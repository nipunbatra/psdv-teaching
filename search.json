[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This webpage contains the course materials for the course “PSDV” that I (Nipun Batra) teach at Indian Institute of Technology, Gandhinagar. These materials have been developed over several years by me and excellent teaching assistants who have helped me in teaching this course."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "Decision Trees Entropy\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/entropy.html",
    "href": "notebooks/entropy.html",
    "title": "Decision Trees Entropy",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom latexify import latexify, format_axes\nfrom scipy.special import xlogy\n\n# Function to calculate entropy\ndef entropy(p):\n    return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n# Generate data\nx_values = np.linspace(0.000, 1.0, 100)  # Avoid log(0) in the calculation\ny_values = entropy(x_values)\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73626/845472961.py:6: RuntimeWarning: divide by zero encountered in log2\n  return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73626/845472961.py:6: RuntimeWarning: invalid value encountered in multiply\n  return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n\n\ny_values\n\narray([       nan, 0.08146203, 0.14257333, 0.19590927, 0.24414164,\n       0.28853851, 0.32984607, 0.36855678, 0.40502013, 0.43949699,\n       0.47218938, 0.50325833, 0.53283506, 0.56102849, 0.58793037,\n       0.61361902, 0.63816195, 0.66161791, 0.68403844, 0.70546904,\n       0.72595015, 0.74551784, 0.76420451, 0.78203929, 0.79904852,\n       0.81525608, 0.83068364, 0.84535094, 0.85927598, 0.87247521,\n       0.88496364, 0.89675502, 0.90786192, 0.91829583, 0.92806728,\n       0.93718586, 0.9456603 , 0.95349858, 0.9607079 , 0.96729478,\n       0.97326507, 0.97862399, 0.98337619, 0.98752571, 0.99107606,\n       0.99403021, 0.99639062, 0.99815923, 0.9993375 , 0.9999264 ,\n       0.9999264 , 0.9993375 , 0.99815923, 0.99639062, 0.99403021,\n       0.99107606, 0.98752571, 0.98337619, 0.97862399, 0.97326507,\n       0.96729478, 0.9607079 , 0.95349858, 0.9456603 , 0.93718586,\n       0.92806728, 0.91829583, 0.90786192, 0.89675502, 0.88496364,\n       0.87247521, 0.85927598, 0.84535094, 0.83068364, 0.81525608,\n       0.79904852, 0.78203929, 0.76420451, 0.74551784, 0.72595015,\n       0.70546904, 0.68403844, 0.66161791, 0.63816195, 0.61361902,\n       0.58793037, 0.56102849, 0.53283506, 0.50325833, 0.47218938,\n       0.43949699, 0.40502013, 0.36855678, 0.32984607, 0.28853851,\n       0.24414164, 0.19590927, 0.14257333, 0.08146203,        nan])\n\n\n\n# Replace NaN values with 0\ny_values = np.nan_to_num(y_values, nan=0.0)\n\n\nlatexify(columns=2)\n\n\nplt.plot(x_values, y_values, color='black')\n\n# Set labels and title\nplt.xlabel('$P(+)$')\nplt.ylabel('Entropy')\nplt.title('Entropy vs. $P(+)$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/entropy.pdf\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n# Function to calculate entropy with numerical stability\ndef entropy_numerically_stable(p):\n    return (-xlogy(p, p) - xlogy(1 - p, 1 - p))/np.log(2)\n\ny_values = entropy_numerically_stable(x_values)\n\n\nplt.plot(x_values, y_values)\n\n\n\n\n\n\n\n\nHow does xlogy handle the corner case?\n\nxlogy??\n\nCall signature:  xlogy(*args, **kwargs)\nType:            ufunc\nString form:     &lt;ufunc 'xlogy'&gt;\nFile:            ~/miniconda3/lib/python3.9/site-packages/numpy/__init__.py\nDocstring:      \nxlogy(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nxlogy(x, y, out=None)\n\nCompute ``x*log(y)`` so that the result is 0 if ``x = 0``.\n\nParameters\n----------\nx : array_like\n    Multiplier\ny : array_like\n    Argument\nout : ndarray, optional\n    Optional output array for the function results\n\nReturns\n-------\nz : scalar or ndarray\n    Computed x*log(y)\n\nNotes\n-----\nThe log function used in the computation is the natural log.\n\n.. versionadded:: 0.13.0\n\nExamples\n--------\nWe can use this function to calculate the binary logistic loss also\nknown as the binary cross entropy. This loss function is used for\nbinary classification problems and is defined as:\n\n.. math::\n    L = 1/n * \\sum_{i=0}^n -(y_i*log(y\\_pred_i) + (1-y_i)*log(1-y\\_pred_i))\n\nWe can define the parameters `x` and `y` as y and y_pred respectively.\ny is the array of the actual labels which over here can be either 0 or 1.\ny_pred is the array of the predicted probabilities with respect to\nthe positive class (1).\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.special import xlogy\n&gt;&gt;&gt; y = np.array([0, 1, 0, 1, 1, 0])\n&gt;&gt;&gt; y_pred = np.array([0.3, 0.8, 0.4, 0.7, 0.9, 0.2])\n&gt;&gt;&gt; n = len(y)\n&gt;&gt;&gt; loss = -(xlogy(y, y_pred) + xlogy(1 - y, 1 - y_pred)).sum()\n&gt;&gt;&gt; loss /= n\n&gt;&gt;&gt; loss\n0.29597052165495025\n\nA lower loss is usually better as it indicates that the predictions are\nsimilar to the actual labels. In this example since our predicted\nprobabilties are close to the actual labels, we get an overall loss\nthat is reasonably low and appropriate.\nClass docstring:\nFunctions that operate element by element on whole arrays.\n\nTo see the documentation for a specific ufunc, use `info`.  For\nexample, ``np.info(np.sin)``.  Because ufuncs are written in C\n(for speed) and linked into Python with NumPy's ufunc facility,\nPython's help() function finds this page whenever help() is called\non a ufunc.\n\nA detailed explanation of ufuncs can be found in the docs for :ref:`ufuncs`.\n\n**Calling ufuncs:** ``op(*x[, out], where=True, **kwargs)``\n\nApply `op` to the arguments `*x` elementwise, broadcasting the arguments.\n\nThe broadcasting rules are:\n\n* Dimensions of length 1 may be prepended to either array.\n* Arrays may be repeated along dimensions of length 1.\n\nParameters\n----------\n*x : array_like\n    Input arrays.\nout : ndarray, None, or tuple of ndarray and None, optional\n    Alternate array object(s) in which to put the result; if provided, it\n    must have a shape that the inputs broadcast to. A tuple of arrays\n    (possible only as a keyword argument) must have length equal to the\n    number of outputs; use None for uninitialized outputs to be\n    allocated by the ufunc.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the :ref:`ufunc docs &lt;ufuncs.kwargs&gt;`.\n\nReturns\n-------\nr : ndarray or tuple of ndarray\n    `r` will have the shape that the arrays in `x` broadcast to; if `out` is\n    provided, it will be returned. If not, `r` will be allocated and\n    may contain uninitialized values. If the function has more than one\n    output, then the result will be a tuple of arrays."
  }
]
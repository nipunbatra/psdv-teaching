[
  {
    "objectID": "notebooks/quiz1.html",
    "href": "notebooks/quiz1.html",
    "title": "Data Analysis Quiz - Pandas GroupBy Operations",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\n\nnum_rows = 10\ndepartment = np.random.choice(['HR', 'IT', 'Finance','Sales'], num_rows)\ndepartment\n\narray(['Sales', 'IT', 'HR', 'HR', 'IT', 'Sales', 'Finance', 'Sales',\n       'Finance', 'Sales'], dtype='&lt;U7')",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Data Analysis Quiz - Pandas GroupBy Operations"
    ]
  },
  {
    "objectID": "notebooks/quiz1.html#introduction",
    "href": "notebooks/quiz1.html#introduction",
    "title": "Data Analysis Quiz - Pandas GroupBy Operations",
    "section": "Introduction",
    "text": "Introduction\nThis notebook contains practical exercises focusing on pandas GroupBy operations - one of the most powerful features in pandas for data aggregation and analysis. These exercises will test your understanding of grouping data, applying functions, and extracting insights from grouped datasets.\n\nLearning Objectives\nThis quiz will assess your ability to: - Create and manipulate sample datasets with pandas - Apply GroupBy operations effectively - Use various aggregation functions (max, min, mean, etc.) - Access specific groups and their properties - Understand method chaining in pandas operations\n\n\nKey Concepts Covered\n\nGroupBy Mechanics: How pandas splits data into groups\nAggregation Functions: Computing summary statistics per group\nGroup Selection: Accessing specific groups and their data\nMethod Syntax: Different ways to call aggregation methods",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Data Analysis Quiz - Pandas GroupBy Operations"
    ]
  },
  {
    "objectID": "notebooks/quiz1.html#problem-setup",
    "href": "notebooks/quiz1.html#problem-setup",
    "title": "Data Analysis Quiz - Pandas GroupBy Operations",
    "section": "Problem Setup",
    "text": "Problem Setup\nWe’ll work with a simulated employee dataset containing information about departments, employee IDs, salaries, and office locations.\n\nlocation = np.random.choice(['Bangalore', 'Chennai', 'Hyderabad','Pune'], num_rows)\n\n\nemployee_id = np.random.choice(range(1000, 2000), num_rows)\n\n\nemployee_id\n\narray([1372, 1109, 1981, 1185, 1502, 1073, 1291, 1058, 1648, 1401])\n\n\n\nsalary = np.random.normal(50000, 10000, num_rows).astype(int)\nsalary\n\narray([41751, 43223, 49878, 45469, 50338, 53113, 70930, 37430, 53010,\n       52789])",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Data Analysis Quiz - Pandas GroupBy Operations"
    ]
  },
  {
    "objectID": "notebooks/quiz1.html#understanding-the-dataset",
    "href": "notebooks/quiz1.html#understanding-the-dataset",
    "title": "Data Analysis Quiz - Pandas GroupBy Operations",
    "section": "Understanding the Dataset",
    "text": "Understanding the Dataset\nLet’s examine our employee dataset. Notice the different departments, varying salaries, and multiple office locations. This diversity makes it perfect for practicing GroupBy operations.",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Data Analysis Quiz - Pandas GroupBy Operations"
    ]
  },
  {
    "objectID": "notebooks/quiz1.html#groupby-fundamentals",
    "href": "notebooks/quiz1.html#groupby-fundamentals",
    "title": "Data Analysis Quiz - Pandas GroupBy Operations",
    "section": "GroupBy Fundamentals",
    "text": "GroupBy Fundamentals\n\nCreating a GroupBy Object\nWhen we call df.groupby('department'), pandas creates a GroupBy object that represents the data split into groups based on the specified column.\n\n\nAccessing Individual Groups\nThe get_group() method allows us to extract data for a specific group. This is useful for examining subsets of your data.\n\ndf = pd.DataFrame({'department': department, \n                   'employee_id': employee_id, \n                   'salary': salary,\n                   'location': location})\n\n\ndf\n\n\n\n\n\n\n\n\ndepartment\nemployee_id\nsalary\nlocation\n\n\n\n\n0\nSales\n1372\n41751\nPune\n\n\n1\nIT\n1109\n43223\nChennai\n\n\n2\nHR\n1981\n49878\nBangalore\n\n\n3\nHR\n1185\n45469\nChennai\n\n\n4\nIT\n1502\n50338\nPune\n\n\n5\nSales\n1073\n53113\nHyderabad\n\n\n6\nFinance\n1291\n70930\nChennai\n\n\n7\nSales\n1058\n37430\nBangalore\n\n\n8\nFinance\n1648\n53010\nChennai\n\n\n9\nSales\n1401\n52789\nPune",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Data Analysis Quiz - Pandas GroupBy Operations"
    ]
  },
  {
    "objectID": "notebooks/quiz1.html#aggregation-methods",
    "href": "notebooks/quiz1.html#aggregation-methods",
    "title": "Data Analysis Quiz - Pandas GroupBy Operations",
    "section": "Aggregation Methods",
    "text": "Aggregation Methods\n\nUnderstanding Different Aggregation Approaches\nPandas provides multiple ways to apply aggregation functions to grouped data. Let’s explore the different syntaxes and their implications.\n\n\nControlling Numeric vs. All Columns\nThe numeric_only parameter helps control which columns are included in aggregation operations.\n\ng = df.groupby('department')\n\n\n\nColumn-Specific Aggregation\nDifferent syntax approaches for focusing on specific columns during aggregation.\n\ng.get_group('HR')\n\n\n\n\n\n\n\n\ndepartment\nemployee_id\nsalary\nlocation\n\n\n\n\n2\nHR\n1981\n49878\nBangalore\n\n\n3\nHR\n1185\n45469\nChennai\n\n\n\n\n\n\n\n\n\nExploring Method Documentation\nUnderstanding the parameters and options available for aggregation methods is crucial for effective data analysis.",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Data Analysis Quiz - Pandas GroupBy Operations"
    ]
  },
  {
    "objectID": "notebooks/quiz1.html#key-learning-points",
    "href": "notebooks/quiz1.html#key-learning-points",
    "title": "Data Analysis Quiz - Pandas GroupBy Operations",
    "section": "Key Learning Points",
    "text": "Key Learning Points\n\nGroupBy Operation Summary\nFrom this exercise, we’ve learned several important concepts:\n\nGroupBy Object Creation: df.groupby('column') splits data into groups\nGroup Access: get_group('group_name') retrieves specific group data\n\nAggregation Methods: Multiple ways to apply functions like max(), min(), mean()\nColumn Selection: Different syntax for targeting specific columns\nParameter Control: Using numeric_only to control which columns are processed\n\n\n\nPractical Applications\nGroupBy operations are fundamental for: - Business Analytics: Analyzing performance by department, region, or time period - Scientific Research: Grouping experimental data by conditions or categories\n- Financial Analysis: Summarizing transactions by account, date, or category - Marketing: Analyzing customer behavior by demographics or segments\n\n\nBest Practices\n\nUnderstand Your Data: Know the structure before grouping\nChoose Appropriate Aggregations: Select functions that make sense for your analysis\nHandle Missing Data: Consider how NaN values affect your groupings\nPerformance: For large datasets, consider using vectorized operations\nReadability: Use clear, descriptive group column names\n\n\n\nFurther Exploration\nTo deepen your GroupBy skills, explore: - Multiple column grouping: df.groupby(['col1', 'col2']) - Custom aggregation functions with agg() - Transformation operations with transform() - Filtering groups with filter() - Advanced indexing with grouped data\n\ng.get_group('Sales')\n\n\n\n\n\n\n\n\ndepartment\nemployee_id\nsalary\nlocation\n\n\n\n\n0\nSales\n1372\n41751\nPune\n\n\n5\nSales\n1073\n53113\nHyderabad\n\n\n7\nSales\n1058\n37430\nBangalore\n\n\n9\nSales\n1401\n52789\nPune\n\n\n\n\n\n\n\n\ng.get_group('IT')\n\n\n\n\n\n\n\n\ndepartment\nemployee_id\nsalary\nlocation\n\n\n\n\n1\nIT\n1109\n43223\nChennai\n\n\n4\nIT\n1502\n50338\nPune\n\n\n\n\n\n\n\n\ng.max()\n\n\n\n\n\n\n\n\nemployee_id\nsalary\nlocation\n\n\ndepartment\n\n\n\n\n\n\n\nFinance\n1648\n70930\nChennai\n\n\nHR\n1981\n49878\nChennai\n\n\nIT\n1502\n50338\nPune\n\n\nSales\n1401\n53113\nPune\n\n\n\n\n\n\n\n\ng.max(numeric_only=True)\n\n\n\n\n\n\n\n\nemployee_id\nsalary\n\n\ndepartment\n\n\n\n\n\n\nFinance\n1648\n70930\n\n\nHR\n1981\n49878\n\n\nIT\n1502\n50338\n\n\nSales\n1401\n53113\n\n\n\n\n\n\n\n\ng.max('salary')\n\n\n\n\n\n\n\n\nemployee_id\nsalary\n\n\ndepartment\n\n\n\n\n\n\nFinance\n1648\n70930\n\n\nHR\n1981\n49878\n\n\nIT\n1502\n50338\n\n\nSales\n1401\n53113\n\n\n\n\n\n\n\n\ng['salary'].max()\n\ndepartment\nFinance    70930\nHR         49878\nIT         50338\nSales      53113\nName: salary, dtype: int64\n\n\n\ng.max()['salary']\n\ndepartment\nFinance    70930\nHR         49878\nIT         50338\nSales      53113\nName: salary, dtype: int64\n\n\n\ng.max?\n\n\nSignature:\n\ng.max(\n\n    numeric_only: 'bool' = False,\n\n    min_count: 'int' = -1,\n\n    engine: \"Literal['cython', 'numba'] | None\" = None,\n\n    engine_kwargs: 'dict[str, bool] | None' = None,\n\n)\n\nDocstring:\n\nCompute max of group values.\n\n\n\nParameters\n\n----------\n\nnumeric_only : bool, default False\n\n    Include only float, int, boolean columns.\n\n\n\n    .. versionchanged:: 2.0.0\n\n\n\n        numeric_only no longer accepts ``None``.\n\n\n\nmin_count : int, default -1\n\n    The required number of valid values to perform the operation. If fewer\n\n    than ``min_count`` non-NA values are present the result will be NA.\n\n\n\nengine : str, default None None\n\n    * ``'cython'`` : Runs rolling apply through C-extensions from cython.\n\n    * ``'numba'`` : Runs rolling apply through JIT compiled code from numba.\n\n        Only available when ``raw`` is set to ``True``.\n\n    * ``None`` : Defaults to ``'cython'`` or globally setting ``compute.use_numba``\n\n\n\nengine_kwargs : dict, default None None\n\n    * For ``'cython'`` engine, there are no accepted ``engine_kwargs``\n\n    * For ``'numba'`` engine, the engine can accept ``nopython``, ``nogil``\n\n        and ``parallel`` dictionary keys. The values must either be ``True`` or\n\n        ``False``. The default ``engine_kwargs`` for the ``'numba'`` engine is\n\n        ``{'nopython': True, 'nogil': False, 'parallel': False}`` and will be\n\n        applied to both the ``func`` and the ``apply`` groupby aggregation.\n\n\n\nReturns\n\n-------\n\nSeries or DataFrame\n\n    Computed max of values within each group.\n\n\n\nExamples\n\n--------\n\nFor SeriesGroupBy:\n\n\n\n&gt;&gt;&gt; lst = ['a', 'a', 'b', 'b']\n\n&gt;&gt;&gt; ser = pd.Series([1, 2, 3, 4], index=lst)\n\n&gt;&gt;&gt; ser\n\na    1\n\na    2\n\nb    3\n\nb    4\n\ndtype: int64\n\n&gt;&gt;&gt; ser.groupby(level=0).max()\n\na    2\n\nb    4\n\ndtype: int64\n\n\n\nFor DataFrameGroupBy:\n\n\n\n&gt;&gt;&gt; data = [[1, 8, 2], [1, 2, 5], [2, 5, 8], [2, 6, 9]]\n\n&gt;&gt;&gt; df = pd.DataFrame(data, columns=[\"a\", \"b\", \"c\"],\n\n...                   index=[\"tiger\", \"leopard\", \"cheetah\", \"lion\"])\n\n&gt;&gt;&gt; df\n\n          a  b  c\n\n  tiger   1  8  2\n\nleopard   1  2  5\n\ncheetah   2  5  8\n\n   lion   2  6  9\n\n&gt;&gt;&gt; df.groupby(\"a\").max()\n\n    b  c\n\na\n\n1   8  5\n\n2   6  9\n\nFile:      ~/mambaforge/lib/python3.12/site-packages/pandas/core/groupby/groupby.py\n\nType:      method",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Data Analysis Quiz - Pandas GroupBy Operations"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html",
    "href": "notebooks/pmf-discrete.html",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "",
    "text": "By the end of this notebook, you will understand:\n\nProbability Mass Functions (PMF): Mathematical definition and properties\nCommon Discrete Distributions: Bernoulli, Categorical, Binomial, Poisson, and Geometric\nReal-World Applications: Quality control, classification, and count modeling\nParameter Estimation: Fitting distributions to data using PyTorch\nModel Selection: Choosing appropriate distributions for different scenarios\nPractical Implementation: Working with PyTorch distributions for data science\n\n\n\n\nDiscrete random variables take on countable values (like integers), and their behavior is completely described by their Probability Mass Function (PMF). The PMF tells us the probability of each possible outcome, providing a complete probabilistic picture of the random variable.\n\n\nFor a discrete random variable \\(X\\), the PMF is defined as:\n\\[p_X(x) = P(X = x)\\]\nThe PMF must satisfy two fundamental properties: 1. Non-negativity: \\(p_X(x) \\geq 0\\) for all \\(x\\) 2. Normalization: \\(\\sum_{x} p_X(x) = 1\\)\n\n\n\nDiscrete distributions are everywhere in data science and statistics: - Classification problems (Bernoulli for binary, Categorical for multi-class) - Count data (Poisson for rare events, Binomial for fixed trials) - Quality control (Binomial for defect rates) - Waiting times (Geometric for time to first success) - Natural language processing (Categorical for word distributions)\n\n\n\nWe’ll explore these essential distributions:\n\n\n\n\n\n\n\n\n\nDistribution\nUse Case\nParameters\nExample\n\n\n\n\nBernoulli\nBinary outcomes\n\\(p\\) (success prob)\nCoin flip, Pass/Fail\n\n\nCategorical\nMulti-class outcomes\n\\(\\boldsymbol{\\theta}\\) (prob vector)\nDie roll, Image classification\n\n\nBinomial\nFixed trials, binary outcome\n\\(n, p\\)\nQuality control, Survey responses\n\n\nPoisson\nCount of rare events\n\\(\\lambda\\) (rate)\nEmail arrivals, Defects per unit\n\n\nGeometric\nTrials to first success\n\\(p\\) (success prob)\nSales calls, Equipment failure\n\n\n\nLet’s dive into each distribution with theory, implementation, and real-world applications!\n\n\n\n\nWe’ll use PyTorch for probability distributions, NumPy for numerical operations, and Matplotlib for visualizations:\n\n\n\nThe Bernoulli distribution is the simplest discrete distribution, modeling a single binary trial with two possible outcomes: success (1) or failure (0).\n\n\nLet \\(X\\) be a Bernoulli random variable with parameter \\(p \\in [0,1]\\). The PMF is:\n\\[p_X(x) = \\begin{cases}\n1-p, & \\text{if } x = 0 \\\\\np, & \\text{if } x = 1\n\\end{cases}\\]\nEquivalently: \\(p_X(x) = p^x(1-p)^{1-x}\\) for \\(x \\in \\{0, 1\\}\\)\n\n\n\n\nMean: \\(E[X] = p\\)\nVariance: \\(\\text{Var}(X) = p(1-p)\\)\nMaximum variance: Occurs when \\(p = 0.5\\)\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch \nimport torch.nn as nn\n\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html#learning-objectives",
    "href": "notebooks/pmf-discrete.html#learning-objectives",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "",
    "text": "By the end of this notebook, you will understand:\n\nProbability Mass Functions (PMF): Mathematical definition and properties\nCommon Discrete Distributions: Bernoulli, Categorical, Binomial, Poisson, and Geometric\nReal-World Applications: Quality control, classification, and count modeling\nParameter Estimation: Fitting distributions to data using PyTorch\nModel Selection: Choosing appropriate distributions for different scenarios\nPractical Implementation: Working with PyTorch distributions for data science",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html#introduction",
    "href": "notebooks/pmf-discrete.html#introduction",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "",
    "text": "Discrete random variables take on countable values (like integers), and their behavior is completely described by their Probability Mass Function (PMF). The PMF tells us the probability of each possible outcome, providing a complete probabilistic picture of the random variable.\n\n\nFor a discrete random variable \\(X\\), the PMF is defined as:\n\\[p_X(x) = P(X = x)\\]\nThe PMF must satisfy two fundamental properties: 1. Non-negativity: \\(p_X(x) \\geq 0\\) for all \\(x\\) 2. Normalization: \\(\\sum_{x} p_X(x) = 1\\)\n\n\n\nDiscrete distributions are everywhere in data science and statistics: - Classification problems (Bernoulli for binary, Categorical for multi-class) - Count data (Poisson for rare events, Binomial for fixed trials) - Quality control (Binomial for defect rates) - Waiting times (Geometric for time to first success) - Natural language processing (Categorical for word distributions)\n\n\n\nWe’ll explore these essential distributions:\n\n\n\n\n\n\n\n\n\nDistribution\nUse Case\nParameters\nExample\n\n\n\n\nBernoulli\nBinary outcomes\n\\(p\\) (success prob)\nCoin flip, Pass/Fail\n\n\nCategorical\nMulti-class outcomes\n\\(\\boldsymbol{\\theta}\\) (prob vector)\nDie roll, Image classification\n\n\nBinomial\nFixed trials, binary outcome\n\\(n, p\\)\nQuality control, Survey responses\n\n\nPoisson\nCount of rare events\n\\(\\lambda\\) (rate)\nEmail arrivals, Defects per unit\n\n\nGeometric\nTrials to first success\n\\(p\\) (success prob)\nSales calls, Equipment failure\n\n\n\nLet’s dive into each distribution with theory, implementation, and real-world applications!",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html#setting-up-the-environment",
    "href": "notebooks/pmf-discrete.html#setting-up-the-environment",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "",
    "text": "We’ll use PyTorch for probability distributions, NumPy for numerical operations, and Matplotlib for visualizations:",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html#bernoulli-distribution",
    "href": "notebooks/pmf-discrete.html#bernoulli-distribution",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "",
    "text": "The Bernoulli distribution is the simplest discrete distribution, modeling a single binary trial with two possible outcomes: success (1) or failure (0).\n\n\nLet \\(X\\) be a Bernoulli random variable with parameter \\(p \\in [0,1]\\). The PMF is:\n\\[p_X(x) = \\begin{cases}\n1-p, & \\text{if } x = 0 \\\\\np, & \\text{if } x = 1\n\\end{cases}\\]\nEquivalently: \\(p_X(x) = p^x(1-p)^{1-x}\\) for \\(x \\in \\{0, 1\\}\\)\n\n\n\n\nMean: \\(E[X] = p\\)\nVariance: \\(\\text{Var}(X) = p(1-p)\\)\nMaximum variance: Occurs when \\(p = 0.5\\)\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch \nimport torch.nn as nn\n\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html#bernoulli-distribution-in-pytorch",
    "href": "notebooks/pmf-discrete.html#bernoulli-distribution-in-pytorch",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "Bernoulli Distribution in PyTorch",
    "text": "Bernoulli Distribution in PyTorch\n\n# Create a Bernoulli distribution with p=0.9\ndist = torch.distributions.Bernoulli(probs=0.9)\n\n\ndist\n\nBernoulli(probs: 0.8999999761581421)\n\n\n\n# Print all attributes of the Bernoulli distribution -- do not have __ or _ in the beginning\nattrs = [attr for attr in dir(dist) if not attr.startswith('_')]\npd.Series(attrs)\n\n0               arg_constraints\n1                   batch_shape\n2                           cdf\n3                       entropy\n4             enumerate_support\n5                   event_shape\n6                        expand\n7         has_enumerate_support\n8                   has_rsample\n9                          icdf\n10                     log_prob\n11                       logits\n12                         mean\n13                         mode\n14                  param_shape\n15                   perplexity\n16                        probs\n17                      rsample\n18                       sample\n19                     sample_n\n20    set_default_validate_args\n21                       stddev\n22                      support\n23                     variance\ndtype: object\n\n\n\ndist.mean\n\ntensor(0.9000)\n\n\n\ndist.probs\n\ntensor(0.9000)\n\n\n\ndist.support\n\nBoolean()\n\n\n\ndist.log_prob(torch.tensor(1.0)).exp()\n\ntensor(0.9000)\n\n\n\ndist.log_prob(torch.tensor(0.0)).exp()\n\ntensor(0.1000)\n\n\n\ntry:\n    dist.log_prob(torch.tensor(0.5)).exp()\nexcept Exception as e:\n    print(e)\n\nExpected value argument (Tensor of shape ()) to be within the support (Boolean()) of the distribution Bernoulli(probs: 0.8999999761581421, logits: 2.1972243785858154), but found invalid values:\n0.5\n\n\nNotice that even though the expected frequency is 0.9, individual samples can vary significantly due to randomness.",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html#real-world-application-fruit-classification",
    "href": "notebooks/pmf-discrete.html#real-world-application-fruit-classification",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "Real-World Application: Fruit Classification",
    "text": "Real-World Application: Fruit Classification\nLet’s explore a practical machine learning application using the Bernoulli distribution for binary classification.\n\ndist.sample()\n\ntensor(0.)\n\n\n\nsamples = dist.sample(torch.Size([1000]))\n\n\nsamples[:10]\n\ntensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\nProblem Setup: Lime vs Lemon Classification\nWe want to classify fruits as limes (0) or lemons (1) based on their radius. This is a classic logistic regression problem where we model:\n\\[P(\\text{Lemon}|\\text{radius}) = \\text{sigmoid}(w \\cdot \\text{radius} + b)\\]\nThe sigmoid function ensures probabilities stay between 0 and 1, making it perfect for Bernoulli distributions.\n\nsamples.mean()\n\ntensor(0.9110)",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html#main-question",
    "href": "notebooks/pmf-discrete.html#main-question",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "Main question",
    "text": "Main question\nGiven a fruit (lime or lemon) and its radius, we want to predict if it is a lime or a lemon.\nLet us denote the radius of the fruit by \\(r\\) and the type of the fruit by \\(y\\) where \\(y=0\\) if the fruit is a lime and \\(y=1\\) if the fruit is a lemon.\nWe want to model the probability of the fruit being a lemon given its radius, i.e., we want to model \\(p(y=1|r)\\).",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html#generative-process",
    "href": "notebooks/pmf-discrete.html#generative-process",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "Generative process",
    "text": "Generative process\n\n# Set random seed\ntorch.manual_seed(42)\n\n&lt;torch._C.Generator at 0x12ab80ab0&gt;\n\n\n\nradius_array = torch.distributions.Uniform(0, 3).sample((1000,))\n\n\nradius_array[:10]\n\ntensor([2.6468, 2.7450, 1.1486, 2.8779, 1.1713, 1.8027, 0.7697, 2.3809, 2.8223,\n        0.3996])\n\n\n\n_ = plt.hist(radius_array)\n\n\n\n\n\n\n\n\nWe start by modeling the generative process of the data.\nWe assume if w*r + b &gt; 0, then the fruit is a lemon, otherwise it is a lime.\nLet us assume that w_true = 1.2 and b_true = -2.0.\n\ndef linear(r, w, b):\n    return w * r + b\n\nw_true = 1.2\nb_true = -2.0\n\nlogits = linear(radius_array, w_true, b_true)\n\n\npd.Series(logits.numpy()).describe()\n\ncount    1000.000000\nmean       -0.249277\nstd         1.045096\nmin        -1.991556\n25%        -1.134019\n50%        -0.342291\n75%         0.646800\nmax         1.599296\ndtype: float64\n\n\nCan we use logits to model the probability of the fruit being a lemon given its radius?\nNo! These logits can be any real number, but we want to model the probability of the fruit being a lemon given its radius, which is a number between 0 and 1.\nWe can use the sigmoid function to map the logits to a number between 0 and 1.\n\\[\n\\sigma(x) = \\frac{1}{1 + e^{-x}}\n\\]\n\ndef sigmoid(x):\n    return 1 / (1 + torch.exp(-x))\n\nprobs = sigmoid(logits)\n\n\ndf = pd.DataFrame({\n    'radius': radius_array.numpy(),\n    'logits': logits.numpy(),\n    'probabilities': probs.numpy()\n})\n\n\ndf.head()\n\n\n\n\n\n\n\n\nradius\nlogits\nprobabilities\n\n\n\n\n0\n2.646808\n1.176169\n0.764258\n\n\n1\n2.745012\n1.294014\n0.784826\n\n\n2\n1.148591\n-0.621690\n0.349397\n\n\n3\n2.877917\n1.453500\n0.810537\n\n\n4\n1.171345\n-0.594386\n0.355629\n\n\n\n\n\n\n\n\ndf.query('radius &lt; 0.2').head()\n\n\n\n\n\n\n\n\nradius\nlogits\nprobabilities\n\n\n\n\n29\n0.018481\n-1.977822\n0.121551\n\n\n50\n0.015137\n-1.981835\n0.121123\n\n\n74\n0.012186\n-1.985377\n0.120747\n\n\n99\n0.048608\n-1.941670\n0.125464\n\n\n108\n0.187182\n-1.775382\n0.144874\n\n\n\n\n\n\n\nWe can observe as per our model, smaller fruits are more likely to be limes (probability of being a lemon is less) and larger fruits are more likely to be lemons (probability of being a lemon is more).",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html#generate-a-dataset",
    "href": "notebooks/pmf-discrete.html#generate-a-dataset",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "Generate a dataset",
    "text": "Generate a dataset\n\ny_true = torch.distributions.Bernoulli(probs=probs).sample()\n\n\ndf['y_true'] = y_true.numpy()\n\n\nModel Training and Results\nOur logistic regression successfully learned the underlying relationship! The learned parameters closely match the true parameters used to generate the data.\n\ndf.query('y_true == 0').head(10)\n\n\n\n\n\n\n\n\nradius\nlogits\nprobabilities\ny_true\n\n\n\n\n6\n0.769717\n-1.076339\n0.254199\n0.0\n\n\n9\n0.399558\n-1.520531\n0.179383\n0.0\n\n\n15\n1.288213\n-0.454144\n0.388376\n0.0\n\n\n17\n1.721713\n0.066056\n0.516508\n0.0\n\n\n18\n0.799740\n-1.040312\n0.261090\n0.0\n\n\n19\n1.882347\n0.258817\n0.564345\n0.0\n\n\n24\n0.315945\n-1.620866\n0.165085\n0.0\n\n\n25\n0.808484\n-1.029819\n0.263119\n0.0\n\n\n26\n1.076438\n-0.708274\n0.329980\n0.0\n\n\n28\n1.641575\n-0.030110\n0.492473\n0.0\n\n\n\n\n\n\n\n\ndf.query('y_true == 1').head(10)\n\n\n\n\n\n\n\n\nradius\nlogits\nprobabilities\ny_true\n\n\n\n\n0\n2.646808\n1.176169\n0.764258\n1.0\n\n\n1\n2.745012\n1.294014\n0.784826\n1.0\n\n\n2\n1.148591\n-0.621690\n0.349397\n1.0\n\n\n3\n2.877917\n1.453500\n0.810537\n1.0\n\n\n4\n1.171345\n-0.594386\n0.355629\n1.0\n\n\n5\n1.802686\n0.163223\n0.540715\n1.0\n\n\n7\n2.380924\n0.857109\n0.702056\n1.0\n\n\n8\n2.822314\n1.386777\n0.800077\n1.0\n\n\n10\n2.803794\n1.364553\n0.796499\n1.0\n\n\n11\n1.780739\n0.136887\n0.534168\n1.0\n\n\n\n\n\n\n\nWe can notice that even though the probability of the event is very low, it still happens. This is the nature of the Bernoulli distribution.\n\ndf.query('y_true == 0').head(10)\n\n\n\n\n\n\n\n\nradius\nlogits\nprobabilities\ny_true\n\n\n\n\n0\n2.646808\n1.176169\n0.764258\n0.0\n\n\n3\n2.877917\n1.453500\n0.810537\n0.0\n\n\n4\n1.171345\n-0.594386\n0.355629\n0.0\n\n\n5\n1.802686\n0.163223\n0.540715\n0.0\n\n\n7\n2.380924\n0.857109\n0.702056\n0.0\n\n\n9\n0.399558\n-1.520531\n0.179383\n0.0\n\n\n10\n2.803794\n1.364553\n0.796499\n0.0\n\n\n14\n2.223282\n0.667939\n0.661041\n0.0\n\n\n15\n1.288213\n-0.454144\n0.388376\n0.0\n\n\n18\n0.799740\n-1.040312\n0.261090\n0.0\n\n\n\n\n\n\n\n\n# Plot the data\nplt.scatter(radius_array, y_true, alpha=0.1, marker='|', color='k')\nplt.xlabel('Radius')\n\n# Use Limes and Lemon markers only on y-axis\nplt.yticks([0, 1], ['Limes', 'Lemons'])\nplt.ylabel('Fruit')\n\nText(0, 0.5, 'Fruit')\n\n\n\n\n\n\n\n\n\nThe decision boundary is at radius ≈ 1.67, where P(Lemon) = 0.5.",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html#categorical-distribution",
    "href": "notebooks/pmf-discrete.html#categorical-distribution",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "2. Categorical Distribution",
    "text": "2. Categorical Distribution\nThe Categorical distribution generalizes the Bernoulli distribution to more than two outcomes. It’s fundamental for multi-class classification problems.\n\nMathematical Definition\nLet \\(X\\) be a categorical random variable with \\(K\\) possible outcomes \\(\\{1, 2, \\ldots, K\\}\\) and parameter vector \\(\\boldsymbol{\\theta} = (\\theta_1, \\theta_2, \\ldots, \\theta_K)\\). The PMF is:\n\\[p_X(x = k) = \\theta_k \\quad \\text{for } k \\in \\{1, 2, \\ldots, K\\}\\]\nConstraints: \\(\\theta_k \\geq 0\\) and \\(\\sum_{k=1}^K \\theta_k = 1\\)\n\n\nApplications\n\nImage classification (cat, dog, bird, …)\nSentiment analysis (positive, negative, neutral)\nDie rolling (1, 2, 3, 4, 5, 6)\nMarket research (product preferences)\n\n\n# Logistic regression model\nclass LogisticRegression(nn.Module):\n    def __init__(self):\n        super(LogisticRegression, self).__init__()\n        self.linear = nn.Linear(1, 1)\n        \n    def forward(self, x):\n        return self.linear(x)\n    \nmodel = LogisticRegression()\n\n# Training the model\ncriterion = nn.BCEWithLogitsLoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n# Convert the data to PyTorch tensors\nradius_tensor = radius_array.unsqueeze(1)\ny_true_tensor = y_true.unsqueeze(1)\n\n# Training loop\nn_epochs = 1000\nfor epoch in range(n_epochs):\n    model.train()\n    optimizer.zero_grad()\n    \n    # Forward pass\n    y_pred = model(radius_tensor)\n    \n    # Compute loss\n    loss = criterion(y_pred, y_true_tensor)\n    \n    # Backward pass\n    loss.backward()\n    \n    # Update weights\n    optimizer.step()\n    \n    if epoch % 100 == 0:\n        print(f'Epoch: {epoch}, Loss: {loss.item()}')\n\n\n\nEpoch: 0, Loss: 0.8052948117256165\nEpoch: 100, Loss: 0.6282714605331421\nEpoch: 200, Loss: 0.593073308467865\nEpoch: 300, Loss: 0.5790674686431885\nEpoch: 400, Loss: 0.5744827389717102\nEpoch: 500, Loss: 0.5731877088546753\nEpoch: 600, Loss: 0.5728737711906433\nEpoch: 700, Loss: 0.5728095769882202\nEpoch: 800, Loss: 0.5727986693382263\nEpoch: 900, Loss: 0.5727971196174622\n\n\n\n# Learned weights and bias\nw_learned = model.linear.weight.item()\nb_learned = model.linear.bias.item()\n\n# Compare the true and learned weights and bias\nprint(f'True weights: {w_true}, Learned weights: {w_learned}')\nprint(f'True bias: {b_true}, Learned bias: {b_learned}')\n\nTrue weights: 1.2, Learned weights: 1.2514334917068481\nTrue bias: -2.0, Learned bias: -2.0183537006378174\n\n\n\n# Test if a new fruit is a lime or a lemon\n\ndef predict_fruit(radius, model):\n    model.eval()\n    radius_tensor = torch.tensor([[radius]])\n    logits = model(radius_tensor)\n    prob = sigmoid(logits).item()\n    fruit = ['Lime', 'Lemon'][int(prob &gt; 0.5)]\n    return fruit, prob\n    \n\n\npredict_fruit(0.5, model)\n\n('Lime', 0.19898711144924164)\n\n\n\npredict_fruit(1.5, model)\n\n('Lime', 0.46475765109062195)\n\n\n\npredict_fruit(2.0, model)\n\n('Lemon', 0.6188130378723145)\n\n\n\n# Decision surface\nradius_values = torch.linspace(0, 3, 100).unsqueeze(1)\nprobs = sigmoid(model(radius_values)).detach()\n\nplt.plot(radius_values, probs)\nplt.xlabel('Radius')\nplt.ylabel('Probability of being a lemon')\nplt.title('Decision surface')\nplt.ylim(0, 1)\nplt.axhline(0.5, color='r', linestyle='--')",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html#imagenet",
    "href": "notebooks/pmf-discrete.html#imagenet",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "Imagenet",
    "text": "Imagenet\nThe ImageNet project is a large visual database designed for use in visual object recognition research.\nThe empirical frequencies closely match our theoretical probabilities, validating our implementation.",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html#binomial-distribution",
    "href": "notebooks/pmf-discrete.html#binomial-distribution",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "3. Binomial Distribution",
    "text": "3. Binomial Distribution\nThe Binomial distribution models the number of successes in a fixed number of independent Bernoulli trials.\n\nMathematical Definition\nLet \\(X\\) be the number of successes in \\(n\\) independent Bernoulli trials, each with success probability \\(p\\). Then \\(X \\sim \\text{Binomial}(n, p)\\) with PMF:\n\\[p_X(x) = \\binom{n}{x} p^x (1-p)^{n-x} \\quad \\text{for } x \\in \\{0, 1, 2, \\ldots, n\\}\\]\nwhere \\(\\binom{n}{x} = \\frac{n!}{x!(n-x)!}\\) is the binomial coefficient.\n\n\nProperties\n\nMean: \\(E[X] = np\\)\nVariance: \\(\\text{Var}(X) = np(1-p)\\)\nMode: \\(\\lfloor (n+1)p \\rfloor\\)\n\n\n\nReal-World Example: Quality Control\nA factory produces electronic chips with a 10% defect rate. Quality control randomly selects 10 chips. How many defects should we expect?\n\n\ntheta_vec = torch.tensor([0.1, 0.2, 0.3, 0.4])\n\n#\nser = pd.Series(theta_vec.numpy())\nser.plot(kind='bar', rot=0)\nplt.xlabel('Outcome')\nplt.ylabel('Probability')\n\nText(0, 0.5, 'Probability')\n\n\n\n\n\n\n\n\n\n\ndist = torch.distributions.Categorical(probs=theta_vec)\nprint(dist)\n\nCategorical(probs: torch.Size([4]))\n\n\n\ndist.support\n\nIntegerInterval(lower_bound=0, upper_bound=3)\n\n\n\ndist.log_prob(torch.tensor(0.0)).exp()\n\ntensor(0.1000)\n\n\n\ntry:\n    dist.log_prob(torch.tensor(4.0)).exp()\nexcept Exception as e:\n    print(e)\n\nExpected value argument (Tensor of shape ()) to be within the support (IntegerInterval(lower_bound=0, upper_bound=3)) of the distribution Categorical(probs: torch.Size([4]), logits: torch.Size([4])), but found invalid values:\n4.0\n\n\n\nsamples = dist.sample(torch.Size([1000]))\n\nThe distribution shows that finding 0 or 1 defects is most likely, with very low probability of finding 5+ defects in a sample of 10.\n\n\nSimulation Validation\n\nsamples[:10]\n\ntensor([2, 3, 2, 1, 0, 3, 3, 3, 2, 3])",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html#poisson-distribution",
    "href": "notebooks/pmf-discrete.html#poisson-distribution",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "4. Poisson Distribution",
    "text": "4. Poisson Distribution\nThe Poisson distribution models the number of events occurring in a fixed interval when events happen independently at a constant average rate.\n\npd.value_counts(samples.numpy(), normalize=True).sort_index()\n\n0    0.115\n1    0.192\n2    0.276\n3    0.417\nName: proportion, dtype: float64",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html#geometric-distribution",
    "href": "notebooks/pmf-discrete.html#geometric-distribution",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "5. Geometric Distribution",
    "text": "5. Geometric Distribution\nThe Geometric distribution models the number of trials needed to achieve the first success in a sequence of independent Bernoulli trials.\n\npd.Series(samples.numpy()).value_counts().sort_index()\n\n0.0    354\n1.0    390\n2.0    184\n3.0     58\n4.0     12\n5.0      1\n6.0      1\nName: count, dtype: int64",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/pmf-discrete.html#summary-and-key-insights",
    "href": "notebooks/pmf-discrete.html#summary-and-key-insights",
    "title": "Probability Mass Functions and Common Discrete Distributions",
    "section": "Summary and Key Insights",
    "text": "Summary and Key Insights\nIn this comprehensive notebook, we’ve explored the fundamental discrete probability distributions that form the backbone of statistical modeling and machine learning:\n\nDistributions Covered\n\nBernoulli: Binary outcomes (classification, A/B testing)\nCategorical: Multi-class outcomes (image classification, NLP)\nBinomial: Fixed trials with binary outcomes (quality control, surveys)\n\nPoisson: Count of rare events (arrivals, defects, clicks)\nGeometric: Trials to first success (sales, reliability testing)\n\n\n\nKey Mathematical Insights\n\nPMF Properties: Non-negativity and normalization are universal\nParameter Relationships: Mean and variance depend on distribution parameters\nSupport: Each distribution has a specific domain of possible values\nApplications: Choice depends on the underlying data-generating process\n\n\n\nPractical Implementation Skills\n\nPyTorch Integration: Using built-in distributions for modeling\nSampling and Validation: Generating samples and verifying against theory\nReal-World Modeling: Applying distributions to practical problems\nParameter Estimation: Learning distribution parameters from data\n\n\n\nWhen to Use Each Distribution\n\n\n\n\n\n\n\n\nSituation\nDistribution\nKey Question\n\n\n\n\nBinary classification\nBernoulli\nSuccess/failure with fixed probability?\n\n\nMulti-class classification\nCategorical\nMultiple exclusive outcomes?\n\n\nFixed number of trials\nBinomial\nHow many successes in n trials?\n\n\nCount of rare events\nPoisson\nHow many events in fixed time/space?\n\n\nTime to first success\nGeometric\nHow long until first success?\n\n\n\n\n\nAdvanced Topics for Further Study\n\nMaximum Likelihood Estimation for parameter fitting\nBayesian approaches to parameter uncertainty\nMixture models combining multiple distributions\nZero-inflated models for count data with excess zeros\nTruncated distributions for bounded domains\n\n\n\nMachine Learning Connections\nThese distributions are essential for: - Generative models (VAEs, GANs) - Classification (logistic regression, neural networks) - Reinforcement learning (policy gradients) - Bayesian machine learning (priors and posteriors) - Natural language processing (language models)\nUnderstanding discrete distributions provides the foundation for advanced statistical modeling and machine learning applications!\n\nurl = \"https://raw.githubusercontent.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/refs/heads/master/Chapter1_Introduction/data/txtdata.csv\"\ndata = pd.read_csv(url, header=None)\n\n\ndata.index.name = 'Day'\ndata.columns = ['Count']\n\n\ndata\n\n\n\n\n\n\n\n\nCount\n\n\nDay\n\n\n\n\n\n0\n13.0\n\n\n1\n24.0\n\n\n2\n8.0\n\n\n3\n24.0\n\n\n4\n7.0\n\n\n...\n...\n\n\n69\n37.0\n\n\n70\n5.0\n\n\n71\n14.0\n\n\n72\n13.0\n\n\n73\n22.0\n\n\n\n\n74 rows × 1 columns\n\n\n\n\nfig, ax = plt.subplots(figsize=(21, 7))\ndata.plot(kind='bar', rot=0, ax=ax)\n\n\n\n\n\n\n\n\nHow can you model the number of SMS messages you receive per day?\nWe can model the number of SMS messages you receive per day using a Poisson distribution.\nLet \\(X\\) be the number of SMS messages you receive per day. \\(X\\) can take on values \\(0, 1, 2, \\ldots\\). The probability mass function (PMF) of a Poisson distribution is given by:\n\\[\np_X(x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad x \\in \\{0, 1, 2, \\ldots\\}.\n\\]\nwhere \\(\\lambda &gt; 0\\) is the average number of SMS messages you receive per day.\n\nrate_param = 6\ndist = torch.distributions.Poisson(rate_param)\n\n\ndist\n\nPoisson(rate: 6.0)\n\n\n\ndist.support\n\nIntegerGreaterThan(lower_bound=0)\n\n\n\nx_range = torch.arange(0, 20)\ny = dist.log_prob(x_range).exp()\ndf_prob_poisson = pd.DataFrame({\n    'x': x_range.numpy(),\n    'P(X=x)': y.numpy().round(6)\n})\n\ndf_prob_poisson\n\n\n\n\n\n\n\n\nx\nP(X=x)\n\n\n\n\n0\n0\n0.002479\n\n\n1\n1\n0.014873\n\n\n2\n2\n0.044618\n\n\n3\n3\n0.089235\n\n\n4\n4\n0.133853\n\n\n5\n5\n0.160623\n\n\n6\n6\n0.160623\n\n\n7\n7\n0.137677\n\n\n8\n8\n0.103258\n\n\n9\n9\n0.068839\n\n\n10\n10\n0.041303\n\n\n11\n11\n0.022529\n\n\n12\n12\n0.011264\n\n\n13\n13\n0.005199\n\n\n14\n14\n0.002228\n\n\n15\n15\n0.000891\n\n\n16\n16\n0.000334\n\n\n17\n17\n0.000118\n\n\n18\n18\n0.000039\n\n\n19\n19\n0.000012\n\n\n\n\n\n\n\n\ndf_prob_poisson.plot(kind='bar', x='x', y='P(X=x)', rot=0)",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Mass Functions and Common Discrete Distributions"
    ]
  },
  {
    "objectID": "notebooks/introduction-pandas.html",
    "href": "notebooks/introduction-pandas.html",
    "title": "Introduction to Pandas",
    "section": "",
    "text": "## 1. Introduction\nPandas is a powerful Python library for data manipulation, offering labeled data structures that make tasks like cleaning, transformation, merging, and analysis more convenient.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nprint(\"Using Pandas version:\", pd.__version__)\nprint(\"Using NumPy version:\", np.__version__)\nprint(\"Using Seaborn version:\", sns.__version__)\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nUsing Pandas version: 2.2.3\nUsing NumPy version: 2.1.2\nUsing Seaborn version: 0.13.2",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "notebooks/introduction-pandas.html#numpy-vs.-pandas-for-student-scores",
    "href": "notebooks/introduction-pandas.html#numpy-vs.-pandas-for-student-scores",
    "title": "Introduction to Pandas",
    "section": "2. Numpy vs. Pandas for Student Scores",
    "text": "2. Numpy vs. Pandas for Student Scores\n\n2.1 Creating and Saving Student Data to Excel/CSV\nWe’ll first generate some random student data: - Name (string) - Maths (integer) - Science (integer)\nThen we’ll save to both .xlsx and .csv for demonstration.\n\nnum_students = 20\n\nstudent_data = {\n    \"Name\": [\n        \"Aarav\", \"Vivaan\", \"Aditya\", \"Ananya\", \"Ishita\", \"Kabir\", \"Nisha\", \"Rohan\", \"Priya\", \"Sneha\",\n        \"Aryan\", \"Meera\", \"Tanya\", \"Siddharth\", \"Neha\", \"Laksh\", \"Pooja\", \"Rahul\", \"Simran\", \"Kiran\"\n    ],\n    \"Maths\": np.random.randint(50, 100, size=num_students),\n    \"Science\": np.random.randint(50, 100, size=num_students),\n    \"Economics\": np.random.randint(50, 100, size=num_students),\n}\n\ndf_students = pd.DataFrame(student_data)\n\ndf_students.to_excel(\"student_scores.xlsx\", index=False)\ndf_students.to_csv(\"student_scores.csv\", index=False)\n\ndf_students.head(10)\n\n\n\n\n\n\n\n\nName\nMaths\nScience\nEconomics\n\n\n\n\n0\nAarav\n53\n56\n67\n\n\n1\nVivaan\n94\n88\n85\n\n\n2\nAditya\n84\n65\n69\n\n\n3\nAnanya\n98\n90\n96\n\n\n4\nIshita\n72\n74\n78\n\n\n5\nKabir\n79\n56\n60\n\n\n6\nNisha\n76\n52\n99\n\n\n7\nRohan\n71\n78\n50\n\n\n8\nPriya\n76\n72\n68\n\n\n9\nSneha\n64\n92\n94\n\n\n\n\n\n\n\n\n\n2.2 Reading CSV in NumPy & Basic Analysis\nNumPy’s loadtxt can be used to read numeric data easily, but handling mixed types (e.g. strings + numbers) can be trickier. We’ll demonstrate a simple approach:\n\nRead the entire CSV (skipping the header) using np.loadtxt.\nWe’ll parse the Name column as a string and the three score columns as integers.\nCompute the mean of Maths and Science.\nFind which student got the maximum in Science.\nFind which student got the maximum of |Maths - Science|.\n\n\n!head student_scores.csv\n\nName,Maths,Science,Economics\nAarav,53,56,67\nVivaan,94,88,85\nAditya,84,65,69\nAnanya,98,90,96\nIshita,72,74,78\nKabir,79,56,60\nNisha,76,52,99\nRohan,71,78,50\nPriya,76,72,68\n\n\n\ndata = np.loadtxt(\"student_scores.csv\", delimiter = \",\", skiprows = 1, dtype = str)\nprint(data)\n\n[['Aarav' '53' '56' '67']\n ['Vivaan' '94' '88' '85']\n ['Aditya' '84' '65' '69']\n ['Ananya' '98' '90' '96']\n ['Ishita' '72' '74' '78']\n ['Kabir' '79' '56' '60']\n ['Nisha' '76' '52' '99']\n ['Rohan' '71' '78' '50']\n ['Priya' '76' '72' '68']\n ['Sneha' '64' '92' '94']\n ['Aryan' '60' '92' '97']\n ['Meera' '62' '56' '89']\n ['Tanya' '70' '78' '88']\n ['Siddharth' '73' '72' '83']\n ['Neha' '79' '85' '95']\n ['Laksh' '68' '62' '88']\n ['Pooja' '63' '62' '57']\n ['Rahul' '81' '63' '67']\n ['Simran' '69' '98' '78']\n ['Kiran' '85' '82' '95']]\n\n\n\nprint(data.dtype)\n\n&lt;U9\n\n\n\nnames = data[:, 0].tolist()\nmaths_np = np.array(data[:, 1].astype(int))\nscience_np = np.array(data[:, 2].astype(int))\neco_np = np.array(data[:, 3].astype(int))\n\nprint(\"Names:\", names)\nprint(\"Maths scores:\", maths_np)\nprint(\"Science scores:\", science_np)\nprint(\"Economics scores:\", eco_np)\n\nNames: ['Aarav', 'Vivaan', 'Aditya', 'Ananya', 'Ishita', 'Kabir', 'Nisha', 'Rohan', 'Priya', 'Sneha', 'Aryan', 'Meera', 'Tanya', 'Siddharth', 'Neha', 'Laksh', 'Pooja', 'Rahul', 'Simran', 'Kiran']\nMaths scores: [53 94 84 98 72 79 76 71 76 64 60 62 70 73 79 68 63 81 69 85]\nScience scores: [56 88 65 90 74 56 52 78 72 92 92 56 78 72 85 62 62 63 98 82]\nEconomics scores: [67 85 69 96 78 60 99 50 68 94 97 89 88 83 95 88 57 67 78 95]\n\n\n\nprint(type(maths_np))\nprint(maths_np.dtype)\n\n&lt;class 'numpy.ndarray'&gt;\nint64\n\n\n\nmean_maths = np.mean(maths_np)\nmean_science = np.mean(science_np)\nmean_eco = np.mean(eco_np)\n\nprint(f\"\\nMean Maths score: {mean_maths:.3f}\")\nprint(f\"Mean Science score: {mean_science:.3f}\")\nprint(f\"Mean Economics score: {mean_eco:.3f}\")\n\nprint(f\"\\nMean Maths score (integer): {mean_maths:.2f}\")\n\n\nMean Maths score: 73.850\nMean Science score: 73.650\nMean Economics score: 80.150\n\nMean Maths score (integer): 73.85\n\n\n\n# Finding student with maximum science\nmax_sci_idx = np.argmax(science_np)\nprint(max_sci_idx)\n\n18\n\n\n\n# since we need the name of the student, we use the index (from argmax) to find the name\nmax_sci_student = names[max_sci_idx]\nmax_sci_val = science_np[max_sci_idx]\nprint(f\"\\nStudent with maximum science score: {max_sci_student} ({max_sci_val})\")\n\n\nStudent with maximum science score: Simran (98)\n\n\n\n# Likewise for finding student with maximum |Maths - Science| score\ndiff = np.abs(maths_np - science_np)\nmax_diff_idx = np.argmax(diff)\nmax_diff_student = names[max_diff_idx]\nprint(f\"\\nStudent with max |Maths - Science|: {max_diff_student} (|{maths_np[max_diff_idx]} - {science_np[max_diff_idx]}| = {diff[max_diff_idx]})\")\n\n\nStudent with max |Maths - Science|: Aryan (|60 - 92| = 32)\n\n\n\n\n2.3 Plotting Student Scores with NumPy/Seaborn\n\n# Define bar width and positions\nbar_width = 0.25\nx = np.arange(len(names))\n\n# Create the figure and axis\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Plot the bars\nax.bar(x - bar_width, maths_np, width=bar_width, label=\"Maths\", color='blue')\nax.bar(x, science_np, width=bar_width, label=\"Science\", color='green')\nax.bar(x + bar_width, eco_np, width=bar_width, label=\"Economics\", color='red')\n\n# Customize the plot\nax.set_title(\"Student Marks in Three Subjects\", fontsize=16)\nax.set_xlabel(\"Students\", fontsize=12)\nax.set_ylabel(\"Marks\", fontsize=12)\nax.set_xticks(x)\nax.set_xticklabels(names, rotation=45)\nax.legend(title=\"Subjects\")\n\n\n\n\n\n\n\n\n ### 2.4 Reading CSV in Pandas & Repeating Analysis With Pandas, we can directly do:\ndf = pd.read_csv(\"student_scores.csv\")\nand the DataFrame will automatically separate columns into name, maths, and science. Then we can easily compute means, maxima, etc.\n\ndf_students_pandas = pd.read_csv(\"student_scores.csv\")\ndf_students_pandas\n\n\n\n\n\n\n\n\nName\nMaths\nScience\nEconomics\n\n\n\n\n0\nAarav\n53\n56\n67\n\n\n1\nVivaan\n94\n88\n85\n\n\n2\nAditya\n84\n65\n69\n\n\n3\nAnanya\n98\n90\n96\n\n\n4\nIshita\n72\n74\n78\n\n\n5\nKabir\n79\n56\n60\n\n\n6\nNisha\n76\n52\n99\n\n\n7\nRohan\n71\n78\n50\n\n\n8\nPriya\n76\n72\n68\n\n\n9\nSneha\n64\n92\n94\n\n\n10\nAryan\n60\n92\n97\n\n\n11\nMeera\n62\n56\n89\n\n\n12\nTanya\n70\n78\n88\n\n\n13\nSiddharth\n73\n72\n83\n\n\n14\nNeha\n79\n85\n95\n\n\n15\nLaksh\n68\n62\n88\n\n\n16\nPooja\n63\n62\n57\n\n\n17\nRahul\n81\n63\n67\n\n\n18\nSimran\n69\n98\n78\n\n\n19\nKiran\n85\n82\n95\n\n\n\n\n\n\n\n\ndf_students_pandas.describe()\n\n\n\n\n\n\n\n\nMaths\nScience\nEconomics\n\n\n\n\ncount\n20.000000\n20.000000\n20.000000\n\n\nmean\n73.850000\n73.650000\n80.150000\n\n\nstd\n11.230949\n14.224793\n14.928866\n\n\nmin\n53.000000\n52.000000\n50.000000\n\n\n25%\n67.000000\n62.000000\n67.750000\n\n\n50%\n72.500000\n73.000000\n84.000000\n\n\n75%\n79.500000\n85.750000\n94.250000\n\n\nmax\n98.000000\n98.000000\n99.000000\n\n\n\n\n\n\n\n\n!head student_scores.csv\n\nName,Maths,Science,Economics\nAarav,53,56,67\nVivaan,94,88,85\nAditya,84,65,69\nAnanya,98,90,96\nIshita,72,74,78\nKabir,79,56,60\nNisha,76,52,99\nRohan,71,78,50\nPriya,76,72,68\n\n\n\ndf_students_pandas.head?\n\n\nSignature: df_students_pandas.head(n: 'int' = 5) -&gt; 'Self'\nDocstring:\nReturn the first `n` rows.\nThis function returns the first `n` rows for the object based\non position. It is useful for quickly testing if your object\nhas the right type of data in it.\nFor negative values of `n`, this function returns all rows except\nthe last `|n|` rows, equivalent to ``df[:n]``.\nIf n is larger than the number of rows, this function returns all rows.\nParameters\n----------\nn : int, default 5\n    Number of rows to select.\nReturns\n-------\nsame type as caller\n    The first `n` rows of the caller object.\nSee Also\n--------\nDataFrame.tail: Returns the last `n` rows.\nExamples\n--------\n&gt;&gt;&gt; df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion',\n...                    'monkey', 'parrot', 'shark', 'whale', 'zebra']})\n&gt;&gt;&gt; df\n      animal\n0  alligator\n1        bee\n2     falcon\n3       lion\n4     monkey\n5     parrot\n6      shark\n7      whale\n8      zebra\nViewing the first 5 lines\n&gt;&gt;&gt; df.head()\n      animal\n0  alligator\n1        bee\n2     falcon\n3       lion\n4     monkey\nViewing the first `n` lines (three in this case)\n&gt;&gt;&gt; df.head(3)\n      animal\n0  alligator\n1        bee\n2     falcon\nFor negative values of `n`\n&gt;&gt;&gt; df.head(-3)\n      animal\n0  alligator\n1        bee\n2     falcon\n3       lion\n4     monkey\n5     parrot\nFile:      ~/mambaforge/lib/python3.12/site-packages/pandas/core/generic.py\nType:      method\n\n\n\n\ndf_students_pandas.head(n=10)\n\n\n\n\n\n\n\n\nName\nMaths\nScience\nEconomics\n\n\n\n\n0\nAarav\n53\n56\n67\n\n\n1\nVivaan\n94\n88\n85\n\n\n2\nAditya\n84\n65\n69\n\n\n3\nAnanya\n98\n90\n96\n\n\n4\nIshita\n72\n74\n78\n\n\n5\nKabir\n79\n56\n60\n\n\n6\nNisha\n76\n52\n99\n\n\n7\nRohan\n71\n78\n50\n\n\n8\nPriya\n76\n72\n68\n\n\n9\nSneha\n64\n92\n94\n\n\n\n\n\n\n\n\ndf_students_pandas.tail()\n\n\n\n\n\n\n\n\nName\nMaths\nScience\nEconomics\n\n\n\n\n15\nLaksh\n68\n62\n88\n\n\n16\nPooja\n63\n62\n57\n\n\n17\nRahul\n81\n63\n67\n\n\n18\nSimran\n69\n98\n78\n\n\n19\nKiran\n85\n82\n95\n\n\n\n\n\n\n\n\n# Displaying the indices and columns\nprint(f\"Indices: {df_students_pandas.index}\")\nprint(f\"Columns: {df_students_pandas.columns}\")\n\nIndices: RangeIndex(start=0, stop=20, step=1)\nColumns: Index(['Name', 'Maths', 'Science', 'Economics'], dtype='object')\n\n\n\n# Displaying the data types of each column\nprint(\"\\nData Info:\")\ndf_students_pandas.info()\n\n\nData Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 20 entries, 0 to 19\nData columns (total 4 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   Name       20 non-null     object\n 1   Maths      20 non-null     int64 \n 2   Science    20 non-null     int64 \n 3   Economics  20 non-null     int64 \ndtypes: int64(3), object(1)\nmemory usage: 772.0+ bytes\n\n\n\n# Mean\nprint(\"\\nMean Maths:\", df_students_pandas['Maths'].mean())\nprint(\"Mean Science:\", df_students_pandas['Science'].mean())\n\n\nMean Maths: 73.85\nMean Science: 73.65\n\n\n\n# Student with maximum science using different indexing methods\ndf_students_pandas.loc[df_students_pandas['Science'].idxmax()]\n\nName         Simran\nMaths            69\nScience          98\nEconomics        78\nName: 18, dtype: object\n\n\n\n# Student with maximum |Maths - Science|\ndiff_pd = (df_students_pandas['Maths'] - df_students_pandas['Science']).abs()\nmax_diff_idx_pd = diff_pd.idxmax()\nprint(\"\\nStudent with max |Maths - Science|:\", df_students_pandas.loc[max_diff_idx_pd, 'Name'], diff_pd[max_diff_idx_pd])\n\n\nStudent with max |Maths - Science|: Aryan 32\n\n\n\n\nHistogram of Student Scores using df.plot()\n\ndf_students_pandas.plot.hist(subplots=True, layout=(1, 3), figsize=(18, 6), bins=10, alpha=0.6)\n\narray([[&lt;Axes: ylabel='Frequency'&gt;, &lt;Axes: ylabel='Frequency'&gt;,\n        &lt;Axes: ylabel='Frequency'&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\n\ndf_students_pandas.plot.hist(subplots=True, layout=(3, 1), figsize=(6, 9), bins=10, alpha=0.6)\n\narray([[&lt;Axes: ylabel='Frequency'&gt;],\n       [&lt;Axes: ylabel='Frequency'&gt;],\n       [&lt;Axes: ylabel='Frequency'&gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\n\ndf_students_pandas.plot.hist(subplots=False, alpha=0.6)\n\n\n\n\n\n\n\n\n\ndf_students_pandas.plot.density()\n\n\n\n\n\n\n\n\n\n\nSome more plotting examples with Pandas df.plot()\n\nnormal = pd.Series(np.random.normal(loc = 10, scale = 2, size = 2000)) # loc is mean, scale is standard deviation\ngamma = pd.Series(np.random.gamma(shape = 2, scale = 2, size = 2000)) # shape is k, scale is theta\nuniform = pd.Series(np.random.uniform(low = 0, high = 10, size = 2000)) # low is a, high is b\n\ndf = pd.DataFrame({'Normal': normal, 'Gamma': gamma, 'Uniform': uniform})\ndf.head()\n\n\n\n\n\n\n\n\nNormal\nGamma\nUniform\n\n\n\n\n0\n8.566490\n4.439362\n2.169578\n\n\n1\n10.946834\n5.073406\n0.512258\n\n\n2\n7.099340\n1.638423\n2.346859\n\n\n3\n11.799476\n0.733187\n2.480262\n\n\n4\n9.174528\n4.601980\n5.228461\n\n\n\n\n\n\n\n\ndf.plot(kind='hist', bins=200, alpha=0.6, figsize=(12, 6))\nplt.title(\"Histogram of Distributions\", fontsize=16)\nplt.xlabel(\"Value\", fontsize=12)\nplt.ylabel(\"Frequency\", fontsize=12)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSeeing them separately\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\nsns.histplot(df['Normal'], bins=100, kde=True, color=\"skyblue\", ax=axes[0])\naxes[0].set_title(\"Normal Distribution\", fontsize=14)\nsns.histplot(df['Gamma'], bins=100, kde=True, color=\"salmon\", ax=axes[1])\naxes[1].set_title(\"Gamma Distribution\", fontsize=14)\nsns.histplot(df['Uniform'], bins=100, kde=True, color=\"limegreen\", ax=axes[2])\naxes[2].set_title(\"Uniform Distribution\", fontsize=14)\n\nfor ax in axes:\n    ax.set_xlabel(\"Value\", fontsize=12)\n    ax.set_ylabel(\"Frequency\", fontsize=12)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "notebooks/introduction-pandas.html#core-pandas-data-structures",
    "href": "notebooks/introduction-pandas.html#core-pandas-data-structures",
    "title": "Introduction to Pandas",
    "section": "3. Core Pandas Data Structures",
    "text": "3. Core Pandas Data Structures\n\nSeries\nA Series is a one-dimensional labeled array. It can be created from a Python list or NumPy array, optionally providing a custom index. The index labels let you reference elements by name instead of by integer position.\n\n# Creating a Pandas Series\nlabels = ['a','b','c','d']\ndata = [10, 0, 50, 40]\nser = pd.Series(data, index=labels, dtype=np.float64)\nser\n\na    10.0\nb     0.0\nc    50.0\nd    40.0\ndtype: float64\n\n\n\n# Other methods to create a Pandas Series\nd = {'a': 10, 'b': 0, 'c': 50, 'd': 40}\nser = pd.Series(d)\nser\n\na    10\nb     0\nc    50\nd    40\ndtype: int64\n\n\n\n# Accessing by label - like a python dictionary\nprint(\"\\nValue at index 'c':\", ser['c'])\nprint(d['c'])\n\n\nValue at index 'c': 50\n50\n\n\n\n# Vectorized ops\nprint(\"\\nMultiply entire Series by 2:\")\nser*2 \n\n\nMultiply entire Series by 2:\n\n\na     20\nb      0\nc    100\nd     80\ndtype: int64\n\n\n\nser.sort_values()\n\nb     0\na    10\nd    40\nc    50\ndtype: int64\n\n\n\nser\n\na    10\nb     0\nc    50\nd    40\ndtype: int64\n\n\n\nser.sort_values(inplace=True)\nser\n\nb     0\na    10\nd    40\nc    50\ndtype: int64\n\n\n\nser.plot()\n\n\n\n\n\n\n\n\n\nser.sort_index(inplace=True)\nser\n\na    10\nb     0\nc    50\nd    40\ndtype: int64\n\n\n\nser.plot.bar()\n\n\n\n\n\n\n\n\n\nser.plot(kind='bar', rot=0)\n\n\n\n\n\n\n\n\n\nser.plot(kind='barh')\n\n\n\n\n\n\n\n\n\n# Accessing by position - like a numpy array\nprint(\"\\nValue at position 2:\", ser.iloc[2])\n\n\nValue at position 2: 50\n\n\n\nser\n\na    10\nb     0\nc    50\nd    40\ndtype: int64\n\n\n\n# Access by key - like a python dictionary\nprint(\"\\nValue at index 'c':\", ser.loc['c'])\nprint(\"\\nValue at index 'c':\", ser['c'])\n\n\nValue at index 'c': 50\n\nValue at index 'c': 50\n\n\n\nser['c'], ser.loc['c']\n\n(np.int64(50), np.int64(50))\n\n\n\n# Access range\nprint(\"\\nValues in range 1 to 3:\")\nser.iloc[1:3]\n\n\nValues in range 1 to 3:\n\n\nb     0\nc    50\ndtype: int64\n\n\n\n# access range by label\nprint(\"\\nValues in range 'b' to 'd':\")\nser.loc['b':'d']\n\n\nValues in range 'b' to 'd':\n\n\nb     0\nc    50\nd    40\ndtype: int64\n\n\n\n# Other methods\nser.describe()\n\ncount     4.000000\nmean     25.000000\nstd      23.804761\nmin       0.000000\n25%       7.500000\n50%      25.000000\n75%      42.500000\nmax      50.000000\ndtype: float64\n\n\n\nprint(ser.mean(), type(ser.mean()), ser.mean().item(), type(ser.mean().item()))\n\n25.0 &lt;class 'numpy.float64'&gt; 25.0 &lt;class 'float'&gt;\n\n\n\nser\n\na    10\nb     0\nc    50\nd    40\ndtype: int64\n\n\n\nser.rank()\n\na    2.0\nb    1.0\nc    4.0\nd    3.0\ndtype: float64\n\n\n\n# Access numpy array from Pandas Series\narr = ser.values\nprint(\"\\nNumpy array:\", arr)\nprint(\"Type of array:\", type(arr))\n\n\nNumpy array: [10  0 50 40]\nType of array: &lt;class 'numpy.ndarray'&gt;\n\n\n\nser\n\na    10\nb     0\nc    50\nd    40\ndtype: int64\n\n\n\n# Print series where elements are greater than 20\nmask = ser &gt; 20\nprint(mask)\n\na    False\nb    False\nc     True\nd     True\ndtype: bool\n\n\n\nser[mask]\n\nc    50\nd    40\ndtype: int64\n\n\n\n\nDataFrame\nA DataFrame is a 2D tabular data structure with labeled rows (index) and columns. Each column is essentially a Pandas Series.\nWe can build one from a dictionary of lists, or by reading data from external sources (CSV, SQL, Excel, etc.).\n\n# Creating a DataFrame from a dictionary\ndata_dict = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [24, 42, 18],\n    'Score': [88.5, 93.0, 78.0]\n}\ndf = pd.DataFrame(data_dict)\ndf.index = df['Name']\ndf = df.drop(['Name'], axis=1)\ndf\n\n\n\n\n\n\n\n\nAge\nScore\n\n\nName\n\n\n\n\n\n\nAlice\n24\n88.5\n\n\nBob\n42\n93.0\n\n\nCharlie\n18\n78.0\n\n\n\n\n\n\n\n\ndf.plot(kind='bar')\n\n\n\n\n\n\n\n\n\ndata_dict = {\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [24, 42, 18],\n    'Score': [88.5, 93.0, 78.0]\n}\ndf = pd.DataFrame(data_dict)\ndf\n\n\n\n\n\n\n\n\nName\nAge\nScore\n\n\n\n\n0\nAlice\n24\n88.5\n\n\n1\nBob\n42\n93.0\n\n\n2\nCharlie\n18\n78.0",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "notebooks/introduction-pandas.html#indexing-selection",
    "href": "notebooks/introduction-pandas.html#indexing-selection",
    "title": "Introduction to Pandas",
    "section": "4. Indexing & Selection",
    "text": "4. Indexing & Selection\nDataFrame selection can occur by column name, row label/index name (.loc), or row position/numpy-array like indexing (.iloc). Boolean masks also apply.\n\ndf[col] =&gt; a Series of that column.\ndf.loc[row_label] =&gt; row by label.\ndf.iloc[row_position] =&gt; row by integer position.\nBoolean indexing =&gt; df[df['Age'] &gt; 20].\nSlicing =&gt; df.loc['a':'b'], df.iloc[0:2].\n\n\ndf2 = pd.DataFrame({\n    'X': np.random.rand(5),\n    'Y': np.random.rand(5),\n    'Z': np.random.randint(1,10,size=5)\n}, index=['a','b','c','d','e'])\n\ndf2\n\n\n\n\n\n\n\n\nX\nY\nZ\n\n\n\n\na\n0.996172\n0.280775\n7\n\n\nb\n0.578735\n0.931498\n4\n\n\nc\n0.654491\n0.699822\n4\n\n\nd\n0.172585\n0.014937\n9\n\n\ne\n0.858278\n0.753386\n8\n\n\n\n\n\n\n\n\ndf2\n\n\n\n\n\n\n\n\nX\nY\nZ\n\n\n\n\na\n0.967366\n0.753478\n2\n\n\nb\n0.401003\n0.238826\n6\n\n\nc\n0.157301\n0.384844\n8\n\n\nd\n0.016926\n0.387394\n7\n\n\ne\n0.659675\n0.817511\n4\n\n\n\n\n\n\n\n\nprint(\"\\nSelect column 'Y':\")\nprint(df2['Y'])\n\n\nSelect column 'Y':\na    0.280775\nb    0.931498\nc    0.699822\nd    0.014937\ne    0.753386\nName: Y, dtype: float64\n\n\n\ntype(df2['Y'])\n\npandas.core.series.Series\n\n\n\nprint(\"\\nSelect row 'c' using loc:\")\ndisplay(df2.loc['c'])\n\n\nSelect row 'c' using loc:\n\n\nX    0.654491\nY    0.699822\nZ    4.000000\nName: c, dtype: float64\n\n\n\nprint(\"\\nSelect row at position 2 using iloc:\")\ndisplay(df2.iloc[2])\n\n\nSelect row at position 2 using iloc:\n\n\nX    0.654491\nY    0.699822\nZ    4.000000\nName: c, dtype: float64\n\n\n\nprint(\"\\nBoolean mask: rows where Z &gt; 5\")\nmask = df2['Z'] &gt; 5\ndisplay(df2[mask])\n\n\nBoolean mask: rows where Z &gt; 5\n\n\n\n\n\n\n\n\n\nX\nY\nZ\n\n\n\n\na\n0.996172\n0.280775\n7\n\n\nd\n0.172585\n0.014937\n9\n\n\ne\n0.858278\n0.753386\n8\n\n\n\n\n\n\n\n\n# loc to address by row, col \"names\" AND iloc to address by row, col \"indices\"\nprint(df2.loc['c', 'Y'], df2.iloc[2, 1])\n\n0.6998221861720456 0.6998221861720456\n\n\n\n# Select multiple rows\nprint(\"\\nSelect rows 'a' and 'c':\")\ndisplay(df2.loc[['a', 'c']])\n\n\nSelect rows 'a' and 'c':\n\n\n\n\n\n\n\n\n\nX\nY\nZ\n\n\n\n\na\n0.996172\n0.280775\n7\n\n\nc\n0.654491\n0.699822\n4\n\n\n\n\n\n\n\n\n# Select multiple columns\nprint(\"\\nSelect columns 'X' and 'Z':\")\ndisplay(df2[['X', 'Z']])\n\n\nSelect columns 'X' and 'Z':\n\n\n\n\n\n\n\n\n\nX\nZ\n\n\n\n\na\n0.610954\n6\n\n\nb\n0.059152\n3\n\n\nc\n0.483286\n9\n\n\nd\n0.325020\n8\n\n\ne\n0.059134\n3\n\n\n\n\n\n\n\n\n# Use loc notation to select multiple column\nprint(\"\\nSelect columns 'X' and 'Z' using loc:\")\n#display(df2.loc[:, ['X', 'Z']])\n\n\n# Select rows 'b' and 'd' and columns 'X' and 'Z'\nrows_to_select = ['b', 'd']\ncols_to_select = ['X', 'Z']\ndf2.loc[rows_to_select, cols_to_select]\n\n\ndf2.loc['b':'d', 'Y':'Z']\n\n\nSelect columns 'X' and 'Z' using loc:\n\n\n\n\n\n\n\n\n\nY\nZ\n\n\n\n\nb\n0.931498\n4\n\n\nc\n0.699822\n4\n\n\nd\n0.014937\n9\n\n\n\n\n\n\n\n\n# Select rows and columns\nprint(\"\\nSelect rows 'b' and 'd', columns 'Y' and 'Z':\")\ndisplay(df2.loc[['b', 'd'], ['Y', 'Z']])\n\n\nSelect rows 'b' and 'd', columns 'Y' and 'Z':\n\n\n\n\n\n\n\n\n\nY\nZ\n\n\n\n\nb\n0.895559\n3\n\n\nd\n0.573120\n8",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "notebooks/introduction-pandas.html#merging-joining-data",
    "href": "notebooks/introduction-pandas.html#merging-joining-data",
    "title": "Introduction to Pandas",
    "section": "5. Merging & Joining Data",
    "text": "5. Merging & Joining Data\nPandas provides efficient ways to combine datasets:\n\npd.concat([df1, df2]): Stack DataFrames (row or column-wise). Preferred for simple stacking horizontally or vertically.\ndf1.append(df2): Similar to concat row-wise but less efficient since it involves creation of a new index.\npd.merge(df1, df2, on='key'): Database-style merges. Also left_on, right_on, left_index, right_index.\n\nWe can specify how to merge: ‘inner’, ‘outer’, ‘left’, ‘right’.\n\nConcatenation of two dataframes with same columns - just stacks them vertically along with their indices\n\nx = pd.DataFrame({'A': ['A0', 'A1'], 'B': ['B0', 'B1']}, index=[0, 1])\ny = pd.DataFrame({'A': ['A2', 'A3'], 'B': ['B2', 'B3']}, index=[0, 1])\ndf_concat = pd.concat([x, y])\ndisplay(\"x:\")\ndisplay(x)\ndisplay(\"y:\")\ndisplay(y)\ndisplay(\"pd.concat([x, y]):\")\ndisplay(df_concat)\n\n'x:'\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nA0\nB0\n\n\n1\nA1\nB1\n\n\n\n\n\n\n\n'y:'\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nA2\nB2\n\n\n1\nA3\nB3\n\n\n\n\n\n\n\n'pd.concat([x, y]):'\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nA0\nB0\n\n\n1\nA1\nB1\n\n\n0\nA2\nB2\n\n\n1\nA3\nB3\n\n\n\n\n\n\n\n\n\nHaving similar indices will result in ambuiguity in addressing the rows, so we need to reset the index or use ignore_index=True\n\ndf_concat.loc[0]\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nA0\nB0\n\n\n0\nA2\nB2\n\n\n\n\n\n\n\n\ndf = pd.concat([x, y], ignore_index=True)\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nA0\nB0\n\n\n1\nA1\nB1\n\n\n2\nA2\nB2\n\n\n3\nA3\nB3\n\n\n\n\n\n\n\n\n\nExample specifying the axis of concatenation - row-wise or column-wise.\n\nz = pd.DataFrame({'C': ['C0', 'C1'], 'D': ['D0', 'D1']}, index=[0, 1])\ndf_col_combine = pd.concat([x, z], axis='columns')\n\ndisplay(\"x:\")\ndisplay(x)\ndisplay(\"z:\")\ndisplay(z)\ndisplay(\"pd.concat([x, z], axis='columns'):\")\ndisplay(df_col_combine)\n\n'x:'\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nA0\nB0\n\n\n1\nA1\nB1\n\n\n\n\n\n\n\n'z:'\n\n\n\n\n\n\n\n\n\nC\nD\n\n\n\n\n0\nC0\nD0\n\n\n1\nC1\nD1\n\n\n\n\n\n\n\n\"pd.concat([x, z], axis='columns'):\"\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\nA0\nB0\nC0\nD0\n\n\n1\nA1\nB1\nC1\nD1\n\n\n\n\n\n\n\n\n\nLet’s use seaborn’s ‘tips’ dataset to demonstrate merges.\n\ntips = sns.load_dataset('tips')  # Load the 'tips' dataset from seaborn\nprint(\"'tips' dataset shape:\", tips.shape)  # Print the shape (rows, columns) of the dataset\ndisplay(tips.head())  # Show the first 5 rows of the dataset\n\n'tips' dataset shape: (244, 7)\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\nis_vip = np.random.choice([True, False], size=len(tips))  # Randomly assign VIP status (True/False) to each row\n\ncustomer_ids = np.arange(1, len(tips) + 1)  # Create unique customer IDs starting from 1\nnp.random.shuffle(customer_ids)  # Shuffle the customer IDs randomly\n\nvip_info = pd.DataFrame({\n    'customer_id': customer_ids,  # Assign customer IDs\n    'vip': is_vip  # Assign the corresponding VIP status\n})\n\nprint(\"VIP info:\")\ndisplay(vip_info)  # Display the VIP information table\n\nVIP info:\n\n\n\n\n\n\n\n\n\ncustomer_id\nvip\n\n\n\n\n0\n119\nTrue\n\n\n1\n148\nTrue\n\n\n2\n2\nTrue\n\n\n3\n84\nTrue\n\n\n4\n76\nFalse\n\n\n...\n...\n...\n\n\n239\n218\nTrue\n\n\n240\n146\nTrue\n\n\n241\n176\nTrue\n\n\n242\n164\nFalse\n\n\n243\n19\nTrue\n\n\n\n\n244 rows × 2 columns\n\n\n\n\ntips_ext = tips.copy()  # Create a copy of the original 'tips' dataset\nnew_customer_ids = np.arange(1, len(tips) + 1)  # Generate new unique customer IDs\nnp.random.shuffle(new_customer_ids)  # Shuffle the customer IDs randomly\n\ntips_ext['customer_id'] = new_customer_ids  # Add the shuffled customer IDs as a new column\nprint(\"Extended tips:\")\ndisplay(tips_ext)  # Show the extended dataset\n\nExtended tips:\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ncustomer_id\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n32\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n22\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n136\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n137\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n156\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n228\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n84\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n219\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n241\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n209\n\n\n\n\n244 rows × 8 columns\n\n\n\n\nmerged = pd.merge(tips_ext, vip_info, on='customer_id', how='left')  # Merge the datasets using 'customer_id'\nprint(\"Merged Data:\")\ndisplay(merged)  # Display the merged dataset\n\nMerged Data:\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ncustomer_id\nvip\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n32\nTrue\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n22\nFalse\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n136\nTrue\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n137\nTrue\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n156\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n228\nFalse\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n84\nTrue\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n219\nFalse\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n241\nTrue\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n209\nTrue\n\n\n\n\n244 rows × 9 columns",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "notebooks/introduction-pandas.html#groupby-aggregation",
    "href": "notebooks/introduction-pandas.html#groupby-aggregation",
    "title": "Introduction to Pandas",
    "section": "6. GroupBy & Aggregation",
    "text": "6. GroupBy & Aggregation\nThe GroupBy abstraction splits data into groups, applies operations, then combines results. Common for summarizing numeric columns by categories.\n\nExample with tips data\nWe’ll group by day of the week and compute average tip, total tip, etc.\n\ntips.loc[:, 'day']\n\n0       Sun\n1       Sun\n2       Sun\n3       Sun\n4       Sun\n       ... \n239     Sat\n240     Sat\n241     Sat\n242     Sat\n243    Thur\nName: day, Length: 244, dtype: category\nCategories (4, object): ['Thur', 'Fri', 'Sat', 'Sun']\n\n\n\ntips['day'].value_counts()\n\nday\nSat     87\nSun     76\nThur    62\nFri     19\nName: count, dtype: int64\n\n\n\n# Unique values in the 'day' column\nunique_days = tips['day'].unique()\nprint(\"Unique days:\", unique_days)\n\nUnique days: ['Sun', 'Sat', 'Thur', 'Fri']\nCategories (4, object): ['Thur', 'Fri', 'Sat', 'Sun']\n\n\n\nmask = tips['day'] == 'Sun'\nmask\n\n0       True\n1       True\n2       True\n3       True\n4       True\n       ...  \n239    False\n240    False\n241    False\n242    False\n243    False\nName: day, Length: 244, dtype: bool\n\n\n\ntips[mask]['tip'].mean().item()\n\n3.2551315789473687\n\n\n\nout = {}\nfor day in unique_days:\n    mask = tips['day'] == day\n    out[day] = tips[mask]['tip'].mean().item()\n\nout\n\n{'Sun': 3.2551315789473687,\n 'Sat': 2.993103448275862,\n 'Thur': 2.771451612903225,\n 'Fri': 2.7347368421052627}\n\n\n\nser = pd.Series(out)\nser.plot(kind='bar')\n\n\n\n\n\n\n\n\n\ngrpby = tips.groupby('day', observed=True)['tip']\ngrpby.mean()\n\nday\nThur    2.771452\nFri     2.734737\nSat     2.993103\nSun     3.255132\nName: tip, dtype: float64\n\n\n\n# Group tips by 'day' column, aggregate 'tip' in different ways\ngrouped = tips.groupby('day', observed=True)['tip']\nprint(\"Mean tip by day:\")\ndisplay(grouped.mean())\n\nprint(\"\\nMultiple Aggregations (count, sum, mean):\")\ndisplay(grouped.agg(['count','sum','mean','std']))\n\nMean tip by day:\n\n\nday\nThur    2.771452\nFri     2.734737\nSat     2.993103\nSun     3.255132\nName: tip, dtype: float64\n\n\n\nMultiple Aggregations (count, sum, mean):\n\n\n\n\n\n\n\n\n\ncount\nsum\nmean\nstd\n\n\nday\n\n\n\n\n\n\n\n\nThur\n62\n171.83\n2.771452\n1.240223\n\n\nFri\n19\n51.96\n2.734737\n1.019577\n\n\nSat\n87\n260.40\n2.993103\n1.631014\n\n\nSun\n76\n247.39\n3.255132\n1.234880\n\n\n\n\n\n\n\nMultiple Grouping Keys: We can group by multiple columns, e.g. day and time (Lunch/Dinner).\n\nmulti_grouped = tips.groupby(['day', 'time'], observed=True)['total_bill'].agg(['mean','size'])\nmulti_grouped\n\n\n\n\n\n\n\n\n\nmean\nsize\n\n\nday\ntime\n\n\n\n\n\n\nThur\nLunch\n17.664754\n61\n\n\nDinner\n18.780000\n1\n\n\nFri\nLunch\n12.845714\n7\n\n\nDinner\n19.663333\n12\n\n\nSat\nDinner\n20.441379\n87\n\n\nSun\nDinner\n21.410000\n76",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "notebooks/introduction-pandas.html#pivot-tables",
    "href": "notebooks/introduction-pandas.html#pivot-tables",
    "title": "Introduction to Pandas",
    "section": "7. Pivot Tables",
    "text": "7. Pivot Tables\nPivot tables provide a 2D summarization akin to spreadsheets:\ndf.pivot_table(values='col', index='rows', columns='cols', aggfunc='mean')\nWe can also specify margins (margins=True) to get row/column totals.\n\n# Pivot example using 'tips'\npivot_tips = tips.pivot_table(\n    values='tip',\n    index='day',\n    columns='time',\n    aggfunc='mean',\n    margins=True, observed=True\n)\npivot_tips\n\n\n\n\n\n\n\ntime\nLunch\nDinner\nAll\n\n\nday\n\n\n\n\n\n\n\nThur\n2.767705\n3.000000\n2.771452\n\n\nFri\n2.382857\n2.940000\n2.734737\n\n\nSat\nNaN\n2.993103\n2.993103\n\n\nSun\nNaN\n3.255132\n3.255132\n\n\nAll\n2.728088\n3.102670\n2.998279\n\n\n\n\n\n\n\n\n# handling nan values by filling them with 0\ntips_fillna = pivot_tips.fillna(0, inplace=False)\n\n# handling nan values by dropping them\ntips_dropna = pivot_tips.dropna()\n\ndisplay(tips_fillna)\ndisplay(tips_dropna)\n\n\n\n\n\n\n\ntime\nLunch\nDinner\nAll\n\n\nday\n\n\n\n\n\n\n\nThur\n2.767705\n3.000000\n2.771452\n\n\nFri\n2.382857\n2.940000\n2.734737\n\n\nSat\n0.000000\n2.993103\n2.993103\n\n\nSun\n0.000000\n3.255132\n3.255132\n\n\nAll\n2.728088\n3.102670\n2.998279\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime\nLunch\nDinner\nAll\n\n\nday\n\n\n\n\n\n\n\nThur\n2.767705\n3.00000\n2.771452\n\n\nFri\n2.382857\n2.94000\n2.734737\n\n\nAll\n2.728088\n3.10267\n2.998279",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "notebooks/introduction-pandas.html#string-operations",
    "href": "notebooks/introduction-pandas.html#string-operations",
    "title": "Introduction to Pandas",
    "section": "8. String Operations",
    "text": "8. String Operations\nPandas offers vectorized string methods under str. They handle missing data gracefully and allow powerful regex usage.\nKey methods: - case changes: .str.lower(), .str.upper(), .str.title(), etc. - trimming: .str.strip(), .str.rstrip(), etc. - Regex: .str.contains(), .str.extract(), .str.replace(). - split: .str.split(), .str.get(), etc.\n\n# Example: string cleaning\ns_str = pd.Series([\"  HELLO  \", \"world! \", None, \"PyTHon 3.9 \", \"pandas is COOL \"], name='mystrings')\nprint(\"Original:\")\ndisplay(s_str)\n\n# Lower + strip\ncleaned = s_str.str.lower().str.strip()\n\nprint(\"Cleaned:\")\ndisplay(cleaned)\n\n# Contains 'python'?\nprint(\"\\nContains 'python'?\")\ndisplay(cleaned.str.contains('python'))\n\n# Replace 'is' with 'IS'\nreplaced = cleaned.str.replace('is', 'IS', case=False)\nprint(\"\\nReplaced 'is':\")\ndisplay(replaced)\n\nOriginal:\n\n\n0            HELLO  \n1            world! \n2               None\n3        PyTHon 3.9 \n4    pandas is COOL \nName: mystrings, dtype: object\n\n\nCleaned:\n\n\n0             hello\n1            world!\n2              None\n3        python 3.9\n4    pandas is cool\nName: mystrings, dtype: object\n\n\n\nContains 'python'?\n\n\n0    False\n1    False\n2     None\n3     True\n4    False\nName: mystrings, dtype: object\n\n\n\nReplaced 'is':\n\n\n0             hello\n1            world!\n2              None\n3        python 3.9\n4    pandas IS cool\nName: mystrings, dtype: object\n\n\nThe presence of None doesn’t break things: the .str accessor handles missing data by returning NaN in operations.",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "notebooks/introduction-pandas.html#using-eval-query",
    "href": "notebooks/introduction-pandas.html#using-eval-query",
    "title": "Introduction to Pandas",
    "section": "9. Using eval() & query()",
    "text": "9. Using eval() & query()\neval() allows efficient expression evaluation on DataFrame columns:\ndf.eval('NewCol = (A + B) / C', inplace=True)\nYou can treat column names like variables, and skip creation of large temporary arrays.\nquery() provides a more readable syntax for row selection:\ndf.query('A &lt; 5 and B &gt; 2')\n\ndf_eval = pd.DataFrame({\n    'A': np.random.randn(5),\n    'B': np.random.randn(5),\n    'C': np.random.randn(5)\n})\nprint(\"Before eval:\")\ndisplay(df_eval)\n\ndf_eval.eval('D = (A + B) * C', inplace=True)\nprint(\"\\nAfter eval, new column 'D':\")\ndisplay(df_eval)\n\n# query\nres_q = df_eval.query('D &gt; 0')\nprint(\"\\nRows where D &gt; 0:\")\ndisplay(res_q)\n\nBefore eval:\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n-1.065378\n-0.126780\n0.644445\n\n\n1\n-2.225845\n-0.966845\n-1.219053\n\n\n2\n-1.012120\n0.980824\n-0.766431\n\n\n3\n-1.453329\n0.220464\n-2.387369\n\n\n4\n-0.756387\n0.964827\n1.534876\n\n\n\n\n\n\n\n\nAfter eval, new column 'D':\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n0\n-1.065378\n-0.126780\n0.644445\n-0.768281\n\n\n1\n-2.225845\n-0.966845\n-1.219053\n3.892059\n\n\n2\n-1.012120\n0.980824\n-0.766431\n0.023986\n\n\n3\n-1.453329\n0.220464\n-2.387369\n2.943305\n\n\n4\n-0.756387\n0.964827\n1.534876\n0.319929\n\n\n\n\n\n\n\n\nRows where D &gt; 0:\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\n\n\n\n\n1\n-2.225845\n-0.966845\n-1.219053\n3.892059\n\n\n2\n-1.012120\n0.980824\n-0.766431\n0.023986\n\n\n3\n-1.453329\n0.220464\n-2.387369\n2.943305\n\n\n4\n-0.756387\n0.964827\n1.534876\n0.319929",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Pandas"
    ]
  },
  {
    "objectID": "notebooks/widgets.html",
    "href": "notebooks/widgets.html",
    "title": "Interactive Data Exploration with Jupyter Widgets",
    "section": "",
    "text": "import ipywidgets\nprint(ipywidgets.__version__)\n\n8.1.5",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Interactive Data Exploration with Jupyter Widgets"
    ]
  },
  {
    "objectID": "notebooks/widgets.html#introduction",
    "href": "notebooks/widgets.html#introduction",
    "title": "Interactive Data Exploration with Jupyter Widgets",
    "section": "Introduction",
    "text": "Introduction\nThis notebook demonstrates the power of Jupyter widgets for creating interactive data exploration experiences. Widgets allow us to build dynamic interfaces directly within Jupyter notebooks, making data analysis more engaging and intuitive.\n\nLearning Objectives\nBy the end of this notebook, you will understand: - How to use ipywidgets for interactive data filtering - Creating interactive visualizations with @interact decorator - Building custom widget interfaces for data exploration - Combining widgets with pandas for dynamic data analysis - Best practices for interactive notebook design\n\n\nPrerequisites\n\nBasic understanding of pandas and matplotlib\nFamiliarity with Jupyter notebooks\nBasic Python programming concepts",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Interactive Data Exploration with Jupyter Widgets"
    ]
  },
  {
    "objectID": "notebooks/widgets.html#why-use-interactive-widgets",
    "href": "notebooks/widgets.html#why-use-interactive-widgets",
    "title": "Interactive Data Exploration with Jupyter Widgets",
    "section": "Why Use Interactive Widgets?",
    "text": "Why Use Interactive Widgets?\nInteractive widgets provide several advantages: 1. Immediate Feedback: See results change in real-time as you modify parameters 2. Enhanced Exploration: Easily test different scenarios without rewriting code 3. Better Communication: Make presentations and demos more engaging 4. Reduced Code Repetition: One function can handle multiple parameter combinations\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ipywidgets import interact, widgets\nfrom IPython.display import clear_output\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Interactive Data Exploration with Jupyter Widgets"
    ]
  },
  {
    "objectID": "notebooks/widgets.html#setup-and-data-preparation",
    "href": "notebooks/widgets.html#setup-and-data-preparation",
    "title": "Interactive Data Exploration with Jupyter Widgets",
    "section": "Setup and Data Preparation",
    "text": "Setup and Data Preparation\nLet’s start by importing the necessary libraries and creating a sample student dataset for our interactive exploration.\n\nnames = [\n    'Aarav', 'Aditi', 'Arjun', 'Ananya', 'Dhruv', 'Diya', \n    'Ishaan', 'Isha', 'Krishna', 'Kavya', 'Mira', 'Mihir',\n    'Neha', 'Nikhil', 'Priya', 'Pranav', 'Riya', 'Rohan'\n]\n\ndf = pd.DataFrame({\n    'Name': names,\n    'Age': np.random.randint(18, 23, len(names)),\n    'Grade': np.random.randint(65, 100, len(names)),\n    'Subject': np.random.choice(['Math', 'Physics', 'Chemistry'], len(names))\n})\n\n\nCreating Sample Student Data\nWe’ll create a realistic student dataset with multiple attributes to demonstrate various filtering capabilities.\n\ndisplay(df)\n\n\n\n\n\n\n\n\nName\nAge\nGrade\nSubject\n\n\n\n\n0\nAarav\n20\n69\nChemistry\n\n\n1\nAditi\n21\n68\nPhysics\n\n\n2\nArjun\n19\n72\nPhysics\n\n\n3\nAnanya\n21\n65\nChemistry\n\n\n4\nDhruv\n20\n75\nPhysics\n\n\n5\nDiya\n21\n88\nPhysics\n\n\n6\nIshaan\n22\n84\nMath\n\n\n7\nIsha\n18\n99\nMath\n\n\n8\nKrishna\n19\n77\nChemistry\n\n\n9\nKavya\n21\n87\nPhysics\n\n\n10\nMira\n21\n69\nChemistry\n\n\n11\nMihir\n19\n77\nChemistry\n\n\n12\nNeha\n18\n66\nMath\n\n\n13\nNikhil\n18\n78\nPhysics\n\n\n14\nPriya\n22\n89\nPhysics\n\n\n15\nPranav\n20\n99\nMath\n\n\n16\nRiya\n18\n84\nMath\n\n\n17\nRohan\n21\n65\nPhysics\n\n\n\n\n\n\n\n\nprint(\"\\nBasic Subsetting Examples:\")\nprint(\"\\nMath Students:\")\nmath_students = df.query('Subject == \"Math\"')\nmath_students\n\n\nBasic Subsetting Examples:\n\nMath Students:\n\n\n\n\n\n\n\n\n\nName\nAge\nGrade\nSubject\n\n\n\n\n6\nIshaan\n22\n84\nMath\n\n\n7\nIsha\n18\n99\nMath\n\n\n12\nNeha\n18\n66\nMath\n\n\n15\nPranav\n20\n99\nMath\n\n\n16\nRiya\n18\n84\nMath",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Interactive Data Exploration with Jupyter Widgets"
    ]
  },
  {
    "objectID": "notebooks/widgets.html#basic-data-filtering-examples",
    "href": "notebooks/widgets.html#basic-data-filtering-examples",
    "title": "Interactive Data Exploration with Jupyter Widgets",
    "section": "Basic Data Filtering Examples",
    "text": "Basic Data Filtering Examples\nBefore diving into interactive widgets, let’s see some traditional filtering approaches to understand what we’re trying to make interactive.\n\nprint(\"\\nHigh Performers (Grade &gt; 85):\")\nhigh_performers = df.query(\"Grade &gt; 85\")\ndisplay(high_performers)\n\n\nHigh Performers (Grade &gt; 85):\n\n\n\n\n\n\n\n\n\nName\nAge\nGrade\nSubject\n\n\n\n\n5\nDiya\n21\n88\nPhysics\n\n\n7\nIsha\n18\n99\nMath\n\n\n9\nKavya\n21\n87\nPhysics\n\n\n14\nPriya\n22\n89\nPhysics\n\n\n15\nPranav\n20\n99\nMath\n\n\n\n\n\n\n\n\n@interact\ndef filter_by_subject(subject=['All'] + list(df['Subject'].unique())):\n    print(f\"Showing {subject} students\")\n    \"\"\"Simple filtering using @interact\"\"\"\n    if subject == 'All':\n        return df\n    return df.query('Subject == @subject')",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Interactive Data Exploration with Jupyter Widgets"
    ]
  },
  {
    "objectID": "notebooks/widgets.html#simple-interactive-filtering-with-interact",
    "href": "notebooks/widgets.html#simple-interactive-filtering-with-interact",
    "title": "Interactive Data Exploration with Jupyter Widgets",
    "section": "Simple Interactive Filtering with @interact",
    "text": "Simple Interactive Filtering with @interact\nThe @interact decorator is the simplest way to create interactive widgets. It automatically generates appropriate widgets based on the function parameters.\n\ndef filter_students(subject='All', min_grade=0):\n    \"\"\"More controlled filtering using interactive\"\"\"\n    print(f\"Showing {subject} students with grade &gt;= {min_grade:0.1f}\")\n    filtered = df if subject == 'All' else df.query('Subject == @subject')\n    out = filtered.query('Grade &gt;= @min_grade')\n    return out\n\n\nfilter_students()\n\nShowing All students with grade &gt;= 0.0\n\n\n\n\n\n\n\n\n\nName\nAge\nGrade\nSubject\n\n\n\n\n0\nAarav\n20\n69\nChemistry\n\n\n1\nAditi\n21\n68\nPhysics\n\n\n2\nArjun\n19\n72\nPhysics\n\n\n3\nAnanya\n21\n65\nChemistry\n\n\n4\nDhruv\n20\n75\nPhysics\n\n\n5\nDiya\n21\n88\nPhysics\n\n\n6\nIshaan\n22\n84\nMath\n\n\n7\nIsha\n18\n99\nMath\n\n\n8\nKrishna\n19\n77\nChemistry\n\n\n9\nKavya\n21\n87\nPhysics\n\n\n10\nMira\n21\n69\nChemistry\n\n\n11\nMihir\n19\n77\nChemistry\n\n\n12\nNeha\n18\n66\nMath\n\n\n13\nNikhil\n18\n78\nPhysics\n\n\n14\nPriya\n22\n89\nPhysics\n\n\n15\nPranav\n20\n99\nMath\n\n\n16\nRiya\n18\n84\nMath\n\n\n17\nRohan\n21\n65\nPhysics\n\n\n\n\n\n\n\n\nstudent_filter_viz = interact(\n    filter_students,\n    subject = ['All'] + list(df['Subject'].unique()),\n    min_grade=(0.0, 99.5, 5.0)\n)",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Interactive Data Exploration with Jupyter Widgets"
    ]
  },
  {
    "objectID": "notebooks/widgets.html#advanced-interactive-filtering",
    "href": "notebooks/widgets.html#advanced-interactive-filtering",
    "title": "Interactive Data Exploration with Jupyter Widgets",
    "section": "Advanced Interactive Filtering",
    "text": "Advanced Interactive Filtering\nNow let’s create a more sophisticated interface with multiple filter criteria. Notice how we can combine different widget types for a richer user experience.\n\ndef plot_wave(freq=1):\n    \"\"\"Basic plotting function\"\"\"\n    x = np.linspace(-1*np.pi, 1*np.pi, 500)\n    \n    plt.plot(x, np.sin(np.pi*2*freq*x), label='sin(x)')\n    plt.plot(x, np.cos(np.pi*2*freq*x), label='cos(x)')\n    plt.grid(True)\n    plt.legend()\n    plt.title('Trigonometric Functions')\n    plt.show()\n\nprint(\"\\nBasic Plot (Direct Function Call):\")\nplot_wave()\n\n\nBasic Plot (Direct Function Call):",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Interactive Data Exploration with Jupyter Widgets"
    ]
  },
  {
    "objectID": "notebooks/widgets.html#interactive-data-visualization",
    "href": "notebooks/widgets.html#interactive-data-visualization",
    "title": "Interactive Data Exploration with Jupyter Widgets",
    "section": "Interactive Data Visualization",
    "text": "Interactive Data Visualization\nWidgets aren’t just for filtering data - they’re also powerful for creating interactive visualizations. Let’s explore mathematical functions with interactive parameters.\n\nprint(\"\\nPlotting with @interact:\")\n\n@interact\ndef plot_wave_interact(freq=(1, 5, 1)):\n    \"\"\"Simple interactive plotting with @interact\"\"\"\n    plot_wave(freq)\n\n\nPlotting with @interact:\n\n\n\n\n\n\nSimple Interactive Plotting\nThe @interact decorator makes it easy to create interactive plots with minimal code.\n\ndef plot_wave_advanced(func_type='sine', frequency=1, color='blue', show_grid=True):\n    \"\"\"Advanced plotting with more control\"\"\"\n    fig, ax = plt.subplots(figsize=(6, 4))\n    \n    num_points = 1000\n    \n    x = np.linspace(-2 * np.pi, 2 * np.pi, num_points)\n    \n    if func_type == 'sine':\n        y = np.sin(2 * np.pi * frequency * x)\n        title = 'Sine Wave'\n    elif func_type == 'cosine':\n        y = np.cos(2 * np.pi * frequency * x)\n        title = 'Cosine Wave'\n    \n    plt.plot(x, y, color=color, label=func_type)\n    plt.title(title)\n    plt.legend()\n    plt.grid(show_grid)\n    \n    plt.show()\n\n# Create the interact widget and store it in plot_viz\nplot_viz = interact(\n    plot_wave_advanced,\n    func_type=widgets.RadioButtons(\n        options=['sine', 'cosine'],\n        description='Function:',\n        style={'description_width': 'initial'},\n        layout={'width': '200px'}\n    ),\n    frequency=widgets.FloatSlider(\n        value=1,\n        min=0.1,\n        max=5,\n        step=0.1,\n        description='Frequency:',\n        style={'description_width': 'initial'},\n        layout={'width': '300px'}\n    ),\n    color=widgets.Dropdown(\n        options=['blue', 'red', 'green', 'purple'],\n        value='blue',\n        description='Color:',\n        style={'description_width': 'initial'},\n        layout={'width': '200px'}\n    ),\n    show_grid=widgets.Checkbox(\n        value=True,\n        description='Show Grid',\n        style={'description_width': 'initial'}\n    )\n)",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Interactive Data Exploration with Jupyter Widgets"
    ]
  },
  {
    "objectID": "notebooks/widgets.html#advanced-widget-customization",
    "href": "notebooks/widgets.html#advanced-widget-customization",
    "title": "Interactive Data Exploration with Jupyter Widgets",
    "section": "Advanced Widget Customization",
    "text": "Advanced Widget Customization\nFor more control over the interface, we can explicitly define widget types and their properties. This gives us fine-grained control over the user experience.",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Interactive Data Exploration with Jupyter Widgets"
    ]
  },
  {
    "objectID": "notebooks/widgets.html#key-takeaways-and-best-practices",
    "href": "notebooks/widgets.html#key-takeaways-and-best-practices",
    "title": "Interactive Data Exploration with Jupyter Widgets",
    "section": "Key Takeaways and Best Practices",
    "text": "Key Takeaways and Best Practices\n\nWhat We’ve Learned\n\nSimple Interactivity: The @interact decorator provides quick interactive capabilities\nCustom Widgets: Explicit widget definitions offer more control over user interface\nReal-time Updates: Widgets provide immediate feedback for data exploration\nMultiple Widget Types: Different widgets (sliders, dropdowns, checkboxes) serve different purposes\n\n\n\nBest Practices for Interactive Widgets\n\nStart Simple: Begin with @interact and add complexity as needed\nMeaningful Parameters: Choose parameters that significantly affect results\nReasonable Ranges: Set appropriate min/max values for sliders\nClear Labels: Use descriptive names for widget labels\nPerformance: Be mindful of computational cost for real-time updates\n\n\n\nCommon Widget Types\n\nIntSlider/FloatSlider: For numeric ranges\nDropdown: For categorical selections\nRadioButtons: For mutually exclusive choices\n\nCheckbox: For boolean options\nText: For string input\n\n\n\nReal-World Applications\nInteractive widgets are valuable for: - Parameter Tuning: Machine learning hyperparameter optimization - Data Exploration: Quick filtering and subsetting of datasets - Educational Content: Teaching concepts with interactive examples - Dashboards: Creating simple analytical interfaces - Prototyping: Rapid development of interactive tools\n\n\nNext Steps\nTo extend your widget skills: 1. Explore advanced layout options with HBox and VBox 2. Learn about widget events and callbacks 3. Investigate integration with other visualization libraries (Plotly, Bokeh) 4. Consider building full dashboard applications with Voila",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Interactive Data Exploration with Jupyter Widgets"
    ]
  },
  {
    "objectID": "notebooks/joint-distribution-properties.html",
    "href": "notebooks/joint-distribution-properties.html",
    "title": "Joint Distribution Properties",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nprint(np.__version__)\nimport torch \nimport torch.nn as nn\n\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n1.26.4\n\n\n\nX = torch.distributions.Uniform(-1, 1)\nY = torch.distributions.Uniform(-1, 1)\n\nx_samples = X.sample((1000,))\ny_samples = Y.sample((1000,))\n\nplt.scatter(x_samples, y_samples)\n# aspect ratio\nplt.gca().set_aspect('equal')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nText(0, 0.5, 'Y')\n\n\n\n\n\n\n\n\n\n\n## E[XY] between U(-1, 1) and U(-1, 1)\n\nE_XY = torch.mean(x_samples * y_samples)\nprint(E_XY)\n\ntensor(-0.0064)\n\n\nAnalytical solution for the expected value of the product of two random variables: \\(X\\sim U(0,1)\\) and \\(Y\\sim U(0,1)\\).\nWe know that \\(f_{X, Y}(x, y) = 1\\) for \\(0 \\leq x \\leq 1\\) and \\(0 \\leq y \\leq 1\\). The expected value of the product of two random variables is given by:\n\\[\\begin{equation}\n\\begin{aligned}\nE[XY] &= \\int_{0}^{1} \\int_{0}^{1} xy \\, dx \\, dy \\\\\n&= \\int_{0}^{1} \\left[ \\frac{1}{2}x^2y \\right]_{0}^{1} \\, dy \\\\\n&= \\int_{0}^{1} \\frac{1}{2}y \\, dy \\\\\n&= \\left[ \\frac{1}{4}y^2 \\right]_{0}^{1} \\\\\n&= \\frac{1}{4}\n\\end{aligned}\n\\end{equation}\\]\n\ndef E_XY(X, Y, n=1000):\n    x_samples = X.sample((n,))\n    y_samples = Y.sample((n,))\n    return torch.mean(x_samples * y_samples)\n\n\nX = torch.distributions.Uniform(0, 1)\nY = torch.distributions.Uniform(0, 1)\n\nprint(E_XY(X, Y, 1000000))\n\n# This is the same as E[XY] = E[X]E[Y] for independent random variables\n\ntensor(0.2502)\n\n\nFinding \\(\\cos(\\theta)\\) for \\(X\\sim U(0,1)\\) and \\(Y\\sim U(0,1)\\):\n\\(E[X^2] = \\int_{0}^{1} x^2 \\, dx = \\left[ \\frac{1}{3}x^3 \\right]_{0}^{1} = \\frac{1}{3}\\)\n\\(\\cos(\\theta) = \\frac{E[XY]}{\\sqrt{E[X^2]E[Y^2]}} = \\frac{\\frac{1}{4}}{\\sqrt{\\frac{1}{3}\\frac{1}{3}}} = \\frac{3}{4}\\)\nFor any general \\(X\\sim U(a,b)\\) and \\(Y\\sim U(a,b)\\), the expected value of the product of two random variables is given by:\n\\[\\begin{equation}\n\\begin{aligned}\nE[XY] &= \\int_{a}^{b} \\int_{a}^{b} xy \\, dx \\, dy \\\\\n&= \\int_{a}^{b} \\left[ \\frac{1}{2}x^2y \\right]_{a}^{b} \\, dy \\\\\n&= \\int_{a}^{b} \\frac{1}{2}(b^2 - a^2)y \\, dy \\\\\n&= \\frac{1}{2}(b^2 - a^2) \\int_{a}^{b} y \\, dy \\\\\n&= \\frac{1}{2}(b^2 - a^2) \\left[ \\frac{1}{2}y^2 \\right]_{a}^{b} \\\\\n&= \\frac{1}{2}(b^2 - a^2) \\left( \\frac{1}{2}b^2 - \\frac{1}{2}a^2 \\right) \\\\\n&= \\frac{1}{4}(b^2 - a^2)(b^2 - a^2) \\\\\n&= \\frac{1}{4}(b - a)^2(b + a)^2\n\\end{aligned}\n\\end{equation}\\]\n\\(E[X^2] = \\int_{a}^{b} x^2 \\, dx = \\left[ \\frac{1}{3}x^3 \\right]_{a}^{b} = \\frac{1}{3}(b^3 - a^3)\\)\n\\(\\sqrt{E[X^2]E[Y^2]} = \\sqrt{\\frac{1}{3}(b^3 - a^3)\\frac{1}{3}(b^3 - a^3)} = \\frac{1}{3}(b^3 - a^3)\\)\nAnd \\(\\cos(\\theta)\\) for \\(X\\sim U(a,b)\\) and \\(Y\\sim U(a,b)\\) is given by:\n\\[\\begin{equation}\n\\begin{aligned}\n\\cos(\\theta) &= \\frac{E[XY]}{\\sqrt{E[X^2]E[Y^2]}} \\\\\n&= \\frac{\\frac{1}{4}(b - a)^2(b + a)^2}{\\frac{1}{3}(b^3 - a^3)} \\\\\n\n\\end{aligned}\n\\end{equation}\\]\n\n### cos(theta)\n\ndef E_cos_theta(X, Y, n=1000):\n    x_samples = X.sample((n,))\n    y_samples = Y.sample((n,))\n    EXY = torch.mean(x_samples * y_samples)\n    EX2 = torch.mean(x_samples ** 2)\n    EY2 = torch.mean(y_samples ** 2)\n    print(EXY, EX2, EY2)\n\n    return EXY / torch.sqrt(EX2 * EY2)\n\n\nX = torch.distributions.Uniform(0, 1)\nY = torch.distributions.Uniform(0, 1)\n\nprint(E_cos_theta(X, Y, 10000))\n\ntensor(0.2479) tensor(0.3288) tensor(0.3344)\ntensor(0.7475)\n\n\n\n# Now increasing the range of the random variables\nX = torch.distributions.Uniform(10, 11)\nY = torch.distributions.Uniform(10, 11)\n\nprint(E_XY(X, Y))\n\nEX2 = torch.mean(x_samples ** 2)\nEY2 = torch.mean(y_samples ** 2)\n\nprint(EX2, EY2)\n\ntensor(110.4675)\ntensor(0.3374) tensor(0.3341)\n\n\n\nX = torch.distributions.Uniform(10, 11)\nY = torch.distributions.Uniform(10, 11)\n\nprint(E_cos_theta(X, Y, 10000))\n\ntensor(110.3001) tensor(110.3019) tensor(110.4660)\ntensor(0.9992)\n\n\n\n## E[XY] between N(0, 1) and N(0, 1)\n\nX = torch.distributions.Normal(0, 1)\nY = torch.distributions.Normal(0, 1)\n\nx_samples = X.sample((1000,))\ny_samples = Y.sample((1000,))\n\nplt.scatter(x_samples, y_samples)\n# aspect ratio\nplt.gca().set_aspect('equal')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nE_XY = torch.mean(x_samples * y_samples)\nprint(E_XY)\n\ntensor(0.0701)\n\n\n\n\n\n\n\n\n\n\ndist = torch.distributions.Normal(0, 1)\nx = dist.sample((1000,))\nplt.hist(x.numpy(), bins=50, density=True)\n\nx_range = torch.linspace(-3, 3, 1000)\ny = dist.log_prob(x_range).exp()\nplt.plot(x_range.numpy(), y.numpy())\n\n\n\n\n\n\n\n\n\ndist.sample([10])\n\ntensor([-1.9083,  0.3758,  0.0051,  0.5140,  0.9852, -0.5989,  0.5222, -0.7744,\n         0.9462, -1.7868])\n\n\n\ndist_2d_normal = torch.distributions.MultivariateNormal(torch.tensor([0.0, 0.0]), torch.eye(2))\n#dist_2d_normal = torch.distributions.MultivariateNormal(torch.tensor([0.0, 0.0]), torch.tensor([[1.0, 0.5], [0.5, 1.0]]))\n\ndist_2d_normal.sample([10])\n\ntensor([[ 0.0438, -0.0310],\n        [ 0.0487, -0.3790],\n        [-0.7872,  0.9880],\n        [ 1.0010, -0.9025],\n        [ 0.5449,  0.1047],\n        [ 1.6466,  0.0925],\n        [ 0.9357,  0.2228],\n        [-1.2721,  2.5194],\n        [-0.3306, -0.1152],\n        [ 1.2249, -1.7330]])\n\n\n\nplt.scatter(*dist_2d_normal.sample([1000]).numpy().T, alpha=0.2, color='k')\n\n\n\n\n\n\n\n\n\n# Plot 2D normal distribution surface plot of PDF\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = torch.linspace(-3, 3, 100)\ny = torch.linspace(-3, 3, 100)\n\nX, Y = torch.meshgrid(x, y)\nxy = torch.stack([X, Y], 2)\nz = dist_2d_normal.log_prob(xy).exp()\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X.numpy(), Y.numpy(), z.numpy())\n\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('PDF')\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\ndf = pd.read_html(\"http://socr.ucla.edu/docs/resources/SOCR_Data/SOCR_Data_Dinov_020108_HeightsWeights.html\")\n\n\nstore_df = df[0]\nstore_df.columns = store_df.iloc[0]\nstore_df = store_df.iloc[1:]\nstore_df = store_df.astype(float)\nstore_df = store_df.drop(columns=[\"Index\"])\nstore_df = store_df.dropna()\n\n\nstore_df.head()\n\n\n\n\n\n\n\n\nHeight(Inches)\nWeight(Pounds)\n\n\n\n\n1\n65.78331\n112.9925\n\n\n2\n71.51521\n136.4873\n\n\n3\n69.39874\n153.0269\n\n\n4\n68.21660\n142.3354\n\n\n5\n67.78781\n144.2971\n\n\n\n\n\n\n\n\n### Fiting a bi-variate normal distribution to the data\ndata = torch.tensor(store_df.values)\nmean = data.mean(0)\ncov = torch.cov(data.T)\ndist = torch.distributions.MultivariateNormal(mean, cov)\n\n\ndist.loc\n\ntensor([ 67.9931, 127.0794], dtype=torch.float64)\n\n\n\ndist.covariance_matrix\n\ntensor([[  3.6164,  11.1510],\n        [ 11.1510, 135.9765]], dtype=torch.float64)\n\n\n\n# Plot the data\n\nplt.scatter(data[:, 0], data[:, 1], alpha=0.1, color='k', facecolors='k')\nplt.xlabel(\"Height\")\nplt.ylabel(\"Weight\")\n\n\nText(0, 0.5, 'Weight')\n\n\n\n\n\n\n\n\n\n\n# plot the PDF\nx = torch.linspace(50, 80, 100)\ny = torch.linspace(80, 280, 100)\nX, Y = torch.meshgrid(x, y)\nxy = torch.stack([X, Y], 2)\nz = dist.log_prob(xy).exp()\n\nimport plotly.graph_objects as go\n\n# Create surface plot with custom hover labels\nfig = go.Figure(data=[go.Surface(\n    x=X, y=Y, z=z, colorscale=\"viridis\",\n    hovertemplate=\"Height: %{x:0.2f}&lt;br&gt;Weight: %{y:0.2f}&lt;br&gt;PDF: %{z:0.5f}&lt;extra&gt;&lt;/extra&gt;\"\n)])\n\n# Maximize figure size and reduce whitespace\nfig.update_layout(\n    autosize=True,\n    width=1200,  # Set wider figure\n    height=700,  # Set taller figure\n    margin=dict(l=0, r=0, t=40, b=0),  # Remove extra whitespace\n    title=\"2D Gaussian PDF\",\n    scene=dict(\n        xaxis_title=\"Height\",\n        yaxis_title=\"Weight\",\n        zaxis_title=\"PDF\"\n    )\n)\n\n# Show plot\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n# uniform distribution\ndist_uniform = torch.distributions.Uniform(0, 1)\nx = dist_uniform.sample((1000,))\nplt.hist(x.numpy(), bins=50, density=True)\n\nx_range = torch.linspace(0, 1, 1000)\ny = dist_uniform.log_prob(x_range).exp()\nplt.plot(x_range.numpy(), y.numpy())\n\n\n\n\n\n\n\n\n\ndist_uniform_2d = torch.distributions.Uniform(torch.tensor([0.0, 0.0]), torch.tensor([1.0, 1.0]))\ndist_uniform_2d.sample([10])\n\ntensor([[0.5493, 0.3478],\n        [0.7661, 0.2568],\n        [0.7199, 0.2975],\n        [0.9114, 0.2916],\n        [0.0045, 0.4948],\n        [0.0156, 0.7434],\n        [0.6856, 0.1037],\n        [0.4446, 0.1913],\n        [0.1995, 0.5009],\n        [0.0716, 0.6085]])\n\n\n\nplt.scatter(*dist_uniform_2d.sample([1000]).numpy().T, alpha=0.5)\n\n\n\n\n\n\n\n\n\nplt.scatter(*dist_uniform_2d.sample([10000]).numpy().T, alpha=0.1)\n\n\n\n\n\n\n\n\n\n# surface plot of PDF\n\nx = torch.linspace(0.0, 1.0, 100)\ny = torch.linspace(0.0, 1.0, 100)\n\nX, Y = torch.meshgrid(x, y)\nxy = torch.stack([X, Y], 2)\n\n\n\n## Important:\n## f(x, y) = f(x) * f(y) for independent random variables\n## log(f(x, y)) = log(f(x)) + log(f(y))\nz1 = dist_uniform_2d.log_prob(xy).sum(-1).exp()\nz2 = dist_uniform.log_prob(X).exp() * dist_uniform.log_prob(Y).exp()\nassert torch.allclose(z1, z2) \n\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X.numpy(), Y.numpy(), z1.numpy())\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('PDF')\n\nText(0.5, 0, 'PDF')",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "Joint Distribution Properties"
    ]
  },
  {
    "objectID": "notebooks/iid.html",
    "href": "notebooks/iid.html",
    "title": "Independent and Identically Distributed (i.i.d) Random Variables",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nprint(np.__version__)\nimport torch \nimport torch.nn as nn\n\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n2.2.4",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Independent and Identically Distributed (i.i.d) Random Variables"
    ]
  },
  {
    "objectID": "notebooks/iid.html#introduction",
    "href": "notebooks/iid.html#introduction",
    "title": "Independent and Identically Distributed (i.i.d) Random Variables",
    "section": "Introduction",
    "text": "Introduction\nIndependent and Identically Distributed (i.i.d) random variables are fundamental building blocks in probability theory and statistics. This concept forms the theoretical foundation for many statistical methods, from simple sampling to complex machine learning algorithms. When we say random variables are i.i.d, we mean two crucial things: they are independent (the outcome of one doesn’t affect another) and identically distributed (they all follow the same probability distribution).\nUnderstanding i.i.d random variables is essential for: - Statistical inference and hypothesis testing - The Law of Large Numbers and Central Limit Theorem - Monte Carlo simulations - Machine learning model assumptions - Data sampling and experimental design",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Independent and Identically Distributed (i.i.d) Random Variables"
    ]
  },
  {
    "objectID": "notebooks/iid.html#learning-objectives",
    "href": "notebooks/iid.html#learning-objectives",
    "title": "Independent and Identically Distributed (i.i.d) Random Variables",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this notebook, you will be able to:\n\nDefine independence and identical distribution for random variables\nCompute joint probability density functions for i.i.d random variables\nApply the multiplication rule for independent random variables\nRecognize when the i.i.d assumption is appropriate in real-world scenarios\nImplement simulations involving i.i.d random variables using Python\nAnalyze the properties and implications of i.i.d assumptions",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Independent and Identically Distributed (i.i.d) Random Variables"
    ]
  },
  {
    "objectID": "notebooks/iid.html#theoretical-background",
    "href": "notebooks/iid.html#theoretical-background",
    "title": "Independent and Identically Distributed (i.i.d) Random Variables",
    "section": "Theoretical Background",
    "text": "Theoretical Background\n\nIndependence of Random Variables\nTwo random variables \\(X_1\\) and \\(X_2\\) are independent if:\n\\[P(X_1 = x_1, X_2 = x_2) = P(X_1 = x_1) \\cdot P(X_2 = x_2)\\]\nFor continuous random variables, this becomes:\n\\[f_{X_1,X_2}(x_1, x_2) = f_{X_1}(x_1) \\cdot f_{X_2}(x_2)\\]\nwhere \\(f_{X_1,X_2}(x_1, x_2)\\) is the joint probability density function.\n\n\nIdentical Distribution\nRandom variables are identically distributed if they have the same probability distribution. This means: - Same probability density function (PDF) or probability mass function (PMF) - Same parameters (mean, variance, etc.) - Same support (the set of possible values)\n\n\nThe i.i.d Property\nWhen random variables \\(X_1, X_2, \\ldots, X_n\\) are i.i.d:\n\nIndependence: \\(f_{X_1,\\ldots,X_n}(x_1,\\ldots,x_n) = \\prod_{i=1}^n f_{X_i}(x_i)\\)\nIdentical Distribution: \\(f_{X_1}(x) = f_{X_2}(x) = \\cdots = f_{X_n}(x) = f(x)\\)\n\nCombined: \\(f_{X_1,\\ldots,X_n}(x_1,\\ldots,x_n) = \\prod_{i=1}^n f(x_i)\\)",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Independent and Identically Distributed (i.i.d) Random Variables"
    ]
  },
  {
    "objectID": "notebooks/iid.html#practical-implementation",
    "href": "notebooks/iid.html#practical-implementation",
    "title": "Independent and Identically Distributed (i.i.d) Random Variables",
    "section": "Practical Implementation",
    "text": "Practical Implementation\nLet’s explore these concepts through computational examples.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Independent and Identically Distributed (i.i.d) Random Variables"
    ]
  },
  {
    "objectID": "notebooks/iid.html#example-1-two-independent-normal-random-variables",
    "href": "notebooks/iid.html#example-1-two-independent-normal-random-variables",
    "title": "Independent and Identically Distributed (i.i.d) Random Variables",
    "section": "Example 1: Two Independent Normal Random Variables",
    "text": "Example 1: Two Independent Normal Random Variables\nWe’ll create two independent normal random variables, both following \\(N(0,1)\\) (standard normal distribution). Since they have the same distribution parameters and are independent, they are i.i.d.\n\nComputing Individual Probabilities\nFor independent random variables, we can compute their individual probability densities separately:\n\nX1 = torch.distributions.Normal(0, 1)\nX2 = torch.distributions.Normal(0, 1)\n\n\n\nComputing Joint Probability for i.i.d Variables\nFor i.i.d random variables, the joint probability density is the product of individual densities:\n\\[f_{X_1,X_2}(x_1, x_2) = f_{X_1}(x_1) \\cdot f_{X_2}(x_2)\\]\nLet’s verify this with our example:",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Independent and Identically Distributed (i.i.d) Random Variables"
    ]
  },
  {
    "objectID": "notebooks/iid.html#summary-and-key-takeaways",
    "href": "notebooks/iid.html#summary-and-key-takeaways",
    "title": "Independent and Identically Distributed (i.i.d) Random Variables",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nWhat We’ve Learned:\n\nDefinition: i.i.d random variables are both independent (outcomes don’t affect each other) and identically distributed (same probability distribution)\nMathematical Property: For i.i.d variables \\(X_1, \\ldots, X_n\\): \\[f_{X_1,\\ldots,X_n}(x_1,\\ldots,x_n) = \\prod_{i=1}^n f(x_i)\\]\nVisual Indicators:\n\nZero correlation between variables (independence)\nIdentical marginal distributions (identical distribution)\nCircular scatter plots for bivariate normal i.i.d variables\n\nPractical Importance:\n\nFoundation for statistical inference\nEnables the Law of Large Numbers\nAssumption in many machine learning algorithms\nCritical for sampling theory\n\n\n\n\nKey Connections to Broader Concepts:\n\nLaw of Large Numbers: Sample means of i.i.d variables converge to population mean\nCentral Limit Theorem: Sums of i.i.d variables approach normal distribution\nStatistical Inference: Many hypothesis tests assume i.i.d observations\nMachine Learning: Training examples are often assumed to be i.i.d\nMonte Carlo Methods: Rely on i.i.d random sampling\n\n\n\nWhen to Question i.i.d Assumptions:\n\nTime series data (autocorrelation)\nSpatial data (spatial correlation)\n\nClustered data (within-cluster correlation)\nSequential learning (changing distributions)\nMeasurement instruments (systematic errors)\n\nUnderstanding i.i.d random variables provides the foundation for advanced topics in probability, statistics, and machine learning. This concept bridges theoretical probability with practical data analysis applications.\n\n# Example 4: Simulating Coin Flips (Classic i.i.d Example)\ntorch.manual_seed(42)  # For reproducibility\n\n# Simulate 1000 coin flips (Bernoulli random variables)\nn_flips = 1000\np_heads = 0.5  # Fair coin\n\n# Each flip is an i.i.d Bernoulli(0.5) random variable\nflips = torch.distributions.Bernoulli(p_heads).sample((n_flips,))\n\n# Plot results\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# 1. Sequence of flips (first 100)\naxes[0].plot(range(100), flips[:100].numpy(), 'o-', markersize=3, alpha=0.7)\naxes[0].set_title('First 100 Coin Flips\\n(0=Tails, 1=Heads)')\naxes[0].set_xlabel('Flip Number')\naxes[0].set_ylabel('Outcome')\naxes[0].set_ylim(-0.1, 1.1)\naxes[0].grid(True, alpha=0.3)\n\n# 2. Running proportion of heads\ncumulative_heads = torch.cumsum(flips, dim=0)\nproportion_heads = cumulative_heads / torch.arange(1, n_flips + 1)\n\naxes[1].plot(range(1, n_flips + 1), proportion_heads.numpy(), 'b-', alpha=0.7)\naxes[1].axhline(y=0.5, color='red', linestyle='--', label='True probability (0.5)')\naxes[1].set_title('Running Proportion of Heads\\n(Converges to true probability)')\naxes[1].set_xlabel('Number of Flips')\naxes[1].set_ylabel('Proportion of Heads')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# 3. Histogram of outcomes\naxes[2].hist(flips.numpy(), bins=[-0.25, 0.25, 0.75, 1.25], alpha=0.7, \n             density=True, rwidth=0.8)\naxes[2].set_title(f'Distribution of Outcomes\\n({int(flips.sum())} heads, {n_flips - int(flips.sum())} tails)')\naxes[2].set_xlabel('Outcome')\naxes[2].set_ylabel('Probability')\naxes[2].set_xticks([0, 1])\naxes[2].set_xticklabels(['Tails', 'Heads'])\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Final proportion of heads: {proportion_heads[-1]:.4f}\")\nprint(f\"Expected proportion: {p_heads}\")\nprint(f\"Difference from expected: {abs(proportion_heads[-1] - p_heads):.4f}\")\nprint(\"\\nThis demonstrates the Law of Large Numbers:\")\nprint(\"As n increases, the sample proportion converges to the true probability.\")",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Independent and Identically Distributed (i.i.d) Random Variables"
    ]
  },
  {
    "objectID": "notebooks/iid.html#real-world-applications-and-when-i.i.d-assumptions-hold",
    "href": "notebooks/iid.html#real-world-applications-and-when-i.i.d-assumptions-hold",
    "title": "Independent and Identically Distributed (i.i.d) Random Variables",
    "section": "Real-World Applications and When i.i.d Assumptions Hold",
    "text": "Real-World Applications and When i.i.d Assumptions Hold\n\nCommon Examples of i.i.d Random Variables:\n\nCoin Flips: Each flip is independent of previous flips and has the same probability distribution\nMeasurement Errors: In well-controlled experiments, measurement errors are often i.i.d\nRandom Sampling: Drawing samples with replacement from a population\nManufacturing Quality: Products from a stable manufacturing process\nNetwork Packet Arrivals: In some network models\n\n\n\nWhen i.i.d Assumptions Break Down:\n\nTime Series Data: Today’s stock price depends on yesterday’s price (not independent)\nSpatial Data: Nearby locations are often similar (not independent)\nLearning Systems: Performance improves over time (not identically distributed)\nBatch Effects: Different experimental batches may have different distributions\n\n\n# Case 1: i.i.d variables (both N(0,1))\nX1_iid = torch.distributions.Normal(0, 1).sample((1000,))\nX2_iid = torch.distributions.Normal(0, 1).sample((1000,))\n\n# Case 2: Independent but NOT identically distributed\nX1_ind = torch.distributions.Normal(0, 1).sample((1000,))    # N(0,1)\nX2_ind = torch.distributions.Normal(2, 0.5).sample((1000,))  # N(2,0.5)\n\n# Case 3: Identically distributed but NOT independent (correlated)\n# Using multivariate normal with correlation\nmean = torch.tensor([0.0, 0.0])\ncov = torch.tensor([[1.0, 0.7], [0.7, 1.0]])  # correlation = 0.7\ncorrelated_samples = torch.distributions.MultivariateNormal(mean, cov).sample((1000,))\nX1_cor = correlated_samples[:, 0]\nX2_cor = correlated_samples[:, 1]\n\n# Create comparison plot\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Row 1: Scatter plots\ntitles = ['i.i.d Variables', 'Independent, Not Identical', 'Identical, Not Independent']\nX_pairs = [(X1_iid, X2_iid), (X1_ind, X2_ind), (X1_cor, X2_cor)]\n\nfor i, (X1, X2) in enumerate(X_pairs):\n    axes[0, i].scatter(X1.numpy(), X2.numpy(), alpha=0.5, s=10)\n    axes[0, i].set_title(titles[i])\n    axes[0, i].set_xlabel('X1')\n    axes[0, i].set_ylabel('X2')\n    axes[0, i].grid(True, alpha=0.3)\n    \n    # Add correlation info\n    corr = torch.corrcoef(torch.stack([X1, X2]))[0, 1]\n    axes[0, i].text(0.05, 0.95, f'Corr: {corr:.3f}', transform=axes[0, i].transAxes,\n                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\"))\n\n# Row 2: Histograms\nfor i, (X1, X2) in enumerate(X_pairs):\n    axes[1, i].hist(X1.numpy(), bins=30, alpha=0.6, label='X1', density=True)\n    axes[1, i].hist(X2.numpy(), bins=30, alpha=0.6, label='X2', density=True)\n    axes[1, i].set_title(f'Marginal Distributions')\n    axes[1, i].set_xlabel('Value')\n    axes[1, i].set_ylabel('Density')\n    axes[1, i].legend()\n    axes[1, i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nprint(\"COMPARISON SUMMARY:\")\nprint(\"=\"*50)\nfor i, (name, (X1, X2)) in enumerate(zip(titles, X_pairs)):\n    corr = torch.corrcoef(torch.stack([X1, X2]))[0, 1]\n    print(f\"\\n{name}:\")\n    print(f\"  X1: mean={X1.mean():.3f}, std={X1.std():.3f}\")\n    print(f\"  X2: mean={X2.mean():.3f}, std={X2.std():.3f}\")\n    print(f\"  Correlation: {corr:.3f}\")\n    \n    # Check properties\n    same_mean = abs(X1.mean() - X2.mean()) &lt; 0.2\n    same_std = abs(X1.std() - X2.std()) &lt; 0.2\n    independent = abs(corr) &lt; 0.1\n    \n    print(f\"  ✓ Identically distributed: {same_mean and same_std}\")\n    print(f\"  ✓ Independent: {independent}\")\n    print(f\"  ✓ i.i.d: {same_mean and same_std and independent}\")",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Independent and Identically Distributed (i.i.d) Random Variables"
    ]
  },
  {
    "objectID": "notebooks/iid.html#example-3-comparing-i.i.d-vs-non-i.i.d-variables",
    "href": "notebooks/iid.html#example-3-comparing-i.i.d-vs-non-i.i.d-variables",
    "title": "Independent and Identically Distributed (i.i.d) Random Variables",
    "section": "Example 3: Comparing i.i.d vs Non-i.i.d Variables",
    "text": "Example 3: Comparing i.i.d vs Non-i.i.d Variables\nLet’s contrast i.i.d variables with non-i.i.d ones to understand the difference.\n\n# Generate samples from i.i.d normal random variables\nn_samples = 1000\n\n# Two i.i.d normal random variables\nX1_samples = torch.distributions.Normal(0, 1).sample((n_samples,))\nX2_samples = torch.distributions.Normal(0, 1).sample((n_samples,))\n\n# Plot the samples\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Individual distributions\naxes[0].hist(X1_samples.numpy(), bins=30, alpha=0.7, label='X1', color='blue', density=True)\naxes[0].hist(X2_samples.numpy(), bins=30, alpha=0.7, label='X2', color='red', density=True)\naxes[0].set_title('Individual Distributions\\n(Should be identical)')\naxes[0].set_xlabel('Value')\naxes[0].set_ylabel('Density')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Joint distribution (scatter plot)\naxes[1].scatter(X1_samples.numpy(), X2_samples.numpy(), alpha=0.5, s=10)\naxes[1].set_title('Joint Distribution\\n(Should show no correlation)')\naxes[1].set_xlabel('X1')\naxes[1].set_ylabel('X2')\naxes[1].grid(True, alpha=0.3)\n\n# Correlation check\ncorrelation = torch.corrcoef(torch.stack([X1_samples, X2_samples]))[0, 1]\naxes[2].text(0.1, 0.7, f'Sample Correlation: {correlation:.4f}', fontsize=12, \n             transform=axes[2].transAxes, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\naxes[2].text(0.1, 0.5, f'Expected (theory): 0.0000', fontsize=12, \n             transform=axes[2].transAxes, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\naxes[2].text(0.1, 0.3, 'Independence verified if\\ncorrelation ≈ 0', fontsize=11, \n             transform=axes[2].transAxes)\naxes[2].set_title('Independence Check')\naxes[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Sample means: X1 = {X1_samples.mean():.4f}, X2 = {X2_samples.mean():.4f}\")\nprint(f\"Sample stds:  X1 = {X1_samples.std():.4f}, X2 = {X2_samples.std():.4f}\")\nprint(f\"Sample correlation: {correlation:.4f}\")\nprint(\"\\nFor i.i.d N(0,1) variables, we expect:\")\nprint(\"- Means ≈ 0, Standard deviations ≈ 1, Correlation ≈ 0\")",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Independent and Identically Distributed (i.i.d) Random Variables"
    ]
  },
  {
    "objectID": "notebooks/iid.html#example-2-visualizing-i.i.d-random-variables",
    "href": "notebooks/iid.html#example-2-visualizing-i.i.d-random-variables",
    "title": "Independent and Identically Distributed (i.i.d) Random Variables",
    "section": "Example 2: Visualizing i.i.d Random Variables",
    "text": "Example 2: Visualizing i.i.d Random Variables\nLet’s generate samples from i.i.d random variables and visualize their properties.\nResult Interpretation: - \\(P(X_1 = 0.2) \\approx 0.391\\) - Individual probability density at \\(x_1 = 0.2\\) - \\(P(X_2 = 0.4) \\approx 0.368\\) - Individual probability density at \\(x_2 = 0.4\\)\n- \\(P(X_1 = 0.2, X_2 = 0.4) \\approx 0.144\\) - Joint probability density\nNotice that the joint probability equals the product of individual probabilities, confirming independence: \\(0.391 \\times 0.368 \\approx 0.144\\).\n\n# say sample is\nsample = torch.tensor([0.2, 0.4])\n\n\nP_X_x1_ = X1.log_prob(sample[0]).exp()\nP_X_x2_ = X2.log_prob(sample[1]).exp()\n\nprint(P_X_x1_, P_X_x2_)\n\ntensor(0.3910) tensor(0.3683)\n\n\n\njoint_pdf = P_X_x1_ * P_X_x2_\nprint(joint_pdf)\n\ntensor(0.1440)",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Independent and Identically Distributed (i.i.d) Random Variables"
    ]
  },
  {
    "objectID": "notebooks/geogebra.html",
    "href": "notebooks/geogebra.html",
    "title": "PSDV Teaching Resources",
    "section": "",
    "text": "f(x,y)=If(0≤x≤2 ∧ 0≤y≤2, 0.25, 0) g(x,y)=If(0≤x≤2 ∧ 0≤y≤2 ∧ x+y≤2, 0.25, 0) a = Slider(0, 2) Segment((0,a,0),(2-a,a,0)) Polygon((0,a,0),(2-a,a,0),(2-a,a,0.25),(0,a,0.25))"
  },
  {
    "objectID": "notebooks/random-vector.html",
    "href": "notebooks/random-vector.html",
    "title": "Random Vector",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nprint(np.__version__)\nimport torch \nimport torch.nn as nn\n\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n1.26.4\n\n\n\ndf = pd.read_html(\"http://socr.ucla.edu/docs/resources/SOCR_Data/SOCR_Data_Dinov_020108_HeightsWeights.html\")\n\n\nstore_df = df[0]\nstore_df.columns = store_df.iloc[0]\nstore_df = store_df.iloc[1:]\nstore_df = store_df.astype(float)\nstore_df = store_df.drop(columns=[\"Index\"])\nstore_df = store_df.dropna()\n\n\nstore_df.head()\n\n\n\n\n\n\n\n\nHeight(Inches)\nWeight(Pounds)\n\n\n\n\n1\n65.78331\n112.9925\n\n\n2\n71.51521\n136.4873\n\n\n3\n69.39874\n153.0269\n\n\n4\n68.21660\n142.3354\n\n\n5\n67.78781\n144.2971\n\n\n\n\n\n\n\n\nmu_vec = store_df.mean()\nprint(mu_vec)\n\n0\nHeight(Inches)     67.993114\nWeight(Pounds)    127.079421\ndtype: float64\n\n\n\ncov_matrix = store_df.cov()\nprint(cov_matrix)\n\n0               Height(Inches)  Weight(Pounds)\n0                                             \nHeight(Inches)        3.616382       11.151029\nWeight(Pounds)       11.151029      135.976532\n\n\n\nstore_df - mu_vec\n\n\n\n\n\n\n\n\nHeight(Inches)\nWeight(Pounds)\n\n\n\n\n1\n-2.209804\n-14.086921\n\n\n2\n3.522096\n9.407879\n\n\n3\n1.405626\n25.947479\n\n\n4\n0.223486\n15.255979\n\n\n5\n-0.205304\n17.217679\n\n\n...\n...\n...\n\n\n24996\n1.509036\n-9.048221\n\n\n24997\n-3.444854\n-6.886221\n\n\n24998\n-3.294564\n-8.813921\n\n\n24999\n-0.463934\n5.188779\n\n\n25000\n0.884496\n-2.205221\n\n\n\n\n25000 rows × 2 columns\n\n\n\n\n(store_df - mu_vec).shape\n\n(25000, 2)\n\n\n\n((store_df - mu_vec).values.T@(store_df - mu_vec).values)/(store_df - mu_vec).shape[0]\n\narray([[  3.61623749,  11.15058313],\n       [ 11.15058313, 135.97109293]])\n\n\n\nstore_df.cov()\n\n\n\n\n\n\n\n\nHeight(Inches)\nWeight(Pounds)\n\n\n0\n\n\n\n\n\n\nHeight(Inches)\n3.616382\n11.151029\n\n\nWeight(Pounds)\n11.151029\n135.976532\n\n\n\n\n\n\n\n\n# Plot the data\n\nplt.scatter(store_df[\"Height(Inches)\"], store_df[\"Weight(Pounds)\"], alpha=0.1, color='k')\nplt.xlabel(\"Height\")\nplt.ylabel(\"Weight\")\n\n\nText(0, 0.5, 'Weight')\n\n\n\n\n\n\n\n\n\n\n# Diagonal covariance matrix\n\nX = torch.distributions.multivariate_normal.MultivariateNormal(\n    loc = torch.tensor([0.0, 0.0]),\n    covariance_matrix = torch.tensor([[1.0, 0.0], [0.0, 1.0]])\n)\n\nX_samples = X.sample((5000,))\nplt.scatter(X_samples[:, 0], X_samples[:, 1], alpha=0.1, color='k')\n\n\n\n\n\n\n\n\n\ntorch.round(torch.cov(X_samples.T))\n\ntensor([[1., 0.],\n        [0., 1.]])",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "Random Vector"
    ]
  },
  {
    "objectID": "notebooks/expectation.html",
    "href": "notebooks/expectation.html",
    "title": "Mathematical Expectation and Law of Large Numbers",
    "section": "",
    "text": "We’ll use PyTorch for probability distributions, NumPy for numerical computations, Pandas for data manipulation, and Matplotlib for visualizations.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Mathematical Expectation and Law of Large Numbers"
    ]
  },
  {
    "objectID": "notebooks/expectation.html#setting-up-the-environment",
    "href": "notebooks/expectation.html#setting-up-the-environment",
    "title": "Mathematical Expectation and Law of Large Numbers",
    "section": "",
    "text": "We’ll use PyTorch for probability distributions, NumPy for numerical computations, Pandas for data manipulation, and Matplotlib for visualizations.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Mathematical Expectation and Law of Large Numbers"
    ]
  },
  {
    "objectID": "notebooks/expectation.html#dice-rolling-simulation-understanding-expected-value",
    "href": "notebooks/expectation.html#dice-rolling-simulation-understanding-expected-value",
    "title": "Mathematical Expectation and Law of Large Numbers",
    "section": "Dice Rolling Simulation: Understanding Expected Value",
    "text": "Dice Rolling Simulation: Understanding Expected Value\nLet’s start with a classic example - rolling a fair six-sided die. The theoretical expected value of a fair die is:\n\\[E[X] = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \\frac{21}{6} = 3.5\\]\nEven though we can never actually roll a 3.5, this is the average value we expect over many rolls.\n\nimport numpy as np \nimport torch\nimport torch.distributions as dist\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\ndice = dist.Categorical(probs=torch.tensor([1/6]*6))\n\n# Sample N dice rolls\nN = 1000\n\nrolls = dice.sample((N,)) + 1\n# 1 is added to the sample because the sample is 0-indexed\n\n\nObserving Individual Dice Rolls\nLet’s examine the frequency distribution of our dice rolls to see how they compare to the theoretical uniform distribution:\n\n\nComputing Running Averages\nNow let’s compute the running average to see how it converges to the theoretical expectation of 3.5:\nThe red dashed line shows the theoretical expected value of 3.5. Notice how the running average oscillates around this value and gradually converges to it as the number of rolls increases. This is a visual demonstration of the Law of Large Numbers.\n\nrolls[:10]\n\ntensor([2, 3, 2, 2, 5, 5, 5, 5, 2, 2])",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Mathematical Expectation and Law of Large Numbers"
    ]
  },
  {
    "objectID": "notebooks/expectation.html#bernoulli-distribution-and-expected-value",
    "href": "notebooks/expectation.html#bernoulli-distribution-and-expected-value",
    "title": "Mathematical Expectation and Law of Large Numbers",
    "section": "Bernoulli Distribution and Expected Value",
    "text": "Bernoulli Distribution and Expected Value\nLet’s explore the Bernoulli distribution, which models binary outcomes (success/failure, heads/tails, etc.). For a Bernoulli random variable with probability \\(p\\) of success:\n\\[E[X] = 1 \\cdot p + 0 \\cdot (1-p) = p\\]\nSo for a fair coin (\\(p = 0.5\\)), the expected value is 0.5.\n\nfig, ax = plt.subplots(figsize=(12, 4))\nax.plot(rolls, lw=0.5, marker='o', markersize=1)\nax.set_xlabel('Roll number')\n\nText(0.5, 0, 'Roll number')\n\n\n\n\n\n\n\n\n\n\npd.Series(rolls).value_counts().sort_index()\n\n1    183\n2    160\n3    188\n4    148\n5    165\n6    156\nName: count, dtype: int64",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Mathematical Expectation and Law of Large Numbers"
    ]
  },
  {
    "objectID": "notebooks/expectation.html#financial-application-coin-flip-game",
    "href": "notebooks/expectation.html#financial-application-coin-flip-game",
    "title": "Mathematical Expectation and Law of Large Numbers",
    "section": "Financial Application: Coin Flip Game",
    "text": "Financial Application: Coin Flip Game\nNow let’s explore a more complex application - a financial game based on coin flips. This demonstrates how expectation and variance work together in risk assessment.\n\nGame Setup\nIn this game: - Each player plays 100 coin flips - Win ₹2000 for each heads - Lose ₹2000 for each tails - We simulate millions of players to understand the distribution of outcomes\n\n# Running average\nruning_avg = torch.cumsum(rolls, dim=0) / torch.arange(1, N+1)\n\n\nplt.plot(runing_avg, lw=1)\nplt.xlabel('Roll number')\nplt.ylabel('Running average')\nplt.axhline(3.5, color='red', lw=1, ls='--')\n\n\n\n\n\n\n\n\n\ndef compute_running_avg(N):\n    rolls = dice.sample((N,)) + 1\n    runing_avg = torch.cumsum(rolls, dim=0) / torch.arange(1, N+1)\n    return runing_avg\n\n\ndist = torch.distributions.Bernoulli(probs=0.5)\nsamples = dist.sample((100,))\n\n\nsamples.mean()\n\ntensor(0.5100)\n\n\n\npd.Series(samples.numpy()).value_counts()\n\n1.0    51\n0.0    49\nName: count, dtype: int64\n\n\n\n# Game of winning and losing money based on a coin flip\n\n\nwin_amt = 2000\nloss_amt = -2000\ntotal_count_N = 100 # Number of coin flips\n\ndist = torch.distributions.Binomial(total_count_N, probs=0.5)\n\n\nnum_samples = 5000000 # number of players/Number of times we perform total_count_N coin flips\n\n\n\nDistribution of Winnings\nLet’s visualize the distribution of net money won across all players:\n\n\nExpected Value and Variance Analysis\nLet’s analyze the statistical properties of our game:\n\nnum_times_wins_across_samples = dist.sample((num_samples,))\n\nThe ratio close to 1 confirms that our simulation matches the theoretical variance formula for the sum of independent random variables.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Mathematical Expectation and Law of Large Numbers"
    ]
  },
  {
    "objectID": "notebooks/expectation.html#alternative-simulation-approach-individual-coin-flips",
    "href": "notebooks/expectation.html#alternative-simulation-approach-individual-coin-flips",
    "title": "Mathematical Expectation and Law of Large Numbers",
    "section": "Alternative Simulation Approach: Individual Coin Flips",
    "text": "Alternative Simulation Approach: Individual Coin Flips\nLet’s implement the same game using individual Bernoulli trials instead of the Binomial distribution to verify our results:\n\nnum_times_wins_across_samples[:100]\n\ntensor([46., 54., 58., 58., 58., 55., 55., 47., 50., 54., 54., 40., 52., 54.,\n        54., 44., 49., 53., 59., 43., 54., 47., 53., 46., 45., 55., 50., 48.,\n        51., 56., 49., 42., 50., 52., 48., 53., 58., 47., 46., 46., 58., 60.,\n        40., 44., 53., 47., 50., 52., 42., 50., 45., 52., 52., 51., 46., 33.,\n        53., 52., 49., 42., 52., 44., 56., 49., 51., 55., 50., 53., 57., 50.,\n        49., 53., 56., 53., 49., 48., 55., 50., 50., 48., 59., 53., 54., 37.,\n        48., 57., 41., 60., 55., 59., 53., 55., 50., 46., 51., 47., 51., 44.,\n        43., 55.])\n\n\n\nif num_samples&lt;=50:\n    fig, ax = plt.subplots(figsize=(12, 4))\n    pd.Series(num_times_wins_across_samples.numpy()).plot(kind='bar', rot=0, ax=ax)\n    plt.xlabel('Sample number')\n    plt.ylabel('Number of wins')\n\n\nnet_money_won = num_times_wins_across_samples * win_amt + (total_count_N - num_times_wins_across_samples) * loss_amt\nnet_money_won_series = pd.Series(net_money_won.numpy())\n\n\nnet_money_won_series\n\n0         -16000.0\n1          16000.0\n2          32000.0\n3          32000.0\n4          32000.0\n            ...   \n4999995   -16000.0\n4999996    52000.0\n4999997   -44000.0\n4999998    -4000.0\n4999999   -20000.0\nLength: 5000000, dtype: float32\n\n\n\nnet_money_won_series.var()\n\n399787200.0\n\n\n\nnet_money_won_series.max() - net_money_won_series.min()\n\n192000.0\n\n\n\nif num_samples&lt;=50:\n    fig, ax = plt.subplots(figsize=(12, 4))\n    net_money_won_series.plot(kind='bar', rot=0, ax=ax)\n    ax.axhline(0, color='red', lw=1, ls='--')\n\n\nnet_money_won_series.hist(grid=False)\n\n\n\n\n\n\n\n\n\nnet_money_won_series.mean()\n\n-6.3688\n\n\n\nnet_money_won_series.var()\n\n399787200.0\n\n\n\nnet_money_won_series.var()/((win_amt**2) * total_count_N)\n\n0.999468\n\n\n\nVisualizing Individual Player Trajectories\nLet’s examine how individual players’ fortunes evolve over the course of their 100 coin flips:\n\n# same analysis but starting from Bernoulli distribution\n\nThis powerful visualization shows all player trajectories in gray (left) and the final distribution of outcomes (right). Notice how most trajectories cluster around zero net winnings, consistent with the fair game’s expected value of zero.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Mathematical Expectation and Law of Large Numbers"
    ]
  },
  {
    "objectID": "notebooks/expectation.html#mathematical-properties-variance-and-its-maximum",
    "href": "notebooks/expectation.html#mathematical-properties-variance-and-its-maximum",
    "title": "Mathematical Expectation and Law of Large Numbers",
    "section": "Mathematical Properties: Variance and Its Maximum",
    "text": "Mathematical Properties: Variance and Its Maximum\nLet’s explore an important mathematical property - when does the variance of a Bernoulli distribution reach its maximum?\n\nVerifying Expected Value of Binomial Distribution\nLet’s verify that our simulation correctly captures the expected value of a Binomial distribution:\n\nwin_amt = 2000\nloss_amt = -2000\ntotal_count_N = 100 # Number of coin flips\n\n\n# Simulate game\ndist = torch.distributions.Bernoulli(probs=0.5)",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Mathematical Expectation and Law of Large Numbers"
    ]
  },
  {
    "objectID": "notebooks/expectation.html#advanced-example-expectation-of-gaussian-distribution",
    "href": "notebooks/expectation.html#advanced-example-expectation-of-gaussian-distribution",
    "title": "Mathematical Expectation and Law of Large Numbers",
    "section": "Advanced Example: Expectation of Gaussian Distribution",
    "text": "Advanced Example: Expectation of Gaussian Distribution\nLet’s conclude with a more advanced example - computing the expected value of a Gaussian distribution using integration. For a standard normal distribution \\(X \\sim \\mathcal{N}(0, \\sigma^2)\\):\n\\[E[X] = \\int_{-\\infty}^{\\infty} x \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{x^2}{2\\sigma^2}} dx = 0\\]\nThis integral evaluates to zero because the function is odd and symmetric around zero.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Mathematical Expectation and Law of Large Numbers"
    ]
  },
  {
    "objectID": "notebooks/expectation.html#summary",
    "href": "notebooks/expectation.html#summary",
    "title": "Mathematical Expectation and Law of Large Numbers",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we’ve explored the fundamental concept of mathematical expectation through:\n\nTheoretical Foundation: Understanding expected value as the long-run average\nLaw of Large Numbers: Observing convergence through simulation\nPractical Applications: Financial modeling and risk assessment\nStatistical Properties: Variance and its relationship to expectation\nAdvanced Concepts: Integration for continuous distributions\n\n\nKey Takeaways\n\nExpected value provides the central tendency of a random variable\nThe Law of Large Numbers guarantees convergence of sample means\nVariance measures the spread around the expected value\nEven fair games (zero expected value) can have high variance and risk\nSimulation is a powerful tool for understanding theoretical concepts\n\n\n\nNext Steps\n\nExplore conditional expectation and its applications\nStudy the Central Limit Theorem and its relationship to the Law of Large Numbers\nInvestigate more complex probability distributions and their moments\nApply these concepts to real-world data analysis problems\n\n\noverall_samples = dist.sample(torch.Size([num_samples, total_count_N]))\n\n\noverall_samples.shape\n\ntorch.Size([5000, 100])\n\n\n\noverall_samples_df = pd.DataFrame(overall_samples.numpy())\n\n\noverall_samples_df.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n\n\n\n\n0\n0.0\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n...\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n...\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n1.0\n\n\n2\n0.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.0\n0.0\n1.0\n1.0\n...\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n\n\n3\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n\n\n4\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n...\n1.0\n0.0\n1.0\n0.0\n1.0\n1.0\n1.0\n0.0\n1.0\n1.0\n\n\n\n\n5 rows × 100 columns\n\n\n\n\n# win and loss -- replace 0 with -2000 and 1 with 2000\nwin_amount_df = overall_samples_df.replace({0: loss_amt, 1: win_amt})\nwin_amount_df.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n\n\n\n\n0\n-2000.0\n-2000.0\n2000.0\n-2000.0\n2000.0\n2000.0\n2000.0\n2000.0\n2000.0\n-2000.0\n...\n2000.0\n-2000.0\n2000.0\n2000.0\n2000.0\n2000.0\n2000.0\n-2000.0\n-2000.0\n2000.0\n\n\n1\n-2000.0\n-2000.0\n-2000.0\n2000.0\n2000.0\n-2000.0\n2000.0\n-2000.0\n-2000.0\n2000.0\n...\n-2000.0\n2000.0\n2000.0\n-2000.0\n-2000.0\n2000.0\n-2000.0\n2000.0\n-2000.0\n2000.0\n\n\n2\n-2000.0\n2000.0\n2000.0\n2000.0\n2000.0\n2000.0\n-2000.0\n-2000.0\n2000.0\n2000.0\n...\n2000.0\n2000.0\n-2000.0\n2000.0\n2000.0\n2000.0\n2000.0\n-2000.0\n2000.0\n2000.0\n\n\n3\n2000.0\n2000.0\n2000.0\n-2000.0\n-2000.0\n-2000.0\n2000.0\n2000.0\n-2000.0\n-2000.0\n...\n-2000.0\n-2000.0\n-2000.0\n2000.0\n2000.0\n-2000.0\n2000.0\n2000.0\n2000.0\n-2000.0\n\n\n4\n-2000.0\n2000.0\n-2000.0\n-2000.0\n-2000.0\n2000.0\n2000.0\n-2000.0\n-2000.0\n2000.0\n...\n2000.0\n-2000.0\n2000.0\n-2000.0\n2000.0\n2000.0\n2000.0\n-2000.0\n2000.0\n2000.0\n\n\n\n\n5 rows × 100 columns\n\n\n\n\n# Net money won\nnet_money_won = win_amount_df.sum(axis=1)\nnet_money_won\n\n0       16000.0\n1       -4000.0\n2       40000.0\n3       24000.0\n4       -4000.0\n         ...   \n4995        0.0\n4996   -12000.0\n4997    16000.0\n4998    12000.0\n4999    40000.0\nLength: 5000, dtype: float32\n\n\n\nnet_money_won.hist(grid=False)\n\n\n\n\n\n\n\n\n\nnet_money_won.var()\n\n397101630.0\n\n\n\nnet_money_won.mean()\n\n561.6\n\n\n\n# Plotting cumulative sum of net money won for first 5 players\nfig, ax = plt.subplots(figsize=(12, 4))\nfor i in range(5):\n    #win_amount_df.iloc[i].values.cumsum()\n    ax.plot(win_amount_df.iloc[i].values.cumsum(), lw=1)\n\n\n\n\n\n\n\n\n\ncumsum_df = win_amount_df.cumsum(axis=1)\ncumsum_df.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n\n\n\n\n0\n-2000.0\n-4000.0\n-2000.0\n-4000.0\n-2000.0\n0.0\n2000.0\n4000.0\n6000.0\n4000.0\n...\n10000.0\n8000.0\n10000.0\n12000.0\n14000.0\n16000.0\n18000.0\n16000.0\n14000.0\n16000.0\n\n\n1\n-2000.0\n-4000.0\n-6000.0\n-4000.0\n-2000.0\n-4000.0\n-2000.0\n-4000.0\n-6000.0\n-4000.0\n...\n-6000.0\n-4000.0\n-2000.0\n-4000.0\n-6000.0\n-4000.0\n-6000.0\n-4000.0\n-6000.0\n-4000.0\n\n\n2\n-2000.0\n0.0\n2000.0\n4000.0\n6000.0\n8000.0\n6000.0\n4000.0\n6000.0\n8000.0\n...\n30000.0\n32000.0\n30000.0\n32000.0\n34000.0\n36000.0\n38000.0\n36000.0\n38000.0\n40000.0\n\n\n3\n2000.0\n4000.0\n6000.0\n4000.0\n2000.0\n0.0\n2000.0\n4000.0\n2000.0\n0.0\n...\n22000.0\n20000.0\n18000.0\n20000.0\n22000.0\n20000.0\n22000.0\n24000.0\n26000.0\n24000.0\n\n\n4\n-2000.0\n0.0\n-2000.0\n-4000.0\n-6000.0\n-4000.0\n-2000.0\n-4000.0\n-6000.0\n-4000.0\n...\n-10000.0\n-12000.0\n-10000.0\n-12000.0\n-10000.0\n-8000.0\n-6000.0\n-8000.0\n-6000.0\n-4000.0\n\n\n\n\n5 rows × 100 columns\n\n\n\n\nfig = plt.figure(figsize=(14, 6))\ngs = fig.add_gridspec(1, 2, width_ratios=[4, 1])  # 4:1 ratio for main plot and histogram\n\n# Main plot (left plot for cumulative sum)\nax1 = fig.add_subplot(gs[0])\ncumsum_df = win_amount_df.cumsum(axis=1)\ncumsum_df.T.plot(legend=False, lw=1, alpha=0.1, color='k', ax=ax1)\nax1.set_xlabel(\"Number of coin flips\")\nax1.set_ylabel(\"Net money won\")\n\n# Right plot (histogram)\nax2 = fig.add_subplot(gs[1])\nax2.hist(net_money_won, color='gray', alpha=0.5, orientation='horizontal', bins=30)\nax2.set_xlabel(\"Frequency\")\n\n# Adjust layout\nplt.tight_layout()\nplt.savefig('coin_flip_game.png', dpi=600)\n\n\n\n\n\n\n\n\n\np = np.linspace(0, 1, 1000)\n\nplt.plot(p, p*(1-p), label='p(1-p)')\n\n\n\n\n\n\n\n\n\np = 0.5\ndist = torch.distributions.Binomial(total_count=2, probs=p)\n\n\nN = 100000000\nsamples = dist.sample((N,))\n\n\nsamples.mean().item()\n\n0.9999343752861023\n\n\n\n### Finding the mean of Gaussian distrubution\n\ny_lin = torch.linspace(-10, 10, 1000)\nsigma = 2.0\n\nout = y_lin*torch.exp(-y_lin**2/(2*sigma**2))\nplt.plot(y_lin, out)\n\n# Drawing coordinate axis at x = 0, y = 0\nplt.axhline(0, color='black', lw=0.5)\nplt.axvline(0, color='black', lw=0.5)\n\nplt.fill_between(y_lin, out, color='gray', alpha=0.5)",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Mathematical Expectation and Law of Large Numbers"
    ]
  },
  {
    "objectID": "notebooks/law-large-numbers.html",
    "href": "notebooks/law-large-numbers.html",
    "title": "Law of Large Numbers and Central Limit Theorem",
    "section": "",
    "text": "import torch\nimport torch.distributions as dist\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Law of Large Numbers and Central Limit Theorem"
    ]
  },
  {
    "objectID": "notebooks/law-large-numbers.html#introduction",
    "href": "notebooks/law-large-numbers.html#introduction",
    "title": "Law of Large Numbers and Central Limit Theorem",
    "section": "Introduction",
    "text": "Introduction\nThis notebook provides an interactive exploration of two fundamental theorems in probability theory: the Law of Large Numbers (LLN) and the Central Limit Theorem (CLT). These concepts are cornerstones of statistical inference and help us understand how sample statistics behave as sample sizes increase.\n\nLearning Objectives\nBy the end of this notebook, you will understand: - The Law of Large Numbers and its practical implications - How sample means converge to population means - The Central Limit Theorem and the distribution of sample means - The relationship between sample size and statistical accuracy - How to visualize convergence using Monte Carlo simulations\n\n\nKey Concepts\n\nLaw of Large Numbers: As sample size increases, sample means converge to the true population mean\nCentral Limit Theorem: Sample means are approximately normally distributed, regardless of the population distribution\nMonte Carlo Simulation: Using random sampling to understand statistical properties",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Law of Large Numbers and Central Limit Theorem"
    ]
  },
  {
    "objectID": "notebooks/law-large-numbers.html#setting-up-the-experiment",
    "href": "notebooks/law-large-numbers.html#setting-up-the-experiment",
    "title": "Law of Large Numbers and Central Limit Theorem",
    "section": "Setting Up the Experiment",
    "text": "Setting Up the Experiment\nWe’ll use Bernoulli random variables for our demonstration. A Bernoulli distribution with parameter \\(p = 0.5\\) represents a fair coin flip where: - Success (heads) = 1 with probability 0.5\n- Failure (tails) = 0 with probability 0.5\nThe theoretical mean \\(\\mathbb{E}[X] = p = 0.5\\).\n\nExperimental Design\nWe’ll generate: - K = 1000 independent experiments (sample paths) - Each experiment contains N = 5000 coin flips - This allows us to observe both LLN and CLT simultaneously\n\nX1 = dist.Bernoulli(probs=0.5)\nX2 = dist.Bernoulli(probs=0.5)\n\nprint(X1.sample(), X2.sample())\n\nX = dist.Bernoulli(probs=0.5)\nprint(X.sample((2,)))\n\ntensor(1.) tensor(0.)\ntensor([1., 1.])",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Law of Large Numbers and Central Limit Theorem"
    ]
  },
  {
    "objectID": "notebooks/law-large-numbers.html#observing-a-single-sample-path",
    "href": "notebooks/law-large-numbers.html#observing-a-single-sample-path",
    "title": "Law of Large Numbers and Central Limit Theorem",
    "section": "Observing a Single Sample Path",
    "text": "Observing a Single Sample Path\nLet’s first examine how a single sequence of coin flips converges to the expected value. The running average should approach 0.5 as we add more samples.\n\nX = dist.Bernoulli(probs=0.5)\n\n# Sample N samples (X1, X2, ..., XN) from the Bernoulli distribution\n# and store them in a tensor\nN = 5000\n\n# Num of samples from population\nK = 1000\nsamples = X.sample((K, N))",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Law of Large Numbers and Central Limit Theorem"
    ]
  },
  {
    "objectID": "notebooks/law-large-numbers.html#law-of-large-numbers-in-action",
    "href": "notebooks/law-large-numbers.html#law-of-large-numbers-in-action",
    "title": "Law of Large Numbers and Central Limit Theorem",
    "section": "Law of Large Numbers in Action",
    "text": "Law of Large Numbers in Action\nNow let’s visualize the Law of Large Numbers by plotting many sample paths simultaneously. Each gray line represents one experiment, and the red dashed line shows the true expected value.",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Law of Large Numbers and Central Limit Theorem"
    ]
  },
  {
    "objectID": "notebooks/law-large-numbers.html#combined-visualization-lln-and-clt",
    "href": "notebooks/law-large-numbers.html#combined-visualization-lln-and-clt",
    "title": "Law of Large Numbers and Central Limit Theorem",
    "section": "Combined Visualization: LLN and CLT",
    "text": "Combined Visualization: LLN and CLT\nThis enhanced visualization shows both theorems simultaneously:\n\nLeft Panel: Law of Large Numbers\n\nMultiple sample paths showing convergence to the true mean\nEach path represents running averages over time\nNotice how variability decreases as sample size increases\n\n\n\nRight Panel: Central Limit Theorem\n\nDistribution of final sample means (from all K=1000 experiments)\nThis distribution is approximately normal, even though individual coin flips are Bernoulli\nThe center of this distribution is at the true mean (0.5)",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Law of Large Numbers and Central Limit Theorem"
    ]
  },
  {
    "objectID": "notebooks/law-large-numbers.html#key-insights-and-implications",
    "href": "notebooks/law-large-numbers.html#key-insights-and-implications",
    "title": "Law of Large Numbers and Central Limit Theorem",
    "section": "Key Insights and Implications",
    "text": "Key Insights and Implications\n\nLaw of Large Numbers\n\nConvergence: Individual sample means approach the true population mean as N → ∞\nConsistency: This convergence happens for each individual experiment\nPractical Meaning: Larger samples give more accurate estimates\n\n\n\nCentral Limit Theorem\n\nNormality: The distribution of sample means becomes normal regardless of the original distribution\nUniversality: This holds for any distribution with finite mean and variance\nStatistical Inference: This is why normal distributions are so important in statistics\n\n\n\nReal-World Applications\n\nQuality Control: Manufacturing processes use these principles to ensure product quality\nPolling: Election polls rely on CLT to estimate population preferences from samples\n\nA/B Testing: Digital experiments use these concepts to make decisions from limited data\nRisk Management: Financial models use LLN to predict long-term outcomes\n\n\n\nMathematical Formulation\nFor i.i.d. random variables \\(X_1, X_2, \\ldots, X_n\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\):\nLaw of Large Numbers: \\[\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i \\xrightarrow{P} \\mu \\text{ as } n \\to \\infty\\]\nCentral Limit Theorem: \\[\\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{D} N(0,1) \\text{ as } n \\to \\infty\\]\n\nsamples[0, :]\n\ntensor([0., 0., 1.,  ..., 0., 0., 1.])\n\n\n\nsamples[1, :]\n\ntensor([1., 1., 0.,  ..., 0., 0., 0.])\n\n\n\n\nplt.plot(torch.cumsum(samples[0, :], dim=0)/torch.arange(1, N+1).float())\nplt.ylim(0, 1)\n\n\n\n\n\n\n\n\n\nplt.plot(torch.cumsum(samples[1, :], dim=0)/torch.arange(1, N+1).float())\nplt.ylim(0, 1)\n\n\n\n\n\n\n\n\n\n# Plot K draws\nrunning_means = torch.cumsum(samples, dim=1) / torch.arange(1, N+1).float()\nfor i in range(K):\n    plt.plot(running_means[i], alpha=0.02, color='gray')\n\nplt.ylim(0, 1)\nplt.axhline(0.5, color='red', linestyle='--', label=r'E[X]')\nplt.legend()\n\n\n\n\n\n\n\n\n\nimport torch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Parameters\nK = 1000  # number of repetitions (sample paths)\nN = 100   # samples per repetition\np = 0.5   # Bernoulli parameter\n\n# Generate samples\nX = torch.distributions.Bernoulli(probs=p)\nsamples = X.sample((K, N))\n\n# Running averages (LLN)\nrunning_means = torch.cumsum(samples, dim=1) / torch.arange(1, N + 1).float()\n\n# Final sample means (for CLT)\nfinal_means = running_means[:, -1].numpy()\n\n# --- Plotting ---\nfig, (ax_lln, ax_clt) = plt.subplots(1, 2, figsize=(10, 6), gridspec_kw={'width_ratios': [3, 1]}, sharey=True)\n\n# LLN plot (running means)\nfor i in range(K):\n    ax_lln.plot(running_means[i], color='gray', alpha=0.02)\n\nax_lln.axhline(p, color='red', linestyle='--', label=r'$\\mathbb{E}[X]$')\nax_lln.set_xlabel(\"n\")\nax_lln.set_ylabel(r\"$\\bar{X}_n$\")\nax_lln.set_title(\"LLN: Running Averages\")\nax_lln.set_ylim(0, 1)\nax_lln.legend()\n\n# CLT plot (rotated histogram + KDE)\n#sns.histplot(final_means, bins=30, stat='density', orientation='horizontal',\n#             ax=ax_clt, color='skyblue', edgecolor='white', alpha=0.6)\nsns.kdeplot(final_means, ax=ax_clt, color='black', linewidth=2, vertical=True, bw_adjust=4)\n\nax_clt.set_xlabel(\"Density\")\nax_clt.set_ylabel(\"\")  # shared y-axis\nax_clt.set_title(\"CLT: Distribution of Sample Means\")\nax_clt.set_ylim(0, 1)\nax_clt.grid(False)\n\nplt.tight_layout()",
    "crumbs": [
      "Home",
      "Core Concepts",
      "Law of Large Numbers and Central Limit Theorem"
    ]
  },
  {
    "objectID": "notebooks/introduction-matplotlib.html",
    "href": "notebooks/introduction-matplotlib.html",
    "title": "Introduction to Matplotlib",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n%config InlineBackend.figure_format = 'retina'\n\n\nurl = \"https://raw.githubusercontent.com/YashBachwana/ES114-2025--Car-Price-Dataset/refs/heads/main/Car%20Price/CarPrice_Assignment.csv\"\nData = pd.read_csv(url,index_col = 0)\n\n\nData.head()\n\n\n\n\n\n\n\n\nsymboling\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\ncarlength\n...\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\ncar_ID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n3\nalfa-romero giulia\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495.0\n\n\n2\n3\nalfa-romero stelvio\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500.0\n\n\n3\n1\nalfa-romero Quadrifoglio\ngas\nstd\ntwo\nhatchback\nrwd\nfront\n94.5\n171.2\n...\n152\nmpfi\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500.0\n\n\n4\n2\naudi 100 ls\ngas\nstd\nfour\nsedan\nfwd\nfront\n99.8\n176.6\n...\n109\nmpfi\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950.0\n\n\n5\n2\naudi 100ls\ngas\nstd\nfour\nsedan\n4wd\nfront\n99.4\n176.6\n...\n136\nmpfi\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450.0\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n\n\nA line plot is the most basic type of plot in Matplotlib. It is used to display information as a series of data points connected by straight lines.\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Plot y = sin(x) on the ax object\nax.plot(x, y)\n\n\n\n\n\n\n\n\n\n\n\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Plot y = sin(x) on the ax object\nax.plot(x, y)\n\n# Add title and labels\nax.set_title(\"Sine Wave: Basic Plot\")\nax.set_xlabel(\"Time (in seconds)\")\nax.set_ylabel(\"Amplitude\")\n\n# Add grid for better visibility of the plot\nax.grid(True)\n\n\n\n\n\n\n\n\n\n\nsine_series = pd.Series(y, index=x, name=\"Amplitude\")\nsine_series\n\n0.00000     0.000000\n0.10101     0.100838\n0.20202     0.200649\n0.30303     0.298414\n0.40404     0.393137\n              ...   \n9.59596    -0.170347\n9.69697    -0.268843\n9.79798    -0.364599\n9.89899    -0.456637\n10.00000   -0.544021\nName: Amplitude, Length: 100, dtype: float64\n\n\n\n# Plot the sine wave using pandas Series\nax = sine_series.plot(\n    title=\"Sine Wave: Basic Plot\", \n    xlabel=\"Time (in seconds)\", \n    ylabel=\"Amplitude\", \n    grid=True\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\n# Plot y = sin(x) on the ax object with label\nax.plot(x, y1, label=\"sin(x)\", color='b')\n\n# Plot y = cos(x) on the ax object with label\nax.plot(x, y2, label=\"cos(x)\",color='r')\n\n# Add title and labels\nax.set_title(\"Sine and Cosine Waves\")\nax.set_xlabel(\"Time (in seconds)\")\nax.set_ylabel(\"Amplitude\")\n\n# Add legend to distinguish the curves\nax.legend(loc=\"upper right\", title=\"Functions\")\n\n# Add grid for better visibility\nax.grid(True)\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\nOther ways to specify colors\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\n# Plot y = sin(x) on the ax object with label\nax.plot(x, y1, label=\"sin(x)\", color='C0')\n\n# Plot y = cos(x) on the ax object with label\nax.plot(x, y2, label=\"cos(x)\",color='C1')\n\n# Add title and labels\nax.set_title(\"Sine and Cosine Waves\")\nax.set_xlabel(\"Time (in seconds)\")\nax.set_ylabel(\"Amplitude\")\n\n# Add legend to distinguish the curves\nax.legend(loc=\"upper right\")\n\n# Add grid for better visibility\nax.grid(True)\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 10, 100)\n\n# Create a figure with 4 subplots arranged in a 2x2 grid\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))  # 2 rows, 2 columns\n\n# First subplot: sin(x)\naxes[0, 0].plot(x, np.sin(x))\naxes[0, 0].set_title(\"sin(x)\")\n\n# Second subplot: cos(x)\naxes[0, 1].plot(x, np.cos(x), color='red')\naxes[0, 1].set_title(\"cos(x)\")\n\n# Third subplot: tan(x)\naxes[1, 0].plot(x, np.tan(x), color='green')\naxes[1, 0].set_title(\"tan(x)\")\n\n# Fourth subplot: exp(-x)\naxes[1, 1].plot(x, np.exp(-x), color='purple')\naxes[1, 1].set_title(\"exp(-x)\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n# Above same plot using pandas\n\n# Create a pandas DataFrame with the sine, cosine, and tangent values\ndf = pd.DataFrame({\n    \"sin(x)\": np.sin(x),\n    \"cos(x)\": np.cos(x),\n    \"tan(x)\": np.tan(x),\n    \"exp(-x)\": np.exp(-x)\n}, index=x)\n\ndf\n\n\n\n\n\n\n\n\nsin(x)\ncos(x)\ntan(x)\nexp(-x)\n\n\n\n\n0.00000\n0.000000\n1.000000\n0.000000\n1.000000\n\n\n0.10101\n0.100838\n0.994903\n0.101355\n0.903924\n\n\n0.20202\n0.200649\n0.979663\n0.204814\n0.817078\n\n\n0.30303\n0.298414\n0.954437\n0.312660\n0.738577\n\n\n0.40404\n0.393137\n0.919480\n0.427564\n0.667617\n\n\n...\n...\n...\n...\n...\n\n\n9.59596\n-0.170347\n-0.985384\n0.172874\n0.000068\n\n\n9.69697\n-0.268843\n-0.963184\n0.279119\n0.000061\n\n\n9.79798\n-0.364599\n-0.931165\n0.391551\n0.000056\n\n\n9.89899\n-0.456637\n-0.889653\n0.513276\n0.000050\n\n\n10.00000\n-0.544021\n-0.839072\n0.648361\n0.000045\n\n\n\n\n100 rows × 4 columns\n\n\n\n\n# Create a figure with 4 subplots arranged in a 2x2 grid\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))  # 2 rows, 2 columns\n\n# Plot each column of the DataFrame on a separate subplot\ndf[\"sin(x)\"].plot(ax=axes[0, 0], color='blue', title=\"sin(x)\")\ndf[\"cos(x)\"].plot(ax=axes[0, 1], color='red', title=\"cos(x)\")\ndf[\"tan(x)\"].plot(ax=axes[1, 0], color='green', title=\"tan(x)\")\ndf[\"exp(-x)\"].plot(ax=axes[1, 1], color='purple', title=\"exp(-x)\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n# Above same plot using pandas\n\n# Create a pandas DataFrame with the sine, cosine, and tangent values\ndf = pd.DataFrame({\n    \"sin(x)\": np.sin(x),\n    \"cos(x)\": np.cos(x),\n    \"tan(x)\": np.tan(x),\n    \"exp(-x)\": np.exp(-x)\n}, index=x)\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))\n\ndf.plot(subplots=True, ax=axes, figsize=(10, 6),\n         title=[\"sin(x)\", \"cos(x)\", \"tan(x)\", \"exp(-x)\"]\n         ,legend=False)\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))\n\ndf.plot(subplots=True, ax=axes, figsize=(10, 6),\n         title=[\"sin(x)\", \"cos(x)\", \"tan(x)\", \"exp(-x)\"]\n         ,legend=False)\n\n\n# Add super title to the figure\nfig.suptitle(\"Trigonometric Functions and Exponential Decay\", fontsize=20)\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Daily temperature variations (in °C) over a week\ndays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\ncity_a = [22, 24, 23, 25, 26, 27, 28]  # City A temperatures\ncity_b = [18, 19, 20, 21, 22, 21, 20]  # City B temperatures\ncity_c = [30, 31, 32, 33, 34, 35, 36]  # City C temperatures\n\n# Create a figure and axis\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot trends with customizations\nax.plot(days, city_a, color='blue', linestyle='-', linewidth=2, marker='o', label='City A')\nax.plot(days, city_b, color='green', linestyle='--', linewidth=2, marker='s', label='City B')\nax.plot(days, city_c, color='red', linestyle='-.', linewidth=2, marker='^', label='City C')\n\n# Add title and labels\nax.set_title(\"Temperature Trends Over a Week\", fontsize=16)\nax.set_xlabel(\"Day of the Week\", fontsize=14)\nax.set_ylabel(\"Temperature (°C)\", fontsize=14)\n\n# Customize ticks\nax.set_xticks(days)  # Use day names for x-axis\nax.tick_params(axis='both', which='major', labelsize=12)\n\n# Add legend\nax.legend(fontsize=12, title=\"Cities\")\n\n# Add grid for better readability\nax.grid(True, linestyle='--', alpha=0.6)\n\n# Display the plot\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n# Create a pandas DataFrame\ndata = pd.DataFrame({\n    \"Day\": days,\n    \"City A\": city_a,\n    \"City B\": city_b,\n    \"City C\": city_c\n})\n\n# Define a list of markers\nmarkers = [\"o\", \"s\", \"^\"]\n\n# Plot in one go\nax = data.plot(\n    x=\"Day\",\n    y=[\"City A\", \"City B\", \"City C\"],\n    figsize=(10, 6),\n    linestyle=\"-\",\n    linewidth=2,\n    title=\"Temperature Trends Over a Week\",\n    xlabel=\"Day of the Week\",\n    ylabel=\"Temperature (°C)\"\n)\n\n# Apply markers\nfor line, marker in zip(ax.lines, markers):\n    line.set_marker(marker)\n\n# Customize legend\nax.legend(fontsize=12, title=\"Cities\")\n\n# Customize grid and ticks\nax.grid(True, linestyle=\"--\", alpha=0.6)\nax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Hypothetical temperature data (in °C) for 24 hours\nhours = np.arange(24)  # Hours from 0 to 23\ntemperature = [12, 11, 10, 9, 9, 8, 8, 10, 14, 18, 22, 25, 27, 29, 28, 26, 23, 21, 18, 16, 15, 14, 13, 12]\n\n# Create a figure and axis\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the temperature data\nax.plot(hours, temperature, marker='o', color='orange', label='Temperature (°C)')\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Hypothetical temperature data (in °C) for 24 hours\nhours = np.arange(24)  # Hours from 0 to 23\ntemperature = [12, 11, 10, 9, 9, 8, 8, 10, 14, 18, 22, 25, 27, 29, 28, 26, 23, 21, 18, 16, 15, 14, 13, 12]\n\n# Create a figure and axis\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the temperature data\nax.plot(hours, temperature, marker='o', color='orange', label='Temperature (°C)')\n\n# Add title and labels\nax.set_title(\"Hourly Temperature Variations\", fontsize=16)\nax.set_xlabel(\"Hour of the Day\", fontsize=14)\nax.set_ylabel(\"Temperature (°C)\", fontsize=14)\n\n# Modify axis limits to focus on the specific hours and temperature range\nax.set_xlim(6, 18)  # Focus on hours 6 AM to 6 PM\nax.set_ylim(15, 30)  # Focus on the relevant temperature range\n\n# Customize ticks\nax.set_xticks(range(6, 19, 2))  # Show ticks every 2 hours in the focused range\nax.set_xticklabels([f\"{h} AM\" if h &lt; 12 else f\"{h-12} PM\" for h in range(6, 19, 2)])  # Format as AM/PM\nax.set_yticks(range(15, 31, 5))  # Show y-axis ticks every 5°C\n\n# Add gridlines for better visibility\nax.grid(which='both', axis='both', linestyle='--', alpha=0.6)\n\n# Add legend\nax.legend(fontsize=12, loc='upper left')\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Save the plot as a PNG file\nfig.savefig(\"temperature_variations.png\", dpi=300, bbox_inches='tight')\n\n# Save the plot as a PDF file\nfig.savefig(\"temperature_variations.pdf\", dpi=300, bbox_inches='tight')\n\n# Save the plot as an SVG file\nfig.savefig(\"temperature_variations.svg\", dpi=300, bbox_inches='tight')\n\n\n\n\n\n\nUsed for categorical or discrete data to compare counts or summarized values.\n\n# Count occurrences of each fuel type\nfueltype_counts = Data['carbody'].value_counts()\nfueltype_counts\n\ncarbody\nsedan          96\nhatchback      70\nwagon          25\nhardtop         8\nconvertible     6\nName: count, dtype: int64\n\n\n\n\n\n# Parameters for bar color and width\nbar_color = ['skyblue']  # Example color list, you can change it to any color you prefer\nbar_width = 0.6\n\nfig, ax = plt.subplots(figsize=(8, 6))\nax.bar(fueltype_counts.index, fueltype_counts.values, color=bar_color, width=bar_width)\n\nax.set_title('Distribution of Car Body', fontsize=16)\n#ax.set_xlabel('Car Body', fontsize=14)\n#ax.set_ylabel('Count', fontsize=14)\n#ax.set_xticks(range(len(fueltype_counts.index)))\n#ax.set_xticklabels(fueltype_counts.index, fontsize=12)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# In pandas\nfueltype_counts.plot(kind='bar', color='skyblue', \n                     figsize=(8, 6),\n                     xlabel='Car Body', ylabel='Count'\n                     , title='Distribution of Car Body')\n\n\n\n\n\n\n\n\n\n\n\n\n# Count occurrences of each fuel type\nfueltype_counts = Data['carbody'].value_counts()\n\n# Parameters for bar color and width\nbar_color = ['skyblue']  # Example color list, you can change it to any color you prefer\nbar_width = 0.6\n\nfig, ax = plt.subplots(figsize=(8, 6))\nbars = ax.bar(fueltype_counts.index, fueltype_counts.values, color=bar_color, width=bar_width)\n\n# Add annotations on top of each bar\nfor bar in bars:\n    yval = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width() / 2, yval + 1,  # Position the text above the bar\n            round(yval, 2),  # Display the count value (rounded to 2 decimal places)\n            ha='center', va='bottom', fontsize=12)  # Align text at the center of the bar and just above it\n\nax.set_title('Distribution of Car Body', fontsize=16)\nax.set_xlabel('Car Body', fontsize=14)\nax.set_ylabel('Count', fontsize=14)\nax.set_xticks(range(len(fueltype_counts.index)))\nax.set_xticklabels(fueltype_counts.index, fontsize=12)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot the bar chart\nax = fueltype_counts.plot(\n    kind='bar', \n    color='skyblue', \n    figsize=(8, 6), \n    rot=0, \n    xlabel='Car Body', \n    ylabel='Count', \n    title='Distribution of Car Body'\n)\n\n# Annotate the bars\nfor bar in ax.patches:\n    # Get the height of the bar (count value)\n    bar_height = bar.get_height()\n    # Annotate the bar with its value\n    ax.annotate(\n        f'{bar_height}',  # Annotation text\n        xy=(bar.get_x() + bar.get_width() / 2, bar_height),  # Position above bar\n        xytext=(0, 5),  # Offset for the annotation\n        textcoords='offset points',\n        ha='center', \n        fontsize=10\n    )\n\n# Adjust layout and show the plot\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nUsed for bivariate data to examine relationships or correlations between two continuous variables.\n\nx = Data['horsepower']\ny = Data['price']\n\n# Create the scatter plot\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, color='blue', alpha=0.7)\n\nax.set_title('Horsepower vs. Car Price', fontsize=16)\nax.set_xlabel('Horsepower', fontsize=14)\nax.set_ylabel('Price ($)', fontsize=14)\n\nax.grid(True)\n\n\n\n\n\n\n\n\n\nData\n\n\n\n\n\n\n\n\nsymboling\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\ncarlength\n...\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\ncar_ID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n3\nalfa-romero giulia\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495.0\n\n\n2\n3\nalfa-romero stelvio\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500.0\n\n\n3\n1\nalfa-romero Quadrifoglio\ngas\nstd\ntwo\nhatchback\nrwd\nfront\n94.5\n171.2\n...\n152\nmpfi\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500.0\n\n\n4\n2\naudi 100 ls\ngas\nstd\nfour\nsedan\nfwd\nfront\n99.8\n176.6\n...\n109\nmpfi\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950.0\n\n\n5\n2\naudi 100ls\ngas\nstd\nfour\nsedan\n4wd\nfront\n99.4\n176.6\n...\n136\nmpfi\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n201\n-1\nvolvo 145e (sw)\ngas\nstd\nfour\nsedan\nrwd\nfront\n109.1\n188.8\n...\n141\nmpfi\n3.78\n3.15\n9.5\n114\n5400\n23\n28\n16845.0\n\n\n202\n-1\nvolvo 144ea\ngas\nturbo\nfour\nsedan\nrwd\nfront\n109.1\n188.8\n...\n141\nmpfi\n3.78\n3.15\n8.7\n160\n5300\n19\n25\n19045.0\n\n\n203\n-1\nvolvo 244dl\ngas\nstd\nfour\nsedan\nrwd\nfront\n109.1\n188.8\n...\n173\nmpfi\n3.58\n2.87\n8.8\n134\n5500\n18\n23\n21485.0\n\n\n204\n-1\nvolvo 246\ndiesel\nturbo\nfour\nsedan\nrwd\nfront\n109.1\n188.8\n...\n145\nidi\n3.01\n3.40\n23.0\n106\n4800\n26\n27\n22470.0\n\n\n205\n-1\nvolvo 264gl\ngas\nturbo\nfour\nsedan\nrwd\nfront\n109.1\n188.8\n...\n141\nmpfi\n3.78\n3.15\n9.5\n114\n5400\n19\n25\n22625.0\n\n\n\n\n205 rows × 25 columns\n\n\n\n\nData.plot.scatter(x='horsepower', y='price', color='blue', alpha=0.7, figsize=(8, 6),\n                    title='Horsepower vs. Car Price', xlabel='Horsepower', ylabel='Price ($)', grid=True)\n\n\n\n\n\n\n\n\n\n\n\nUsed for proportional or compositional data to represent parts of a whole\n\n# Count occurrences of each fuel type\nfueltype_counts = Data['carbody'].value_counts()\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nsizes = fueltype_counts.values\nlabels = fueltype_counts.index\ncolors = ['gold', 'lightblue', 'lightgreen', 'pink','lightgrey']  # Adjust colors as needed\n\nax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n\nax.set_title(\"Distribution of Car Fuel Types\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pie in pandas\nfueltype_counts.plot(kind='pie', figsize=(8, 6), autopct='%1.1f%%', startangle=140,\n                     title='Distribution of Car Fuel Types', colors=['gold', 'lightblue', 'lightgreen', 'pink','lightgrey'])\n\n\n\n\n\n\n\n\n\n\n\nUsed for univariate continuous data to visualize frequency distribution.\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\ndata = Data['price']\n\nax.hist(data, bins=40, color='skyblue', edgecolor='black')\n\nax.set_title('Histogram of Price', fontsize=16)\nax.set_xlabel('Price ($)', fontsize=14)\nax.set_ylabel('Frequency', fontsize=14)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# In pandas\nData['price'].plot(kind='hist', bins=20, color='skyblue', edgecolor='black',\n                   figsize=(8, 6), title='Histogram of Price', xlabel='Price ($)', ylabel='Frequency')\n\n\n\n\n\n\n\n\n\n\n\nUsed for data with measurements from multiple experiments, showing variability or uncertainty, often from multiple experiments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate a physics experiment: Measuring spring constant (k) at different masses\n# Each mass is measured multiple times to account for experimental uncertainty\n\n# Create sample data\nmasses = np.array([50, 100, 150, 200, 250])  # mass in grams\nnum_trials = 10\n\n# Simulate multiple measurements for each mass\n# Each measurement has some random variation to simulate real experimental conditions\nmeasurements = []\nfor mass in masses:\n    # Simulate spring constant measurements with some random noise\n    # True k = 10 N/m with measurement errors\n    k_measurements = 10 + np.random.normal(0, 0.5, num_trials)\n    measurements.append(k_measurements)\n\nmeasurements\n\n[array([ 9.60059384,  9.15385593, 10.28497197,  9.85293433, 10.02889485,\n        10.34334378,  9.08639395,  9.82668512,  9.7685277 ,  9.4138344 ]),\n array([ 9.51136846, 11.50160258, 10.15063726, 10.8276022 ,  8.97931269,\n        10.13273831, 10.38841646,  8.64112354, 10.33124531, 10.69226999]),\n array([10.26489637,  9.81086493, 10.65842485,  9.47632847,  9.2579158 ,\n         9.60256365, 10.60278096,  9.38020745, 10.54828873, 10.95357033]),\n array([10.10126388, 10.76131134, 10.30305548,  9.50037411,  9.24211105,\n        10.82680171,  9.91489558,  9.66617419, 10.18283327,  9.85033258]),\n array([10.18823263, 10.01292153, 10.57612678,  9.92321915, 10.25308004,\n        10.6656051 , 10.08219377,  9.35058485, 11.04976976,  9.32749305])]\n\n\n\n# Calculate means and standard errors\nmeans = [np.mean(m) for m in measurements]\nerrors = [np.std(m) / np.sqrt(num_trials) for m in measurements]  # Standard error of the mean\n\n# Create the error bar plot\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.errorbar(masses, means, yerr=errors, fmt='o',\n            color='blue', ecolor='black',\n            capsize=5, capthick=1.5,\n            label='Measured Values')\n\n# Add true value line\nax.axhline(y=10, color='r', linestyle='--', label='True Spring Constant')\n\nax.set_title('Spring Constant Measurements vs Mass', fontsize=12)\nax.set_xlabel('Mass (g)', fontsize=10)\nax.set_ylabel('Spring Constant (N/m)', fontsize=10)\nax.grid(True, linestyle='--', alpha=0.7)\nax.legend()\n\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(measurements, index=masses).T\ndf\n\n\n\n\n\n\n\n\n50\n100\n150\n200\n250\n\n\n\n\n0\n9.600594\n9.511368\n10.264896\n10.101264\n10.188233\n\n\n1\n9.153856\n11.501603\n9.810865\n10.761311\n10.012922\n\n\n2\n10.284972\n10.150637\n10.658425\n10.303055\n10.576127\n\n\n3\n9.852934\n10.827602\n9.476328\n9.500374\n9.923219\n\n\n4\n10.028895\n8.979313\n9.257916\n9.242111\n10.253080\n\n\n5\n10.343344\n10.132738\n9.602564\n10.826802\n10.665605\n\n\n6\n9.086394\n10.388416\n10.602781\n9.914896\n10.082194\n\n\n7\n9.826685\n8.641124\n9.380207\n9.666174\n9.350585\n\n\n8\n9.768528\n10.331245\n10.548289\n10.182833\n11.049770\n\n\n9\n9.413834\n10.692270\n10.953570\n9.850333\n9.327493\n\n\n\n\n\n\n\n\n\n\nUsed for univariate or grouped data to summarize distributions and highlight outliers.\n\nimport matplotlib.pyplot as plt\n\n# Create a boxplot to compare price distribution by car body type\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the boxplot\nData.boxplot(column='price', by='carbody', ax=ax, grid=False, patch_artist=True,\n             boxprops=dict(facecolor='lightblue', color='blue'),\n             whiskerprops=dict(color='blue'),\n             medianprops=dict(color='red', linewidth=2))\n\nax.set_title(\"Price Distribution by Car Body Type\", fontsize=16)\nax.set_xlabel(\"Car Body Type\", fontsize=14)\nax.set_ylabel(\"Price (USD)\", fontsize=14)\nplt.suptitle(\"\")  # This is to remove the default title\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n#exponential growth\nx = np.linspace(1, 100, 500)\ny = np.exp(x / 20)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot with normal scale\nax1.plot(x, y, label=\"Exponential Growth\", color=\"blue\")\nax1.set_title(\"Normal Scale\")\nax1.set_xlabel(\"X-axis\")\nax1.set_ylabel(\"Y-axis\")\nax1.legend()\nax1.grid(True)\n\n# Plot with log scale (Y-axis)\nax2.plot(x, y, label=\"Exponential Growth\", color=\"green\")\nax2.set_yscale(\"log\")\nax2.set_title(\"Logarithmic Scale (Y-axis)\")\nax2.set_xlabel(\"X-axis\")\nax2.set_ylabel(\"Log(Y-axis)\")\nax2.legend()\nax2.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create grid for paraboloid\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\n\nX, Y = np.meshgrid(x,y)\n\n\npd.DataFrame(X)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n\n\n\n\n0\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n1\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n2\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n3\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n4\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n96\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n97\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n98\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n99\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n\n\n100 rows × 100 columns\n\n\n\n\npd.DataFrame(Y)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n\n\n\n\n0\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n...\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n\n\n1\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n...\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n\n\n2\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n...\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n\n\n3\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n...\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n\n\n4\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n...\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n...\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n\n\n96\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n...\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n\n\n97\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n...\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n\n\n98\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n...\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n\n\n99\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n...\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n100 rows × 100 columns\n\n\n\n\nZ = X**2 + Y**2\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n\nsurface = ax.plot_surface(X, Y, Z, cmap='viridis')\n\n# Add labels and title\nax.set_title('3D Plot of Paraboloid', fontsize=16)\nax.set_xlabel('X-axis', fontsize=12)\nax.set_ylabel('Y-axis', fontsize=12)\nax.set_zlabel('Z-axis', fontsize=12)\n\nfig.colorbar(surface,pad=0.1)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create grid for paraboloid\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\n\nX, Y = np.meshgrid(x, y)\n\nZ = X**2 + Y**2\n\n# Create figure and subplots\nfig, axs = plt.subplots(1, 2, figsize=(14, 6))\n\n# 3D Plot\nax1 = fig.add_subplot(121, projection='3d')\nsurface = ax1.plot_surface(X, Y, Z, cmap='viridis')\nax1.set_title('3D Plot of Paraboloid', fontsize=16)\nax1.set_xlabel('X-axis', fontsize=12)\nax1.set_ylabel('Y-axis', fontsize=12)\nax1.set_zlabel('Z-axis', fontsize=12)\n\n\n# 2D Contour Plot\nax2 = axs[1]\ncontour = ax2.contourf(X, Y, Z, levels=20, cmap='viridis')\nax2.set_title('2D Contour Plot of Paraboloid', fontsize=16)\nax2.set_xlabel('X-axis', fontsize=12)\nax2.set_ylabel('Y-axis', fontsize=12)\nfig.colorbar(contour, ax=ax2, pad=0.1)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nimshow is crucial for displaying 2D data as color-coded images. It’s commonly used for heatmaps, matrices, and actual images.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create sample data\ndata = np.random.rand(10, 10)\n\npd.DataFrame(data)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n0.490860\n0.929045\n0.572919\n0.964711\n0.980014\n0.186145\n0.427508\n0.608805\n0.893498\n0.695284\n\n\n1\n0.966428\n0.422390\n0.348150\n0.525304\n0.331108\n0.580977\n0.562608\n0.560270\n0.939938\n0.808085\n\n\n2\n0.021255\n0.326444\n0.765063\n0.515920\n0.515554\n0.121355\n0.830749\n0.594427\n0.062847\n0.371122\n\n\n3\n0.222631\n0.762149\n0.229365\n0.335383\n0.635986\n0.205019\n0.852074\n0.897168\n0.055277\n0.186129\n\n\n4\n0.987063\n0.835088\n0.654150\n0.181315\n0.012669\n0.086974\n0.903000\n0.829740\n0.716490\n0.834996\n\n\n5\n0.180575\n0.426452\n0.881832\n0.496820\n0.524836\n0.539941\n0.650598\n0.490325\n0.337512\n0.901674\n\n\n6\n0.752542\n0.284082\n0.979989\n0.957409\n0.929105\n0.480490\n0.818328\n0.123510\n0.564911\n0.189244\n\n\n7\n0.992347\n0.311162\n0.411070\n0.600753\n0.495184\n0.069100\n0.372265\n0.431248\n0.456604\n0.238691\n\n\n8\n0.258555\n0.691445\n0.463634\n0.021314\n0.333122\n0.435857\n0.207976\n0.866270\n0.231009\n0.749924\n\n\n9\n0.905349\n0.911525\n0.675188\n0.804228\n0.966661\n0.572587\n0.021375\n0.688160\n0.815374\n0.508309\n\n\n\n\n\n\n\n\n\n\n# Basic imshow example\nfig, ax = plt.subplots(figsize=(8, 6))\nim = ax.imshow(data, cmap='viridis')\nfig.colorbar(im, ax=ax)\nax.set_title('Basic imshow example')\n\n# Multiple imshow with different interpolations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nim1 = ax1.imshow(data, interpolation='nearest', cmap='coolwarm')\nax1.set_title('nearest interpolation')\nfig.colorbar(im1, ax=ax1)\n\nim2 = ax2.imshow(data, interpolation='bilinear', cmap='coolwarm')\nax2.set_title('bilinear interpolation')\nfig.colorbar(im2, ax=ax2)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Advanced Plot Features\n##Custom Markers and Lines\nWhat are Custom Markers and Lines?\nMarkers and lines are elements used in plots to differentiate data points and highlight trends. Matplotlib allows customization of their shapes, colors, and styles to make plots more informative and visually appealing.\n\nCustomizing Markers\n- Markers represent individual data points on a plot.\n- Use the marker parameter in plotting functions.\nCommon Marker Options:\n- 'o': Circle\n- 's': Square\n- '^': Triangle up\n- 'x': Cross\n- '*': Star\n- '.': Point\n\nx = np.linspace(0, 10, 50)\ny = np.sin(x)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(x, y, 'o-', label='Default')\nax.plot(x, y + 1, 'D--', markersize=2, label='Diamond markers')\nax.plot(x, y - 1, 's:', markerfacecolor='red',\n        markeredgecolor='black', label='Square markers')\nax.legend()\nax.set_title('Custom Markers and Lines')\n\nText(0.5, 1.0, 'Custom Markers and Lines')\n\n\n\n\n\n\n\n\n\n\n\nWhat is fill_between?\nThe fill_between function in Matplotlib is used to fill the area between two curves or between a curve and a horizontal line.\nWhere to Use:\n- To visually emphasize the range of values or uncertainty in data.\n- To highlight areas under a curve or between curves.\nWhy Use fill_between?\n- Makes plots more intuitive by shading regions of interest.\n- Helps in representing data variability, confidence intervals, or integrals.\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.fill_between(x, y - 0.2, y + 0.2, alpha=0.1, color='red')\nax.plot(x, y, 'r-', label='Main line')\nax.set_title('Fill Between Example')\nax.legend()\n\n\n\n\n\n\n\n\n#Stylesheets\nWhat are Stylesheets?\nMatplotlib stylesheets are pre-defined sets of style parameters that help you create visually appealing and consistent plots.\nWhere to Use:\n- Use stylesheets when you want your plots to have a cohesive appearance across a project or to match a publication’s style guide.\nWhy Use Stylesheets?\n- Simplify customization by applying a uniform theme with a single line of code.\n- Save time and maintain consistency in multi-plot projects.\n\n# List available styles\nprint(plt.style.available)\n\n['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn-v0_8', 'seaborn-v0_8-bright', 'seaborn-v0_8-colorblind', 'seaborn-v0_8-dark', 'seaborn-v0_8-dark-palette', 'seaborn-v0_8-darkgrid', 'seaborn-v0_8-deep', 'seaborn-v0_8-muted', 'seaborn-v0_8-notebook', 'seaborn-v0_8-paper', 'seaborn-v0_8-pastel', 'seaborn-v0_8-poster', 'seaborn-v0_8-talk', 'seaborn-v0_8-ticks', 'seaborn-v0_8-white', 'seaborn-v0_8-whitegrid', 'tableau-colorblind10']\n\n\n\n\n\n# Example using different styles\ndata = np.random.randn(1000)\nstyles = ['default', 'seaborn-v0_8-bright', 'dark_background', 'seaborn-v0_8-dark-palette']\n\n# Create separate figures for each style\nfig = plt.figure(figsize=(15, 10))\nfor idx, style in enumerate(styles):\n    with plt.style.context(style):\n        ax = fig.add_subplot(2, 2, idx + 1)\n        ax.hist(data, bins=30)\n        ax.set_title(f'Style: {style}')\nfig.tight_layout()",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introduction-matplotlib.html#data-and-library-import",
    "href": "notebooks/introduction-matplotlib.html#data-and-library-import",
    "title": "Introduction to Matplotlib",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n%config InlineBackend.figure_format = 'retina'\n\n\nurl = \"https://raw.githubusercontent.com/YashBachwana/ES114-2025--Car-Price-Dataset/refs/heads/main/Car%20Price/CarPrice_Assignment.csv\"\nData = pd.read_csv(url,index_col = 0)\n\n\nData.head()\n\n\n\n\n\n\n\n\nsymboling\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\ncarlength\n...\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\ncar_ID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n3\nalfa-romero giulia\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495.0\n\n\n2\n3\nalfa-romero stelvio\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500.0\n\n\n3\n1\nalfa-romero Quadrifoglio\ngas\nstd\ntwo\nhatchback\nrwd\nfront\n94.5\n171.2\n...\n152\nmpfi\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500.0\n\n\n4\n2\naudi 100 ls\ngas\nstd\nfour\nsedan\nfwd\nfront\n99.8\n176.6\n...\n109\nmpfi\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950.0\n\n\n5\n2\naudi 100ls\ngas\nstd\nfour\nsedan\n4wd\nfront\n99.4\n176.6\n...\n136\nmpfi\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450.0\n\n\n\n\n5 rows × 25 columns",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introduction-matplotlib.html#a-simple-line-plot",
    "href": "notebooks/introduction-matplotlib.html#a-simple-line-plot",
    "title": "Introduction to Matplotlib",
    "section": "",
    "text": "A line plot is the most basic type of plot in Matplotlib. It is used to display information as a series of data points connected by straight lines.\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Plot y = sin(x) on the ax object\nax.plot(x, y)\n\n\n\n\n\n\n\n\n\n\n\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\nx = np.linspace(0, 10, 100)\ny = np.sin(x)\n\n# Plot y = sin(x) on the ax object\nax.plot(x, y)\n\n# Add title and labels\nax.set_title(\"Sine Wave: Basic Plot\")\nax.set_xlabel(\"Time (in seconds)\")\nax.set_ylabel(\"Amplitude\")\n\n# Add grid for better visibility of the plot\nax.grid(True)\n\n\n\n\n\n\n\n\n\n\nsine_series = pd.Series(y, index=x, name=\"Amplitude\")\nsine_series\n\n0.00000     0.000000\n0.10101     0.100838\n0.20202     0.200649\n0.30303     0.298414\n0.40404     0.393137\n              ...   \n9.59596    -0.170347\n9.69697    -0.268843\n9.79798    -0.364599\n9.89899    -0.456637\n10.00000   -0.544021\nName: Amplitude, Length: 100, dtype: float64\n\n\n\n# Plot the sine wave using pandas Series\nax = sine_series.plot(\n    title=\"Sine Wave: Basic Plot\", \n    xlabel=\"Time (in seconds)\", \n    ylabel=\"Amplitude\", \n    grid=True\n)",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introduction-matplotlib.html#organizing-the-plots",
    "href": "notebooks/introduction-matplotlib.html#organizing-the-plots",
    "title": "Introduction to Matplotlib",
    "section": "",
    "text": "# Create a figure and axis\nfig, ax = plt.subplots()\n\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\n# Plot y = sin(x) on the ax object with label\nax.plot(x, y1, label=\"sin(x)\", color='b')\n\n# Plot y = cos(x) on the ax object with label\nax.plot(x, y2, label=\"cos(x)\",color='r')\n\n# Add title and labels\nax.set_title(\"Sine and Cosine Waves\")\nax.set_xlabel(\"Time (in seconds)\")\nax.set_ylabel(\"Amplitude\")\n\n# Add legend to distinguish the curves\nax.legend(loc=\"upper right\", title=\"Functions\")\n\n# Add grid for better visibility\nax.grid(True)\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\nOther ways to specify colors\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\nx = np.linspace(0, 10, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n\n# Plot y = sin(x) on the ax object with label\nax.plot(x, y1, label=\"sin(x)\", color='C0')\n\n# Plot y = cos(x) on the ax object with label\nax.plot(x, y2, label=\"cos(x)\",color='C1')\n\n# Add title and labels\nax.set_title(\"Sine and Cosine Waves\")\nax.set_xlabel(\"Time (in seconds)\")\nax.set_ylabel(\"Amplitude\")\n\n# Add legend to distinguish the curves\nax.legend(loc=\"upper right\")\n\n# Add grid for better visibility\nax.grid(True)\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(0, 10, 100)\n\n# Create a figure with 4 subplots arranged in a 2x2 grid\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))  # 2 rows, 2 columns\n\n# First subplot: sin(x)\naxes[0, 0].plot(x, np.sin(x))\naxes[0, 0].set_title(\"sin(x)\")\n\n# Second subplot: cos(x)\naxes[0, 1].plot(x, np.cos(x), color='red')\naxes[0, 1].set_title(\"cos(x)\")\n\n# Third subplot: tan(x)\naxes[1, 0].plot(x, np.tan(x), color='green')\naxes[1, 0].set_title(\"tan(x)\")\n\n# Fourth subplot: exp(-x)\naxes[1, 1].plot(x, np.exp(-x), color='purple')\naxes[1, 1].set_title(\"exp(-x)\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n# Above same plot using pandas\n\n# Create a pandas DataFrame with the sine, cosine, and tangent values\ndf = pd.DataFrame({\n    \"sin(x)\": np.sin(x),\n    \"cos(x)\": np.cos(x),\n    \"tan(x)\": np.tan(x),\n    \"exp(-x)\": np.exp(-x)\n}, index=x)\n\ndf\n\n\n\n\n\n\n\n\nsin(x)\ncos(x)\ntan(x)\nexp(-x)\n\n\n\n\n0.00000\n0.000000\n1.000000\n0.000000\n1.000000\n\n\n0.10101\n0.100838\n0.994903\n0.101355\n0.903924\n\n\n0.20202\n0.200649\n0.979663\n0.204814\n0.817078\n\n\n0.30303\n0.298414\n0.954437\n0.312660\n0.738577\n\n\n0.40404\n0.393137\n0.919480\n0.427564\n0.667617\n\n\n...\n...\n...\n...\n...\n\n\n9.59596\n-0.170347\n-0.985384\n0.172874\n0.000068\n\n\n9.69697\n-0.268843\n-0.963184\n0.279119\n0.000061\n\n\n9.79798\n-0.364599\n-0.931165\n0.391551\n0.000056\n\n\n9.89899\n-0.456637\n-0.889653\n0.513276\n0.000050\n\n\n10.00000\n-0.544021\n-0.839072\n0.648361\n0.000045\n\n\n\n\n100 rows × 4 columns\n\n\n\n\n# Create a figure with 4 subplots arranged in a 2x2 grid\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))  # 2 rows, 2 columns\n\n# Plot each column of the DataFrame on a separate subplot\ndf[\"sin(x)\"].plot(ax=axes[0, 0], color='blue', title=\"sin(x)\")\ndf[\"cos(x)\"].plot(ax=axes[0, 1], color='red', title=\"cos(x)\")\ndf[\"tan(x)\"].plot(ax=axes[1, 0], color='green', title=\"tan(x)\")\ndf[\"exp(-x)\"].plot(ax=axes[1, 1], color='purple', title=\"exp(-x)\")\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n# Above same plot using pandas\n\n# Create a pandas DataFrame with the sine, cosine, and tangent values\ndf = pd.DataFrame({\n    \"sin(x)\": np.sin(x),\n    \"cos(x)\": np.cos(x),\n    \"tan(x)\": np.tan(x),\n    \"exp(-x)\": np.exp(-x)\n}, index=x)\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))\n\ndf.plot(subplots=True, ax=axes, figsize=(10, 6),\n         title=[\"sin(x)\", \"cos(x)\", \"tan(x)\", \"exp(-x)\"]\n         ,legend=False)\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(2, 2, figsize=(10, 6))\n\ndf.plot(subplots=True, ax=axes, figsize=(10, 6),\n         title=[\"sin(x)\", \"cos(x)\", \"tan(x)\", \"exp(-x)\"]\n         ,legend=False)\n\n\n# Add super title to the figure\nfig.suptitle(\"Trigonometric Functions and Exponential Decay\", fontsize=20)\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Daily temperature variations (in °C) over a week\ndays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\ncity_a = [22, 24, 23, 25, 26, 27, 28]  # City A temperatures\ncity_b = [18, 19, 20, 21, 22, 21, 20]  # City B temperatures\ncity_c = [30, 31, 32, 33, 34, 35, 36]  # City C temperatures\n\n# Create a figure and axis\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot trends with customizations\nax.plot(days, city_a, color='blue', linestyle='-', linewidth=2, marker='o', label='City A')\nax.plot(days, city_b, color='green', linestyle='--', linewidth=2, marker='s', label='City B')\nax.plot(days, city_c, color='red', linestyle='-.', linewidth=2, marker='^', label='City C')\n\n# Add title and labels\nax.set_title(\"Temperature Trends Over a Week\", fontsize=16)\nax.set_xlabel(\"Day of the Week\", fontsize=14)\nax.set_ylabel(\"Temperature (°C)\", fontsize=14)\n\n# Customize ticks\nax.set_xticks(days)  # Use day names for x-axis\nax.tick_params(axis='both', which='major', labelsize=12)\n\n# Add legend\nax.legend(fontsize=12, title=\"Cities\")\n\n# Add grid for better readability\nax.grid(True, linestyle='--', alpha=0.6)\n\n# Display the plot\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n# Create a pandas DataFrame\ndata = pd.DataFrame({\n    \"Day\": days,\n    \"City A\": city_a,\n    \"City B\": city_b,\n    \"City C\": city_c\n})\n\n# Define a list of markers\nmarkers = [\"o\", \"s\", \"^\"]\n\n# Plot in one go\nax = data.plot(\n    x=\"Day\",\n    y=[\"City A\", \"City B\", \"City C\"],\n    figsize=(10, 6),\n    linestyle=\"-\",\n    linewidth=2,\n    title=\"Temperature Trends Over a Week\",\n    xlabel=\"Day of the Week\",\n    ylabel=\"Temperature (°C)\"\n)\n\n# Apply markers\nfor line, marker in zip(ax.lines, markers):\n    line.set_marker(marker)\n\n# Customize legend\nax.legend(fontsize=12, title=\"Cities\")\n\n# Customize grid and ticks\nax.grid(True, linestyle=\"--\", alpha=0.6)\nax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Hypothetical temperature data (in °C) for 24 hours\nhours = np.arange(24)  # Hours from 0 to 23\ntemperature = [12, 11, 10, 9, 9, 8, 8, 10, 14, 18, 22, 25, 27, 29, 28, 26, 23, 21, 18, 16, 15, 14, 13, 12]\n\n# Create a figure and axis\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the temperature data\nax.plot(hours, temperature, marker='o', color='orange', label='Temperature (°C)')\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Hypothetical temperature data (in °C) for 24 hours\nhours = np.arange(24)  # Hours from 0 to 23\ntemperature = [12, 11, 10, 9, 9, 8, 8, 10, 14, 18, 22, 25, 27, 29, 28, 26, 23, 21, 18, 16, 15, 14, 13, 12]\n\n# Create a figure and axis\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the temperature data\nax.plot(hours, temperature, marker='o', color='orange', label='Temperature (°C)')\n\n# Add title and labels\nax.set_title(\"Hourly Temperature Variations\", fontsize=16)\nax.set_xlabel(\"Hour of the Day\", fontsize=14)\nax.set_ylabel(\"Temperature (°C)\", fontsize=14)\n\n# Modify axis limits to focus on the specific hours and temperature range\nax.set_xlim(6, 18)  # Focus on hours 6 AM to 6 PM\nax.set_ylim(15, 30)  # Focus on the relevant temperature range\n\n# Customize ticks\nax.set_xticks(range(6, 19, 2))  # Show ticks every 2 hours in the focused range\nax.set_xticklabels([f\"{h} AM\" if h &lt; 12 else f\"{h-12} PM\" for h in range(6, 19, 2)])  # Format as AM/PM\nax.set_yticks(range(15, 31, 5))  # Show y-axis ticks every 5°C\n\n# Add gridlines for better visibility\nax.grid(which='both', axis='both', linestyle='--', alpha=0.6)\n\n# Add legend\nax.legend(fontsize=12, loc='upper left')\n\n# Display the plot\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introduction-matplotlib.html#saving-the-plots",
    "href": "notebooks/introduction-matplotlib.html#saving-the-plots",
    "title": "Introduction to Matplotlib",
    "section": "",
    "text": "# Save the plot as a PNG file\nfig.savefig(\"temperature_variations.png\", dpi=300, bbox_inches='tight')\n\n# Save the plot as a PDF file\nfig.savefig(\"temperature_variations.pdf\", dpi=300, bbox_inches='tight')\n\n# Save the plot as an SVG file\nfig.savefig(\"temperature_variations.svg\", dpi=300, bbox_inches='tight')",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introduction-matplotlib.html#choose-your-plot",
    "href": "notebooks/introduction-matplotlib.html#choose-your-plot",
    "title": "Introduction to Matplotlib",
    "section": "",
    "text": "Used for categorical or discrete data to compare counts or summarized values.\n\n# Count occurrences of each fuel type\nfueltype_counts = Data['carbody'].value_counts()\nfueltype_counts\n\ncarbody\nsedan          96\nhatchback      70\nwagon          25\nhardtop         8\nconvertible     6\nName: count, dtype: int64\n\n\n\n\n\n# Parameters for bar color and width\nbar_color = ['skyblue']  # Example color list, you can change it to any color you prefer\nbar_width = 0.6\n\nfig, ax = plt.subplots(figsize=(8, 6))\nax.bar(fueltype_counts.index, fueltype_counts.values, color=bar_color, width=bar_width)\n\nax.set_title('Distribution of Car Body', fontsize=16)\n#ax.set_xlabel('Car Body', fontsize=14)\n#ax.set_ylabel('Count', fontsize=14)\n#ax.set_xticks(range(len(fueltype_counts.index)))\n#ax.set_xticklabels(fueltype_counts.index, fontsize=12)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# In pandas\nfueltype_counts.plot(kind='bar', color='skyblue', \n                     figsize=(8, 6),\n                     xlabel='Car Body', ylabel='Count'\n                     , title='Distribution of Car Body')\n\n\n\n\n\n\n\n\n\n\n\n\n# Count occurrences of each fuel type\nfueltype_counts = Data['carbody'].value_counts()\n\n# Parameters for bar color and width\nbar_color = ['skyblue']  # Example color list, you can change it to any color you prefer\nbar_width = 0.6\n\nfig, ax = plt.subplots(figsize=(8, 6))\nbars = ax.bar(fueltype_counts.index, fueltype_counts.values, color=bar_color, width=bar_width)\n\n# Add annotations on top of each bar\nfor bar in bars:\n    yval = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width() / 2, yval + 1,  # Position the text above the bar\n            round(yval, 2),  # Display the count value (rounded to 2 decimal places)\n            ha='center', va='bottom', fontsize=12)  # Align text at the center of the bar and just above it\n\nax.set_title('Distribution of Car Body', fontsize=16)\nax.set_xlabel('Car Body', fontsize=14)\nax.set_ylabel('Count', fontsize=14)\nax.set_xticks(range(len(fueltype_counts.index)))\nax.set_xticklabels(fueltype_counts.index, fontsize=12)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot the bar chart\nax = fueltype_counts.plot(\n    kind='bar', \n    color='skyblue', \n    figsize=(8, 6), \n    rot=0, \n    xlabel='Car Body', \n    ylabel='Count', \n    title='Distribution of Car Body'\n)\n\n# Annotate the bars\nfor bar in ax.patches:\n    # Get the height of the bar (count value)\n    bar_height = bar.get_height()\n    # Annotate the bar with its value\n    ax.annotate(\n        f'{bar_height}',  # Annotation text\n        xy=(bar.get_x() + bar.get_width() / 2, bar_height),  # Position above bar\n        xytext=(0, 5),  # Offset for the annotation\n        textcoords='offset points',\n        ha='center', \n        fontsize=10\n    )\n\n# Adjust layout and show the plot\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nUsed for bivariate data to examine relationships or correlations between two continuous variables.\n\nx = Data['horsepower']\ny = Data['price']\n\n# Create the scatter plot\nfig, ax = plt.subplots(figsize=(8, 6))\nax.scatter(x, y, color='blue', alpha=0.7)\n\nax.set_title('Horsepower vs. Car Price', fontsize=16)\nax.set_xlabel('Horsepower', fontsize=14)\nax.set_ylabel('Price ($)', fontsize=14)\n\nax.grid(True)\n\n\n\n\n\n\n\n\n\nData\n\n\n\n\n\n\n\n\nsymboling\nCarName\nfueltype\naspiration\ndoornumber\ncarbody\ndrivewheel\nenginelocation\nwheelbase\ncarlength\n...\nenginesize\nfuelsystem\nboreratio\nstroke\ncompressionratio\nhorsepower\npeakrpm\ncitympg\nhighwaympg\nprice\n\n\ncar_ID\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n3\nalfa-romero giulia\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n13495.0\n\n\n2\n3\nalfa-romero stelvio\ngas\nstd\ntwo\nconvertible\nrwd\nfront\n88.6\n168.8\n...\n130\nmpfi\n3.47\n2.68\n9.0\n111\n5000\n21\n27\n16500.0\n\n\n3\n1\nalfa-romero Quadrifoglio\ngas\nstd\ntwo\nhatchback\nrwd\nfront\n94.5\n171.2\n...\n152\nmpfi\n2.68\n3.47\n9.0\n154\n5000\n19\n26\n16500.0\n\n\n4\n2\naudi 100 ls\ngas\nstd\nfour\nsedan\nfwd\nfront\n99.8\n176.6\n...\n109\nmpfi\n3.19\n3.40\n10.0\n102\n5500\n24\n30\n13950.0\n\n\n5\n2\naudi 100ls\ngas\nstd\nfour\nsedan\n4wd\nfront\n99.4\n176.6\n...\n136\nmpfi\n3.19\n3.40\n8.0\n115\n5500\n18\n22\n17450.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n201\n-1\nvolvo 145e (sw)\ngas\nstd\nfour\nsedan\nrwd\nfront\n109.1\n188.8\n...\n141\nmpfi\n3.78\n3.15\n9.5\n114\n5400\n23\n28\n16845.0\n\n\n202\n-1\nvolvo 144ea\ngas\nturbo\nfour\nsedan\nrwd\nfront\n109.1\n188.8\n...\n141\nmpfi\n3.78\n3.15\n8.7\n160\n5300\n19\n25\n19045.0\n\n\n203\n-1\nvolvo 244dl\ngas\nstd\nfour\nsedan\nrwd\nfront\n109.1\n188.8\n...\n173\nmpfi\n3.58\n2.87\n8.8\n134\n5500\n18\n23\n21485.0\n\n\n204\n-1\nvolvo 246\ndiesel\nturbo\nfour\nsedan\nrwd\nfront\n109.1\n188.8\n...\n145\nidi\n3.01\n3.40\n23.0\n106\n4800\n26\n27\n22470.0\n\n\n205\n-1\nvolvo 264gl\ngas\nturbo\nfour\nsedan\nrwd\nfront\n109.1\n188.8\n...\n141\nmpfi\n3.78\n3.15\n9.5\n114\n5400\n19\n25\n22625.0\n\n\n\n\n205 rows × 25 columns\n\n\n\n\nData.plot.scatter(x='horsepower', y='price', color='blue', alpha=0.7, figsize=(8, 6),\n                    title='Horsepower vs. Car Price', xlabel='Horsepower', ylabel='Price ($)', grid=True)\n\n\n\n\n\n\n\n\n\n\n\nUsed for proportional or compositional data to represent parts of a whole\n\n# Count occurrences of each fuel type\nfueltype_counts = Data['carbody'].value_counts()\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nsizes = fueltype_counts.values\nlabels = fueltype_counts.index\ncolors = ['gold', 'lightblue', 'lightgreen', 'pink','lightgrey']  # Adjust colors as needed\n\nax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n\nax.set_title(\"Distribution of Car Fuel Types\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Pie in pandas\nfueltype_counts.plot(kind='pie', figsize=(8, 6), autopct='%1.1f%%', startangle=140,\n                     title='Distribution of Car Fuel Types', colors=['gold', 'lightblue', 'lightgreen', 'pink','lightgrey'])\n\n\n\n\n\n\n\n\n\n\n\nUsed for univariate continuous data to visualize frequency distribution.\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\ndata = Data['price']\n\nax.hist(data, bins=40, color='skyblue', edgecolor='black')\n\nax.set_title('Histogram of Price', fontsize=16)\nax.set_xlabel('Price ($)', fontsize=14)\nax.set_ylabel('Frequency', fontsize=14)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# In pandas\nData['price'].plot(kind='hist', bins=20, color='skyblue', edgecolor='black',\n                   figsize=(8, 6), title='Histogram of Price', xlabel='Price ($)', ylabel='Frequency')\n\n\n\n\n\n\n\n\n\n\n\nUsed for data with measurements from multiple experiments, showing variability or uncertainty, often from multiple experiments.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulate a physics experiment: Measuring spring constant (k) at different masses\n# Each mass is measured multiple times to account for experimental uncertainty\n\n# Create sample data\nmasses = np.array([50, 100, 150, 200, 250])  # mass in grams\nnum_trials = 10\n\n# Simulate multiple measurements for each mass\n# Each measurement has some random variation to simulate real experimental conditions\nmeasurements = []\nfor mass in masses:\n    # Simulate spring constant measurements with some random noise\n    # True k = 10 N/m with measurement errors\n    k_measurements = 10 + np.random.normal(0, 0.5, num_trials)\n    measurements.append(k_measurements)\n\nmeasurements\n\n[array([ 9.60059384,  9.15385593, 10.28497197,  9.85293433, 10.02889485,\n        10.34334378,  9.08639395,  9.82668512,  9.7685277 ,  9.4138344 ]),\n array([ 9.51136846, 11.50160258, 10.15063726, 10.8276022 ,  8.97931269,\n        10.13273831, 10.38841646,  8.64112354, 10.33124531, 10.69226999]),\n array([10.26489637,  9.81086493, 10.65842485,  9.47632847,  9.2579158 ,\n         9.60256365, 10.60278096,  9.38020745, 10.54828873, 10.95357033]),\n array([10.10126388, 10.76131134, 10.30305548,  9.50037411,  9.24211105,\n        10.82680171,  9.91489558,  9.66617419, 10.18283327,  9.85033258]),\n array([10.18823263, 10.01292153, 10.57612678,  9.92321915, 10.25308004,\n        10.6656051 , 10.08219377,  9.35058485, 11.04976976,  9.32749305])]\n\n\n\n# Calculate means and standard errors\nmeans = [np.mean(m) for m in measurements]\nerrors = [np.std(m) / np.sqrt(num_trials) for m in measurements]  # Standard error of the mean\n\n# Create the error bar plot\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.errorbar(masses, means, yerr=errors, fmt='o',\n            color='blue', ecolor='black',\n            capsize=5, capthick=1.5,\n            label='Measured Values')\n\n# Add true value line\nax.axhline(y=10, color='r', linestyle='--', label='True Spring Constant')\n\nax.set_title('Spring Constant Measurements vs Mass', fontsize=12)\nax.set_xlabel('Mass (g)', fontsize=10)\nax.set_ylabel('Spring Constant (N/m)', fontsize=10)\nax.grid(True, linestyle='--', alpha=0.7)\nax.legend()\n\n\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(measurements, index=masses).T\ndf\n\n\n\n\n\n\n\n\n50\n100\n150\n200\n250\n\n\n\n\n0\n9.600594\n9.511368\n10.264896\n10.101264\n10.188233\n\n\n1\n9.153856\n11.501603\n9.810865\n10.761311\n10.012922\n\n\n2\n10.284972\n10.150637\n10.658425\n10.303055\n10.576127\n\n\n3\n9.852934\n10.827602\n9.476328\n9.500374\n9.923219\n\n\n4\n10.028895\n8.979313\n9.257916\n9.242111\n10.253080\n\n\n5\n10.343344\n10.132738\n9.602564\n10.826802\n10.665605\n\n\n6\n9.086394\n10.388416\n10.602781\n9.914896\n10.082194\n\n\n7\n9.826685\n8.641124\n9.380207\n9.666174\n9.350585\n\n\n8\n9.768528\n10.331245\n10.548289\n10.182833\n11.049770\n\n\n9\n9.413834\n10.692270\n10.953570\n9.850333\n9.327493\n\n\n\n\n\n\n\n\n\n\nUsed for univariate or grouped data to summarize distributions and highlight outliers.\n\nimport matplotlib.pyplot as plt\n\n# Create a boxplot to compare price distribution by car body type\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the boxplot\nData.boxplot(column='price', by='carbody', ax=ax, grid=False, patch_artist=True,\n             boxprops=dict(facecolor='lightblue', color='blue'),\n             whiskerprops=dict(color='blue'),\n             medianprops=dict(color='red', linewidth=2))\n\nax.set_title(\"Price Distribution by Car Body Type\", fontsize=16)\nax.set_xlabel(\"Car Body Type\", fontsize=14)\nax.set_ylabel(\"Price (USD)\", fontsize=14)\nplt.suptitle(\"\")  # This is to remove the default title\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introduction-matplotlib.html#scale-transformations",
    "href": "notebooks/introduction-matplotlib.html#scale-transformations",
    "title": "Introduction to Matplotlib",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\n#exponential growth\nx = np.linspace(1, 100, 500)\ny = np.exp(x / 20)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# Plot with normal scale\nax1.plot(x, y, label=\"Exponential Growth\", color=\"blue\")\nax1.set_title(\"Normal Scale\")\nax1.set_xlabel(\"X-axis\")\nax1.set_ylabel(\"Y-axis\")\nax1.legend()\nax1.grid(True)\n\n# Plot with log scale (Y-axis)\nax2.plot(x, y, label=\"Exponential Growth\", color=\"green\")\nax2.set_yscale(\"log\")\nax2.set_title(\"Logarithmic Scale (Y-axis)\")\nax2.set_xlabel(\"X-axis\")\nax2.set_ylabel(\"Log(Y-axis)\")\nax2.legend()\nax2.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introduction-matplotlib.html#d-visualization-and-contour",
    "href": "notebooks/introduction-matplotlib.html#d-visualization-and-contour",
    "title": "Introduction to Matplotlib",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create grid for paraboloid\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\n\nX, Y = np.meshgrid(x,y)\n\n\npd.DataFrame(X)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n\n\n\n\n0\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n1\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n2\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n3\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n4\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n96\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n97\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n98\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n99\n-10.0\n-9.79798\n-9.59596\n-9.393939\n-9.191919\n-8.989899\n-8.787879\n-8.585859\n-8.383838\n-8.181818\n...\n8.181818\n8.383838\n8.585859\n8.787879\n8.989899\n9.191919\n9.393939\n9.59596\n9.79798\n10.0\n\n\n\n\n100 rows × 100 columns\n\n\n\n\npd.DataFrame(Y)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n\n\n\n\n0\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n...\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n-10.000000\n\n\n1\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n...\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n-9.797980\n\n\n2\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n...\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n-9.595960\n\n\n3\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n...\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n-9.393939\n\n\n4\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n...\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n-9.191919\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n95\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n...\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n9.191919\n\n\n96\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n...\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n9.393939\n\n\n97\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n...\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n9.595960\n\n\n98\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n...\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n9.797980\n\n\n99\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n...\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n100 rows × 100 columns\n\n\n\n\nZ = X**2 + Y**2\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n\nsurface = ax.plot_surface(X, Y, Z, cmap='viridis')\n\n# Add labels and title\nax.set_title('3D Plot of Paraboloid', fontsize=16)\nax.set_xlabel('X-axis', fontsize=12)\nax.set_ylabel('Y-axis', fontsize=12)\nax.set_zlabel('Z-axis', fontsize=12)\n\nfig.colorbar(surface,pad=0.1)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create grid for paraboloid\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\n\nX, Y = np.meshgrid(x, y)\n\nZ = X**2 + Y**2\n\n# Create figure and subplots\nfig, axs = plt.subplots(1, 2, figsize=(14, 6))\n\n# 3D Plot\nax1 = fig.add_subplot(121, projection='3d')\nsurface = ax1.plot_surface(X, Y, Z, cmap='viridis')\nax1.set_title('3D Plot of Paraboloid', fontsize=16)\nax1.set_xlabel('X-axis', fontsize=12)\nax1.set_ylabel('Y-axis', fontsize=12)\nax1.set_zlabel('Z-axis', fontsize=12)\n\n\n# 2D Contour Plot\nax2 = axs[1]\ncontour = ax2.contourf(X, Y, Z, levels=20, cmap='viridis')\nax2.set_title('2D Contour Plot of Paraboloid', fontsize=16)\nax2.set_xlabel('X-axis', fontsize=12)\nax2.set_ylabel('Y-axis', fontsize=12)\nfig.colorbar(contour, ax=ax2, pad=0.1)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/introduction-matplotlib.html#using-imshow-for-image-like-data",
    "href": "notebooks/introduction-matplotlib.html#using-imshow-for-image-like-data",
    "title": "Introduction to Matplotlib",
    "section": "",
    "text": "imshow is crucial for displaying 2D data as color-coded images. It’s commonly used for heatmaps, matrices, and actual images.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create sample data\ndata = np.random.rand(10, 10)\n\npd.DataFrame(data)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n0.490860\n0.929045\n0.572919\n0.964711\n0.980014\n0.186145\n0.427508\n0.608805\n0.893498\n0.695284\n\n\n1\n0.966428\n0.422390\n0.348150\n0.525304\n0.331108\n0.580977\n0.562608\n0.560270\n0.939938\n0.808085\n\n\n2\n0.021255\n0.326444\n0.765063\n0.515920\n0.515554\n0.121355\n0.830749\n0.594427\n0.062847\n0.371122\n\n\n3\n0.222631\n0.762149\n0.229365\n0.335383\n0.635986\n0.205019\n0.852074\n0.897168\n0.055277\n0.186129\n\n\n4\n0.987063\n0.835088\n0.654150\n0.181315\n0.012669\n0.086974\n0.903000\n0.829740\n0.716490\n0.834996\n\n\n5\n0.180575\n0.426452\n0.881832\n0.496820\n0.524836\n0.539941\n0.650598\n0.490325\n0.337512\n0.901674\n\n\n6\n0.752542\n0.284082\n0.979989\n0.957409\n0.929105\n0.480490\n0.818328\n0.123510\n0.564911\n0.189244\n\n\n7\n0.992347\n0.311162\n0.411070\n0.600753\n0.495184\n0.069100\n0.372265\n0.431248\n0.456604\n0.238691\n\n\n8\n0.258555\n0.691445\n0.463634\n0.021314\n0.333122\n0.435857\n0.207976\n0.866270\n0.231009\n0.749924\n\n\n9\n0.905349\n0.911525\n0.675188\n0.804228\n0.966661\n0.572587\n0.021375\n0.688160\n0.815374\n0.508309\n\n\n\n\n\n\n\n\n\n\n# Basic imshow example\nfig, ax = plt.subplots(figsize=(8, 6))\nim = ax.imshow(data, cmap='viridis')\nfig.colorbar(im, ax=ax)\nax.set_title('Basic imshow example')\n\n# Multiple imshow with different interpolations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nim1 = ax1.imshow(data, interpolation='nearest', cmap='coolwarm')\nax1.set_title('nearest interpolation')\nfig.colorbar(im1, ax=ax1)\n\nim2 = ax2.imshow(data, interpolation='bilinear', cmap='coolwarm')\nax2.set_title('bilinear interpolation')\nfig.colorbar(im2, ax=ax2)\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#Advanced Plot Features\n##Custom Markers and Lines\nWhat are Custom Markers and Lines?\nMarkers and lines are elements used in plots to differentiate data points and highlight trends. Matplotlib allows customization of their shapes, colors, and styles to make plots more informative and visually appealing.\n\nCustomizing Markers\n- Markers represent individual data points on a plot.\n- Use the marker parameter in plotting functions.\nCommon Marker Options:\n- 'o': Circle\n- 's': Square\n- '^': Triangle up\n- 'x': Cross\n- '*': Star\n- '.': Point\n\nx = np.linspace(0, 10, 50)\ny = np.sin(x)\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(x, y, 'o-', label='Default')\nax.plot(x, y + 1, 'D--', markersize=2, label='Diamond markers')\nax.plot(x, y - 1, 's:', markerfacecolor='red',\n        markeredgecolor='black', label='Square markers')\nax.legend()\nax.set_title('Custom Markers and Lines')\n\nText(0.5, 1.0, 'Custom Markers and Lines')\n\n\n\n\n\n\n\n\n\n\n\nWhat is fill_between?\nThe fill_between function in Matplotlib is used to fill the area between two curves or between a curve and a horizontal line.\nWhere to Use:\n- To visually emphasize the range of values or uncertainty in data.\n- To highlight areas under a curve or between curves.\nWhy Use fill_between?\n- Makes plots more intuitive by shading regions of interest.\n- Helps in representing data variability, confidence intervals, or integrals.\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.fill_between(x, y - 0.2, y + 0.2, alpha=0.1, color='red')\nax.plot(x, y, 'r-', label='Main line')\nax.set_title('Fill Between Example')\nax.legend()\n\n\n\n\n\n\n\n\n#Stylesheets\nWhat are Stylesheets?\nMatplotlib stylesheets are pre-defined sets of style parameters that help you create visually appealing and consistent plots.\nWhere to Use:\n- Use stylesheets when you want your plots to have a cohesive appearance across a project or to match a publication’s style guide.\nWhy Use Stylesheets?\n- Simplify customization by applying a uniform theme with a single line of code.\n- Save time and maintain consistency in multi-plot projects.\n\n# List available styles\nprint(plt.style.available)\n\n['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn-v0_8', 'seaborn-v0_8-bright', 'seaborn-v0_8-colorblind', 'seaborn-v0_8-dark', 'seaborn-v0_8-dark-palette', 'seaborn-v0_8-darkgrid', 'seaborn-v0_8-deep', 'seaborn-v0_8-muted', 'seaborn-v0_8-notebook', 'seaborn-v0_8-paper', 'seaborn-v0_8-pastel', 'seaborn-v0_8-poster', 'seaborn-v0_8-talk', 'seaborn-v0_8-ticks', 'seaborn-v0_8-white', 'seaborn-v0_8-whitegrid', 'tableau-colorblind10']\n\n\n\n\n\n# Example using different styles\ndata = np.random.randn(1000)\nstyles = ['default', 'seaborn-v0_8-bright', 'dark_background', 'seaborn-v0_8-dark-palette']\n\n# Create separate figures for each style\nfig = plt.figure(figsize=(15, 10))\nfor idx, style in enumerate(styles):\n    with plt.style.context(style):\n        ax = fig.add_subplot(2, 2, idx + 1)\n        ax.hist(data, bins=30)\n        ax.set_title(f'Style: {style}')\nfig.tight_layout()",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to Matplotlib"
    ]
  },
  {
    "objectID": "notebooks/2d-distributions.html",
    "href": "notebooks/2d-distributions.html",
    "title": "Distributions in 2D",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nprint(np.__version__)\nimport torch \nimport torch.nn as nn\n\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n1.26.4\n\n\n\ndist = torch.distributions.Normal(0, 1)\nx = dist.sample((1000,))\nplt.hist(x.numpy(), bins=50, density=True)\n\nx_range = torch.linspace(-3, 3, 1000)\ny = dist.log_prob(x_range).exp()\nplt.plot(x_range.numpy(), y.numpy())\n\n\n\n\n\n\n\n\n\ndist.sample([10])\n\ntensor([-1.9083,  0.3758,  0.0051,  0.5140,  0.9852, -0.5989,  0.5222, -0.7744,\n         0.9462, -1.7868])\n\n\n\ndist_2d_normal = torch.distributions.MultivariateNormal(torch.tensor([0.0, 0.0]), torch.eye(2))\n#dist_2d_normal = torch.distributions.MultivariateNormal(torch.tensor([0.0, 0.0]), torch.tensor([[1.0, 0.5], [0.5, 1.0]]))\n\ndist_2d_normal.sample([10])\n\ntensor([[ 0.0438, -0.0310],\n        [ 0.0487, -0.3790],\n        [-0.7872,  0.9880],\n        [ 1.0010, -0.9025],\n        [ 0.5449,  0.1047],\n        [ 1.6466,  0.0925],\n        [ 0.9357,  0.2228],\n        [-1.2721,  2.5194],\n        [-0.3306, -0.1152],\n        [ 1.2249, -1.7330]])\n\n\n\nplt.scatter(*dist_2d_normal.sample([1000]).numpy().T, alpha=0.2, color='k')\n\n\n\n\n\n\n\n\n\n# Plot 2D normal distribution surface plot of PDF\nfrom mpl_toolkits.mplot3d import Axes3D\n\nx = torch.linspace(-3, 3, 100)\ny = torch.linspace(-3, 3, 100)\n\nX, Y = torch.meshgrid(x, y)\nxy = torch.stack([X, Y], 2)\nz = dist_2d_normal.log_prob(xy).exp()\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X.numpy(), Y.numpy(), z.numpy())\n\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('PDF')\n\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\ndf = pd.read_html(\"http://socr.ucla.edu/docs/resources/SOCR_Data/SOCR_Data_Dinov_020108_HeightsWeights.html\")\n\n\nstore_df = df[0]\nstore_df.columns = store_df.iloc[0]\nstore_df = store_df.iloc[1:]\nstore_df = store_df.astype(float)\nstore_df = store_df.drop(columns=[\"Index\"])\nstore_df = store_df.dropna()\n\n\nstore_df.head()\n\n\n\n\n\n\n\n\nHeight(Inches)\nWeight(Pounds)\n\n\n\n\n1\n65.78331\n112.9925\n\n\n2\n71.51521\n136.4873\n\n\n3\n69.39874\n153.0269\n\n\n4\n68.21660\n142.3354\n\n\n5\n67.78781\n144.2971\n\n\n\n\n\n\n\n\n### Fiting a bi-variate normal distribution to the data\ndata = torch.tensor(store_df.values)\nmean = data.mean(0)\ncov = torch.cov(data.T)\ndist = torch.distributions.MultivariateNormal(mean, cov)\n\n\ndist.loc\n\ntensor([ 67.9931, 127.0794], dtype=torch.float64)\n\n\n\ndist.covariance_matrix\n\ntensor([[  3.6164,  11.1510],\n        [ 11.1510, 135.9765]], dtype=torch.float64)\n\n\n\n# Plot the data\n\nplt.scatter(data[:, 0], data[:, 1], alpha=0.1, color='k', facecolors='k')\nplt.xlabel(\"Height\")\nplt.ylabel(\"Weight\")\n\n\nText(0, 0.5, 'Weight')\n\n\n\n\n\n\n\n\n\n\n# plot the PDF\nx = torch.linspace(50, 80, 100)\ny = torch.linspace(80, 280, 100)\nX, Y = torch.meshgrid(x, y)\nxy = torch.stack([X, Y], 2)\nz = dist.log_prob(xy).exp()\n\nimport plotly.graph_objects as go\n\n# Create surface plot with custom hover labels\nfig = go.Figure(data=[go.Surface(\n    x=X, y=Y, z=z, colorscale=\"viridis\",\n    hovertemplate=\"Height: %{x:0.2f}&lt;br&gt;Weight: %{y:0.2f}&lt;br&gt;PDF: %{z:0.5f}&lt;extra&gt;&lt;/extra&gt;\"\n)])\n\n# Maximize figure size and reduce whitespace\nfig.update_layout(\n    autosize=True,\n    width=1200,  # Set wider figure\n    height=700,  # Set taller figure\n    margin=dict(l=0, r=0, t=40, b=0),  # Remove extra whitespace\n    title=\"2D Gaussian PDF\",\n    scene=dict(\n        xaxis_title=\"Height\",\n        yaxis_title=\"Weight\",\n        zaxis_title=\"PDF\"\n    )\n)\n\n# Show plot\nfig.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n\n# uniform distribution\ndist_uniform = torch.distributions.Uniform(0, 1)\nx = dist_uniform.sample((1000,))\nplt.hist(x.numpy(), bins=50, density=True)\n\nx_range = torch.linspace(0, 1, 1000)\ny = dist_uniform.log_prob(x_range).exp()\nplt.plot(x_range.numpy(), y.numpy())\n\n\n\n\n\n\n\n\n\ndist_uniform_2d = torch.distributions.Uniform(torch.tensor([0.0, 0.0]), torch.tensor([1.0, 1.0]))\ndist_uniform_2d.sample([10])\n\ntensor([[0.5493, 0.3478],\n        [0.7661, 0.2568],\n        [0.7199, 0.2975],\n        [0.9114, 0.2916],\n        [0.0045, 0.4948],\n        [0.0156, 0.7434],\n        [0.6856, 0.1037],\n        [0.4446, 0.1913],\n        [0.1995, 0.5009],\n        [0.0716, 0.6085]])\n\n\n\nplt.scatter(*dist_uniform_2d.sample([1000]).numpy().T, alpha=0.5)\n\n\n\n\n\n\n\n\n\nplt.scatter(*dist_uniform_2d.sample([10000]).numpy().T, alpha=0.1)\n\n\n\n\n\n\n\n\n\n# surface plot of PDF\n\nx = torch.linspace(0.0, 1.0, 100)\ny = torch.linspace(0.0, 1.0, 100)\n\nX, Y = torch.meshgrid(x, y)\nxy = torch.stack([X, Y], 2)\n\n\n\n## Important:\n## f(x, y) = f(x) * f(y) for independent random variables\n## log(f(x, y)) = log(f(x)) + log(f(y))\nz1 = dist_uniform_2d.log_prob(xy).sum(-1).exp()\nz2 = dist_uniform.log_prob(X).exp() * dist_uniform.log_prob(Y).exp()\nassert torch.allclose(z1, z2) \n\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X.numpy(), Y.numpy(), z1.numpy())\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('PDF')\n\nText(0.5, 0, 'PDF')",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "Distributions in 2D"
    ]
  },
  {
    "objectID": "notebooks/logistic-regression-generative.html",
    "href": "notebooks/logistic-regression-generative.html",
    "title": "PMF and their applications",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch \nimport torch.nn as nn\n\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'",
    "crumbs": [
      "Home",
      "Machine Learning",
      "PMF and their applications"
    ]
  },
  {
    "objectID": "notebooks/logistic-regression-generative.html#introduction",
    "href": "notebooks/logistic-regression-generative.html#introduction",
    "title": "PMF and their applications",
    "section": "Introduction",
    "text": "Introduction\nProbability Mass Functions (PMF) are fundamental tools for describing the behavior of discrete random variables. A PMF tells us the probability that a discrete random variable takes each of its possible values. This concept is essential for understanding many real-world phenomena, from coin flips and dice rolls to classification problems in machine learning.\nIn this notebook, we’ll explore PMFs through both theoretical foundations and practical applications, including their connection to logistic regression - one of the most important classification algorithms in machine learning. We’ll see how the Bernoulli distribution (a special PMF) naturally leads to logistic regression for binary classification problems.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "PMF and their applications"
    ]
  },
  {
    "objectID": "notebooks/logistic-regression-generative.html#learning-objectives",
    "href": "notebooks/logistic-regression-generative.html#learning-objectives",
    "title": "PMF and their applications",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this notebook, you will be able to:\n\nDefine and interpret Probability Mass Functions (PMF)\nDistinguish between PMF, PDF, and CDF\nWork with common discrete distributions (Bernoulli, Binomial, Categorical)\nApply PMFs to real-world modeling scenarios\nConnect PMFs to logistic regression for classification\nImplement and visualize PMF-based models using Python\nUnderstand the generative vs. discriminative modeling paradigm",
    "crumbs": [
      "Home",
      "Machine Learning",
      "PMF and their applications"
    ]
  },
  {
    "objectID": "notebooks/logistic-regression-generative.html#theoretical-background",
    "href": "notebooks/logistic-regression-generative.html#theoretical-background",
    "title": "PMF and their applications",
    "section": "Theoretical Background",
    "text": "Theoretical Background\n\nDefinition of Probability Mass Function\nFor a discrete random variable \\(X\\) that can take values \\(x_1, x_2, \\ldots, x_k\\), the Probability Mass Function (PMF) is:\n\\[P(X = x_i) = p_i\\]\nwhere \\(p_i \\geq 0\\) for all \\(i\\), and \\(\\sum_{i=1}^k p_i = 1\\).\n\n\nKey Properties of PMF:\n\nNon-negativity: \\(P(X = x) \\geq 0\\) for all \\(x\\)\nNormalization: \\(\\sum_{\\text{all } x} P(X = x) = 1\\)\nAdditivity: \\(P(X \\in A) = \\sum_{x \\in A} P(X = x)\\) for any set \\(A\\)\n\n\n\nCommon Discrete Distributions:\n\n1. Bernoulli Distribution\nFor binary outcomes (success/failure): \\[P(X = 1) = p, \\quad P(X = 0) = 1-p\\]\n\n\n2. Binomial Distribution\nFor \\(n\\) independent Bernoulli trials: \\[P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\]\n\n\n3. Categorical Distribution\nFor \\(k\\) mutually exclusive outcomes: \\[P(X = i) = p_i, \\quad \\sum_{i=1}^k p_i = 1\\]\n\n\n\nConnection to Machine Learning\nPMFs are fundamental to many machine learning algorithms: - Classification: Predicting discrete class labels - Generative Models: Modeling the joint distribution \\(P(X, Y)\\) - Discriminative Models: Modeling the conditional distribution \\(P(Y|X)\\)",
    "crumbs": [
      "Home",
      "Machine Learning",
      "PMF and their applications"
    ]
  },
  {
    "objectID": "notebooks/logistic-regression-generative.html#practical-implementation",
    "href": "notebooks/logistic-regression-generative.html#practical-implementation",
    "title": "PMF and their applications",
    "section": "Practical Implementation",
    "text": "Practical Implementation",
    "crumbs": [
      "Home",
      "Machine Learning",
      "PMF and their applications"
    ]
  },
  {
    "objectID": "notebooks/logistic-regression-generative.html#example-1-understanding-basic-pmfs",
    "href": "notebooks/logistic-regression-generative.html#example-1-understanding-basic-pmfs",
    "title": "PMF and their applications",
    "section": "Example 1: Understanding Basic PMFs",
    "text": "Example 1: Understanding Basic PMFs\nLet’s start with simple examples to build intuition about PMFs.\n\n# Example 1: Basic PMF Examples\n\n# 1. Fair Die PMF\ndie_outcomes = np.arange(1, 7)  # {1, 2, 3, 4, 5, 6}\ndie_pmf = np.ones(6) / 6  # Each outcome has probability 1/6\n\n# 2. Loaded Die PMF  \nloaded_die_pmf = np.array([0.1, 0.1, 0.1, 0.1, 0.2, 0.4])  # Favors 5 and 6\n\n# 3. Bernoulli PMF (Coin flip)\ncoin_outcomes = np.array([0, 1])  # {Tails, Heads}\ncoin_pmf = np.array([0.3, 0.7])  # Biased toward heads\n\n# Visualization\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Fair die\naxes[0].bar(die_outcomes, die_pmf, alpha=0.7, color='blue')\naxes[0].set_title('Fair Die PMF')\naxes[0].set_xlabel('Outcome')\naxes[0].set_ylabel('Probability')\naxes[0].set_ylim(0, 0.5)\naxes[0].grid(True, alpha=0.3)\n\n# Add probability values on bars\nfor i, p in enumerate(die_pmf):\n    axes[0].text(i+1, p+0.01, f'{p:.3f}', ha='center', va='bottom')\n\n# Loaded die\naxes[1].bar(die_outcomes, loaded_die_pmf, alpha=0.7, color='red')\naxes[1].set_title('Loaded Die PMF')\naxes[1].set_xlabel('Outcome')\naxes[1].set_ylabel('Probability')\naxes[1].set_ylim(0, 0.5)\naxes[1].grid(True, alpha=0.3)\n\nfor i, p in enumerate(loaded_die_pmf):\n    axes[1].text(i+1, p+0.01, f'{p:.3f}', ha='center', va='bottom')\n\n# Biased coin\naxes[2].bar(coin_outcomes, coin_pmf, alpha=0.7, color='green')\naxes[2].set_title('Biased Coin PMF')\naxes[2].set_xlabel('Outcome (0=Tails, 1=Heads)')\naxes[2].set_ylabel('Probability')\naxes[2].set_ylim(0, 0.8)\naxes[2].grid(True, alpha=0.3)\n\nfor i, p in enumerate(coin_pmf):\n    axes[2].text(i, p+0.02, f'{p:.3f}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n# Verify PMF properties\nprint(\"VERIFICATION OF PMF PROPERTIES:\")\nprint(\"=\"*40)\nprint(f\"Fair die:\")\nprint(f\"  - All probabilities ≥ 0: {all(die_pmf &gt;= 0)}\")\nprint(f\"  - Sum = 1: {np.sum(die_pmf):.6f}\")\n\nprint(f\"\\nLoaded die:\")\nprint(f\"  - All probabilities ≥ 0: {all(loaded_die_pmf &gt;= 0)}\")\nprint(f\"  - Sum = 1: {np.sum(loaded_die_pmf):.6f}\")\n\nprint(f\"\\nBiased coin:\")\nprint(f\"  - All probabilities ≥ 0: {all(coin_pmf &gt;= 0)}\")\nprint(f\"  - Sum = 1: {np.sum(coin_pmf):.6f}\")\n\nprint(f\"\\nExpected Values:\")\nprint(f\"  - Fair die: {np.sum(die_outcomes * die_pmf):.3f}\")\nprint(f\"  - Loaded die: {np.sum(die_outcomes * loaded_die_pmf):.3f}\")\nprint(f\"  - Biased coin: {np.sum(coin_outcomes * coin_pmf):.3f}\")",
    "crumbs": [
      "Home",
      "Machine Learning",
      "PMF and their applications"
    ]
  },
  {
    "objectID": "notebooks/logistic-regression-generative.html#example-2-bernoulli-distribution-and-binary-classification",
    "href": "notebooks/logistic-regression-generative.html#example-2-bernoulli-distribution-and-binary-classification",
    "title": "PMF and their applications",
    "section": "Example 2: Bernoulli Distribution and Binary Classification",
    "text": "Example 2: Bernoulli Distribution and Binary Classification\nThe Bernoulli distribution is fundamental to binary classification. Let’s explore how it connects to logistic regression.\n\nGenerating Synthetic Binary Classification Data\nWe’ll create a dataset where the class labels follow a Bernoulli distribution whose parameter depends on the input features.\nUnderstanding the Data Generation Process:\n\nLinear Combination: \\(z = w_1 x_1 + w_2 x_2 + b\\) (logits)\nSigmoid Transformation: \\(p = \\sigma(z) = \\frac{1}{1 + e^{-z}}\\)\nBernoulli Sampling: \\(Y \\sim \\text{Bernoulli}(p)\\)\n\nThis creates a natural connection between continuous features and binary outcomes through the Bernoulli PMF.\n\n# Visualize how the Bernoulli PMF varies across feature space\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# 1. Probability surface\nx1_range = np.linspace(0, 5, 50)\nx2_range = np.linspace(0, 5, 50)\nX1_grid, X2_grid = np.meshgrid(x1_range, x2_range)\n\n# Compute probability for each point\nlogits_grid = w1 * X1_grid + w2 * X2_grid + b\nprob_grid = torch.sigmoid(torch.tensor(logits_grid)).numpy()\n\ncontour = axes[0, 0].contourf(X1_grid, X2_grid, prob_grid, levels=20, cmap='RdYlBu_r')\naxes[0, 0].set_title('Bernoulli Parameter p(x₁, x₂)\\nProbability of Class 1')\naxes[0, 0].set_xlabel('Feature 1 (X₁)')\naxes[0, 0].set_ylabel('Feature 2 (X₂)')\nplt.colorbar(contour, ax=axes[0, 0])\n\n# Add decision boundary (p = 0.5)\ncontour_line = axes[0, 0].contour(X1_grid, X2_grid, prob_grid, levels=[0.5], colors='black', linewidths=2)\naxes[0, 0].clabel(contour_line, inline=True, fontsize=10)\n\n# 2. Data points with probability coloring\nX1_np, X2_np, Y_np = X1.numpy(), X2.numpy(), Y.numpy()\nscatter = axes[0, 1].scatter(X1_np, X2_np, c=prob_Y.numpy(), cmap='RdYlBu_r', \n                            alpha=0.7, s=30, edgecolors='black', linewidth=0.5)\naxes[0, 1].set_title('Data Points Colored by\\nBernoulli Parameter p')\naxes[0, 1].set_xlabel('Feature 1 (X₁)')\naxes[0, 1].set_ylabel('Feature 2 (X₂)')\nplt.colorbar(scatter, ax=axes[0, 1])\n\n# 3. Actual class labels\naxes[0, 2].scatter(X1_np[Y_np == 0], X2_np[Y_np == 0], color=\"blue\", label=\"Class 0\", alpha=0.7, s=30)\naxes[0, 2].scatter(X1_np[Y_np == 1], X2_np[Y_np == 1], color=\"red\", label=\"Class 1\", alpha=0.7, s=30)\naxes[0, 2].set_title('Actual Class Labels\\n(Bernoulli Realizations)')\naxes[0, 2].set_xlabel('Feature 1 (X₁)')\naxes[0, 2].set_ylabel('Feature 2 (X₂)')\naxes[0, 2].legend()\n\n# 4. PMF visualization for specific points\nsample_points = [(1, 1), (2.5, 2.5), (4, 1)]\ncolors = ['blue', 'purple', 'red']\n\nfor i, (x1_val, x2_val) in enumerate(sample_points):\n    # Calculate probability for this point\n    logit_val = w1 * x1_val + w2 * x2_val + b\n    p_val = torch.sigmoid(torch.tensor(logit_val)).item()\n    \n    # Plot the Bernoulli PMF for this point\n    outcomes = [0, 1]\n    probabilities = [1 - p_val, p_val]\n    \n    ax = axes[1, i]\n    bars = ax.bar(outcomes, probabilities, alpha=0.7, color=colors[i])\n    ax.set_title(f'Bernoulli PMF at ({x1_val}, {x2_val})\\np = {p_val:.3f}')\n    ax.set_xlabel('Class')\n    ax.set_ylabel('Probability')\n    ax.set_ylim(0, 1)\n    ax.grid(True, alpha=0.3)\n    \n    # Add probability values on bars\n    for j, prob in enumerate(probabilities):\n        ax.text(j, prob + 0.02, f'{prob:.3f}', ha='center', va='bottom', fontweight='bold')\n    \n    # Mark this point on the main plot\n    axes[0, 1].plot(x1_val, x2_val, 'o', color=colors[i], markersize=10, \n                   markeredgecolor='black', markeredgewidth=2)\n    axes[0, 1].text(x1_val + 0.1, x2_val + 0.1, f'Point {i+1}', \n                   color=colors[i], fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"PMF ANALYSIS FOR DIFFERENT POINTS:\")\nprint(\"=\"*50)\nfor i, (x1_val, x2_val) in enumerate(sample_points):\n    logit_val = w1 * x1_val + w2 * x2_val + b\n    p_val = torch.sigmoid(torch.tensor(logit_val)).item()\n    \n    print(f\"\\nPoint {i+1}: ({x1_val}, {x2_val})\")\n    print(f\"  Logit z = {w1:.1f}×{x1_val} + {w2:.1f}×{x2_val} + {b:.1f} = {logit_val:.3f}\")\n    print(f\"  Probability p = σ({logit_val:.3f}) = {p_val:.3f}\")\n    print(f\"  Bernoulli PMF: P(Y=0) = {1-p_val:.3f}, P(Y=1) = {p_val:.3f}\")\n    print(f\"  Most likely class: {1 if p_val &gt; 0.5 else 0}\")\n\nprint(f\"\\nDecision boundary equation: {w1:.1f}×X₁ + {w2:.1f}×X₂ + {b:.1f} = 0\")\nprint(f\"Simplified: X₂ = {-w1/w2:.3f}×X₁ + {-b/w2:.3f}\")\n\n\n\nVisualizing the Bernoulli PMF in Action\nLet’s examine how the Bernoulli PMF varies across our feature space:\n\n# Set random seed\ntorch.manual_seed(42)\n\n&lt;torch._C.Generator at 0x11e27cab0&gt;\n\n\n\n\nUnderstanding the Learning Process: Maximum Likelihood Estimation\nLogistic regression learns by finding parameters that maximize the likelihood of observing our data. This is directly connected to the Bernoulli PMF!",
    "crumbs": [
      "Home",
      "Machine Learning",
      "PMF and their applications"
    ]
  },
  {
    "objectID": "notebooks/logistic-regression-generative.html#summary-and-key-takeaways",
    "href": "notebooks/logistic-regression-generative.html#summary-and-key-takeaways",
    "title": "PMF and their applications",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nWhat We’ve Learned About PMFs:\n\nDefinition and Properties: PMFs describe discrete random variables with non-negative probabilities that sum to 1\nConnection to Machine Learning:\n\nBernoulli Distribution → Binary Classification → Logistic Regression\nCategorical Distribution → Multi-class Classification → Softmax Regression\n\nMaximum Likelihood Estimation: Learning algorithms find parameters that maximize the likelihood of observed data under the assumed PMF\nPractical Applications: PMFs model discrete outcomes in classification, counting processes, and decision-making scenarios\n\n\n\nMathematical Connections:\nBinary Classification (Bernoulli PMF): - \\(P(Y = 1|X) = \\sigma(w^T X + b)\\) where \\(\\sigma(z) = \\frac{1}{1+e^{-z}}\\) - Loss function: \\(-\\sum_i [y_i \\log p_i + (1-y_i) \\log(1-p_i)]\\)\nMulti-class Classification (Categorical PMF): - \\(P(Y = k|X) = \\frac{e^{w_k^T X + b_k}}{\\sum_{j=1}^K e^{w_j^T X + b_j}}\\) (softmax) - Loss function: \\(-\\sum_i \\sum_k y_{ik} \\log p_{ik}\\) (cross-entropy)\n\n\nKey Insights for Data Science:\n\nProbabilistic Foundation: Classification is fundamentally about modeling conditional PMFs\nGenerative vs. Discriminative: PMFs can model \\(P(X,Y)\\) (generative) or \\(P(Y|X)\\) (discriminative)\nUncertainty Quantification: PMFs naturally provide prediction confidence through probabilities\nModel Selection: Different PMF assumptions lead to different algorithms (Naive Bayes vs. Logistic Regression)\n\n\n\nReal-World Applications:\n\nMedical Diagnosis: Modeling disease presence/absence (Bernoulli)\nCustomer Behavior: Predicting purchase categories (Categorical)\n\nQuality Control: Counting defects (Poisson/Binomial)\nNatural Language: Word occurrence in documents (Multinomial)\nRecommendation Systems: Item preferences (Categorical/Multinomial)\n\n\n\nConnection to Broader Topics:\n\nExponential Family: Bernoulli and Categorical are exponential family distributions\nInformation Theory: Cross-entropy loss connects to information theory\nBayesian Statistics: PMFs serve as likelihood functions in Bayesian inference\nCausal Inference: Understanding \\(P(Y|do(X))\\) vs. \\(P(Y|X)\\)\n\nUnderstanding PMFs provides the probabilistic foundation for classification algorithms, uncertainty quantification, and decision-making under uncertainty. This knowledge bridges pure probability theory with practical machine learning applications.\n\n# Example: Categorical Distribution (Multinomial Classification)\n\n# Simulate a 3-class classification problem\ntorch.manual_seed(123)\nn_samples = 500\nn_classes = 3\n\n# Generate 2D features\nX_multi = torch.distributions.Uniform(0, 6).sample((n_samples, 2))\n\n# Define parameters for 3-class softmax\n# Each class has its own linear function\nW = torch.tensor([[1.0, -0.5],   # Class 0 weights\n                  [-0.8, 1.2],   # Class 1 weights  \n                  [0.3, -0.7]])  # Class 2 weights\nb = torch.tensor([-1.5, 0.5, 1.0])  # Class biases\n\n# Compute logits for each class\nlogits_multi = torch.matmul(X_multi, W.T) + b  # (n_samples, n_classes)\n\n# Apply softmax to get class probabilities (categorical PMF parameters)\nprobs_multi = torch.softmax(logits_multi, dim=1)\n\n# Sample class labels from categorical distribution\nY_multi = torch.distributions.Categorical(probs_multi).sample()\n\n# Visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# 1. Data points colored by true class\ncolors = ['blue', 'red', 'green']\nclass_names = ['Class 0', 'Class 1', 'Class 2']\n\nfor i in range(n_classes):\n    mask = (Y_multi == i)\n    axes[0, 0].scatter(X_multi[mask, 0], X_multi[mask, 1], \n                      color=colors[i], label=class_names[i], alpha=0.7, s=30)\n\naxes[0, 0].set_title('3-Class Classification Data\\n(Categorical Distribution Samples)')\naxes[0, 0].set_xlabel('Feature 1')\naxes[0, 0].set_ylabel('Feature 2')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# 2-4. Probability maps for each class\nx1_range = np.linspace(0, 6, 50)\nx2_range = np.linspace(0, 6, 50)\nX1_grid, X2_grid = np.meshgrid(x1_range, x2_range)\ngrid_points = torch.tensor(np.c_[X1_grid.ravel(), X2_grid.ravel()], dtype=torch.float32)\n\n# Compute probabilities for grid\nlogits_grid = torch.matmul(grid_points, W.T) + b\nprobs_grid = torch.softmax(logits_grid, dim=1)\n\nfor class_idx in range(n_classes):\n    ax = axes[0, class_idx + 1]\n    prob_map = probs_grid[:, class_idx].reshape(X1_grid.shape)\n    \n    contour = ax.contourf(X1_grid, X2_grid, prob_map, levels=20, cmap='Reds')\n    ax.set_title(f'P(Y = {class_idx} | X)\\nCategorical PMF Parameter')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    plt.colorbar(contour, ax=ax)\n\n# 5. PMF visualization for specific points\nsample_points = [(1, 5), (3, 3), (5, 1)]\npoint_colors = ['purple', 'orange', 'brown']\n\nfor i, (x1_val, x2_val) in enumerate(sample_points):\n    # Calculate probabilities for this point\n    point_tensor = torch.tensor([[x1_val, x2_val]], dtype=torch.float32)\n    logits_point = torch.matmul(point_tensor, W.T) + b\n    probs_point = torch.softmax(logits_point, dim=1).squeeze()\n    \n    # Plot the categorical PMF for this point\n    ax = axes[1, i]\n    bars = ax.bar(range(n_classes), probs_point.numpy(), alpha=0.7, color=point_colors[i])\n    ax.set_title(f'Categorical PMF at ({x1_val}, {x2_val})')\n    ax.set_xlabel('Class')\n    ax.set_ylabel('Probability')\n    ax.set_ylim(0, 1)\n    ax.set_xticks(range(n_classes))\n    ax.grid(True, alpha=0.3)\n    \n    # Add probability values on bars\n    for j, prob in enumerate(probs_point.numpy()):\n        ax.text(j, prob + 0.02, f'{prob:.3f}', ha='center', va='bottom', fontweight='bold')\n    \n    # Mark this point on the main plot\n    axes[0, 0].plot(x1_val, x2_val, 'o', color=point_colors[i], markersize=12, \n                   markeredgecolor='black', markeredgewidth=2)\n    axes[0, 0].text(x1_val + 0.1, x2_val + 0.1, f'Point {i+1}', \n                   color=point_colors[i], fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Analysis\nprint(\"CATEGORICAL DISTRIBUTION ANALYSIS:\")\nprint(\"=\"*50)\n\n# Class distribution\nclass_counts = torch.bincount(Y_multi)\nprint(f\"Class distribution in sample:\")\nfor i in range(n_classes):\n    count = class_counts[i].item()\n    proportion = count / n_samples\n    print(f\"  Class {i}: {count} samples ({proportion:.3f})\")\n\nprint(f\"\\nPMF Analysis for sample points:\")\nfor i, (x1_val, x2_val) in enumerate(sample_points):\n    point_tensor = torch.tensor([[x1_val, x2_val]], dtype=torch.float32)\n    logits_point = torch.matmul(point_tensor, W.T) + b\n    probs_point = torch.softmax(logits_point, dim=1).squeeze()\n    \n    print(f\"\\nPoint {i+1}: ({x1_val}, {x2_val})\")\n    print(f\"  Logits: {logits_point.squeeze().numpy()}\")\n    print(f\"  Probabilities: {probs_point.numpy()}\")\n    print(f\"  Predicted class: {torch.argmax(probs_point).item()}\")\n    print(f\"  Confidence: {torch.max(probs_point).item():.3f}\")\n\nprint(f\"\\nKey Properties Verified:\")\nprint(f\"- Non-negativity: All probabilities ≥ 0 ✓\")\nprint(f\"- Normalization: Each point's probabilities sum to 1 ✓\")\nprint(f\"- Mutual exclusivity: Each sample belongs to exactly one class ✓\")",
    "crumbs": [
      "Home",
      "Machine Learning",
      "PMF and their applications"
    ]
  },
  {
    "objectID": "notebooks/logistic-regression-generative.html#example-3-multinomial-and-categorical-distributions",
    "href": "notebooks/logistic-regression-generative.html#example-3-multinomial-and-categorical-distributions",
    "title": "PMF and their applications",
    "section": "Example 3: Multinomial and Categorical Distributions",
    "text": "Example 3: Multinomial and Categorical Distributions\nPMFs extend beyond binary outcomes. Let’s explore the Categorical distribution, which generalizes the Bernoulli to multiple classes.\n\n# Demonstrate the connection to Maximum Likelihood Estimation\ndef bernoulli_likelihood(y_true, p_pred):\n    \"\"\"\n    Compute the likelihood of data under Bernoulli model\n    L = ∏ᵢ p_i^{y_i} (1-p_i)^{1-y_i}\n    \"\"\"\n    likelihood = 1.0\n    for i in range(len(y_true)):\n        if y_true[i] == 1:\n            likelihood *= p_pred[i]\n        else:\n            likelihood *= (1 - p_pred[i])\n    return likelihood\n\ndef log_likelihood(y_true, p_pred):\n    \"\"\"\n    Compute log-likelihood (more numerically stable)\n    ℓ = Σᵢ [y_i log(p_i) + (1-y_i) log(1-p_i)]\n    \"\"\"\n    ll = 0.0\n    for i in range(len(y_true)):\n        if y_true[i] == 1:\n            ll += np.log(p_pred[i] + 1e-15)  # Add small epsilon to avoid log(0)\n        else:\n            ll += np.log(1 - p_pred[i] + 1e-15)\n    return ll\n\n# Analyze likelihood for different parameter settings\nY_sample = Y[:100]  # Use subset for clearer visualization\nX1_sample = X1[:100]\nX2_sample = X2[:100]\n\n# Test different parameter settings\ntest_scenarios = [\n    {\"name\": \"True Parameters\", \"w1\": w1, \"w2\": w2, \"b\": b},\n    {\"name\": \"Random Guess 1\", \"w1\": 0.5, \"w2\": -0.3, \"b\": -1.0},\n    {\"name\": \"Random Guess 2\", \"w1\": -0.8, \"w2\": 1.2, \"b\": 1.5},\n    {\"name\": \"Poor Fit\", \"w1\": 0.1, \"w2\": 0.1, \"b\": 0.0}\n]\n\nresults = []\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\naxes = axes.ravel()\n\nfor i, scenario in enumerate(test_scenarios):\n    # Compute predictions\n    logits_test = scenario[\"w1\"] * X1_sample + scenario[\"w2\"] * X2_sample + scenario[\"b\"]\n    p_test = torch.sigmoid(logits_test).numpy()\n    \n    # Compute likelihood metrics\n    likelihood = bernoulli_likelihood(Y_sample.numpy(), p_test)\n    log_ll = log_likelihood(Y_sample.numpy(), p_test)\n    \n    results.append({\n        \"scenario\": scenario[\"name\"],\n        \"likelihood\": likelihood,\n        \"log_likelihood\": log_ll,\n        \"parameters\": (scenario[\"w1\"], scenario[\"w2\"], scenario[\"b\"])\n    })\n    \n    # Visualize decision boundary\n    ax = axes[i]\n    \n    # Plot data points\n    Y_sample_np = Y_sample.numpy()\n    ax.scatter(X1_sample[Y_sample_np == 0], X2_sample[Y_sample_np == 0], \n              color=\"blue\", label=\"Class 0\", alpha=0.7, s=30)\n    ax.scatter(X1_sample[Y_sample_np == 1], X2_sample[Y_sample_np == 1], \n              color=\"red\", label=\"Class 1\", alpha=0.7, s=30)\n    \n    # Plot decision boundary\n    x1_range = np.linspace(0, 5, 100)\n    if scenario[\"w2\"] != 0:\n        x2_boundary = -(scenario[\"w1\"] * x1_range + scenario[\"b\"]) / scenario[\"w2\"]\n        valid_mask = (x2_boundary &gt;= 0) & (x2_boundary &lt;= 5)\n        ax.plot(x1_range[valid_mask], x2_boundary[valid_mask], 'k-', linewidth=2, \n               label='Decision Boundary')\n    \n    ax.set_title(f'{scenario[\"name\"]}\\nLog-Likelihood: {log_ll:.2f}')\n    ax.set_xlabel('Feature 1 (X₁)')\n    ax.set_ylabel('Feature 2 (X₂)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_xlim(0, 5)\n    ax.set_ylim(0, 5)\n\nplt.tight_layout()\nplt.show()\n\n# Print detailed results\nprint(\"MAXIMUM LIKELIHOOD ESTIMATION RESULTS:\")\nprint(\"=\"*60)\nprint(f\"{'Scenario':&lt;20} {'Log-Likelihood':&lt;15} {'Likelihood':&lt;15} {'Parameters (w1, w2, b)'}\")\nprint(\"-\" * 80)\n\n# Sort by log-likelihood (higher is better)\nresults.sort(key=lambda x: x['log_likelihood'], reverse=True)\n\nfor result in results:\n    ll = result['log_likelihood']\n    likelihood = result['likelihood']\n    params = result['parameters']\n    scenario = result['scenario']\n    \n    print(f\"{scenario:&lt;20} {ll:&lt;15.3f} {likelihood:&lt;15.2e} {params}\")\n\nprint(f\"\\nKEY INSIGHTS:\")\nprint(f\"- Higher likelihood = better fit to the data\")\nprint(f\"- Log-likelihood is used for numerical stability\")\nprint(f\"- True parameters achieve highest likelihood (as expected)\")\nprint(f\"- Poor parameters result in low likelihood\")\nprint(f\"\\nThe learning algorithm finds parameters that maximize:\")\nprint(f\"  ℓ = Σᵢ [yᵢ log(pᵢ) + (1-yᵢ) log(1-pᵢ)]\")\nprint(f\"where pᵢ = σ(w₁x₁ᵢ + w₂x₂ᵢ + b) is the Bernoulli parameter\")\n\n\n\n# Generate 2D input features\nn_samples = 1000\nX1 = torch.distributions.Uniform(0, 5).sample((n_samples, 1))  # Feature 1\nX2 = torch.distributions.Uniform(0, 5).sample((n_samples, 1))  # Feature 2\n\n\n# True weights and bias\nw1, w2, b = 1.2, -0.8, -2.5  \n\n\n# Compute logits and apply sigmoid\nlogits = w1 * X1 + w2 * X2 + b\nprob_Y = torch.sigmoid(logits)  # Probabilities\n\n\nprob_Y\n\ntensor([[0.4690],\n        [0.8697],\n        [0.2377],\n        [0.6852],\n        [0.2472],\n        [0.5074],\n        [0.0922],\n        [0.7336],\n        [0.8787],\n        [0.0406],\n        [0.6429],\n        [0.6637],\n        [0.8729],\n        [0.2455],\n        [0.4179],\n        [0.0462],\n        [0.9018],\n        [0.1551],\n        [0.0173],\n        [0.1364],\n        [0.2652],\n        [0.2519],\n        [0.2691],\n        [0.8527],\n        [0.0068],\n        [0.0701],\n        [0.0393],\n        [0.1062],\n        [0.1793],\n        [0.0372],\n        [0.5522],\n        [0.0402],\n        [0.7888],\n        [0.6758],\n        [0.0772],\n        [0.1758],\n        [0.1189],\n        [0.8798],\n        [0.2317],\n        [0.0974],\n        [0.7143],\n        [0.0202],\n        [0.4330],\n        [0.6808],\n        [0.7392],\n        [0.1329],\n        [0.4755],\n        [0.1541],\n        [0.3953],\n        [0.0859],\n        [0.0016],\n        [0.0859],\n        [0.0707],\n        [0.8696],\n        [0.0688],\n        [0.4360],\n        [0.4305],\n        [0.3274],\n        [0.7977],\n        [0.1249],\n        [0.5691],\n        [0.0089],\n        [0.0923],\n        [0.0116],\n        [0.7033],\n        [0.0389],\n        [0.9026],\n        [0.0113],\n        [0.0087],\n        [0.0079],\n        [0.4398],\n        [0.1479],\n        [0.7938],\n        [0.0074],\n        [0.0228],\n        [0.0089],\n        [0.0534],\n        [0.4375],\n        [0.7883],\n        [0.8199],\n        [0.1195],\n        [0.0536],\n        [0.7972],\n        [0.2862],\n        [0.5475],\n        [0.0290],\n        [0.1672],\n        [0.0067],\n        [0.1216],\n        [0.8010],\n        [0.1646],\n        [0.0328],\n        [0.8813],\n        [0.4557],\n        [0.0046],\n        [0.0383],\n        [0.9207],\n        [0.0155],\n        [0.0318],\n        [0.0032],\n        [0.1341],\n        [0.1285],\n        [0.3706],\n        [0.0035],\n        [0.3584],\n        [0.0085],\n        [0.0116],\n        [0.0246],\n        [0.0215],\n        [0.0143],\n        [0.9115],\n        [0.7215],\n        [0.5481],\n        [0.0028],\n        [0.0500],\n        [0.0420],\n        [0.4277],\n        [0.0035],\n        [0.0182],\n        [0.0180],\n        [0.3848],\n        [0.2725],\n        [0.1439],\n        [0.0863],\n        [0.0055],\n        [0.2094],\n        [0.0317],\n        [0.0028],\n        [0.5034],\n        [0.0586],\n        [0.9528],\n        [0.0766],\n        [0.0242],\n        [0.9202],\n        [0.0134],\n        [0.1375],\n        [0.0209],\n        [0.0018],\n        [0.6780],\n        [0.3229],\n        [0.3414],\n        [0.0243],\n        [0.5663],\n        [0.1166],\n        [0.1113],\n        [0.5040],\n        [0.7373],\n        [0.9401],\n        [0.5430],\n        [0.0139],\n        [0.7203],\n        [0.0124],\n        [0.0808],\n        [0.0160],\n        [0.9173],\n        [0.0179],\n        [0.1390],\n        [0.1397],\n        [0.7374],\n        [0.0071],\n        [0.7449],\n        [0.1415],\n        [0.0649],\n        [0.0029],\n        [0.7889],\n        [0.0241],\n        [0.1800],\n        [0.7164],\n        [0.6379],\n        [0.0617],\n        [0.0508],\n        [0.1972],\n        [0.6204],\n        [0.0813],\n        [0.0056],\n        [0.0244],\n        [0.0077],\n        [0.2261],\n        [0.3260],\n        [0.0143],\n        [0.2764],\n        [0.4105],\n        [0.6875],\n        [0.2774],\n        [0.4553],\n        [0.7025],\n        [0.4154],\n        [0.1725],\n        [0.0688],\n        [0.0773],\n        [0.2032],\n        [0.0181],\n        [0.2384],\n        [0.9317],\n        [0.4372],\n        [0.3650],\n        [0.4109],\n        [0.2889],\n        [0.3479],\n        [0.4569],\n        [0.0386],\n        [0.1222],\n        [0.8519],\n        [0.0917],\n        [0.2874],\n        [0.1328],\n        [0.0153],\n        [0.0168],\n        [0.0178],\n        [0.1498],\n        [0.1939],\n        [0.7941],\n        [0.0043],\n        [0.0068],\n        [0.2717],\n        [0.3926],\n        [0.0714],\n        [0.0912],\n        [0.2274],\n        [0.1253],\n        [0.1548],\n        [0.5459],\n        [0.0305],\n        [0.3112],\n        [0.0043],\n        [0.9017],\n        [0.2792],\n        [0.0876],\n        [0.0157],\n        [0.0389],\n        [0.0073],\n        [0.5978],\n        [0.3535],\n        [0.5908],\n        [0.1898],\n        [0.5297],\n        [0.2536],\n        [0.0130],\n        [0.0135],\n        [0.0027],\n        [0.6854],\n        [0.0522],\n        [0.0775],\n        [0.0632],\n        [0.0320],\n        [0.0212],\n        [0.1663],\n        [0.4284],\n        [0.1848],\n        [0.5393],\n        [0.0971],\n        [0.1523],\n        [0.1591],\n        [0.0594],\n        [0.1385],\n        [0.0020],\n        [0.6491],\n        [0.0076],\n        [0.3805],\n        [0.8249],\n        [0.5432],\n        [0.0042],\n        [0.8772],\n        [0.1644],\n        [0.0875],\n        [0.4281],\n        [0.3085],\n        [0.0614],\n        [0.1262],\n        [0.1158],\n        [0.4476],\n        [0.0059],\n        [0.7236],\n        [0.5122],\n        [0.9547],\n        [0.0268],\n        [0.0034],\n        [0.1269],\n        [0.4547],\n        [0.0058],\n        [0.3243],\n        [0.0574],\n        [0.1720],\n        [0.1343],\n        [0.2229],\n        [0.0219],\n        [0.0033],\n        [0.9415],\n        [0.5935],\n        [0.0582],\n        [0.0232],\n        [0.0202],\n        [0.0024],\n        [0.0227],\n        [0.1436],\n        [0.0084],\n        [0.1765],\n        [0.4679],\n        [0.0520],\n        [0.0276],\n        [0.3989],\n        [0.0865],\n        [0.0738],\n        [0.0310],\n        [0.0538],\n        [0.4719],\n        [0.0175],\n        [0.6643],\n        [0.0717],\n        [0.2207],\n        [0.6581],\n        [0.1212],\n        [0.0091],\n        [0.5362],\n        [0.3897],\n        [0.5126],\n        [0.7488],\n        [0.7894],\n        [0.3925],\n        [0.2507],\n        [0.0973],\n        [0.0892],\n        [0.6295],\n        [0.4406],\n        [0.1529],\n        [0.1186],\n        [0.0113],\n        [0.0021],\n        [0.1150],\n        [0.0482],\n        [0.8805],\n        [0.2454],\n        [0.0035],\n        [0.0044],\n        [0.8107],\n        [0.0069],\n        [0.0515],\n        [0.4856],\n        [0.0080],\n        [0.2625],\n        [0.2149],\n        [0.2317],\n        [0.0353],\n        [0.0090],\n        [0.0166],\n        [0.1212],\n        [0.9087],\n        [0.0258],\n        [0.1467],\n        [0.3867],\n        [0.3478],\n        [0.7219],\n        [0.5131],\n        [0.5104],\n        [0.0028],\n        [0.2580],\n        [0.8291],\n        [0.0395],\n        [0.0280],\n        [0.8736],\n        [0.2135],\n        [0.0247],\n        [0.0914],\n        [0.1309],\n        [0.2388],\n        [0.7147],\n        [0.5993],\n        [0.0881],\n        [0.0025],\n        [0.0412],\n        [0.7522],\n        [0.1615],\n        [0.0706],\n        [0.5131],\n        [0.0120],\n        [0.0252],\n        [0.0513],\n        [0.5335],\n        [0.3664],\n        [0.0042],\n        [0.2806],\n        [0.1637],\n        [0.1168],\n        [0.9396],\n        [0.0373],\n        [0.7879],\n        [0.0925],\n        [0.0391],\n        [0.6586],\n        [0.4116],\n        [0.1996],\n        [0.0494],\n        [0.2710],\n        [0.6425],\n        [0.1510],\n        [0.0312],\n        [0.0179],\n        [0.1739],\n        [0.6685],\n        [0.2275],\n        [0.1530],\n        [0.9160],\n        [0.4186],\n        [0.1890],\n        [0.2847],\n        [0.0327],\n        [0.0187],\n        [0.0628],\n        [0.1278],\n        [0.3404],\n        [0.1098],\n        [0.0375],\n        [0.7807],\n        [0.6327],\n        [0.7904],\n        [0.0507],\n        [0.1858],\n        [0.3649],\n        [0.0336],\n        [0.0142],\n        [0.1073],\n        [0.0164],\n        [0.0245],\n        [0.8529],\n        [0.5046],\n        [0.5838],\n        [0.1484],\n        [0.0258],\n        [0.1378],\n        [0.5009],\n        [0.1275],\n        [0.0463],\n        [0.0377],\n        [0.1537],\n        [0.1114],\n        [0.5183],\n        [0.0190],\n        [0.0583],\n        [0.3773],\n        [0.1361],\n        [0.2197],\n        [0.6813],\n        [0.2385],\n        [0.9250],\n        [0.9642],\n        [0.1617],\n        [0.0100],\n        [0.4980],\n        [0.9302],\n        [0.0457],\n        [0.1462],\n        [0.0613],\n        [0.0062],\n        [0.0611],\n        [0.2581],\n        [0.6507],\n        [0.0583],\n        [0.6322],\n        [0.0657],\n        [0.0992],\n        [0.5305],\n        [0.1600],\n        [0.3603],\n        [0.0488],\n        [0.6136],\n        [0.1191],\n        [0.0036],\n        [0.0577],\n        [0.4877],\n        [0.3272],\n        [0.7177],\n        [0.4588],\n        [0.7115],\n        [0.0364],\n        [0.2061],\n        [0.3108],\n        [0.0613],\n        [0.0117],\n        [0.0795],\n        [0.4676],\n        [0.5647],\n        [0.0231],\n        [0.1328],\n        [0.5421],\n        [0.0562],\n        [0.0614],\n        [0.3270],\n        [0.0339],\n        [0.8109],\n        [0.3840],\n        [0.5580],\n        [0.6581],\n        [0.5215],\n        [0.4812],\n        [0.4216],\n        [0.5801],\n        [0.8010],\n        [0.6829],\n        [0.0294],\n        [0.0095],\n        [0.0193],\n        [0.0555],\n        [0.2013],\n        [0.8739],\n        [0.5182],\n        [0.8575],\n        [0.0095],\n        [0.0300],\n        [0.0588],\n        [0.0581],\n        [0.4170],\n        [0.0025],\n        [0.1793],\n        [0.0060],\n        [0.0250],\n        [0.0052],\n        [0.4422],\n        [0.0052],\n        [0.3186],\n        [0.2248],\n        [0.0405],\n        [0.4789],\n        [0.0483],\n        [0.1727],\n        [0.5538],\n        [0.0335],\n        [0.4634],\n        [0.1829],\n        [0.2329],\n        [0.3739],\n        [0.0957],\n        [0.0156],\n        [0.3449],\n        [0.8196],\n        [0.6699],\n        [0.4111],\n        [0.0098],\n        [0.3198],\n        [0.0751],\n        [0.8296],\n        [0.0686],\n        [0.0258],\n        [0.0505],\n        [0.0505],\n        [0.6845],\n        [0.1587],\n        [0.3579],\n        [0.6190],\n        [0.9235],\n        [0.2053],\n        [0.1562],\n        [0.4789],\n        [0.0443],\n        [0.2962],\n        [0.0232],\n        [0.0190],\n        [0.5996],\n        [0.6140],\n        [0.7862],\n        [0.9355],\n        [0.4863],\n        [0.5655],\n        [0.0436],\n        [0.0308],\n        [0.0309],\n        [0.0590],\n        [0.0157],\n        [0.0254],\n        [0.0145],\n        [0.0126],\n        [0.0280],\n        [0.0055],\n        [0.0610],\n        [0.6977],\n        [0.6296],\n        [0.1038],\n        [0.1875],\n        [0.3234],\n        [0.0475],\n        [0.0674],\n        [0.7886],\n        [0.0208],\n        [0.6382],\n        [0.6671],\n        [0.2659],\n        [0.1304],\n        [0.0330],\n        [0.1543],\n        [0.1324],\n        [0.0923],\n        [0.6481],\n        [0.4782],\n        [0.0137],\n        [0.4810],\n        [0.0242],\n        [0.0645],\n        [0.2477],\n        [0.0396],\n        [0.8759],\n        [0.0614],\n        [0.2093],\n        [0.2825],\n        [0.0732],\n        [0.0819],\n        [0.0994],\n        [0.0042],\n        [0.0639],\n        [0.0714],\n        [0.6146],\n        [0.5548],\n        [0.5720],\n        [0.0173],\n        [0.0950],\n        [0.9247],\n        [0.0979],\n        [0.5093],\n        [0.3201],\n        [0.0249],\n        [0.0115],\n        [0.8606],\n        [0.3280],\n        [0.2417],\n        [0.0065],\n        [0.0185],\n        [0.0386],\n        [0.0495],\n        [0.0566],\n        [0.8324],\n        [0.0054],\n        [0.1783],\n        [0.0084],\n        [0.1398],\n        [0.5463],\n        [0.5990],\n        [0.2893],\n        [0.2422],\n        [0.0921],\n        [0.0408],\n        [0.0046],\n        [0.0630],\n        [0.2670],\n        [0.0066],\n        [0.9150],\n        [0.5427],\n        [0.0191],\n        [0.1254],\n        [0.0957],\n        [0.3615],\n        [0.1072],\n        [0.2503],\n        [0.7266],\n        [0.0921],\n        [0.5816],\n        [0.4543],\n        [0.1018],\n        [0.9215],\n        [0.0393],\n        [0.1537],\n        [0.0449],\n        [0.8551],\n        [0.0236],\n        [0.3884],\n        [0.0032],\n        [0.3583],\n        [0.7975],\n        [0.1741],\n        [0.7627],\n        [0.0478],\n        [0.4580],\n        [0.0336],\n        [0.2849],\n        [0.2970],\n        [0.9362],\n        [0.2849],\n        [0.0084],\n        [0.3964],\n        [0.0635],\n        [0.4038],\n        [0.0035],\n        [0.2056],\n        [0.7709],\n        [0.2959],\n        [0.0089],\n        [0.1077],\n        [0.0589],\n        [0.5707],\n        [0.0962],\n        [0.0793],\n        [0.0674],\n        [0.0335],\n        [0.7049],\n        [0.8910],\n        [0.0836],\n        [0.3845],\n        [0.5714],\n        [0.7929],\n        [0.6587],\n        [0.5552],\n        [0.0545],\n        [0.8679],\n        [0.8363],\n        [0.0228],\n        [0.0075],\n        [0.0453],\n        [0.1558],\n        [0.0073],\n        [0.1370],\n        [0.4195],\n        [0.0152],\n        [0.1042],\n        [0.0471],\n        [0.1086],\n        [0.6888],\n        [0.0051],\n        [0.4637],\n        [0.9148],\n        [0.3310],\n        [0.0220],\n        [0.0207],\n        [0.1868],\n        [0.0823],\n        [0.2680],\n        [0.8058],\n        [0.9286],\n        [0.1681],\n        [0.1451],\n        [0.1451],\n        [0.0671],\n        [0.0082],\n        [0.1195],\n        [0.8149],\n        [0.0024],\n        [0.0062],\n        [0.6743],\n        [0.4562],\n        [0.6410],\n        [0.0143],\n        [0.0469],\n        [0.7715],\n        [0.4464],\n        [0.5066],\n        [0.1339],\n        [0.5612],\n        [0.5935],\n        [0.0075],\n        [0.0167],\n        [0.5136],\n        [0.0040],\n        [0.0411],\n        [0.5657],\n        [0.0792],\n        [0.0777],\n        [0.1271],\n        [0.0341],\n        [0.6242],\n        [0.0441],\n        [0.0383],\n        [0.7237],\n        [0.0076],\n        [0.3616],\n        [0.1653],\n        [0.2611],\n        [0.4183],\n        [0.0433],\n        [0.0674],\n        [0.7030],\n        [0.2125],\n        [0.0033],\n        [0.0105],\n        [0.5878],\n        [0.2345],\n        [0.7199],\n        [0.1536],\n        [0.0522],\n        [0.0208],\n        [0.1033],\n        [0.0328],\n        [0.5261],\n        [0.4477],\n        [0.4163],\n        [0.0638],\n        [0.0860],\n        [0.7403],\n        [0.3582],\n        [0.0382],\n        [0.0144],\n        [0.3440],\n        [0.1347],\n        [0.2414],\n        [0.9490],\n        [0.0595],\n        [0.9154],\n        [0.8628],\n        [0.0031],\n        [0.6427],\n        [0.0366],\n        [0.0776],\n        [0.3921],\n        [0.0062],\n        [0.8658],\n        [0.2734],\n        [0.7700],\n        [0.0047],\n        [0.6233],\n        [0.0978],\n        [0.0369],\n        [0.4760],\n        [0.4007],\n        [0.3911],\n        [0.1022],\n        [0.0443],\n        [0.6452],\n        [0.5626],\n        [0.1089],\n        [0.5314],\n        [0.0162],\n        [0.6265],\n        [0.8584],\n        [0.2519],\n        [0.2426],\n        [0.8181],\n        [0.5241],\n        [0.0241],\n        [0.0273],\n        [0.0294],\n        [0.3048],\n        [0.2815],\n        [0.1653],\n        [0.6694],\n        [0.0173],\n        [0.4302],\n        [0.2016],\n        [0.0251],\n        [0.1268],\n        [0.0075],\n        [0.5129],\n        [0.0309],\n        [0.0977],\n        [0.0888],\n        [0.1067],\n        [0.5135],\n        [0.0031],\n        [0.0084],\n        [0.0379],\n        [0.3944],\n        [0.0491],\n        [0.1070],\n        [0.0736],\n        [0.1812],\n        [0.9447],\n        [0.1928],\n        [0.1918],\n        [0.0043],\n        [0.1300],\n        [0.0261],\n        [0.1692],\n        [0.8610],\n        [0.8094],\n        [0.9622],\n        [0.5234],\n        [0.1729],\n        [0.0978],\n        [0.5527],\n        [0.0086],\n        [0.2799],\n        [0.3916],\n        [0.6266],\n        [0.3144],\n        [0.0426],\n        [0.0029],\n        [0.0657],\n        [0.4495],\n        [0.1725],\n        [0.1724],\n        [0.4098],\n        [0.0512],\n        [0.8470],\n        [0.1267],\n        [0.5247],\n        [0.1115],\n        [0.7646],\n        [0.7213],\n        [0.0759],\n        [0.3569],\n        [0.3375],\n        [0.8672],\n        [0.7596],\n        [0.1475],\n        [0.0044],\n        [0.4780],\n        [0.0581],\n        [0.0251],\n        [0.4568],\n        [0.2024],\n        [0.7446],\n        [0.6320],\n        [0.0314],\n        [0.0765],\n        [0.0181],\n        [0.1966],\n        [0.5667],\n        [0.8577],\n        [0.9013],\n        [0.1643],\n        [0.0776],\n        [0.1805],\n        [0.0631],\n        [0.1894],\n        [0.3725],\n        [0.0367],\n        [0.1032],\n        [0.8510],\n        [0.3935],\n        [0.8488],\n        [0.9634],\n        [0.0439],\n        [0.0918],\n        [0.5004],\n        [0.0232],\n        [0.2922],\n        [0.0592],\n        [0.6902],\n        [0.0502],\n        [0.2931],\n        [0.5467],\n        [0.0902],\n        [0.0438],\n        [0.7579],\n        [0.0144],\n        [0.0310],\n        [0.9566],\n        [0.0983],\n        [0.1246],\n        [0.0175],\n        [0.3579],\n        [0.1215],\n        [0.0063],\n        [0.8350],\n        [0.0783],\n        [0.0673],\n        [0.0044],\n        [0.1932],\n        [0.0166],\n        [0.0619],\n        [0.0126],\n        [0.0981],\n        [0.1415],\n        [0.0975],\n        [0.0028],\n        [0.7217],\n        [0.1342],\n        [0.0666],\n        [0.8163],\n        [0.5906],\n        [0.8264],\n        [0.0062],\n        [0.1267],\n        [0.0576],\n        [0.4529],\n        [0.1712],\n        [0.0178],\n        [0.1451],\n        [0.0375],\n        [0.1448],\n        [0.1852],\n        [0.9204],\n        [0.6197],\n        [0.0180],\n        [0.3411],\n        [0.0445],\n        [0.1539],\n        [0.4608],\n        [0.5943],\n        [0.2118],\n        [0.8577],\n        [0.0662],\n        [0.2722],\n        [0.6509],\n        [0.5460],\n        [0.4635],\n        [0.2292],\n        [0.0513],\n        [0.2066],\n        [0.5374],\n        [0.0059],\n        [0.2224],\n        [0.1254],\n        [0.3754],\n        [0.5258],\n        [0.0939],\n        [0.1778],\n        [0.2772],\n        [0.0344],\n        [0.0388],\n        [0.9426],\n        [0.1209],\n        [0.2947],\n        [0.0870],\n        [0.4037],\n        [0.0158]])\n\n\n\n# Sample class labels\nY = torch.distributions.Bernoulli(prob_Y).sample()\n\n\n# Convert to NumPy for visualization\nX1_np, X2_np, Y_np = X1.numpy(), X2.numpy(), Y.numpy()\n\n# Plot data points\nplt.scatter(X1_np[Y_np == 0], X2_np[Y_np == 0], color=\"blue\", label=\"Class 0\", alpha=0.5)\nplt.scatter(X1_np[Y_np == 1], X2_np[Y_np == 1], color=\"red\", label=\"Class 1\", alpha=0.5)\nplt.xlabel(\"Feature 1 (X1)\")\nplt.ylabel(\"Feature 2 (X2)\")\nplt.legend()\nplt.title(\"Generated 2D Logistic Regression Data\")\n\nText(0.5, 1.0, 'Generated 2D Logistic Regression Data')\n\n\n\n\n\n\n\n\n\n\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Stack X1 and X2 into a single tensor\nX_train = torch.cat((X1, X2), dim=1)\n\n# Define logistic regression model\nclass LogisticRegression2D(nn.Module):\n    def __init__(self):\n        super(LogisticRegression2D, self).__init__()\n        self.linear = nn.Linear(2, 1)  # Two inputs, one output\n\n    def forward(self, x):\n        return torch.sigmoid(self.linear(x))  # Sigmoid activation\n\n# Initialize model\nmodel = LogisticRegression2D()\nloss_fn = nn.BCELoss()  # Binary cross-entropy loss\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop\nepochs = 1000\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    Y_pred = model(X_train)  # Forward pass\n    loss = loss_fn(Y_pred, Y.view(-1, 1))  # Compute loss\n    loss.backward()  # Backpropagation\n    optimizer.step()  # Update weights\n\n    if epoch % 200 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\n# Extract learned parameters\nw1_learned, w2_learned = model.linear.weight[0].detach().numpy()\nb_learned = model.linear.bias[0].detach().numpy()\nprint(f\"Learned Parameters: w1 = {w1_learned:.4f}, w2 = {w2_learned:.4f}, b = {b_learned:.4f}\")\n\nEpoch 0, Loss: 2.0864\nEpoch 200, Loss: 0.4884\nEpoch 400, Loss: 0.4536\nEpoch 600, Loss: 0.4395\nEpoch 800, Loss: 0.4318\nLearned Parameters: w1 = 0.6339, w2 = -0.8716, b = -0.4170\n\n\n\ndef plot_decision_boundary(model, X1_np, X2_np, Y_np, w1_true, w2_true, b_true):\n    \"\"\"Plots the true and learned decision boundaries.\"\"\"\n    \n    # Generate mesh grid\n    x1_vals = np.linspace(0, 5, 100)\n    x2_vals = np.linspace(0, 5, 100)\n    X1_grid, X2_grid = np.meshgrid(x1_vals, x2_vals)\n\n    # Compute model's learned decision boundary\n    with torch.no_grad():\n        Z = model(torch.tensor(np.c_[X1_grid.ravel(), X2_grid.ravel()], dtype=torch.float32))\n        Z = Z.view(X1_grid.shape).numpy()\n\n    # Compute true decision boundary\n    x1_boundary = np.linspace(0,5, 100)\n    x2_boundary_true = - (w1_true / w2_true) * x1_boundary - (b_true / w2_true)\n\n    # Plot data points\n    plt.scatter(X1_np[Y_np == 0], X2_np[Y_np == 0], color=\"blue\", label=\"Class 0\", alpha=0.5)\n    plt.scatter(X1_np[Y_np == 1], X2_np[Y_np == 1], color=\"red\", label=\"Class 1\", alpha=0.5)\n\n    # Plot learned decision boundary\n    plt.contour(X1_grid, X2_grid, Z, levels=[0.5], colors=\"black\", linestyles=\"dashed\", label=\"Learned Boundary\")\n\n    # Plot true decision boundary\n    plt.plot(x1_boundary, x2_boundary_true, color=\"green\", linestyle=\"solid\", label=\"True Boundary\")\n\n    plt.xlabel(\"Feature 1 (X1)\")\n    plt.ylabel(\"Feature 2 (X2)\")\n    plt.legend()\n    plt.title(\"Logistic Regression Decision Boundary\")\n    plt.ylim([0, 5])\n\n\n# Call the function with true and learned parameters\nplot_decision_boundary(model, X1_np, X2_np, Y_np, w1_true=1.2, w2_true=-0.8, b_true=-2.5)",
    "crumbs": [
      "Home",
      "Machine Learning",
      "PMF and their applications"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to PSDV Teaching Resources",
    "section": "",
    "text": "Thank you for your interest in contributing to this educational project! We welcome contributions that help improve the learning experience for students of probability, statistics, and data visualization.\n\n\n\n\n\nBugs: Found an error in a notebook? Report it!\nTypos: Mathematical notation or text corrections\nSuggestions: Ideas for improving explanations or examples\nMissing Content: Topics that could be added or expanded\n\n\n\n\n\nFix Errors: Correct mathematical mistakes or code bugs\nEnhance Explanations: Make concepts clearer or more accessible\nAdd Examples: Provide additional practical applications\nImprove Visualizations: Better plots or interactive demonstrations\n\n\n\n\n\nNew Notebooks: Additional topics within the course scope\nInteractive Widgets: Enhance learning with interactive elements\nReal-World Applications: Connect theory to practical problems\nAssessment Materials: Quizzes and exercises\n\n\n\n\n\n\n\n\nPython 3.8+ with Jupyter\nFamiliarity with NumPy, Matplotlib, PyTorch\nBasic understanding of probability and statistics\nExperience with Markdown and LaTeX for mathematical notation\n\n\n\n\n\nFork the repository\nClone your fork: git clone https://github.com/yourusername/psdv-teaching.git\nInstall dependencies: pip install -r requirements.txt\nCreate a feature branch: git checkout -b feature-name\nMake your changes\nTest thoroughly\nSubmit a pull request\n\n\n\n\n\n\n\n\nPython Code: Follow PEP 8 style guidelines\nNotebook Structure: Include clear learning objectives and summaries\nComments: Explain complex mathematical concepts or algorithms\nReproducibility: Ensure all code runs without errors\n\n\n\n\n\nAccuracy: All mathematical statements must be correct\nNotation: Use consistent mathematical notation throughout\nRigor: Balance mathematical rigor with accessibility\nCitations: Cite sources for non-trivial mathematical results\n\n\n\n\n\nClear Explanations: Write for undergraduate-level understanding\nExamples: Include concrete examples for abstract concepts\nLearning Objectives: State what students will learn\nPrerequisites: Mention required background knowledge\n\n\n\n\n\nMetadata: Include proper YAML frontmatter with title, description, categories\nStructure: Use markdown cells for explanations, code cells for implementation\nOutputs: Include cell outputs for key visualizations and results\nSelf-Contained: Each notebook should be complete and runnable\n\n\n\n\n\n\n\n\nTest Your Changes: Run all code cells and verify outputs\nCheck Formatting: Ensure proper markdown and code formatting\nReview Content: Verify mathematical accuracy and clarity\nUpdate Documentation: Modify README or other docs if needed\n\n\n\n\n## Description\nBrief description of changes made\n\n## Type of Change\n- [ ] Bug fix\n- [ ] New content\n- [ ] Enhancement to existing content\n- [ ] Documentation update\n\n## Testing\n- [ ] All code runs without errors\n- [ ] Mathematical content verified\n- [ ] Notebook outputs updated\n\n## Additional Notes\nAny additional context or considerations\n\n\n\n\nAutomated Checks: GitHub Actions will verify notebook structure\nContent Review: Maintainers will check mathematical accuracy\nEducational Review: Assessment of pedagogical value\nFinal Approval: Merge after all checks pass\n\n\n\n\n\n\n\n\nClarity: Use clear, concise language\nAccessibility: Write for diverse educational backgrounds\nEngagement: Make content interesting and motivating\nStructure: Use headings, lists, and emphasis effectively\n\n\n\n\n\nLaTeX: Use proper LaTeX for all mathematical expressions\nConsistency: Follow established notation throughout\nDefinitions: Clearly define new mathematical symbols\nFormatting: Use inline math $...$ and display math $$...$$ appropriately\n\n\n\n\n\nReadability: Write clean, well-commented code\nEfficiency: Use vectorized operations where appropriate\nEducational Value: Code should illustrate concepts clearly\nError Handling: Include appropriate error checking\n\n\n\n\n\nContributors will be acknowledged in: - README: Listed in acknowledgments section - Individual Notebooks: Credit for significant contributions - Course Materials: Recognition in course documentation\n\n\n\n\nGitHub Issues: Technical questions or suggestions\nEmail: Contact Prof. Nipun Batra at nipun.batra@iitgn.ac.in\nCourse Website: Visit https://nipunbatra.github.io/psdv25/\n\n\n\n\nThis project follows academic standards of respectful collaboration: - Respectful Communication: Be courteous in all interactions - Constructive Feedback: Provide helpful, specific suggestions - Academic Integrity: Respect intellectual property and give proper credit - Inclusive Environment: Welcome contributors from all backgrounds\n\n\n\nBy contributing, you agree that your contributions will be licensed under the MIT License.\n\nThank you for helping make these educational resources better for everyone!"
  },
  {
    "objectID": "CONTRIBUTING.html#ways-to-contribute",
    "href": "CONTRIBUTING.html#ways-to-contribute",
    "title": "Contributing to PSDV Teaching Resources",
    "section": "",
    "text": "Bugs: Found an error in a notebook? Report it!\nTypos: Mathematical notation or text corrections\nSuggestions: Ideas for improving explanations or examples\nMissing Content: Topics that could be added or expanded\n\n\n\n\n\nFix Errors: Correct mathematical mistakes or code bugs\nEnhance Explanations: Make concepts clearer or more accessible\nAdd Examples: Provide additional practical applications\nImprove Visualizations: Better plots or interactive demonstrations\n\n\n\n\n\nNew Notebooks: Additional topics within the course scope\nInteractive Widgets: Enhance learning with interactive elements\nReal-World Applications: Connect theory to practical problems\nAssessment Materials: Quizzes and exercises"
  },
  {
    "objectID": "CONTRIBUTING.html#getting-started",
    "href": "CONTRIBUTING.html#getting-started",
    "title": "Contributing to PSDV Teaching Resources",
    "section": "",
    "text": "Python 3.8+ with Jupyter\nFamiliarity with NumPy, Matplotlib, PyTorch\nBasic understanding of probability and statistics\nExperience with Markdown and LaTeX for mathematical notation\n\n\n\n\n\nFork the repository\nClone your fork: git clone https://github.com/yourusername/psdv-teaching.git\nInstall dependencies: pip install -r requirements.txt\nCreate a feature branch: git checkout -b feature-name\nMake your changes\nTest thoroughly\nSubmit a pull request"
  },
  {
    "objectID": "CONTRIBUTING.html#contribution-guidelines",
    "href": "CONTRIBUTING.html#contribution-guidelines",
    "title": "Contributing to PSDV Teaching Resources",
    "section": "",
    "text": "Python Code: Follow PEP 8 style guidelines\nNotebook Structure: Include clear learning objectives and summaries\nComments: Explain complex mathematical concepts or algorithms\nReproducibility: Ensure all code runs without errors\n\n\n\n\n\nAccuracy: All mathematical statements must be correct\nNotation: Use consistent mathematical notation throughout\nRigor: Balance mathematical rigor with accessibility\nCitations: Cite sources for non-trivial mathematical results\n\n\n\n\n\nClear Explanations: Write for undergraduate-level understanding\nExamples: Include concrete examples for abstract concepts\nLearning Objectives: State what students will learn\nPrerequisites: Mention required background knowledge\n\n\n\n\n\nMetadata: Include proper YAML frontmatter with title, description, categories\nStructure: Use markdown cells for explanations, code cells for implementation\nOutputs: Include cell outputs for key visualizations and results\nSelf-Contained: Each notebook should be complete and runnable"
  },
  {
    "objectID": "CONTRIBUTING.html#pull-request-process",
    "href": "CONTRIBUTING.html#pull-request-process",
    "title": "Contributing to PSDV Teaching Resources",
    "section": "",
    "text": "Test Your Changes: Run all code cells and verify outputs\nCheck Formatting: Ensure proper markdown and code formatting\nReview Content: Verify mathematical accuracy and clarity\nUpdate Documentation: Modify README or other docs if needed\n\n\n\n\n## Description\nBrief description of changes made\n\n## Type of Change\n- [ ] Bug fix\n- [ ] New content\n- [ ] Enhancement to existing content\n- [ ] Documentation update\n\n## Testing\n- [ ] All code runs without errors\n- [ ] Mathematical content verified\n- [ ] Notebook outputs updated\n\n## Additional Notes\nAny additional context or considerations\n\n\n\n\nAutomated Checks: GitHub Actions will verify notebook structure\nContent Review: Maintainers will check mathematical accuracy\nEducational Review: Assessment of pedagogical value\nFinal Approval: Merge after all checks pass"
  },
  {
    "objectID": "CONTRIBUTING.html#style-guidelines",
    "href": "CONTRIBUTING.html#style-guidelines",
    "title": "Contributing to PSDV Teaching Resources",
    "section": "",
    "text": "Clarity: Use clear, concise language\nAccessibility: Write for diverse educational backgrounds\nEngagement: Make content interesting and motivating\nStructure: Use headings, lists, and emphasis effectively\n\n\n\n\n\nLaTeX: Use proper LaTeX for all mathematical expressions\nConsistency: Follow established notation throughout\nDefinitions: Clearly define new mathematical symbols\nFormatting: Use inline math $...$ and display math $$...$$ appropriately\n\n\n\n\n\nReadability: Write clean, well-commented code\nEfficiency: Use vectorized operations where appropriate\nEducational Value: Code should illustrate concepts clearly\nError Handling: Include appropriate error checking"
  },
  {
    "objectID": "CONTRIBUTING.html#recognition",
    "href": "CONTRIBUTING.html#recognition",
    "title": "Contributing to PSDV Teaching Resources",
    "section": "",
    "text": "Contributors will be acknowledged in: - README: Listed in acknowledgments section - Individual Notebooks: Credit for significant contributions - Course Materials: Recognition in course documentation"
  },
  {
    "objectID": "CONTRIBUTING.html#questions",
    "href": "CONTRIBUTING.html#questions",
    "title": "Contributing to PSDV Teaching Resources",
    "section": "",
    "text": "GitHub Issues: Technical questions or suggestions\nEmail: Contact Prof. Nipun Batra at nipun.batra@iitgn.ac.in\nCourse Website: Visit https://nipunbatra.github.io/psdv25/"
  },
  {
    "objectID": "CONTRIBUTING.html#code-of-conduct",
    "href": "CONTRIBUTING.html#code-of-conduct",
    "title": "Contributing to PSDV Teaching Resources",
    "section": "",
    "text": "This project follows academic standards of respectful collaboration: - Respectful Communication: Be courteous in all interactions - Constructive Feedback: Provide helpful, specific suggestions - Academic Integrity: Respect intellectual property and give proper credit - Inclusive Environment: Welcome contributors from all backgrounds"
  },
  {
    "objectID": "CONTRIBUTING.html#license",
    "href": "CONTRIBUTING.html#license",
    "title": "Contributing to PSDV Teaching Resources",
    "section": "",
    "text": "By contributing, you agree that your contributions will be licensed under the MIT License.\n\nThank you for helping make these educational resources better for everyone!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability, Statistics & Data Visualization",
    "section": "",
    "text": "This comprehensive collection of interactive Jupyter notebooks provides educational materials for Probability, Statistics, and Data Visualization. Developed by Prof. Nipun Batra at Indian Institute of Technology, Gandhinagar, these resources have evolved over multiple years with contributions from excellent teaching assistants."
  },
  {
    "objectID": "index.html#welcome-to-psdv-teaching-resources",
    "href": "index.html#welcome-to-psdv-teaching-resources",
    "title": "Probability, Statistics & Data Visualization",
    "section": "",
    "text": "This comprehensive collection of interactive Jupyter notebooks provides educational materials for Probability, Statistics, and Data Visualization. Developed by Prof. Nipun Batra at Indian Institute of Technology, Gandhinagar, these resources have evolved over multiple years with contributions from excellent teaching assistants."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Probability, Statistics & Data Visualization",
    "section": "Course Overview",
    "text": "Course Overview\nES 114: Probability, Statistics and Data Visualization is an undergraduate course designed to provide students with foundational knowledge in:\n\nProbability Theory: From basic concepts to advanced distributions\nStatistical Analysis: Expectation, variance, and key theorems\nData Visualization: Creating meaningful insights from data\nPractical Applications: Real-world datasets and machine learning concepts\n\n\nCourse Information\n\nInstitution: Indian Institute of Technology, Gandhinagar\nLevel: Undergraduate (No prerequisites required)\nFormat: Interactive Jupyter notebooks with theoretical explanations and practical implementations\nCurrent Course: ES 114 - Spring 2025"
  },
  {
    "objectID": "index.html#what-youll-find-here",
    "href": "index.html#what-youll-find-here",
    "title": "Probability, Statistics & Data Visualization",
    "section": "What You’ll Find Here",
    "text": "What You’ll Find Here\n\n\n\nInteractive Notebooks\n24 comprehensive Jupyter notebooks covering the complete spectrum from basic probability to advanced machine learning topics like PCA and neural networks.\n\n\nRich Visualizations\nHigh-quality matplotlib plots, interactive widgets, and 3D visualizations to help understand complex mathematical concepts.\n\n\nMathematical Rigor\nProper mathematical notation, derivations, and proofs alongside practical implementations using Python.\n\n\nReal Applications\nWork with actual datasets including MNIST digits, height-weight data, and word embeddings to see theory in action."
  },
  {
    "objectID": "index.html#quick-navigation",
    "href": "index.html#quick-navigation",
    "title": "Probability, Statistics & Data Visualization",
    "section": "Quick Navigation",
    "text": "Quick Navigation\n\n\n\nGetting Started\n\nIntroduction\nSet Theory\nProbability Basics\n\n\n\nCore Topics\n\nRandom Variables\nDistributions\nExpectation\n\n\n\nAdvanced Topics\n\n2D Distributions\nPrincipal Component Analysis\nMachine Learning"
  },
  {
    "objectID": "index.html#features",
    "href": "index.html#features",
    "title": "Probability, Statistics & Data Visualization",
    "section": "Features",
    "text": "Features\n\nInteractive Learning Environment\n\nAll notebooks can be run locally or viewed online with full interactivity\n\nSelf-Contained Modules\n\nEach notebook includes learning objectives, theoretical background, and practical exercises\n\nModern Tools\n\nBuilt using Jupyter, PyTorch, NumPy, Matplotlib, and other state-of-the-art data science tools\n\nOpen Source\n\nAll materials are freely available and contributions are welcome"
  },
  {
    "objectID": "index.html#getting-started-1",
    "href": "index.html#getting-started-1",
    "title": "Probability, Statistics & Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\n\nBrowse Online: Explore all materials directly in your browser\nClone Repository: git clone https://github.com/nipunbatra/psdv-teaching.git\nInstall Dependencies: pip install -r requirements.txt\nLaunch Jupyter: jupyter lab notebooks/\n\n\nThis resource is continuously updated based on student feedback and advances in data science education."
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Interactive Notebooks",
    "section": "",
    "text": "These notebooks are designed to be followed in a logical sequence, building from fundamental concepts to advanced applications. Each notebook is self-contained but references and builds upon previous concepts.\n\n\n\n\n\n\nEssential mathematical and programming foundations for probability and statistics.\n\nIntroduction to Probability: Random number generation, basic probability concepts\nSet Theory: Mathematical foundations, set operations, Venn diagrams\n\nProbability Fundamentals: Sample spaces, events, basic probability rules\n\n\n\n\nCore concepts of random variables and their distributions.\n\nRandom Variables: Definition, types, and basic properties\nPMF for Discrete Variables: Probability mass functions, discrete distributions\nPDF for Continuous Variables: Probability density functions, continuous distributions\nCumulative Distribution Functions: CDFs and inverse sampling methods\nCDF for Discrete Variables: Discrete CDFs and applications\n\n\n\n\nKey statistical theorems and concepts.\n\nExpectation and Variance: Expected values, variance, moments\nIndependent and Identically Distributed: IID random variables and their properties\nLaw of Large Numbers: LLN, Central Limit Theorem demonstrations\n\n\n\n\nAdvanced topics involving multiple variables.\n\n2D Distributions: Bivariate normal, joint distributions\nJoint Distribution Properties: Covariance, correlation, independence\nSum of Random Variables: Convolution, sums of independent variables\nRandom Vectors: Multivariate distributions and operations\n\n\n\n\nPractical applications of probability and statistics in machine learning.\n\nPrincipal Component Analysis: Dimensionality reduction, eigenvalue decomposition\nLogistic Regression: Classification, generative vs discriminative models\nWord Embeddings and Angles: Vector representations, cosine similarity\nImage Joint Distributions: Computer vision applications\n\n\n\n\nEssential programming tools for data science.\n\nNumPy Introduction: Array operations, broadcasting, linear algebra\nPandas Introduction: Data manipulation, analysis, and visualization\nMatplotlib Introduction: Creating publication-quality plots\n\n\n\n\nHands-on learning and self-assessment tools.\n\nInteractive Widgets: Dynamic visualizations and explorations\nQuiz 1: Self-assessment questions and problems"
  },
  {
    "objectID": "notebooks.html#learning-pathway",
    "href": "notebooks.html#learning-pathway",
    "title": "Interactive Notebooks",
    "section": "",
    "text": "These notebooks are designed to be followed in a logical sequence, building from fundamental concepts to advanced applications. Each notebook is self-contained but references and builds upon previous concepts.\n\n\n\n\n\n\nEssential mathematical and programming foundations for probability and statistics.\n\nIntroduction to Probability: Random number generation, basic probability concepts\nSet Theory: Mathematical foundations, set operations, Venn diagrams\n\nProbability Fundamentals: Sample spaces, events, basic probability rules\n\n\n\n\nCore concepts of random variables and their distributions.\n\nRandom Variables: Definition, types, and basic properties\nPMF for Discrete Variables: Probability mass functions, discrete distributions\nPDF for Continuous Variables: Probability density functions, continuous distributions\nCumulative Distribution Functions: CDFs and inverse sampling methods\nCDF for Discrete Variables: Discrete CDFs and applications\n\n\n\n\nKey statistical theorems and concepts.\n\nExpectation and Variance: Expected values, variance, moments\nIndependent and Identically Distributed: IID random variables and their properties\nLaw of Large Numbers: LLN, Central Limit Theorem demonstrations\n\n\n\n\nAdvanced topics involving multiple variables.\n\n2D Distributions: Bivariate normal, joint distributions\nJoint Distribution Properties: Covariance, correlation, independence\nSum of Random Variables: Convolution, sums of independent variables\nRandom Vectors: Multivariate distributions and operations\n\n\n\n\nPractical applications of probability and statistics in machine learning.\n\nPrincipal Component Analysis: Dimensionality reduction, eigenvalue decomposition\nLogistic Regression: Classification, generative vs discriminative models\nWord Embeddings and Angles: Vector representations, cosine similarity\nImage Joint Distributions: Computer vision applications\n\n\n\n\nEssential programming tools for data science.\n\nNumPy Introduction: Array operations, broadcasting, linear algebra\nPandas Introduction: Data manipulation, analysis, and visualization\nMatplotlib Introduction: Creating publication-quality plots\n\n\n\n\nHands-on learning and self-assessment tools.\n\nInteractive Widgets: Dynamic visualizations and explorations\nQuiz 1: Self-assessment questions and problems"
  },
  {
    "objectID": "notebooks.html#notebook-features",
    "href": "notebooks.html#notebook-features",
    "title": "Interactive Notebooks",
    "section": "Notebook Features",
    "text": "Notebook Features\nEach notebook includes:\n\nLearning Objectives: Clear goals for what you’ll learn\nMathematical Theory: Rigorous mathematical explanations with proper notation\nPractical Implementation: Python code demonstrating concepts\nVisualizations: High-quality plots and interactive demonstrations\nReal Data: Applications using actual datasets\nExercises: Problems to test understanding"
  },
  {
    "objectID": "notebooks.html#technical-requirements",
    "href": "notebooks.html#technical-requirements",
    "title": "Interactive Notebooks",
    "section": "Technical Requirements",
    "text": "Technical Requirements\n\nPython 3.8+ with Jupyter Notebook or JupyterLab\nCore packages: NumPy, Matplotlib, Pandas, PyTorch, Scikit-learn\nOptional: Interactive widgets for enhanced visualizations\n\nSee the README for complete installation instructions.\n\nBrowse the complete collection below. Use the search and filter options to find specific topics."
  },
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "bivariate-derivation.pdf\n\n\n7/13/25, 2:36:48 PM\n\n\n\n\n\n\n\n\n\n\n\nexpectation.pdf\n\n\n7/13/25, 2:36:48 PM\n\n\n\n\n\n\n\n\n\n\n\nintegration.pdf\n\n\n7/13/25, 2:36:48 PM\n\n\n\n\n\n\n\n\n\n\n\npca.pdf\n\n\n7/13/25, 2:36:48 PM\n\n\n\n\n\nNo matching items\nReuseMITCopyrightCopyright 2024-2025, Prof. Nipun Batra, IIT GandhinagarCitationBibTeX citation:@online{batra,\n  author = {Batra, Nipun},\n  title = {PSDV {Teaching} {Resources:} {Interactive} {Notebooks} for\n    {Probability,} {Statistics,} and {Data} {Visualization}},\n  url = {https://nipunbatra.github.io/psdv-teaching/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nBatra, Nipun. n.d. “PSDV Teaching Resources: Interactive Notebooks\nfor Probability, Statistics, and Data Visualization.” https://nipunbatra.github.io/psdv-teaching/."
  },
  {
    "objectID": "questions.html",
    "href": "questions.html",
    "title": "Prerequisite",
    "section": "",
    "text": "Instructions:\n\nSolve the following questions to assess your understanding of Python basics, data structures, and statistics.\nCreate a single Juptyer notebook with solutions to all questions.\nInclude output for each code cell.\nAdd explanations and comments to your code where necessary using Markdown cells.\nDO NOT cheat or copy solutions from the internet. The purpose of this exercise is to evaluate your current knowledge and skills and to help you prepare for the course.\n\n\n\n\nList Operations and Basic Statistics:\n\nCreate a list of the first 20 natural numbers.\nCompute the sum of all even numbers in the list.\nCompute the product of all odd numbers in the list.\nCalculate the mean, median, and standard deviation of the list.\n\nMutable vs. Immutable Data Types:\n\nExplain the difference between mutable and immutable data types in Python. Provide examples of each.\n\nFibonacci Sequence:\n\nWrite a Python function that takes a number n and returns a list of the first n Fibonacci numbers.\nPlot the Fibonacci sequence using a line graph.\n\nList Manipulation:\n\nGiven a list [3, 5, 7, 9, 11, 13]:\n\nInsert 6 at the 3rd index.\nRemove the number 7 from the list.\nReverse the list.\nCalculate the mean and median of the modified list.\n\n\nUser Input and Basic Statistics:\n\nWrite a program to read a comma-separated list of numbers from the user (e.g., “10, 20, 30, 40”).\nCalculate the mean of the numbers.\nIdentify and print the smallest and largest numbers.\nPlot a histogram of the numbers.\n\nPalindrome Check:\n\nWrite a program to check if a string is a palindrome (ignoring spaces and case).\nInput: \"A man a plan a canal Panama\"\nOutput: True\n\nDictionary Operations:\n\nCreate a dictionary of student names (keys) and their marks (values).\nCompute the average marks.\nPrint the names of students scoring above the average.\nVisualize the distribution of marks using a bar chart.\n\nTuple List Creation:\n\nCreate a list of tuples, where each tuple contains a number and its square (e.g., [(1, 1), (2, 4), (3, 9), ...] for numbers 1 to 10).\nPlot the numbers and their squares on a scatter plot.\n\nMerging Dictionaries:\n\nWrite Python code to merge two dictionaries. If a key is present in both, sum their values.\nExample:\ndict1 = {'a': 10, 'b': 20}\ndict2 = {'b': 5, 'c': 15}\nOutput: {'a': 10, 'b': 25, 'c': 15}\n\nCharacter Frequency:\n\nWrite a function that accepts a string and:\n\nConverts all characters to lowercase.\nCounts the frequency of each character.\nReturns a dictionary with characters as keys and their counts as values.\nVisualize the character frequencies using a bar chart.\n\n\nRemove Duplicates:\n\nWrite a program to remove duplicate elements from a list while maintaining the order of elements.\nCalculate the mean and median of the unique elements.\n\nString Operations:\n\nGiven a list of strings, write a Python function to:\n\nFind the longest string.\nCount how many strings start with a vowel.\nVisualize the distribution of string lengths using a histogram.\n\n\nSorting Dictionaries:\n\nWrite a function to sort a list of dictionaries by a key.\nExample:\ndata = [{'name': 'Alice', 'age': 25}, {'name': 'Bob', 'age': 22}]\nSort by age → [{'name': 'Bob', 'age': 22}, {'name': 'Alice', 'age': 25}]\n\nWord Filtering and Sorting:\n\nWrite a program to:\n\nSplit a string into a list of words.\nRemove all words that are shorter than 4 characters.\nSort the remaining words alphabetically.\nVisualize the word lengths using a bar chart.\n\n\nAnagram Finder:\n\nWrite a Python function to find all anagrams of a word in a given list.\nInput: word = \"listen\", words = [\"enlist\", \"google\", \"inlets\", \"banana\"]\nOutput: [\"enlist\", \"inlets\"]\n\nSet Operations:\n\nGiven two lists, write Python code to find their intersection, union, and symmetric difference.\nVisualize the results using Venn diagrams.\n\nFile Operations:\n\nWrite Python code to:\n\nCreate a text file.\nWrite a list of numbers to the file (one per line).\nRead the file and compute their sum.\n\n\nWord Count and Frequency:\n\nWrite a program to:\n\nRead a file and count the number of words in it.\nFind the most frequent word.\nVisualize the word frequencies using a bar chart.\n\n\nRandom Number Generation:\n\nWrite a Python program to:\n\nGenerate 100 random integers between 1 and 50.\nSave them in a text file, one per line.\nRead the file and compute the mean of the numbers.\nVisualize the distribution of the numbers using a histogram.\n\n\nCSV File Operations:\n\nWrite Python code to read a CSV file and compute the sum of values in a specific column.\nVisualize the data using a line plot.\n\nBasic Statistics:\n\nWrite Python code to calculate:\n\nMean, median, mode, and standard deviation of a list of numbers.\nVisualize the data using a box plot.\n\n\nNormalization:\n\nWrite a Python function to normalize a list of numbers to a range of 0 to 1.\nVisualize the normalized data using a line plot.\n\nEven or Odd:\n\nWrite a Python program that takes an integer input from the user and prints whether the number is even or odd.\n\nGrade Calculator:\n\nWrite a Python program that takes a numerical grade (0-100) as input and prints the corresponding letter grade (A, B, C, D, F).\n\nLeap Year Checker:\n\nWrite a Python program that takes a year as input and checks if it is a leap year."
  },
  {
    "objectID": "questions.html#python-basics-lists-strings-dictionaries-file-handling-statistics-and-flow-control",
    "href": "questions.html#python-basics-lists-strings-dictionaries-file-handling-statistics-and-flow-control",
    "title": "Prerequisite",
    "section": "",
    "text": "List Operations and Basic Statistics:\n\nCreate a list of the first 20 natural numbers.\nCompute the sum of all even numbers in the list.\nCompute the product of all odd numbers in the list.\nCalculate the mean, median, and standard deviation of the list.\n\nMutable vs. Immutable Data Types:\n\nExplain the difference between mutable and immutable data types in Python. Provide examples of each.\n\nFibonacci Sequence:\n\nWrite a Python function that takes a number n and returns a list of the first n Fibonacci numbers.\nPlot the Fibonacci sequence using a line graph.\n\nList Manipulation:\n\nGiven a list [3, 5, 7, 9, 11, 13]:\n\nInsert 6 at the 3rd index.\nRemove the number 7 from the list.\nReverse the list.\nCalculate the mean and median of the modified list.\n\n\nUser Input and Basic Statistics:\n\nWrite a program to read a comma-separated list of numbers from the user (e.g., “10, 20, 30, 40”).\nCalculate the mean of the numbers.\nIdentify and print the smallest and largest numbers.\nPlot a histogram of the numbers.\n\nPalindrome Check:\n\nWrite a program to check if a string is a palindrome (ignoring spaces and case).\nInput: \"A man a plan a canal Panama\"\nOutput: True\n\nDictionary Operations:\n\nCreate a dictionary of student names (keys) and their marks (values).\nCompute the average marks.\nPrint the names of students scoring above the average.\nVisualize the distribution of marks using a bar chart.\n\nTuple List Creation:\n\nCreate a list of tuples, where each tuple contains a number and its square (e.g., [(1, 1), (2, 4), (3, 9), ...] for numbers 1 to 10).\nPlot the numbers and their squares on a scatter plot.\n\nMerging Dictionaries:\n\nWrite Python code to merge two dictionaries. If a key is present in both, sum their values.\nExample:\ndict1 = {'a': 10, 'b': 20}\ndict2 = {'b': 5, 'c': 15}\nOutput: {'a': 10, 'b': 25, 'c': 15}\n\nCharacter Frequency:\n\nWrite a function that accepts a string and:\n\nConverts all characters to lowercase.\nCounts the frequency of each character.\nReturns a dictionary with characters as keys and their counts as values.\nVisualize the character frequencies using a bar chart.\n\n\nRemove Duplicates:\n\nWrite a program to remove duplicate elements from a list while maintaining the order of elements.\nCalculate the mean and median of the unique elements.\n\nString Operations:\n\nGiven a list of strings, write a Python function to:\n\nFind the longest string.\nCount how many strings start with a vowel.\nVisualize the distribution of string lengths using a histogram.\n\n\nSorting Dictionaries:\n\nWrite a function to sort a list of dictionaries by a key.\nExample:\ndata = [{'name': 'Alice', 'age': 25}, {'name': 'Bob', 'age': 22}]\nSort by age → [{'name': 'Bob', 'age': 22}, {'name': 'Alice', 'age': 25}]\n\nWord Filtering and Sorting:\n\nWrite a program to:\n\nSplit a string into a list of words.\nRemove all words that are shorter than 4 characters.\nSort the remaining words alphabetically.\nVisualize the word lengths using a bar chart.\n\n\nAnagram Finder:\n\nWrite a Python function to find all anagrams of a word in a given list.\nInput: word = \"listen\", words = [\"enlist\", \"google\", \"inlets\", \"banana\"]\nOutput: [\"enlist\", \"inlets\"]\n\nSet Operations:\n\nGiven two lists, write Python code to find their intersection, union, and symmetric difference.\nVisualize the results using Venn diagrams.\n\nFile Operations:\n\nWrite Python code to:\n\nCreate a text file.\nWrite a list of numbers to the file (one per line).\nRead the file and compute their sum.\n\n\nWord Count and Frequency:\n\nWrite a program to:\n\nRead a file and count the number of words in it.\nFind the most frequent word.\nVisualize the word frequencies using a bar chart.\n\n\nRandom Number Generation:\n\nWrite a Python program to:\n\nGenerate 100 random integers between 1 and 50.\nSave them in a text file, one per line.\nRead the file and compute the mean of the numbers.\nVisualize the distribution of the numbers using a histogram.\n\n\nCSV File Operations:\n\nWrite Python code to read a CSV file and compute the sum of values in a specific column.\nVisualize the data using a line plot.\n\nBasic Statistics:\n\nWrite Python code to calculate:\n\nMean, median, mode, and standard deviation of a list of numbers.\nVisualize the data using a box plot.\n\n\nNormalization:\n\nWrite a Python function to normalize a list of numbers to a range of 0 to 1.\nVisualize the normalized data using a line plot.\n\nEven or Odd:\n\nWrite a Python program that takes an integer input from the user and prints whether the number is even or odd.\n\nGrade Calculator:\n\nWrite a Python program that takes a numerical grade (0-100) as input and prints the corresponding letter grade (A, B, C, D, F).\n\nLeap Year Checker:\n\nWrite a Python program that takes a year as input and checks if it is a leap year."
  },
  {
    "objectID": "notebooks/intro.html",
    "href": "notebooks/intro.html",
    "title": "Introduction to Probability and Statistics",
    "section": "",
    "text": "This notebook introduces fundamental concepts in probability theory and demonstrates how to generate random numbers using Python. We’ll explore both built-in random number generators and implement our own simple pseudo-random number generator (PRNG).\n\n\n\nUnderstand probability distributions and sampling\nLearn how random number generation works\nImplement a simple PRNG\nCompare different random number generators\n\nLet’s start by importing the necessary libraries:\n\n\n\nLet’s start with a simple example of creating a probability distribution over characters. We’ll create a categorical distribution where certain vowels have higher probabilities:\n\nfirst_4_chars = \"appl\"\n\nprobs = np.zeros(27) # 26 letters + space (EOS)\n\n# Store i to s\nitos = list(\"abcdefghijklmnopqrstuvwxyz \")\n\n# stoi \nstoi = {char: i for i, char in enumerate(itos)}\n\n# Fill in the probabilities\nprobs[stoi[\"a\"]] = 0.1\nprobs[stoi[\"e\"]] = 0.3\nprobs[stoi[\"i\"]] = 0.2\nprobs[stoi[\"o\"]] = 0.1\nprobs[stoi[\"u\"]] = 0.05\nprobs[stoi[\" \"]] = 0.05\n\nprobability_distribution = torch.distributions.Categorical(torch.tensor(probs))\n\nNow let’s sample from this distribution to see which character gets selected:\n\nsample = probability_distribution.sample()\nprint(f\"Sampled character: {itos[sample]}\")\n\nSampled character: a\n\n\n\n\n\nNumPy provides built-in functions for generating random numbers from various distributions. Let’s generate random numbers from a uniform distribution:\n\nsamples_numpy = np.random.uniform(0, 1, 1000)\nprint(samples_numpy[:10])\nplt.hist(samples_numpy)\n\n[0.18872185 0.29419573 0.05307716 0.4139927  0.80335078 0.64528118\n 0.4575413  0.76490687 0.53933945 0.3135855 ]\n\n\n(array([100.,  86., 105.,  99., 102.,  90., 105., 106., 104., 103.]),\n array([0.00367548, 0.10305437, 0.20243325, 0.30181214, 0.40119103,\n        0.50056992, 0.59994881, 0.6993277 , 0.79870659, 0.89808548,\n        0.99746437]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\n\n\nTo understand how random number generation works under the hood, let’s implement our own simple PRNG using a Linear Congruential Generator (LCG). This is one of the oldest and most well-studied PRNG algorithms:\n\nclass SimplePRNG:\n    def __init__(self, seed=42):\n        # Initialize the generator with a seed\n        self.modulus = 2**31 - 1  # Large prime modulus\n        self.multiplier = 1664525\n        self.increment = 1013904223\n        self.state = seed\n\n    def uniform(self, low=0.0, high=1.0, size=None):\n        # Generate random numbers in the range [low, high)\n        if size is None:\n            size = 1  # Default to a single number if size is None\n        if isinstance(size, int):\n            size = (size,)  # Convert size to tuple\n\n        # Generate random numbers\n        random_numbers = np.empty(size)\n        for idx in np.ndindex(size):\n            self.state = (self.multiplier * self.state + self.increment) % self.modulus\n            random_numbers[idx] = low + (high - low) * (self.state / self.modulus)\n\n        return random_numbers \n\n# Example usage:\nprng = SimplePRNG(seed=12345)\nprint(prng.uniform(0, 1, 50))  \n\n[0.04080538 0.04087837 0.54709049 0.26713204 0.42986223 0.90642683\n 0.59240588 0.8643601  0.45993872 0.96243078 0.56511522 0.88322526\n 0.98992417 0.00597593 0.55550726 0.19979239 0.90062773 0.84657695\n 0.96436855 0.03375009 0.34428985 0.54191328 0.67693687 0.81460587\n 0.30590164 0.39907677 0.73545432 0.56768856 0.26844794 0.78549411\n 0.06020346 0.63806449 0.76347271 0.89276656 0.72407304 0.15397659\n 0.35789549 0.46641842 0.58988864 0.86842092 0.80445417 0.54725703\n 0.48183308 0.68513887 0.24797944 0.44857785 0.52156459 0.77294949\n 0.22528635 0.23579403]\n\n\n\nsamples_our_prng = prng.uniform(0, 1, 1000)\n\n\nplt.hist(samples_our_prng)\n\n(array([ 96.,  94., 101.,  98.,  92.,  97., 107., 129.,  81., 105.]),\n array([0.00156537, 0.10117911, 0.20079286, 0.30040661, 0.40002035,\n        0.4996341 , 0.59924785, 0.69886159, 0.79847534, 0.89808909,\n        0.99770283]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\nLet’s generate more samples and visualize the distribution:\n\n\n\nLet’s visualize how our custom PRNG compares to NumPy’s built-in generator:",
    "crumbs": [
      "Home",
      "Getting Started",
      "Introduction to Probability and Statistics"
    ]
  },
  {
    "objectID": "notebooks/intro.html#learning-objectives",
    "href": "notebooks/intro.html#learning-objectives",
    "title": "Introduction to Probability and Statistics",
    "section": "",
    "text": "Understand probability distributions and sampling\nLearn how random number generation works\nImplement a simple PRNG\nCompare different random number generators\n\nLet’s start by importing the necessary libraries:",
    "crumbs": [
      "Home",
      "Getting Started",
      "Introduction to Probability and Statistics"
    ]
  },
  {
    "objectID": "notebooks/intro.html#probability-distributions-and-sampling",
    "href": "notebooks/intro.html#probability-distributions-and-sampling",
    "title": "Introduction to Probability and Statistics",
    "section": "",
    "text": "Let’s start with a simple example of creating a probability distribution over characters. We’ll create a categorical distribution where certain vowels have higher probabilities:\n\nfirst_4_chars = \"appl\"\n\nprobs = np.zeros(27) # 26 letters + space (EOS)\n\n# Store i to s\nitos = list(\"abcdefghijklmnopqrstuvwxyz \")\n\n# stoi \nstoi = {char: i for i, char in enumerate(itos)}\n\n# Fill in the probabilities\nprobs[stoi[\"a\"]] = 0.1\nprobs[stoi[\"e\"]] = 0.3\nprobs[stoi[\"i\"]] = 0.2\nprobs[stoi[\"o\"]] = 0.1\nprobs[stoi[\"u\"]] = 0.05\nprobs[stoi[\" \"]] = 0.05\n\nprobability_distribution = torch.distributions.Categorical(torch.tensor(probs))\n\nNow let’s sample from this distribution to see which character gets selected:\n\nsample = probability_distribution.sample()\nprint(f\"Sampled character: {itos[sample]}\")\n\nSampled character: a",
    "crumbs": [
      "Home",
      "Getting Started",
      "Introduction to Probability and Statistics"
    ]
  },
  {
    "objectID": "notebooks/intro.html#random-number-generation-with-numpy",
    "href": "notebooks/intro.html#random-number-generation-with-numpy",
    "title": "Introduction to Probability and Statistics",
    "section": "",
    "text": "NumPy provides built-in functions for generating random numbers from various distributions. Let’s generate random numbers from a uniform distribution:\n\nsamples_numpy = np.random.uniform(0, 1, 1000)\nprint(samples_numpy[:10])\nplt.hist(samples_numpy)\n\n[0.18872185 0.29419573 0.05307716 0.4139927  0.80335078 0.64528118\n 0.4575413  0.76490687 0.53933945 0.3135855 ]\n\n\n(array([100.,  86., 105.,  99., 102.,  90., 105., 106., 104., 103.]),\n array([0.00367548, 0.10305437, 0.20243325, 0.30181214, 0.40119103,\n        0.50056992, 0.59994881, 0.6993277 , 0.79870659, 0.89808548,\n        0.99746437]),\n &lt;BarContainer object of 10 artists&gt;)",
    "crumbs": [
      "Home",
      "Getting Started",
      "Introduction to Probability and Statistics"
    ]
  },
  {
    "objectID": "notebooks/intro.html#implementing-a-simple-pseudo-random-number-generator-prng",
    "href": "notebooks/intro.html#implementing-a-simple-pseudo-random-number-generator-prng",
    "title": "Introduction to Probability and Statistics",
    "section": "",
    "text": "To understand how random number generation works under the hood, let’s implement our own simple PRNG using a Linear Congruential Generator (LCG). This is one of the oldest and most well-studied PRNG algorithms:\n\nclass SimplePRNG:\n    def __init__(self, seed=42):\n        # Initialize the generator with a seed\n        self.modulus = 2**31 - 1  # Large prime modulus\n        self.multiplier = 1664525\n        self.increment = 1013904223\n        self.state = seed\n\n    def uniform(self, low=0.0, high=1.0, size=None):\n        # Generate random numbers in the range [low, high)\n        if size is None:\n            size = 1  # Default to a single number if size is None\n        if isinstance(size, int):\n            size = (size,)  # Convert size to tuple\n\n        # Generate random numbers\n        random_numbers = np.empty(size)\n        for idx in np.ndindex(size):\n            self.state = (self.multiplier * self.state + self.increment) % self.modulus\n            random_numbers[idx] = low + (high - low) * (self.state / self.modulus)\n\n        return random_numbers \n\n# Example usage:\nprng = SimplePRNG(seed=12345)\nprint(prng.uniform(0, 1, 50))  \n\n[0.04080538 0.04087837 0.54709049 0.26713204 0.42986223 0.90642683\n 0.59240588 0.8643601  0.45993872 0.96243078 0.56511522 0.88322526\n 0.98992417 0.00597593 0.55550726 0.19979239 0.90062773 0.84657695\n 0.96436855 0.03375009 0.34428985 0.54191328 0.67693687 0.81460587\n 0.30590164 0.39907677 0.73545432 0.56768856 0.26844794 0.78549411\n 0.06020346 0.63806449 0.76347271 0.89276656 0.72407304 0.15397659\n 0.35789549 0.46641842 0.58988864 0.86842092 0.80445417 0.54725703\n 0.48183308 0.68513887 0.24797944 0.44857785 0.52156459 0.77294949\n 0.22528635 0.23579403]\n\n\n\nsamples_our_prng = prng.uniform(0, 1, 1000)\n\n\nplt.hist(samples_our_prng)\n\n(array([ 96.,  94., 101.,  98.,  92.,  97., 107., 129.,  81., 105.]),\n array([0.00156537, 0.10117911, 0.20079286, 0.30040661, 0.40002035,\n        0.4996341 , 0.59924785, 0.69886159, 0.79847534, 0.89808909,\n        0.99770283]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\nLet’s generate more samples and visualize the distribution:",
    "crumbs": [
      "Home",
      "Getting Started",
      "Introduction to Probability and Statistics"
    ]
  },
  {
    "objectID": "notebooks/intro.html#comparing-distributions",
    "href": "notebooks/intro.html#comparing-distributions",
    "title": "Introduction to Probability and Statistics",
    "section": "",
    "text": "Let’s visualize how our custom PRNG compares to NumPy’s built-in generator:",
    "crumbs": [
      "Home",
      "Getting Started",
      "Introduction to Probability and Statistics"
    ]
  },
  {
    "objectID": "notebooks/cdf-discrete.html",
    "href": "notebooks/cdf-discrete.html",
    "title": "CDF for Discrete Random Variables",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch \nimport torch.nn as nn\n\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Set random seed\ntorch.manual_seed(42)\n\n&lt;torch._C.Generator at 0x135180ab0&gt;\n\n\n\nprobs = [0.1, 0.4, 0.2, 0.3]\ndist = torch.distributions.Categorical(probs=torch.tensor(probs))\n\n\ndist\n\nCategorical(probs: torch.Size([4]))\n\n\n\npd.Series(dist.sample(torch.Size([1000]))).value_counts().sort_index()\n\n0     90\n1    409\n2    187\n3    314\nName: count, dtype: int64\n\n\n\n\n# Generate 2D input features\nn_samples = 1000\nX1 = torch.distributions.Uniform(0, 5).sample((n_samples, 1))  # Feature 1\nX2 = torch.distributions.Uniform(0, 5).sample((n_samples, 1))  # Feature 2\n\n\n# True weights and bias\nw1, w2, b = 1.2, -0.8, -2.5  \n\n\n# Compute logits and apply sigmoid\nlogits = w1 * X1 + w2 * X2 + b\nprob_Y = torch.sigmoid(logits)  # Probabilities\n\n\nprob_Y\n\ntensor([[0.4690],\n        [0.8697],\n        [0.2377],\n        [0.6852],\n        [0.2472],\n        [0.5074],\n        [0.0922],\n        [0.7336],\n        [0.8787],\n        [0.0406],\n        [0.6429],\n        [0.6637],\n        [0.8729],\n        [0.2455],\n        [0.4179],\n        [0.0462],\n        [0.9018],\n        [0.1551],\n        [0.0173],\n        [0.1364],\n        [0.2652],\n        [0.2519],\n        [0.2691],\n        [0.8527],\n        [0.0068],\n        [0.0701],\n        [0.0393],\n        [0.1062],\n        [0.1793],\n        [0.0372],\n        [0.5522],\n        [0.0402],\n        [0.7888],\n        [0.6758],\n        [0.0772],\n        [0.1758],\n        [0.1189],\n        [0.8798],\n        [0.2317],\n        [0.0974],\n        [0.7143],\n        [0.0202],\n        [0.4330],\n        [0.6808],\n        [0.7392],\n        [0.1329],\n        [0.4755],\n        [0.1541],\n        [0.3953],\n        [0.0859],\n        [0.0016],\n        [0.0859],\n        [0.0707],\n        [0.8696],\n        [0.0688],\n        [0.4360],\n        [0.4305],\n        [0.3274],\n        [0.7977],\n        [0.1249],\n        [0.5691],\n        [0.0089],\n        [0.0923],\n        [0.0116],\n        [0.7033],\n        [0.0389],\n        [0.9026],\n        [0.0113],\n        [0.0087],\n        [0.0079],\n        [0.4398],\n        [0.1479],\n        [0.7938],\n        [0.0074],\n        [0.0228],\n        [0.0089],\n        [0.0534],\n        [0.4375],\n        [0.7883],\n        [0.8199],\n        [0.1195],\n        [0.0536],\n        [0.7972],\n        [0.2862],\n        [0.5475],\n        [0.0290],\n        [0.1672],\n        [0.0067],\n        [0.1216],\n        [0.8010],\n        [0.1646],\n        [0.0328],\n        [0.8813],\n        [0.4557],\n        [0.0046],\n        [0.0383],\n        [0.9207],\n        [0.0155],\n        [0.0318],\n        [0.0032],\n        [0.1341],\n        [0.1285],\n        [0.3706],\n        [0.0035],\n        [0.3584],\n        [0.0085],\n        [0.0116],\n        [0.0246],\n        [0.0215],\n        [0.0143],\n        [0.9115],\n        [0.7215],\n        [0.5481],\n        [0.0028],\n        [0.0500],\n        [0.0420],\n        [0.4277],\n        [0.0035],\n        [0.0182],\n        [0.0180],\n        [0.3848],\n        [0.2725],\n        [0.1439],\n        [0.0863],\n        [0.0055],\n        [0.2094],\n        [0.0317],\n        [0.0028],\n        [0.5034],\n        [0.0586],\n        [0.9528],\n        [0.0766],\n        [0.0242],\n        [0.9202],\n        [0.0134],\n        [0.1375],\n        [0.0209],\n        [0.0018],\n        [0.6780],\n        [0.3229],\n        [0.3414],\n        [0.0243],\n        [0.5663],\n        [0.1166],\n        [0.1113],\n        [0.5040],\n        [0.7373],\n        [0.9401],\n        [0.5430],\n        [0.0139],\n        [0.7203],\n        [0.0124],\n        [0.0808],\n        [0.0160],\n        [0.9173],\n        [0.0179],\n        [0.1390],\n        [0.1397],\n        [0.7374],\n        [0.0071],\n        [0.7449],\n        [0.1415],\n        [0.0649],\n        [0.0029],\n        [0.7889],\n        [0.0241],\n        [0.1800],\n        [0.7164],\n        [0.6379],\n        [0.0617],\n        [0.0508],\n        [0.1972],\n        [0.6204],\n        [0.0813],\n        [0.0056],\n        [0.0244],\n        [0.0077],\n        [0.2261],\n        [0.3260],\n        [0.0143],\n        [0.2764],\n        [0.4105],\n        [0.6875],\n        [0.2774],\n        [0.4553],\n        [0.7025],\n        [0.4154],\n        [0.1725],\n        [0.0688],\n        [0.0773],\n        [0.2032],\n        [0.0181],\n        [0.2384],\n        [0.9317],\n        [0.4372],\n        [0.3650],\n        [0.4109],\n        [0.2889],\n        [0.3479],\n        [0.4569],\n        [0.0386],\n        [0.1222],\n        [0.8519],\n        [0.0917],\n        [0.2874],\n        [0.1328],\n        [0.0153],\n        [0.0168],\n        [0.0178],\n        [0.1498],\n        [0.1939],\n        [0.7941],\n        [0.0043],\n        [0.0068],\n        [0.2717],\n        [0.3926],\n        [0.0714],\n        [0.0912],\n        [0.2274],\n        [0.1253],\n        [0.1548],\n        [0.5459],\n        [0.0305],\n        [0.3112],\n        [0.0043],\n        [0.9017],\n        [0.2792],\n        [0.0876],\n        [0.0157],\n        [0.0389],\n        [0.0073],\n        [0.5978],\n        [0.3535],\n        [0.5908],\n        [0.1898],\n        [0.5297],\n        [0.2536],\n        [0.0130],\n        [0.0135],\n        [0.0027],\n        [0.6854],\n        [0.0522],\n        [0.0775],\n        [0.0632],\n        [0.0320],\n        [0.0212],\n        [0.1663],\n        [0.4284],\n        [0.1848],\n        [0.5393],\n        [0.0971],\n        [0.1523],\n        [0.1591],\n        [0.0594],\n        [0.1385],\n        [0.0020],\n        [0.6491],\n        [0.0076],\n        [0.3805],\n        [0.8249],\n        [0.5432],\n        [0.0042],\n        [0.8772],\n        [0.1644],\n        [0.0875],\n        [0.4281],\n        [0.3085],\n        [0.0614],\n        [0.1262],\n        [0.1158],\n        [0.4476],\n        [0.0059],\n        [0.7236],\n        [0.5122],\n        [0.9547],\n        [0.0268],\n        [0.0034],\n        [0.1269],\n        [0.4547],\n        [0.0058],\n        [0.3243],\n        [0.0574],\n        [0.1720],\n        [0.1343],\n        [0.2229],\n        [0.0219],\n        [0.0033],\n        [0.9415],\n        [0.5935],\n        [0.0582],\n        [0.0232],\n        [0.0202],\n        [0.0024],\n        [0.0227],\n        [0.1436],\n        [0.0084],\n        [0.1765],\n        [0.4679],\n        [0.0520],\n        [0.0276],\n        [0.3989],\n        [0.0865],\n        [0.0738],\n        [0.0310],\n        [0.0538],\n        [0.4719],\n        [0.0175],\n        [0.6643],\n        [0.0717],\n        [0.2207],\n        [0.6581],\n        [0.1212],\n        [0.0091],\n        [0.5362],\n        [0.3897],\n        [0.5126],\n        [0.7488],\n        [0.7894],\n        [0.3925],\n        [0.2507],\n        [0.0973],\n        [0.0892],\n        [0.6295],\n        [0.4406],\n        [0.1529],\n        [0.1186],\n        [0.0113],\n        [0.0021],\n        [0.1150],\n        [0.0482],\n        [0.8805],\n        [0.2454],\n        [0.0035],\n        [0.0044],\n        [0.8107],\n        [0.0069],\n        [0.0515],\n        [0.4856],\n        [0.0080],\n        [0.2625],\n        [0.2149],\n        [0.2317],\n        [0.0353],\n        [0.0090],\n        [0.0166],\n        [0.1212],\n        [0.9087],\n        [0.0258],\n        [0.1467],\n        [0.3867],\n        [0.3478],\n        [0.7219],\n        [0.5131],\n        [0.5104],\n        [0.0028],\n        [0.2580],\n        [0.8291],\n        [0.0395],\n        [0.0280],\n        [0.8736],\n        [0.2135],\n        [0.0247],\n        [0.0914],\n        [0.1309],\n        [0.2388],\n        [0.7147],\n        [0.5993],\n        [0.0881],\n        [0.0025],\n        [0.0412],\n        [0.7522],\n        [0.1615],\n        [0.0706],\n        [0.5131],\n        [0.0120],\n        [0.0252],\n        [0.0513],\n        [0.5335],\n        [0.3664],\n        [0.0042],\n        [0.2806],\n        [0.1637],\n        [0.1168],\n        [0.9396],\n        [0.0373],\n        [0.7879],\n        [0.0925],\n        [0.0391],\n        [0.6586],\n        [0.4116],\n        [0.1996],\n        [0.0494],\n        [0.2710],\n        [0.6425],\n        [0.1510],\n        [0.0312],\n        [0.0179],\n        [0.1739],\n        [0.6685],\n        [0.2275],\n        [0.1530],\n        [0.9160],\n        [0.4186],\n        [0.1890],\n        [0.2847],\n        [0.0327],\n        [0.0187],\n        [0.0628],\n        [0.1278],\n        [0.3404],\n        [0.1098],\n        [0.0375],\n        [0.7807],\n        [0.6327],\n        [0.7904],\n        [0.0507],\n        [0.1858],\n        [0.3649],\n        [0.0336],\n        [0.0142],\n        [0.1073],\n        [0.0164],\n        [0.0245],\n        [0.8529],\n        [0.5046],\n        [0.5838],\n        [0.1484],\n        [0.0258],\n        [0.1378],\n        [0.5009],\n        [0.1275],\n        [0.0463],\n        [0.0377],\n        [0.1537],\n        [0.1114],\n        [0.5183],\n        [0.0190],\n        [0.0583],\n        [0.3773],\n        [0.1361],\n        [0.2197],\n        [0.6813],\n        [0.2385],\n        [0.9250],\n        [0.9642],\n        [0.1617],\n        [0.0100],\n        [0.4980],\n        [0.9302],\n        [0.0457],\n        [0.1462],\n        [0.0613],\n        [0.0062],\n        [0.0611],\n        [0.2581],\n        [0.6507],\n        [0.0583],\n        [0.6322],\n        [0.0657],\n        [0.0992],\n        [0.5305],\n        [0.1600],\n        [0.3603],\n        [0.0488],\n        [0.6136],\n        [0.1191],\n        [0.0036],\n        [0.0577],\n        [0.4877],\n        [0.3272],\n        [0.7177],\n        [0.4588],\n        [0.7115],\n        [0.0364],\n        [0.2061],\n        [0.3108],\n        [0.0613],\n        [0.0117],\n        [0.0795],\n        [0.4676],\n        [0.5647],\n        [0.0231],\n        [0.1328],\n        [0.5421],\n        [0.0562],\n        [0.0614],\n        [0.3270],\n        [0.0339],\n        [0.8109],\n        [0.3840],\n        [0.5580],\n        [0.6581],\n        [0.5215],\n        [0.4812],\n        [0.4216],\n        [0.5801],\n        [0.8010],\n        [0.6829],\n        [0.0294],\n        [0.0095],\n        [0.0193],\n        [0.0555],\n        [0.2013],\n        [0.8739],\n        [0.5182],\n        [0.8575],\n        [0.0095],\n        [0.0300],\n        [0.0588],\n        [0.0581],\n        [0.4170],\n        [0.0025],\n        [0.1793],\n        [0.0060],\n        [0.0250],\n        [0.0052],\n        [0.4422],\n        [0.0052],\n        [0.3186],\n        [0.2248],\n        [0.0405],\n        [0.4789],\n        [0.0483],\n        [0.1727],\n        [0.5538],\n        [0.0335],\n        [0.4634],\n        [0.1829],\n        [0.2329],\n        [0.3739],\n        [0.0957],\n        [0.0156],\n        [0.3449],\n        [0.8196],\n        [0.6699],\n        [0.4111],\n        [0.0098],\n        [0.3198],\n        [0.0751],\n        [0.8296],\n        [0.0686],\n        [0.0258],\n        [0.0505],\n        [0.0505],\n        [0.6845],\n        [0.1587],\n        [0.3579],\n        [0.6190],\n        [0.9235],\n        [0.2053],\n        [0.1562],\n        [0.4789],\n        [0.0443],\n        [0.2962],\n        [0.0232],\n        [0.0190],\n        [0.5996],\n        [0.6140],\n        [0.7862],\n        [0.9355],\n        [0.4863],\n        [0.5655],\n        [0.0436],\n        [0.0308],\n        [0.0309],\n        [0.0590],\n        [0.0157],\n        [0.0254],\n        [0.0145],\n        [0.0126],\n        [0.0280],\n        [0.0055],\n        [0.0610],\n        [0.6977],\n        [0.6296],\n        [0.1038],\n        [0.1875],\n        [0.3234],\n        [0.0475],\n        [0.0674],\n        [0.7886],\n        [0.0208],\n        [0.6382],\n        [0.6671],\n        [0.2659],\n        [0.1304],\n        [0.0330],\n        [0.1543],\n        [0.1324],\n        [0.0923],\n        [0.6481],\n        [0.4782],\n        [0.0137],\n        [0.4810],\n        [0.0242],\n        [0.0645],\n        [0.2477],\n        [0.0396],\n        [0.8759],\n        [0.0614],\n        [0.2093],\n        [0.2825],\n        [0.0732],\n        [0.0819],\n        [0.0994],\n        [0.0042],\n        [0.0639],\n        [0.0714],\n        [0.6146],\n        [0.5548],\n        [0.5720],\n        [0.0173],\n        [0.0950],\n        [0.9247],\n        [0.0979],\n        [0.5093],\n        [0.3201],\n        [0.0249],\n        [0.0115],\n        [0.8606],\n        [0.3280],\n        [0.2417],\n        [0.0065],\n        [0.0185],\n        [0.0386],\n        [0.0495],\n        [0.0566],\n        [0.8324],\n        [0.0054],\n        [0.1783],\n        [0.0084],\n        [0.1398],\n        [0.5463],\n        [0.5990],\n        [0.2893],\n        [0.2422],\n        [0.0921],\n        [0.0408],\n        [0.0046],\n        [0.0630],\n        [0.2670],\n        [0.0066],\n        [0.9150],\n        [0.5427],\n        [0.0191],\n        [0.1254],\n        [0.0957],\n        [0.3615],\n        [0.1072],\n        [0.2503],\n        [0.7266],\n        [0.0921],\n        [0.5816],\n        [0.4543],\n        [0.1018],\n        [0.9215],\n        [0.0393],\n        [0.1537],\n        [0.0449],\n        [0.8551],\n        [0.0236],\n        [0.3884],\n        [0.0032],\n        [0.3583],\n        [0.7975],\n        [0.1741],\n        [0.7627],\n        [0.0478],\n        [0.4580],\n        [0.0336],\n        [0.2849],\n        [0.2970],\n        [0.9362],\n        [0.2849],\n        [0.0084],\n        [0.3964],\n        [0.0635],\n        [0.4038],\n        [0.0035],\n        [0.2056],\n        [0.7709],\n        [0.2959],\n        [0.0089],\n        [0.1077],\n        [0.0589],\n        [0.5707],\n        [0.0962],\n        [0.0793],\n        [0.0674],\n        [0.0335],\n        [0.7049],\n        [0.8910],\n        [0.0836],\n        [0.3845],\n        [0.5714],\n        [0.7929],\n        [0.6587],\n        [0.5552],\n        [0.0545],\n        [0.8679],\n        [0.8363],\n        [0.0228],\n        [0.0075],\n        [0.0453],\n        [0.1558],\n        [0.0073],\n        [0.1370],\n        [0.4195],\n        [0.0152],\n        [0.1042],\n        [0.0471],\n        [0.1086],\n        [0.6888],\n        [0.0051],\n        [0.4637],\n        [0.9148],\n        [0.3310],\n        [0.0220],\n        [0.0207],\n        [0.1868],\n        [0.0823],\n        [0.2680],\n        [0.8058],\n        [0.9286],\n        [0.1681],\n        [0.1451],\n        [0.1451],\n        [0.0671],\n        [0.0082],\n        [0.1195],\n        [0.8149],\n        [0.0024],\n        [0.0062],\n        [0.6743],\n        [0.4562],\n        [0.6410],\n        [0.0143],\n        [0.0469],\n        [0.7715],\n        [0.4464],\n        [0.5066],\n        [0.1339],\n        [0.5612],\n        [0.5935],\n        [0.0075],\n        [0.0167],\n        [0.5136],\n        [0.0040],\n        [0.0411],\n        [0.5657],\n        [0.0792],\n        [0.0777],\n        [0.1271],\n        [0.0341],\n        [0.6242],\n        [0.0441],\n        [0.0383],\n        [0.7237],\n        [0.0076],\n        [0.3616],\n        [0.1653],\n        [0.2611],\n        [0.4183],\n        [0.0433],\n        [0.0674],\n        [0.7030],\n        [0.2125],\n        [0.0033],\n        [0.0105],\n        [0.5878],\n        [0.2345],\n        [0.7199],\n        [0.1536],\n        [0.0522],\n        [0.0208],\n        [0.1033],\n        [0.0328],\n        [0.5261],\n        [0.4477],\n        [0.4163],\n        [0.0638],\n        [0.0860],\n        [0.7403],\n        [0.3582],\n        [0.0382],\n        [0.0144],\n        [0.3440],\n        [0.1347],\n        [0.2414],\n        [0.9490],\n        [0.0595],\n        [0.9154],\n        [0.8628],\n        [0.0031],\n        [0.6427],\n        [0.0366],\n        [0.0776],\n        [0.3921],\n        [0.0062],\n        [0.8658],\n        [0.2734],\n        [0.7700],\n        [0.0047],\n        [0.6233],\n        [0.0978],\n        [0.0369],\n        [0.4760],\n        [0.4007],\n        [0.3911],\n        [0.1022],\n        [0.0443],\n        [0.6452],\n        [0.5626],\n        [0.1089],\n        [0.5314],\n        [0.0162],\n        [0.6265],\n        [0.8584],\n        [0.2519],\n        [0.2426],\n        [0.8181],\n        [0.5241],\n        [0.0241],\n        [0.0273],\n        [0.0294],\n        [0.3048],\n        [0.2815],\n        [0.1653],\n        [0.6694],\n        [0.0173],\n        [0.4302],\n        [0.2016],\n        [0.0251],\n        [0.1268],\n        [0.0075],\n        [0.5129],\n        [0.0309],\n        [0.0977],\n        [0.0888],\n        [0.1067],\n        [0.5135],\n        [0.0031],\n        [0.0084],\n        [0.0379],\n        [0.3944],\n        [0.0491],\n        [0.1070],\n        [0.0736],\n        [0.1812],\n        [0.9447],\n        [0.1928],\n        [0.1918],\n        [0.0043],\n        [0.1300],\n        [0.0261],\n        [0.1692],\n        [0.8610],\n        [0.8094],\n        [0.9622],\n        [0.5234],\n        [0.1729],\n        [0.0978],\n        [0.5527],\n        [0.0086],\n        [0.2799],\n        [0.3916],\n        [0.6266],\n        [0.3144],\n        [0.0426],\n        [0.0029],\n        [0.0657],\n        [0.4495],\n        [0.1725],\n        [0.1724],\n        [0.4098],\n        [0.0512],\n        [0.8470],\n        [0.1267],\n        [0.5247],\n        [0.1115],\n        [0.7646],\n        [0.7213],\n        [0.0759],\n        [0.3569],\n        [0.3375],\n        [0.8672],\n        [0.7596],\n        [0.1475],\n        [0.0044],\n        [0.4780],\n        [0.0581],\n        [0.0251],\n        [0.4568],\n        [0.2024],\n        [0.7446],\n        [0.6320],\n        [0.0314],\n        [0.0765],\n        [0.0181],\n        [0.1966],\n        [0.5667],\n        [0.8577],\n        [0.9013],\n        [0.1643],\n        [0.0776],\n        [0.1805],\n        [0.0631],\n        [0.1894],\n        [0.3725],\n        [0.0367],\n        [0.1032],\n        [0.8510],\n        [0.3935],\n        [0.8488],\n        [0.9634],\n        [0.0439],\n        [0.0918],\n        [0.5004],\n        [0.0232],\n        [0.2922],\n        [0.0592],\n        [0.6902],\n        [0.0502],\n        [0.2931],\n        [0.5467],\n        [0.0902],\n        [0.0438],\n        [0.7579],\n        [0.0144],\n        [0.0310],\n        [0.9566],\n        [0.0983],\n        [0.1246],\n        [0.0175],\n        [0.3579],\n        [0.1215],\n        [0.0063],\n        [0.8350],\n        [0.0783],\n        [0.0673],\n        [0.0044],\n        [0.1932],\n        [0.0166],\n        [0.0619],\n        [0.0126],\n        [0.0981],\n        [0.1415],\n        [0.0975],\n        [0.0028],\n        [0.7217],\n        [0.1342],\n        [0.0666],\n        [0.8163],\n        [0.5906],\n        [0.8264],\n        [0.0062],\n        [0.1267],\n        [0.0576],\n        [0.4529],\n        [0.1712],\n        [0.0178],\n        [0.1451],\n        [0.0375],\n        [0.1448],\n        [0.1852],\n        [0.9204],\n        [0.6197],\n        [0.0180],\n        [0.3411],\n        [0.0445],\n        [0.1539],\n        [0.4608],\n        [0.5943],\n        [0.2118],\n        [0.8577],\n        [0.0662],\n        [0.2722],\n        [0.6509],\n        [0.5460],\n        [0.4635],\n        [0.2292],\n        [0.0513],\n        [0.2066],\n        [0.5374],\n        [0.0059],\n        [0.2224],\n        [0.1254],\n        [0.3754],\n        [0.5258],\n        [0.0939],\n        [0.1778],\n        [0.2772],\n        [0.0344],\n        [0.0388],\n        [0.9426],\n        [0.1209],\n        [0.2947],\n        [0.0870],\n        [0.4037],\n        [0.0158]])\n\n\n\n# Sample class labels\nY = torch.distributions.Bernoulli(prob_Y).sample()\n\n\n# Convert to NumPy for visualization\nX1_np, X2_np, Y_np = X1.numpy(), X2.numpy(), Y.numpy()\n\n# Plot data points\nplt.scatter(X1_np[Y_np == 0], X2_np[Y_np == 0], color=\"blue\", label=\"Class 0\", alpha=0.5)\nplt.scatter(X1_np[Y_np == 1], X2_np[Y_np == 1], color=\"red\", label=\"Class 1\", alpha=0.5)\nplt.xlabel(\"Feature 1 (X1)\")\nplt.ylabel(\"Feature 2 (X2)\")\nplt.legend()\nplt.title(\"Generated 2D Logistic Regression Data\")\n\nText(0.5, 1.0, 'Generated 2D Logistic Regression Data')\n\n\n\n\n\n\n\n\n\n\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Stack X1 and X2 into a single tensor\nX_train = torch.cat((X1, X2), dim=1)\n\n# Define logistic regression model\nclass LogisticRegression2D(nn.Module):\n    def __init__(self):\n        super(LogisticRegression2D, self).__init__()\n        self.linear = nn.Linear(2, 1)  # Two inputs, one output\n\n    def forward(self, x):\n        return torch.sigmoid(self.linear(x))  # Sigmoid activation\n\n# Initialize model\nmodel = LogisticRegression2D()\nloss_fn = nn.BCELoss()  # Binary cross-entropy loss\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop\nepochs = 1000\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    Y_pred = model(X_train)  # Forward pass\n    loss = loss_fn(Y_pred, Y.view(-1, 1))  # Compute loss\n    loss.backward()  # Backpropagation\n    optimizer.step()  # Update weights\n\n    if epoch % 200 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\n# Extract learned parameters\nw1_learned, w2_learned = model.linear.weight[0].detach().numpy()\nb_learned = model.linear.bias[0].detach().numpy()\nprint(f\"Learned Parameters: w1 = {w1_learned:.4f}, w2 = {w2_learned:.4f}, b = {b_learned:.4f}\")\n\nEpoch 0, Loss: 2.0864\nEpoch 200, Loss: 0.4884\nEpoch 400, Loss: 0.4536\nEpoch 600, Loss: 0.4395\nEpoch 800, Loss: 0.4318\nLearned Parameters: w1 = 0.6339, w2 = -0.8716, b = -0.4170\n\n\n\ndef plot_decision_boundary(model, X1_np, X2_np, Y_np, w1_true, w2_true, b_true):\n    \"\"\"Plots the true and learned decision boundaries.\"\"\"\n    \n    # Generate mesh grid\n    x1_vals = np.linspace(0, 5, 100)\n    x2_vals = np.linspace(0, 5, 100)\n    X1_grid, X2_grid = np.meshgrid(x1_vals, x2_vals)\n\n    # Compute model's learned decision boundary\n    with torch.no_grad():\n        Z = model(torch.tensor(np.c_[X1_grid.ravel(), X2_grid.ravel()], dtype=torch.float32))\n        Z = Z.view(X1_grid.shape).numpy()\n\n    # Compute true decision boundary\n    x1_boundary = np.linspace(0,5, 100)\n    x2_boundary_true = - (w1_true / w2_true) * x1_boundary - (b_true / w2_true)\n\n    # Plot data points\n    plt.scatter(X1_np[Y_np == 0], X2_np[Y_np == 0], color=\"blue\", label=\"Class 0\", alpha=0.5)\n    plt.scatter(X1_np[Y_np == 1], X2_np[Y_np == 1], color=\"red\", label=\"Class 1\", alpha=0.5)\n\n    # Plot learned decision boundary\n    plt.contour(X1_grid, X2_grid, Z, levels=[0.5], colors=\"black\", linestyles=\"dashed\", label=\"Learned Boundary\")\n\n    # Plot true decision boundary\n    plt.plot(x1_boundary, x2_boundary_true, color=\"green\", linestyle=\"solid\", label=\"True Boundary\")\n\n    plt.xlabel(\"Feature 1 (X1)\")\n    plt.ylabel(\"Feature 2 (X2)\")\n    plt.legend()\n    plt.title(\"Logistic Regression Decision Boundary\")\n    plt.ylim([0, 5])\n\n\n# Call the function with true and learned parameters\nplot_decision_boundary(model, X1_np, X2_np, Y_np, w1_true=1.2, w2_true=-0.8, b_true=-2.5)",
    "crumbs": [
      "Home",
      "Foundations",
      "CDF for Discrete Random Variables"
    ]
  },
  {
    "objectID": "notebooks/set.html",
    "href": "notebooks/set.html",
    "title": "Set",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch \nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nA = np.linspace(0, 1, 100)\nA = np.arange(0, 1, 0.01)\n\n\n# uncountable set\nx = np.random.rand(100000000)\n#print(x)\nnp.unique(x).size\n\n100000000\n\n\n\n# Set of lines in 2d\ndef line_fx(x, m, c):\n    return m*x + c\n\nline_fx(1, 2, 3)\n\n5\n\n\n\nx_lin = np.linspace(-10, 10, 100)\ny_lin_2_3 = line_fx(x_lin, 2, 3)\n\nplt.plot(x_lin, y_lin_2_3)\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact\nimport ipywidgets as widgets\n\n\n\ndef plot_line(m, c):\n    x_lin = np.linspace(-10, 10, 100)\n    y_lin = line_fx(x_lin, m, c)\n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(x_lin, y_lin, label=f'Line: y = {m}x + {c}')\n    plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n    plt.axvline(0, color='black', linewidth=0.8, linestyle='--')\n    plt.grid(alpha=0.5)\n    plt.legend()\n    plt.title(\"Interactive Line Plot\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.ylim(-10, 10)\n    plt.show()\n\n# Interactive widget\ninteract(plot_line, \n         m=widgets.FloatSlider(value=1, min=-10, max=10, step=0.1, description='Slope (m)'),\n         c=widgets.FloatSlider(value=0, min=-10, max=10, step=0.1, description='Intercept (c)'));\n\n\n\n\n\nsamples_uniform = np.random.rand(50000)\nplt.hist(samples_uniform)\n\n(array([5044., 4923., 5075., 4798., 4977., 4982., 4989., 5158., 4992.,\n        5062.]),\n array([3.93041625e-05, 1.00032757e-01, 2.00026211e-01, 3.00019664e-01,\n        4.00013117e-01, 5.00006570e-01, 6.00000024e-01, 6.99993477e-01,\n        7.99986930e-01, 8.99980383e-01, 9.99973836e-01]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\nsamples_uniform\n\narray([0.18674884, 0.81968297, 0.80087693, ..., 0.46548473, 0.47943922,\n       0.73968035])\n\n\n\nsamples_normal = np.random.randn(50000)\nplt.hist(samples_normal)\n\n(array([   23.,   285.,  1985.,  7467., 14803., 14914.,  7988.,  2201.,\n          309.,    25.]),\n array([-4.18722189, -3.35305381, -2.51888572, -1.68471764, -0.85054956,\n        -0.01638148,  0.8177866 ,  1.65195468,  2.48612276,  3.32029084,\n         4.15445892]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\nsamples_normal[:50]\n\narray([-1.02610077,  1.30693882, -0.60887913, -2.21509403, -0.49288884,\n       -0.24255606, -0.06006607, -0.43524481,  0.09849432,  0.65723845,\n        0.26736775, -0.23655818, -2.18103935, -0.49017392,  1.62213243,\n        0.38596106,  0.93529816,  1.08752614, -0.4461042 , -0.95299851,\n        1.38512913,  0.09622675, -0.72466762, -0.12871054, -0.50039256,\n        0.11997974, -1.54530777,  0.27708632, -1.59812337,  0.91816234,\n       -0.07142259, -1.00183667,  0.77816444,  0.24435284,  0.91035827,\n        0.60326872,  0.57121044,  1.26167048, -1.15016846, -0.69882365,\n       -1.07502868, -0.11305347,  0.82249031,  0.49697962, -1.21883061,\n       -1.96468898, -0.01928378, -0.56361649,  0.48693249, -0.27086149])\n\n\n\n# Plot some 20 lines for random m and c\nm = np.random.rand(20)*20 - 10\nc = np.random.rand(20)*20 - 10\n\nfor i in range(20):\n    y_lin = line_fx(x_lin, m[i], c[i])\n    plt.plot(x_lin, y_lin, label=f'Line: y = {m[i]:.2f}x + {c[i]:.2f}')\n\n\n\n\n\n\n\n\n\nnp.random.randn?\n\n\nSignature: np.random.randn(*args)\n\nDocstring:\n\nrandn(d0, d1, ..., dn)\n\n\n\nReturn a sample (or samples) from the \"standard normal\" distribution.\n\n\n\n.. note::\n\n    This is a convenience function for users porting code from Matlab,\n\n    and wraps `standard_normal`. That function takes a\n\n    tuple to specify the size of the output, which is consistent with\n\n    other NumPy functions like `numpy.zeros` and `numpy.ones`.\n\n\n\n.. note::\n\n    New code should use the\n\n    `~numpy.random.Generator.standard_normal`\n\n    method of a `~numpy.random.Generator` instance instead;\n\n    please see the :ref:`random-quick-start`.\n\n\n\nIf positive int_like arguments are provided, `randn` generates an array\n\nof shape ``(d0, d1, ..., dn)``, filled\n\nwith random floats sampled from a univariate \"normal\" (Gaussian)\n\ndistribution of mean 0 and variance 1. A single float randomly sampled\n\nfrom the distribution is returned if no argument is provided.\n\n\n\nParameters\n\n----------\n\nd0, d1, ..., dn : int, optional\n\n    The dimensions of the returned array, must be non-negative.\n\n    If no argument is given a single Python float is returned.\n\n\n\nReturns\n\n-------\n\nZ : ndarray or float\n\n    A ``(d0, d1, ..., dn)``-shaped array of floating-point samples from\n\n    the standard normal distribution, or a single such float if\n\n    no parameters were supplied.\n\n\n\nSee Also\n\n--------\n\nstandard_normal : Similar, but takes a tuple as its argument.\n\nnormal : Also accepts mu and sigma arguments.\n\nrandom.Generator.standard_normal: which should be used for new code.\n\n\n\nNotes\n\n-----\n\nFor random samples from the normal distribution with mean ``mu`` and\n\nstandard deviation ``sigma``, use::\n\n\n\n    sigma * np.random.randn(...) + mu\n\n\n\nExamples\n\n--------\n\n&gt;&gt;&gt; np.random.randn()\n\n2.1923875335537315  # random\n\n\n\nTwo-by-four array of samples from the normal distribution with\n\nmean 3 and standard deviation 2.5:\n\n\n\n&gt;&gt;&gt; 3 + 2.5 * np.random.randn(2, 4)\n\narray([[-4.49401501,  4.00950034, -1.81814867,  7.29718677],   # random\n\n       [ 0.39924804,  4.68456316,  4.99394529,  4.84057254]])  # random\n\nType:      method\n\n\n\n\n# Plot some 20 lines for random m and c\nN = 5000\nm = np.random.randn(N)*0.5\nc = np.random.randn(N)*0.5\n\nfor i in range(N):\n    y_lin = line_fx(x_lin, m[i], c[i])\n    plt.plot(x_lin, y_lin, label=f'Line: y = {m[i]:.2f}x + {c[i]:.2f}', \n             color='k', alpha=0.01)\n\n\n\n\n\n\n\n\n\n# Set of cosines with varying phase (fixed amplitude and frequency)\ndef cosine_fx(x, A=1, f=1, phi=0):\n    return A*np.cos(2*np.pi*f*x + phi)\n\n\nx_lin = np.linspace(-10, 10, 1000)\ny_cos_1_1_0 = cosine_fx(x_lin, 1, 1, 0)\n\nplt.plot(x_lin, y_cos_1_1_0)\n\n\n\n\n\n\n\n\n\ndef plot_cosine(A, f, phi):\n    x = np.linspace(0, 2, 500)  # x range for visualization\n    y = cosine_fx(x, A, f, phi)\n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(x, y, label=f'Cosine: y = {A}cos(2π{f}x + {phi})')\n    plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n    plt.grid(alpha=0.5)\n    plt.legend()\n    plt.title(\"Interactive Cosine Plot\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.ylim(-2, 2)\n    \n\n# Interactive widget\ninteract(plot_cosine, \n         A=widgets.FloatSlider(value=1, min=0.1, max=2, step=0.1, description='Amplitude (A)'),\n         f=widgets.FloatSlider(value=1, min=0.1, max=5, step=0.1, description='Frequency (f)'),\n         phi=widgets.FloatSlider(value=0, min=0, max=2*np.pi, step=0.1, description='Phase (φ)'));\n\n\n\n\n\n\n\n\n\n\n\n\n# Set datastructure in Python\n\nA = {1, 2, 3, 4, 5}\nprint(A, type(A))\n\n{1, 2, 3, 4, 5} &lt;class 'set'&gt;\n\n\n\nA = set([1, 2, 3, 4, 5])\nprint(A, type(A))\n\n{1, 2, 3, 4, 5} &lt;class 'set'&gt;\n\n\n\n# unique elements\nA = {1, 2, 3, 4, 5, 1, 2, 3, 4, 5}\nprint(A, len(A))\n\n{1, 2, 3, 4, 5} 5\n\n\n\n# Can set contain a set?\nA = {1, 2, 3}\nfor a in A:\n    print(a)\n\n1\n2\n3\n\n\n\n# why below code doesn't work? homework\ntry:\n    A = {1, 2, 3, {4, 5}}\nexcept Exception as e:\n    print(e)\n\nunhashable type: 'set'\n\n\n\n# subset\nA = {1, 2, 3, 4, 5}\nB = {1, 2, 3}\n\nprint(B.issubset(A))\n\nTrue\n\n\n\nprint(A.issubset(B))\n\nFalse\n\n\n\nprint(A.issuperset(B))\n\nTrue\n\n\n\nA.issubset(A)\n\nTrue\n\n\n\n# Other methods in set\nA\n\n{1, 2, 3, 4, 5}\n\n\n\n# check proper subset\ndef is_proper_subset(A, B):\n    return A.issubset(B) and A != B\n\n\nis_proper_subset({1, 2, 3}, {1, 2, 3, 4})\n\nTrue\n\n\n\nis_proper_subset({1, 2, 3, 4}, {1, 2, 3, 4})\n\nFalse\n\n\n\nis_proper_subset({1, 2, 6}, {1, 2, 3, 4})\n\nFalse\n\n\n\n# empty set subset of every set\nempty_set = set()\nA = {1, 2, 3}\n\nempty_set.issubset(A)\n\nTrue\n\n\n\nis_proper_subset(empty_set, A)\n\nTrue\n\n\n\n# be careful with empty set definition. Below is not an empty set but \n# a dictionary\nempty_set= {}\nprint(type(empty_set))\n\n&lt;class 'dict'&gt;\n\n\n\n# Sets in NumPy\nA = np.array([1, 2, 3, 4, 5])\nB = np.array([1, 2, 3])\n\n# is B a subset of A?\nprint(np.in1d(B, A).all())\n\nTrue\n\n\n\nnp.in1d?\n\n\nSignature:       np.in1d(ar1, ar2, assume_unique=False, invert=False, *, kind=None)\n\nCall signature:  np.in1d(*args, **kwargs)\n\nType:            _ArrayFunctionDispatcher\n\nString form:     &lt;function in1d at 0x107c70fe0&gt;\n\nFile:            ~/mambaforge/lib/python3.12/site-packages/numpy/lib/_arraysetops_impl.py\n\nDocstring:      \n\nTest whether each element of a 1-D array is also present in a second array.\n\n\n\n.. deprecated:: 2.0\n\n    Use :func:`isin` instead of `in1d` for new code.\n\n\n\nReturns a boolean array the same length as `ar1` that is True\n\nwhere an element of `ar1` is in `ar2` and False otherwise.\n\n\n\nParameters\n\n----------\n\nar1 : (M,) array_like\n\n    Input array.\n\nar2 : array_like\n\n    The values against which to test each value of `ar1`.\n\nassume_unique : bool, optional\n\n    If True, the input arrays are both assumed to be unique, which\n\n    can speed up the calculation.  Default is False.\n\ninvert : bool, optional\n\n    If True, the values in the returned array are inverted (that is,\n\n    False where an element of `ar1` is in `ar2` and True otherwise).\n\n    Default is False. ``np.in1d(a, b, invert=True)`` is equivalent\n\n    to (but is faster than) ``np.invert(in1d(a, b))``.\n\nkind : {None, 'sort', 'table'}, optional\n\n    The algorithm to use. This will not affect the final result,\n\n    but will affect the speed and memory use. The default, None,\n\n    will select automatically based on memory considerations.\n\n\n\n    * If 'sort', will use a mergesort-based approach. This will have\n\n      a memory usage of roughly 6 times the sum of the sizes of\n\n      `ar1` and `ar2`, not accounting for size of dtypes.\n\n    * If 'table', will use a lookup table approach similar\n\n      to a counting sort. This is only available for boolean and\n\n      integer arrays. This will have a memory usage of the\n\n      size of `ar1` plus the max-min value of `ar2`. `assume_unique`\n\n      has no effect when the 'table' option is used.\n\n    * If None, will automatically choose 'table' if\n\n      the required memory allocation is less than or equal to\n\n      6 times the sum of the sizes of `ar1` and `ar2`,\n\n      otherwise will use 'sort'. This is done to not use\n\n      a large amount of memory by default, even though\n\n      'table' may be faster in most cases. If 'table' is chosen,\n\n      `assume_unique` will have no effect.\n\n\n\n    .. versionadded:: 1.8.0\n\n\n\nReturns\n\n-------\n\nin1d : (M,) ndarray, bool\n\n    The values `ar1[in1d]` are in `ar2`.\n\n\n\nSee Also\n\n--------\n\nisin                  : Version of this function that preserves the\n\n                        shape of ar1.\n\n\n\nNotes\n\n-----\n\n`in1d` can be considered as an element-wise function version of the\n\npython keyword `in`, for 1-D sequences. ``in1d(a, b)`` is roughly\n\nequivalent to ``np.array([item in b for item in a])``.\n\nHowever, this idea fails if `ar2` is a set, or similar (non-sequence)\n\ncontainer:  As ``ar2`` is converted to an array, in those cases\n\n``asarray(ar2)`` is an object array rather than the expected array of\n\ncontained values.\n\n\n\nUsing ``kind='table'`` tends to be faster than `kind='sort'` if the\n\nfollowing relationship is true:\n\n``log10(len(ar2)) &gt; (log10(max(ar2)-min(ar2)) - 2.27) / 0.927``,\n\nbut may use greater memory. The default value for `kind` will\n\nbe automatically selected based only on memory usage, so one may\n\nmanually set ``kind='table'`` if memory constraints can be relaxed.\n\n\n\n.. versionadded:: 1.4.0\n\n\n\nExamples\n\n--------\n\n&gt;&gt;&gt; import numpy as np\n\n&gt;&gt;&gt; test = np.array([0, 1, 2, 5, 0])\n\n&gt;&gt;&gt; states = [0, 2]\n\n&gt;&gt;&gt; mask = np.in1d(test, states)\n\n&gt;&gt;&gt; mask\n\narray([ True, False,  True, False,  True])\n\n&gt;&gt;&gt; test[mask]\n\narray([0, 2, 0])\n\n&gt;&gt;&gt; mask = np.in1d(test, states, invert=True)\n\n&gt;&gt;&gt; mask\n\narray([False,  True, False,  True, False])\n\n&gt;&gt;&gt; test[mask]\n\narray([1, 5])\n\nClass docstring:\n\nClass to wrap functions with checks for __array_function__ overrides.\n\n\n\nAll arguments are required, and can only be passed by position.\n\n\n\nParameters\n\n----------\n\ndispatcher : function or None\n\n    The dispatcher function that returns a single sequence-like object\n\n    of all arguments relevant.  It must have the same signature (except\n\n    the default values) as the actual implementation.\n\n    If ``None``, this is a ``like=`` dispatcher and the\n\n    ``_ArrayFunctionDispatcher`` must be called with ``like`` as the\n\n    first (additional and positional) argument.\n\nimplementation : function\n\n    Function that implements the operation on NumPy arrays without\n\n    overrides.  Arguments passed calling the ``_ArrayFunctionDispatcher``\n\n    will be forwarded to this (and the ``dispatcher``) as if using\n\n    ``*args, **kwargs``.\n\n\n\nAttributes\n\n----------\n\n_implementation : function\n\n    The original implementation passed in.\n\n\n\n\nnp.isin?\n\n\nSignature:      \n\nnp.isin(\n\n    element,\n\n    test_elements,\n\n    assume_unique=False,\n\n    invert=False,\n\n    *,\n\n    kind=None,\n\n)\n\nCall signature:  np.isin(*args, **kwargs)\n\nType:            _ArrayFunctionDispatcher\n\nString form:     &lt;function isin at 0x107c711c0&gt;\n\nFile:            ~/mambaforge/lib/python3.12/site-packages/numpy/lib/_arraysetops_impl.py\n\nDocstring:      \n\nCalculates ``element in test_elements``, broadcasting over `element` only.\n\nReturns a boolean array of the same shape as `element` that is True\n\nwhere an element of `element` is in `test_elements` and False otherwise.\n\n\n\nParameters\n\n----------\n\nelement : array_like\n\n    Input array.\n\ntest_elements : array_like\n\n    The values against which to test each value of `element`.\n\n    This argument is flattened if it is an array or array_like.\n\n    See notes for behavior with non-array-like parameters.\n\nassume_unique : bool, optional\n\n    If True, the input arrays are both assumed to be unique, which\n\n    can speed up the calculation.  Default is False.\n\ninvert : bool, optional\n\n    If True, the values in the returned array are inverted, as if\n\n    calculating `element not in test_elements`. Default is False.\n\n    ``np.isin(a, b, invert=True)`` is equivalent to (but faster\n\n    than) ``np.invert(np.isin(a, b))``.\n\nkind : {None, 'sort', 'table'}, optional\n\n    The algorithm to use. This will not affect the final result,\n\n    but will affect the speed and memory use. The default, None,\n\n    will select automatically based on memory considerations.\n\n\n\n    * If 'sort', will use a mergesort-based approach. This will have\n\n      a memory usage of roughly 6 times the sum of the sizes of\n\n      `element` and `test_elements`, not accounting for size of dtypes.\n\n    * If 'table', will use a lookup table approach similar\n\n      to a counting sort. This is only available for boolean and\n\n      integer arrays. This will have a memory usage of the\n\n      size of `element` plus the max-min value of `test_elements`.\n\n      `assume_unique` has no effect when the 'table' option is used.\n\n    * If None, will automatically choose 'table' if\n\n      the required memory allocation is less than or equal to\n\n      6 times the sum of the sizes of `element` and `test_elements`,\n\n      otherwise will use 'sort'. This is done to not use\n\n      a large amount of memory by default, even though\n\n      'table' may be faster in most cases. If 'table' is chosen,\n\n      `assume_unique` will have no effect.\n\n\n\n\n\nReturns\n\n-------\n\nisin : ndarray, bool\n\n    Has the same shape as `element`. The values `element[isin]`\n\n    are in `test_elements`.\n\n\n\nNotes\n\n-----\n\n\n\n`isin` is an element-wise function version of the python keyword `in`.\n\n``isin(a, b)`` is roughly equivalent to\n\n``np.array([item in b for item in a])`` if `a` and `b` are 1-D sequences.\n\n\n\n`element` and `test_elements` are converted to arrays if they are not\n\nalready. If `test_elements` is a set (or other non-sequence collection)\n\nit will be converted to an object array with one element, rather than an\n\narray of the values contained in `test_elements`. This is a consequence\n\nof the `array` constructor's way of handling non-sequence collections.\n\nConverting the set to a list usually gives the desired behavior.\n\n\n\nUsing ``kind='table'`` tends to be faster than `kind='sort'` if the\n\nfollowing relationship is true:\n\n``log10(len(test_elements)) &gt;\n\n(log10(max(test_elements)-min(test_elements)) - 2.27) / 0.927``,\n\nbut may use greater memory. The default value for `kind` will\n\nbe automatically selected based only on memory usage, so one may\n\nmanually set ``kind='table'`` if memory constraints can be relaxed.\n\n\n\n.. versionadded:: 1.13.0\n\n\n\nExamples\n\n--------\n\n&gt;&gt;&gt; import numpy as np\n\n&gt;&gt;&gt; element = 2*np.arange(4).reshape((2, 2))\n\n&gt;&gt;&gt; element\n\narray([[0, 2],\n\n       [4, 6]])\n\n&gt;&gt;&gt; test_elements = [1, 2, 4, 8]\n\n&gt;&gt;&gt; mask = np.isin(element, test_elements)\n\n&gt;&gt;&gt; mask\n\narray([[False,  True],\n\n       [ True, False]])\n\n&gt;&gt;&gt; element[mask]\n\narray([2, 4])\n\n\n\nThe indices of the matched values can be obtained with `nonzero`:\n\n\n\n&gt;&gt;&gt; np.nonzero(mask)\n\n(array([0, 1]), array([1, 0]))\n\n\n\nThe test can also be inverted:\n\n\n\n&gt;&gt;&gt; mask = np.isin(element, test_elements, invert=True)\n\n&gt;&gt;&gt; mask\n\narray([[ True, False],\n\n       [False,  True]])\n\n&gt;&gt;&gt; element[mask]\n\narray([0, 6])\n\n\n\nBecause of how `array` handles sets, the following does not\n\nwork as expected:\n\n\n\n&gt;&gt;&gt; test_set = {1, 2, 4, 8}\n\n&gt;&gt;&gt; np.isin(element, test_set)\n\narray([[False, False],\n\n       [False, False]])\n\n\n\nCasting the set to a list gives the expected result:\n\n\n\n&gt;&gt;&gt; np.isin(element, list(test_set))\n\narray([[False,  True],\n\n       [ True, False]])\n\nClass docstring:\n\nClass to wrap functions with checks for __array_function__ overrides.\n\n\n\nAll arguments are required, and can only be passed by position.\n\n\n\nParameters\n\n----------\n\ndispatcher : function or None\n\n    The dispatcher function that returns a single sequence-like object\n\n    of all arguments relevant.  It must have the same signature (except\n\n    the default values) as the actual implementation.\n\n    If ``None``, this is a ``like=`` dispatcher and the\n\n    ``_ArrayFunctionDispatcher`` must be called with ``like`` as the\n\n    first (additional and positional) argument.\n\nimplementation : function\n\n    Function that implements the operation on NumPy arrays without\n\n    overrides.  Arguments passed calling the ``_ArrayFunctionDispatcher``\n\n    will be forwarded to this (and the ``dispatcher``) as if using\n\n    ``*args, **kwargs``.\n\n\n\nAttributes\n\n----------\n\n_implementation : function\n\n    The original implementation passed in.\n\n\n\n\nA, B\n\n(array([1, 2, 3, 4, 5]), array([1, 2, 3]))\n\n\n\nresponse = np.in1d(B, A)\n\n\nif False in response:\n    print(\"B is not a subset of A\")\nelse:\n    print(\"B is a subset of A\")\n\nB is a subset of A\n\n\n\nresponse.all()\n\nnp.True_\n\n\n\nnp.array([False, True]).astype(int).sum()\n\nnp.int64(1)\n\n\n\nresponse.astype(int).sum() == len(B)\n\nnp.True_\n\n\n\nnp.isin(B, A).all()\n\nnp.True_\n\n\n\n# Case where B is not a subset of A\nB = np.array([1, 2, 6])\nA = np.array([1, 2, 3, 4, 5])\n\nnp.isin(B, A).all()\n\nnp.False_\n\n\n\nempty_set = np.array([])\nA = np.array([1, 2, 3])\n\nnp.isin(empty_set, A).all()\n\nnp.True_\n\n\n\n# Visualising sets using Venn diagrams\nfrom matplotlib_venn import venn2\n\n# Define the sets\nset1 = {1, 2, 3}\nset2 = {2, 3, 5}\n\n# Create the Venn diagram\nvenn = venn2([set1, set2], ('Set 1', 'Set 2'))\n\n\n\n\n\n\n\n\n\n# Define the sets\nsetA = {1, 2, 3}\nsetB = {2, 3, 5}\n\n# Create the Venn diagram\nvenn = venn2([setA, setB], ('A', 'B'))\n\n# Customize the labels to show the elements and sizes\ntry:\n    venn.get_label_by_id('10').set_text(\n        f\"A: {', '.join(map(str, setA - setB))}\\n(Size: {len(setA - setB)})\"\n    )  # Only in A\n    venn.get_label_by_id('01').set_text(\n        f\"B: {', '.join(map(str, setB - setA))}\\n(Size: {len(setB - setA)})\"\n    )  # Only in B\n    venn.get_label_by_id('11').set_text(\n        f\"A ∩ B: {', '.join(map(str, setA & setB))}\\n(Size: {len(setA & setB)})\"\n    )  # Intersection (A ∩ B)\nexcept:\n    pass\n\n# Display the plot\nplt.title(\"Venn Diagram with Labels A, B, and A ∩ B\")\n\nText(0.5, 1.0, 'Venn Diagram with Labels A, B, and A ∩ B')\n\n\n\n\n\n\n\n\n\n\nSet_A = set([1,2,3])\nSet_B = set([2,3,5])\n\n# Union\nUnion = Set_A.union(Set_B)\nprint('Union:', Union)\n\nUnion: {1, 2, 3, 5}\n\n\n\n# numpy\nSet_A = np.array(list(Set_A))\nSet_B = np.array(list(Set_B))\n\nUnion = np.union1d(Set_A, Set_B)\nprint('Union:', Union)\n\nUnion: [1 2 3 5]\n\n\n\n# From scratch\na = np.array([1, 2, 3])\nb = np.array([2, 3, 5])\n\nunion = a.copy()\nfor i in b:\n    if i not in union:\n        union = np.append(union, i)\nprint('Union:', union)\n\nUnion: [1 2 3 5]\n\n\n\nunion = []\nfor element in a:\n    if element not in union:\n        union.append(element)\nfor element in b:\n    if element not in union:\n        union.append(element)\n\nunion = np.array(union)\nprint('Union:', union)\n\nUnion: [1 2 3 5]\n\n\n\nnp.unique(np.concatenate([a, b]))\n\narray([1, 2, 3, 5])\n\n\n\n# Intersection\nSet_A = set([1,2,3])\nSet_B = set([2,3,5])\nIntersection = Set_A.intersection(Set_B)\nprint('Intersection:', Intersection)\n\nIntersection: {2, 3}\n\n\n\n# Intersection using numpy\nA = np.array([1, 2, 3])\nB = np.array([2, 3, 5])\n\nIntersection = np.intersect1d(A, B)\nprint('Intersection:', Intersection)\n\nIntersection: [2 3]\n\n\n\n# From scratch\nintersection = []\nfor i in a:\n    if i in b:\n        intersection.append(i)\nintersection = np.array(intersection)\nprint('Intersection:', intersection)\n\nIntersection: [2 3]\n\n\n\n# Difference\nDifference = Set_A.difference(Set_B)\nprint('Difference:', Difference)\n\nDifference: {1}\n\n\n\nSet_B.difference(Set_A)\n\n{5}\n\n\n\n# Difference in numpy\nDifference_A_B = np.setdiff1d(A, B)\nprint('Difference A/B:', Difference_A_B)\nDifference_B_A = np.setdiff1d(B, A)\nprint('Difference B/A:', Difference_B_A)\n\nDifference A/B: [1]\nDifference B/A: [5]\n\n\n\n# From scratch\ndifference_A_B = []\nfor i in a:\n    if i not in b:\n        difference_A_B.append(i)\ndifference_A_B = np.array(difference_A_B)\nprint('Difference A/B:', difference_A_B)\n\n\nDifference A/B: [1]\n\n\n\ndef difference(A, B):\n    \"\"\"\n    Function to find the difference between two sets A and B\n    A: numpy array 1d\n    B: numpy array 1d\n\n    Returns:\n    difference_A_B: numpy array 1d\n    \"\"\"\n    difference_A_B = []\n    for i in A:\n        if i not in B:\n            difference_A_B.append(i)\n    difference_A_B = np.array(difference_A_B)\n    return difference_A_B\n\n\n# Complement\nuniversal_set = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\nA = np.array([1, 2, 3])\n\ncomplement_A = np.setdiff1d(universal_set, A)\nprint('Complement of A:', complement_A)\n\nComplement of A: [ 4  5  6  7  8  9 10]\n\n\n\n# Disjoint sets\nSet_A = set([1,2,3])\nSet_B = set([4,5,6])\n\nIntersection = Set_A.intersection(Set_B)\nprint('Intersection:', Intersection)\n\nif len(Intersection) == 0:\n    print('Sets are disjoint')\nelse:\n    print('Sets are not disjoint')\n\nIntersection: set()\nSets are disjoint\n\n\n\ncollection_sets = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ncollection_sets\n\n# check if all sets are disjoint\ndisjoint = True\n\n\n# Associative property of union\n\n# (A ∪ B) ∪ C = A ∪ (B ∪ C)\n\nA = np.array([1, 2, 3])\nB = np.array([2, 3, 4])\nC = np.array([3, 4, 5])\n\n# (A ∪ B) ∪ C\nlhs = np.union1d(np.union1d(A, B), C)\nprint('(A ∪ B) ∪ C:', lhs)\n\n# A ∪ (B ∪ C)\nrhs = np.union1d(A, np.union1d(B, C))\nprint('A ∪ (B ∪ C):', rhs)\n\n(A ∪ B) ∪ C: [1 2 3 4 5]\nA ∪ (B ∪ C): [1 2 3 4 5]\n\n\n\n# Associative property of intersection \n\n\n\n# De Morgan's laws",
    "crumbs": [
      "Home",
      "Getting Started",
      "Set"
    ]
  },
  {
    "objectID": "notebooks/embeddings-angle.html",
    "href": "notebooks/embeddings-angle.html",
    "title": "Word Embeddings and Vector Angles",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'",
    "crumbs": [
      "Home",
      "Machine Learning",
      "Word Embeddings and Vector Angles"
    ]
  },
  {
    "objectID": "notebooks/embeddings-angle.html#introduction",
    "href": "notebooks/embeddings-angle.html#introduction",
    "title": "Word Embeddings and Vector Angles",
    "section": "Introduction",
    "text": "Introduction\nThis notebook explores the fascinating relationship between vector angles and semantic similarity in word embeddings. We’ll demonstrate how geometric concepts from linear algebra apply to natural language processing, particularly in measuring word similarities.\n\nLearning Objectives\nBy the end of this notebook, you will understand: - How to calculate angles between vectors using dot products - The concept of cosine similarity in vector spaces - How word embeddings capture semantic relationships - The practical application of vector geometry in NLP\n\n\nPrerequisites\n\nBasic linear algebra (vectors, dot products)\nUnderstanding of Python and NumPy\nFamiliarity with the concept of word embeddings",
    "crumbs": [
      "Home",
      "Machine Learning",
      "Word Embeddings and Vector Angles"
    ]
  },
  {
    "objectID": "notebooks/embeddings-angle.html#vector-angles-mathematical-foundation",
    "href": "notebooks/embeddings-angle.html#vector-angles-mathematical-foundation",
    "title": "Word Embeddings and Vector Angles",
    "section": "Vector Angles: Mathematical Foundation",
    "text": "Vector Angles: Mathematical Foundation\nWe begin with a simple example of computing angles between 2D vectors. The angle between two vectors can be calculated using the dot product formula:\n\\[\\cos(\\theta) = \\frac{\\mathbf{v_1} \\cdot \\mathbf{v_2}}{|\\mathbf{v_1}| \\cdot |\\mathbf{v_2}|}\\]\nWhere \\(\\theta\\) is the angle between vectors \\(\\mathbf{v_1}\\) and \\(\\mathbf{v_2}\\).\n\nExploring Different Vector Combinations\nNotice how the angle changes as we modify the vectors. Perpendicular vectors have a 90° angle, while parallel vectors have 0° angle.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "Word Embeddings and Vector Angles"
    ]
  },
  {
    "objectID": "notebooks/embeddings-angle.html#word-embeddings-from-words-to-vectors",
    "href": "notebooks/embeddings-angle.html#word-embeddings-from-words-to-vectors",
    "title": "Word Embeddings and Vector Angles",
    "section": "Word Embeddings: From Words to Vectors",
    "text": "Word Embeddings: From Words to Vectors\nNow we transition to the main topic: word embeddings. Word embeddings are dense vector representations of words that capture semantic relationships. We’ll use pre-trained GloVe embeddings to explore how words relate to each other in vector space.\n\nLoading Pre-trained Word Embeddings\nWe’ll use the GloVe (Global Vectors for Word Representation) model, which creates word embeddings by analyzing word co-occurrence statistics from large text corpora.\n\n\nExample 1: Simple 2D Vector Angles\nLet’s start with basic examples to understand how angles work with simple vectors.\n\n\nExamining Word Vectors\nLet’s look at the actual vector representation of the word “king”. Each word is represented as a 50-dimensional vector where each dimension captures different semantic features.\n\ndef angle_between(v1, v2):\n    cos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n    return np.arccos(np.clip(cos_theta, -1.0, 1.0)) * 180 / np.pi\n\nv1 = np.array([1, 0])\nv2 = np.array([1, 1])\n\nplt.quiver([0, 0], [0, 0], [v1[0], v2[0]], [v1[1], v2[1]], angles='xy', scale_units='xy', scale=1)\nplt.xlim(-1, 2)\nplt.ylim(-1, 2)\n\nprint(f\"Angle: {angle_between(v1, v2):.2f}°\")\n\nAngle: 45.00°",
    "crumbs": [
      "Home",
      "Machine Learning",
      "Word Embeddings and Vector Angles"
    ]
  },
  {
    "objectID": "notebooks/embeddings-angle.html#measuring-semantic-similarity-with-angles",
    "href": "notebooks/embeddings-angle.html#measuring-semantic-similarity-with-angles",
    "title": "Word Embeddings and Vector Angles",
    "section": "Measuring Semantic Similarity with Angles",
    "text": "Measuring Semantic Similarity with Angles\n\nKing and Queen: A Classic Example\nThe angle between “king” and “queen” vectors demonstrates how semantically related words have smaller angles between their vector representations. This is a fundamental principle in word embeddings.\n\n\nFamily Relationships in Vector Space\nFamily relationship words like “uncle” and “aunt” should have relatively small angles since they represent similar family concepts.\n\n\nDissimilar Words: Larger Angles\nWhen we compare semantically unrelated words like “king” and “python” (the programming language), we expect to see much larger angles, indicating low semantic similarity.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "Word Embeddings and Vector Angles"
    ]
  },
  {
    "objectID": "notebooks/embeddings-angle.html#exploring-word-neighborhoods",
    "href": "notebooks/embeddings-angle.html#exploring-word-neighborhoods",
    "title": "Word Embeddings and Vector Angles",
    "section": "Exploring Word Neighborhoods",
    "text": "Exploring Word Neighborhoods\n\nFinding Similar Words\nThe most_similar function finds words with the smallest angles (highest cosine similarity) to a given word. This demonstrates how vector space geometry captures semantic relationships.\n\n\nFinding Dissimilar Words\nConversely, we can find words that are most dissimilar (largest angles) to understand what the model considers semantically distant.\n\n\nVerification: Computing Angles with Dissimilar Words\nLet’s verify our findings by computing the actual angle between “king” and one of the most dissimilar words.",
    "crumbs": [
      "Home",
      "Machine Learning",
      "Word Embeddings and Vector Angles"
    ]
  },
  {
    "objectID": "notebooks/embeddings-angle.html#key-insights-and-conclusions",
    "href": "notebooks/embeddings-angle.html#key-insights-and-conclusions",
    "title": "Word Embeddings and Vector Angles",
    "section": "Key Insights and Conclusions",
    "text": "Key Insights and Conclusions\n\nWhat We’ve Learned\n\nVector Angles and Similarity: Smaller angles between word vectors indicate greater semantic similarity\nCosine Similarity: This metric (cosine of the angle) is widely used in NLP for measuring word similarity\nGeometric Intuition: Word embeddings transform linguistic relationships into geometric relationships in high-dimensional space\nPractical Applications: These concepts are fundamental to search engines, recommendation systems, and many NLP tasks\n\n\n\nMathematical Relationship\nThe relationship between angle \\(\\theta\\) and cosine similarity is: - \\(\\theta = 0°\\) → \\(\\cos(\\theta) = 1\\) → Perfect similarity - \\(\\theta = 90°\\) → \\(\\cos(\\theta) = 0\\) → No correlation\n- \\(\\theta = 180°\\) → \\(\\cos(\\theta) = -1\\) → Perfect opposition\n\n\nReal-world Applications\nUnderstanding vector angles in embeddings is crucial for: - Information Retrieval: Finding relevant documents - Recommendation Systems: Suggesting similar items - Machine Translation: Aligning words across languages - Sentiment Analysis: Understanding emotional relationships between words\n\nprint(f\"Angle: {angle_between(v1, np.array([0, 1])):.2f}°\")\nplt.quiver([0, 0], [0, 0], [v1[0], 0], [v1[1], 1], angles='xy', scale_units='xy', scale=1)\nplt.xlim(-1, 2)\nplt.ylim(-1, 2)\n\nAngle: 90.00°\n\n\n\n\n\n\n\n\n\n\nimport gensim.downloader as api\nfrom scipy.spatial.distance import cosine\n\nmodel = api.load(\"glove-wiki-gigaword-50\")  # Small 50D GloVe model\n\n\ndef get_cosine_similarity(word1, word2):\n    v1 = model[word1]\n    v2 = model[word2]\n    return angle_between(v1, v2)\n\n\nword1 = \"king\"\nv1 = model[word1]\n\n\nv1\n\narray([ 0.50451 ,  0.68607 , -0.59517 , -0.022801,  0.60046 , -0.13498 ,\n       -0.08813 ,  0.47377 , -0.61798 , -0.31012 , -0.076666,  1.493   ,\n       -0.034189, -0.98173 ,  0.68229 ,  0.81722 , -0.51874 , -0.31503 ,\n       -0.55809 ,  0.66421 ,  0.1961  , -0.13495 , -0.11476 , -0.30344 ,\n        0.41177 , -2.223   , -1.0756  , -1.0783  , -0.34354 ,  0.33505 ,\n        1.9927  , -0.04234 , -0.64319 ,  0.71125 ,  0.49159 ,  0.16754 ,\n        0.34344 , -0.25663 , -0.8523  ,  0.1661  ,  0.40102 ,  1.1685  ,\n       -1.0137  , -0.21585 , -0.15155 ,  0.78321 , -0.91241 , -1.6106  ,\n       -0.64426 , -0.51042 ], dtype=float32)\n\n\n\nword2 = \"queen\"\nv2 = model[word2]\nprint(v2)\n\n[ 0.37854    1.8233    -1.2648    -0.1043     0.35829    0.60029\n -0.17538    0.83767   -0.056798  -0.75795    0.22681    0.98587\n  0.60587   -0.31419    0.28877    0.56013   -0.77456    0.071421\n -0.5741     0.21342    0.57674    0.3868    -0.12574    0.28012\n  0.28135   -1.8053    -1.0421    -0.19255   -0.55375   -0.054526\n  1.5574     0.39296   -0.2475     0.34251    0.45365    0.16237\n  0.52464   -0.070272  -0.83744   -1.0326     0.45946    0.25302\n -0.17837   -0.73398   -0.20025    0.2347    -0.56095   -2.2839\n  0.0092753 -0.60284  ]\n\n\n\nangle_between(v1, v2)\n\n38.3805515334704\n\n\n\nangle_between(model[\"uncle\"], model[\"aunt\"])\n\n40.26145236751397\n\n\n\n# Now some dissimilar words\nangle_between(model[\"king\"], model[\"python\"])\n\n79.36413285046953\n\n\n\nmodel.most_similar(\"king\")\n\n[('prince', 0.8236179351806641),\n ('queen', 0.7839044332504272),\n ('ii', 0.7746230363845825),\n ('emperor', 0.7736247777938843),\n ('son', 0.766719400882721),\n ('uncle', 0.7627150416374207),\n ('kingdom', 0.7542160749435425),\n ('throne', 0.7539914846420288),\n ('brother', 0.7492411136627197),\n ('ruler', 0.7434254288673401)]\n\n\n\n# most dissimilar words to \"king\"\nmodel.most_similar(negative=[\"king\"])\n\n[('4,835', 0.729358434677124),\n ('rules-based', 0.7123876810073853),\n ('renos', 0.7085371613502502),\n ('meawhile', 0.706490159034729),\n ('nanobiotechnology', 0.6925080418586731),\n ('m-42', 0.6916395425796509),\n ('poligny', 0.6882078051567078),\n ('onyekwe', 0.6877189874649048),\n ('asie', 0.6861312985420227),\n ('metabolomics', 0.682388961315155)]\n\n\n\nangle_between(model[\"king\"], model[\"rules-based\"])\n\n135.42952204160463",
    "crumbs": [
      "Home",
      "Machine Learning",
      "Word Embeddings and Vector Angles"
    ]
  },
  {
    "objectID": "notebooks/probability.html",
    "href": "notebooks/probability.html",
    "title": "Probability Theory Fundamentals",
    "section": "",
    "text": "This notebook covers the fundamental concepts of probability theory, including sample spaces, events, and probability laws. We’ll explore these concepts through practical examples using Python.\n\n\n\nUnderstand sample spaces and events\nLearn about combinatorics and power sets\nImplement probability laws\nWork with discrete probability distributions\n\nLet’s begin by exploring sample spaces for common probability experiments:\n\nimport numpy as np\n\n# Define the sample space and event space for a fair coin\nsample_space = np.array(['H', 'T'])\nprint(\"Sample Space for Coin Toss:\", sample_space)\n\n# Dice throw\nsample_space_dice = [1, 2, 3, 4, 5, 6]\nprint(\"Sample Space for Dice Throw:\", sample_space_dice)\n\nSample Space for Coin Toss: ['H' 'T']\nSample Space for Dice Throw: [1, 2, 3, 4, 5, 6]\n\n\n\n\n\nIn probability theory, the sample space (Ω) is the set of all possible outcomes of an experiment. An event is any subset of the sample space.\nLet’s start with simple examples:\nNext, let’s define some specific events:\n\n\n\nTo understand all possible events, we need to explore combinatorics. The event space (σ-algebra) consists of all possible events, which is the power set of the sample space.\nLet’s explore combinations and permutations using Python’s itertools:\n\n# Define the event space for getting a head\nevent_space_head = np.array(['H'])\n\n# Define the event space for getting an odd number in a dice throw\nevent_space_odd = [1, 3, 5]\n\n\n# Mini tutorial on itertools\nfrom itertools import combinations, permutations, product\n\nx = [1, 2, 3, 4]\n# Combinations\nprint('Combinations')\nprint(list(combinations(x, 2)))\n\nCombinations\n[(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n\n\n\nlist(combinations(x, 0))\n\n[()]\n\n\n\nfor i in range(0, len(x)+1):\n    print(\"Combinations of length \", i)\n    print(list(combinations(x, i)))\n    print()\n\nCombinations of length  0\n[()]\n\nCombinations of length  1\n[(1,), (2,), (3,), (4,)]\n\nCombinations of length  2\n[(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n\nCombinations of length  3\n[(1, 2, 3), (1, 2, 4), (1, 3, 4), (2, 3, 4)]\n\nCombinations of length  4\n[(1, 2, 3, 4)]\n\n\n\n\n# Combine all using chain \nfrom itertools import chain\npowerset = list(chain.from_iterable(combinations(x, i) for i in range(0, len(x)+1)))\nprint(powerset)\n\n[(), (1,), (2,), (3,), (4,), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4), (1, 2, 3), (1, 2, 4), (1, 3, 4), (2, 3, 4), (1, 2, 3, 4)]\n\n\n\nimport itertools\n# Generate the entire event space (power set)\ndef generate_event_space(sample_space):\n\n    \"\"\"\n    Generates the power set of the sample space, which is the event space.\n\n    Args:\n        sample_space (np.ndarray): The sample space.\n\n    Returns:\n        list: A list of NumPy arrays representing all possible events.\n    \"\"\"\n    n = len(sample_space)\n    # Use itertools to generate all subsets\n    power_set = list(itertools.chain.from_iterable(\n        itertools.combinations(sample_space, r) for r in range(n + 1)\n    ))\n    # Convert tuples to NumPy arrays\n    return [np.array(event) for event in power_set]\n\n\ngenerate_event_space(sample_space)\n\n[array([], dtype=float64),\n array(['H'], dtype='&lt;U1'),\n array(['T'], dtype='&lt;U1'),\n array(['H', 'T'], dtype='&lt;U1')]\n\n\n\nsample_space_dice\n\n[1, 2, 3, 4, 5, 6]\n\n\n\ngenerate_event_space(sample_space_dice)\n\n[array([], dtype=float64),\n array([1]),\n array([2]),\n array([3]),\n array([4]),\n array([5]),\n array([6]),\n array([1, 2]),\n array([1, 3]),\n array([1, 4]),\n array([1, 5]),\n array([1, 6]),\n array([2, 3]),\n array([2, 4]),\n array([2, 5]),\n array([2, 6]),\n array([3, 4]),\n array([3, 5]),\n array([3, 6]),\n array([4, 5]),\n array([4, 6]),\n array([5, 6]),\n array([1, 2, 3]),\n array([1, 2, 4]),\n array([1, 2, 5]),\n array([1, 2, 6]),\n array([1, 3, 4]),\n array([1, 3, 5]),\n array([1, 3, 6]),\n array([1, 4, 5]),\n array([1, 4, 6]),\n array([1, 5, 6]),\n array([2, 3, 4]),\n array([2, 3, 5]),\n array([2, 3, 6]),\n array([2, 4, 5]),\n array([2, 4, 6]),\n array([2, 5, 6]),\n array([3, 4, 5]),\n array([3, 4, 6]),\n array([3, 5, 6]),\n array([4, 5, 6]),\n array([1, 2, 3, 4]),\n array([1, 2, 3, 5]),\n array([1, 2, 3, 6]),\n array([1, 2, 4, 5]),\n array([1, 2, 4, 6]),\n array([1, 2, 5, 6]),\n array([1, 3, 4, 5]),\n array([1, 3, 4, 6]),\n array([1, 3, 5, 6]),\n array([1, 4, 5, 6]),\n array([2, 3, 4, 5]),\n array([2, 3, 4, 6]),\n array([2, 3, 5, 6]),\n array([2, 4, 5, 6]),\n array([3, 4, 5, 6]),\n array([1, 2, 3, 4, 5]),\n array([1, 2, 3, 4, 6]),\n array([1, 2, 3, 5, 6]),\n array([1, 2, 4, 5, 6]),\n array([1, 3, 4, 5, 6]),\n array([2, 3, 4, 5, 6]),\n array([1, 2, 3, 4, 5, 6])]\n\n\n\n# Probability law function for a fair coin\ndef probability(event, sample_space):\n    \"\"\"\n    Computes the probability of an event for a fair coin.\n\n    Args:\n        event (np.ndarray): The event (subset of the sample space).\n\n    Returns:\n        float: The probability of the event.\n    \"\"\"\n    # Convert the event into a NumPy array for comparison\n    event = np.array(event)\n\n\n    # Validate if the event is a subset of the sample space\n    if not np.all(np.isin(event, sample_space)):\n        raise ValueError(\"Invalid event. Event must be a subset of the sample space.\")\n\n    # Probability logic\n    if len(event) == 0:         # Empty set\n        return 0.0\n    elif np.array_equal(event, sample_space):  # Entire sample space\n        return 1.0\n    else:                       # Any single event (like {H} or {T})\n        return len(event) / len(sample_space)\n\n\nfor event in generate_event_space(sample_space):\n    print(f\"Event: {event} -&gt; Probability: {probability(event, sample_space)}\")\n    \n\nEvent: [] -&gt; Probability: 0.0\nEvent: ['H'] -&gt; Probability: 0.5\nEvent: ['T'] -&gt; Probability: 0.5\nEvent: ['H' 'T'] -&gt; Probability: 1.0\n\n\n\nimport pandas as pd\n# Initialize an empty list to store events and probabilities\nevents_data = []\n\n# Generate events and probabilities\nfor event in generate_event_space(sample_space_dice):\n    event_tuple = tuple(event)\n    event_prob = probability(event, sample_space_dice)\n    events_data.append((event_tuple, event_prob))\n\n# Create a pandas DataFrame\ndf = pd.DataFrame(events_data, columns=['Event', 'Probability'])\n\n\ndf\n\n\n\n\n\n\n\n\nEvent\nProbability\n\n\n\n\n0\n()\n0.000000\n\n\n1\n(1,)\n0.166667\n\n\n2\n(2,)\n0.166667\n\n\n3\n(3,)\n0.166667\n\n\n4\n(4,)\n0.166667\n\n\n...\n...\n...\n\n\n59\n(1, 2, 3, 5, 6)\n0.833333\n\n\n60\n(1, 2, 4, 5, 6)\n0.833333\n\n\n61\n(1, 3, 4, 5, 6)\n0.833333\n\n\n62\n(2, 3, 4, 5, 6)\n0.833333\n\n\n63\n(1, 2, 3, 4, 5, 6)\n1.000000\n\n\n\n\n64 rows × 2 columns",
    "crumbs": [
      "Home",
      "Getting Started",
      "Probability Theory Fundamentals"
    ]
  },
  {
    "objectID": "notebooks/probability.html#learning-objectives",
    "href": "notebooks/probability.html#learning-objectives",
    "title": "Probability Theory Fundamentals",
    "section": "",
    "text": "Understand sample spaces and events\nLearn about combinatorics and power sets\nImplement probability laws\nWork with discrete probability distributions\n\nLet’s begin by exploring sample spaces for common probability experiments:\n\nimport numpy as np\n\n# Define the sample space and event space for a fair coin\nsample_space = np.array(['H', 'T'])\nprint(\"Sample Space for Coin Toss:\", sample_space)\n\n# Dice throw\nsample_space_dice = [1, 2, 3, 4, 5, 6]\nprint(\"Sample Space for Dice Throw:\", sample_space_dice)\n\nSample Space for Coin Toss: ['H' 'T']\nSample Space for Dice Throw: [1, 2, 3, 4, 5, 6]",
    "crumbs": [
      "Home",
      "Getting Started",
      "Probability Theory Fundamentals"
    ]
  },
  {
    "objectID": "notebooks/probability.html#sample-spaces-and-events",
    "href": "notebooks/probability.html#sample-spaces-and-events",
    "title": "Probability Theory Fundamentals",
    "section": "",
    "text": "In probability theory, the sample space (Ω) is the set of all possible outcomes of an experiment. An event is any subset of the sample space.\nLet’s start with simple examples:\nNext, let’s define some specific events:",
    "crumbs": [
      "Home",
      "Getting Started",
      "Probability Theory Fundamentals"
    ]
  },
  {
    "objectID": "notebooks/probability.html#combinatorics-and-power-sets",
    "href": "notebooks/probability.html#combinatorics-and-power-sets",
    "title": "Probability Theory Fundamentals",
    "section": "",
    "text": "To understand all possible events, we need to explore combinatorics. The event space (σ-algebra) consists of all possible events, which is the power set of the sample space.\nLet’s explore combinations and permutations using Python’s itertools:\n\n# Define the event space for getting a head\nevent_space_head = np.array(['H'])\n\n# Define the event space for getting an odd number in a dice throw\nevent_space_odd = [1, 3, 5]\n\n\n# Mini tutorial on itertools\nfrom itertools import combinations, permutations, product\n\nx = [1, 2, 3, 4]\n# Combinations\nprint('Combinations')\nprint(list(combinations(x, 2)))\n\nCombinations\n[(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n\n\n\nlist(combinations(x, 0))\n\n[()]\n\n\n\nfor i in range(0, len(x)+1):\n    print(\"Combinations of length \", i)\n    print(list(combinations(x, i)))\n    print()\n\nCombinations of length  0\n[()]\n\nCombinations of length  1\n[(1,), (2,), (3,), (4,)]\n\nCombinations of length  2\n[(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4)]\n\nCombinations of length  3\n[(1, 2, 3), (1, 2, 4), (1, 3, 4), (2, 3, 4)]\n\nCombinations of length  4\n[(1, 2, 3, 4)]\n\n\n\n\n# Combine all using chain \nfrom itertools import chain\npowerset = list(chain.from_iterable(combinations(x, i) for i in range(0, len(x)+1)))\nprint(powerset)\n\n[(), (1,), (2,), (3,), (4,), (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4), (1, 2, 3), (1, 2, 4), (1, 3, 4), (2, 3, 4), (1, 2, 3, 4)]\n\n\n\nimport itertools\n# Generate the entire event space (power set)\ndef generate_event_space(sample_space):\n\n    \"\"\"\n    Generates the power set of the sample space, which is the event space.\n\n    Args:\n        sample_space (np.ndarray): The sample space.\n\n    Returns:\n        list: A list of NumPy arrays representing all possible events.\n    \"\"\"\n    n = len(sample_space)\n    # Use itertools to generate all subsets\n    power_set = list(itertools.chain.from_iterable(\n        itertools.combinations(sample_space, r) for r in range(n + 1)\n    ))\n    # Convert tuples to NumPy arrays\n    return [np.array(event) for event in power_set]\n\n\ngenerate_event_space(sample_space)\n\n[array([], dtype=float64),\n array(['H'], dtype='&lt;U1'),\n array(['T'], dtype='&lt;U1'),\n array(['H', 'T'], dtype='&lt;U1')]\n\n\n\nsample_space_dice\n\n[1, 2, 3, 4, 5, 6]\n\n\n\ngenerate_event_space(sample_space_dice)\n\n[array([], dtype=float64),\n array([1]),\n array([2]),\n array([3]),\n array([4]),\n array([5]),\n array([6]),\n array([1, 2]),\n array([1, 3]),\n array([1, 4]),\n array([1, 5]),\n array([1, 6]),\n array([2, 3]),\n array([2, 4]),\n array([2, 5]),\n array([2, 6]),\n array([3, 4]),\n array([3, 5]),\n array([3, 6]),\n array([4, 5]),\n array([4, 6]),\n array([5, 6]),\n array([1, 2, 3]),\n array([1, 2, 4]),\n array([1, 2, 5]),\n array([1, 2, 6]),\n array([1, 3, 4]),\n array([1, 3, 5]),\n array([1, 3, 6]),\n array([1, 4, 5]),\n array([1, 4, 6]),\n array([1, 5, 6]),\n array([2, 3, 4]),\n array([2, 3, 5]),\n array([2, 3, 6]),\n array([2, 4, 5]),\n array([2, 4, 6]),\n array([2, 5, 6]),\n array([3, 4, 5]),\n array([3, 4, 6]),\n array([3, 5, 6]),\n array([4, 5, 6]),\n array([1, 2, 3, 4]),\n array([1, 2, 3, 5]),\n array([1, 2, 3, 6]),\n array([1, 2, 4, 5]),\n array([1, 2, 4, 6]),\n array([1, 2, 5, 6]),\n array([1, 3, 4, 5]),\n array([1, 3, 4, 6]),\n array([1, 3, 5, 6]),\n array([1, 4, 5, 6]),\n array([2, 3, 4, 5]),\n array([2, 3, 4, 6]),\n array([2, 3, 5, 6]),\n array([2, 4, 5, 6]),\n array([3, 4, 5, 6]),\n array([1, 2, 3, 4, 5]),\n array([1, 2, 3, 4, 6]),\n array([1, 2, 3, 5, 6]),\n array([1, 2, 4, 5, 6]),\n array([1, 3, 4, 5, 6]),\n array([2, 3, 4, 5, 6]),\n array([1, 2, 3, 4, 5, 6])]\n\n\n\n# Probability law function for a fair coin\ndef probability(event, sample_space):\n    \"\"\"\n    Computes the probability of an event for a fair coin.\n\n    Args:\n        event (np.ndarray): The event (subset of the sample space).\n\n    Returns:\n        float: The probability of the event.\n    \"\"\"\n    # Convert the event into a NumPy array for comparison\n    event = np.array(event)\n\n\n    # Validate if the event is a subset of the sample space\n    if not np.all(np.isin(event, sample_space)):\n        raise ValueError(\"Invalid event. Event must be a subset of the sample space.\")\n\n    # Probability logic\n    if len(event) == 0:         # Empty set\n        return 0.0\n    elif np.array_equal(event, sample_space):  # Entire sample space\n        return 1.0\n    else:                       # Any single event (like {H} or {T})\n        return len(event) / len(sample_space)\n\n\nfor event in generate_event_space(sample_space):\n    print(f\"Event: {event} -&gt; Probability: {probability(event, sample_space)}\")\n    \n\nEvent: [] -&gt; Probability: 0.0\nEvent: ['H'] -&gt; Probability: 0.5\nEvent: ['T'] -&gt; Probability: 0.5\nEvent: ['H' 'T'] -&gt; Probability: 1.0\n\n\n\nimport pandas as pd\n# Initialize an empty list to store events and probabilities\nevents_data = []\n\n# Generate events and probabilities\nfor event in generate_event_space(sample_space_dice):\n    event_tuple = tuple(event)\n    event_prob = probability(event, sample_space_dice)\n    events_data.append((event_tuple, event_prob))\n\n# Create a pandas DataFrame\ndf = pd.DataFrame(events_data, columns=['Event', 'Probability'])\n\n\ndf\n\n\n\n\n\n\n\n\nEvent\nProbability\n\n\n\n\n0\n()\n0.000000\n\n\n1\n(1,)\n0.166667\n\n\n2\n(2,)\n0.166667\n\n\n3\n(3,)\n0.166667\n\n\n4\n(4,)\n0.166667\n\n\n...\n...\n...\n\n\n59\n(1, 2, 3, 5, 6)\n0.833333\n\n\n60\n(1, 2, 4, 5, 6)\n0.833333\n\n\n61\n(1, 3, 4, 5, 6)\n0.833333\n\n\n62\n(2, 3, 4, 5, 6)\n0.833333\n\n\n63\n(1, 2, 3, 4, 5, 6)\n1.000000\n\n\n\n\n64 rows × 2 columns",
    "crumbs": [
      "Home",
      "Getting Started",
      "Probability Theory Fundamentals"
    ]
  },
  {
    "objectID": "notebooks/intro-numpy.html",
    "href": "notebooks/intro-numpy.html",
    "title": "Introduction to NumPy",
    "section": "",
    "text": "import numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# config retina mode\n%config InlineBackend.figure_format = 'retina'\n\n\nimg_path = \"../datasets/images/street.jpg\"\nimg = Image.open(img_path)\n\n\nimg_array = np.array(img).astype(np.uint8)\n\n\nplt.imshow(img_array)\n# remove axis\n_ = plt.axis('off')\n\n\n\n\n\n\n\n\n\nimg_array\n\narray([[[ 48,  51,  56],\n        [ 41,  44,  49],\n        [ 31,  34,  39],\n        ...,\n        [153, 154, 149],\n        [129, 125, 116],\n        [120, 112, 101]],\n\n       [[ 47,  50,  55],\n        [ 41,  44,  49],\n        [ 30,  33,  38],\n        ...,\n        [151, 152, 147],\n        [126, 122, 113],\n        [117, 109,  98]],\n\n       [[ 46,  49,  54],\n        [ 39,  42,  47],\n        [ 29,  32,  37],\n        ...,\n        [147, 148, 143],\n        [121, 117, 108],\n        [113, 105,  94]],\n\n       ...,\n\n       [[ 80,  93,  99],\n        [ 85,  98, 104],\n        [ 91, 104, 110],\n        ...,\n        [ 83,  88,  92],\n        [ 85,  90,  94],\n        [ 88,  93,  97]],\n\n       [[ 69,  80,  86],\n        [ 75,  86,  92],\n        [ 82,  93,  99],\n        ...,\n        [ 86,  94,  97],\n        [ 86,  94,  97],\n        [ 87,  95,  98]],\n\n       [[ 58,  65,  75],\n        [ 64,  71,  81],\n        [ 73,  80,  90],\n        ...,\n        [ 90,  98, 101],\n        [ 88,  96,  99],\n        [ 88,  96,  99]]], dtype=uint8)\n\n\n\nimg_array.shape\n\n(2000, 3000, 3)\n\n\n\nimg_array.dtype\n\ndtype('uint8')\n\n\n\n# rotate image by 90 degrees\nrotated_img_array = np.rot90(img_array)\n\nplt.imshow(rotated_img_array.astype(np.uint8))\nplt.axis('off')\n\n\n\n\n\n\n\n\n\n# 0, 0 th pixel\nimg_array[0, 0]\n\narray([48, 51, 56], dtype=uint8)\n\n\n\n# Increase R value of first quarter to max\nnew_img = img_array.copy()\nnew_img[:new_img.shape[0]//2, :new_img.shape[1]//2, 0] = 255\n\nplt.imshow(new_img)\nplt.axis('off')\n\n\n\n\n\n\n\n\n\n%pip install pydub -q\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n# load audio\nfrom email.mime import audio\nfrom pydub import AudioSegment\naudio_path = \"../datasets/audio/pm-answer.mp3\"\n\n\naudio = AudioSegment.from_file(audio_path)\n\n\naudio\n\n\n                    \n                        \n                        Your browser does not support the audio element.\n                    \n                  \n\n\n\naudio_arr = np.array(audio.get_array_of_samples())\n\n\naudio_arr\n\narray([0, 0, 0, ..., 0, 0, 0], dtype=int16)\n\n\n\nplt.plot(audio_arr)\nplt.xlabel('Sample')\nplt.ylabel('Amplitude')\n\nText(0, 0.5, 'Amplitude')\n\n\n\n\n\n\n\n\n\n\naudio_arr.shape\n\n(82368,)\n\n\n\naudio.frame_rate\n\n24000\n\n\n\n# Convert plot to time as x-axis\ntime = np.linspace(0, len(audio_arr) / audio.frame_rate, num=len(audio_arr))\n\nplt.plot(time, audio_arr)\nplt.xlabel('Time (s)')\nplt.ylabel('Amplitude')\n\nText(0, 0.5, 'Amplitude')\n\n\n\n\n\n\n\n\n\n\n# Add a smoothing effect\nfrom scipy.signal import savgol_filter\n\nsmoothed_audio_arr = savgol_filter(audio_arr, 51, 3)\n\nplt.plot(time, smoothed_audio_arr)\n\n\n\n\n\n\n\n\n\nfrom IPython.display import Audio\nAudio(audio_arr, rate=audio.frame_rate)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nAudio(smoothed_audio_arr, rate=audio.frame_rate)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Sample text data\ndocuments = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"Never jump over the lazy dog quickly\"\n]\n\n# Convert text to a bag-of-words representation\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(documents)\n\nprint(\"Feature names:\", vectorizer.get_feature_names_out())\nprint(\"Bag-of-words representation:\\n\", X.toarray())\n\nFeature names: ['brown' 'dog' 'fox' 'jump' 'jumps' 'lazy' 'never' 'over' 'quick'\n 'quickly' 'the']\nBag-of-words representation:\n [[1 1 1 0 1 1 0 1 1 0 2]\n [0 1 0 1 0 1 1 1 0 1 1]]\n\n\nWhy not use Python lists instead of NumPy arrays?\n\nimport time\n\nn_nums = 10000000\n# Using a Python list\nlst = list(range(n_nums))\nstart = time.time()\nlst_squared = [x**2 for x in lst]\nend = time.time()\nprint(f\"Python list computation time: {end - start: .2f} seconds\")\n\n# Using a NumPy array\narr = np.arange(n_nums)\nstart = time.time()\narr_squared = arr ** 2\nend = time.time()\nprint(f\"NumPy array computation time: {end - start: .2f} seconds\")\n\nPython list computation time:  0.21 seconds\nNumPy array computation time:  0.01 seconds\n\n\n\nImport & Version Check\n\nimport numpy as np\nprint(\"Using NumPy version:\", np.__version__)\n\nUsing NumPy version: 2.1.2\n\n\n\n\n\nNumPy arrays can come from Python lists or built-in functions.\n\n# From a Python list\npy_list = [1, 2, 3, 4]\narr_from_list = np.array(py_list)\nprint(\"Array from list:\", arr_from_list)\n\nArray from list: [1 2 3 4]\n\n\n\nprint(py_list)\n\n[1, 2, 3, 4]\n\n\n\nprint(arr_from_list)\n\n[1 2 3 4]\n\n\n\ntype(py_list), type(arr_from_list)\n\n(list, numpy.ndarray)\n\n\n\npy_list = [0, 0, 0, 0, 0, 0, 0]\nnp.array(py_list)\n\nzeros_arr = np.zeros(7, dtype=np.int32)\nzeros_arr, py_list\n\n(array([0, 0, 0, 0, 0, 0, 0], dtype=int32), [0, 0, 0, 0, 0, 0, 0])\n\n\n\n# Using built-in functions\nzeros_arr = np.zeros((2, 3))\nprint(\"Zeros array:\\n\", zeros_arr)\n\nzeros_1d = np.zeros(3)\nprint(\"1D Zeros array:\", zeros_1d)\n\nZeros array:\n [[0. 0. 0.]\n [0. 0. 0.]]\n1D Zeros array: [0. 0. 0.]\n\n\n\nones_arr = np.ones((3, 2))\nprint(\"Ones array:\\n\", ones_arr)\n\nOnes array:\n [[1. 1.]\n [1. 1.]\n [1. 1.]]\n\n\n\nlist(range(0, 10, 2))\n\n[0, 2, 4, 6, 8]\n\n\n\nrange_arr = np.arange(0, 10, 2)\nprint(\"range_arr =\", range_arr)\n\nrange_arr = [0 2 4 6 8]\n\n\n\nnp.arange(0, 10, 2.5)\n\narray([0. , 2.5, 5. , 7.5])\n\n\n\ndef f(x):\n    return np.sin(x)\n\nx_range = np.arange(0, 2*np.pi, 0.001)\ny = f(x_range)\n\n\nx_range\n\narray([0.000e+00, 1.000e-03, 2.000e-03, ..., 6.281e+00, 6.282e+00,\n       6.283e+00])\n\n\n\nplt.plot(x_range, y)\n\n\n\n\n\n\n\n\n\nlinspace_arr = np.linspace(0, 1, 5)\nprint(\"linspace_arr =\", linspace_arr)\n\nlinspace_arr = [0.   0.25 0.5  0.75 1.  ]\n\n\n\nidentity_mat_arr = np.eye(3)\nprint(\"Identity matrix array:\\n\", identity_mat_arr)\n\nIdentity matrix array:\n [[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\n\n\n\nshape, size, ndim, and dtype are particularly important.\n\nrandom_arr = np.random.randint(1, 10, size=(3,4))\n\nprint(\"Array:\\n\", random_arr)\nprint(\"Shape:\", random_arr.shape)\nprint(\"Size:\", random_arr.size)\nprint(\"Dimensions:\", random_arr.ndim)\nprint(\"Data Type:\", random_arr.dtype)\n\nArray:\n [[1 1 2 8]\n [8 4 5 2]\n [3 2 9 8]]\nShape: (3, 4)\nSize: 12\nDimensions: 2\nData Type: int64\n\n\n\n\n\n? and . tab completion are useful for exploring the API.\n\nnp.zeros?\n\n\nDocstring:\nzeros(shape, dtype=float, order='C', *, like=None)\nReturn a new array of given shape and type, filled with zeros.\nParameters\n----------\nshape : int or tuple of ints\n    Shape of the new array, e.g., ``(2, 3)`` or ``2``.\ndtype : data-type, optional\n    The desired data-type for the array, e.g., `numpy.int8`.  Default is\n    `numpy.float64`.\norder : {'C', 'F'}, optional, default: 'C'\n    Whether to store multi-dimensional data in row-major\n    (C-style) or column-major (Fortran-style) order in\n    memory.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n    .. versionadded:: 1.20.0\nReturns\n-------\nout : ndarray\n    Array of zeros with the given shape, dtype, and order.\nSee Also\n--------\nzeros_like : Return an array of zeros with shape and type of input.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nfull : Return a new array of given shape filled with value.\nExamples\n--------\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.zeros(5)\narray([ 0.,  0.,  0.,  0.,  0.])\n&gt;&gt;&gt; np.zeros((5,), dtype=int)\narray([0, 0, 0, 0, 0])\n&gt;&gt;&gt; np.zeros((2, 1))\narray([[ 0.],\n       [ 0.]])\n&gt;&gt;&gt; s = (2,2)\n&gt;&gt;&gt; np.zeros(s)\narray([[ 0.,  0.],\n       [ 0.,  0.]])\n&gt;&gt;&gt; np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype\narray([(0, 0), (0, 0)],\n      dtype=[('x', '&lt;i4'), ('y', '&lt;i4')])\nType:      builtin_function_or_method\n\n\n\n\nhelp(np.zeros)\n\nHelp on built-in function zeros in module numpy:\n\nzeros(...)\n    zeros(shape, dtype=float, order='C', *, like=None)\n\n    Return a new array of given shape and type, filled with zeros.\n\n    Parameters\n    ----------\n    shape : int or tuple of ints\n        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n    dtype : data-type, optional\n        The desired data-type for the array, e.g., `numpy.int8`.  Default is\n        `numpy.float64`.\n    order : {'C', 'F'}, optional, default: 'C'\n        Whether to store multi-dimensional data in row-major\n        (C-style) or column-major (Fortran-style) order in\n        memory.\n    like : array_like, optional\n        Reference object to allow the creation of arrays which are not\n        NumPy arrays. If an array-like passed in as ``like`` supports\n        the ``__array_function__`` protocol, the result will be defined\n        by it. In this case, it ensures the creation of an array object\n        compatible with that passed in via this argument.\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of zeros with the given shape, dtype, and order.\n\n    See Also\n    --------\n    zeros_like : Return an array of zeros with shape and type of input.\n    empty : Return a new uninitialized array.\n    ones : Return a new array setting values to one.\n    full : Return a new array of given shape filled with value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; np.zeros(5)\n    array([ 0.,  0.,  0.,  0.,  0.])\n\n    &gt;&gt;&gt; np.zeros((5,), dtype=int)\n    array([0, 0, 0, 0, 0])\n\n    &gt;&gt;&gt; np.zeros((2, 1))\n    array([[ 0.],\n           [ 0.]])\n\n    &gt;&gt;&gt; s = (2,2)\n    &gt;&gt;&gt; np.zeros(s)\n    array([[ 0.,  0.],\n           [ 0.,  0.]])\n\n    &gt;&gt;&gt; np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype\n    array([(0, 0), (0, 0)],\n          dtype=[('x', '&lt;i4'), ('y', '&lt;i4')])\n\n\n\n\na = np.zeros((2, 3))\na.size\n\n6\n\n\n\n# Gotcha\n# Shape of (N,) v/s (N, 1)\n\na = np.zeros(3)\nprint(\"Shape of a:\", a.shape)\nprint(\"a:\", a)\n\nb = np.zeros((3, 1))\nprint(\"Shape of b:\", b.shape)\nprint(\"b:\\n\", b)\n\nc = np.zeros((1, 3))\nprint(\"Shape of c:\", c.shape)\nprint(\"c:\\n\", c)\n\nShape of a: (3,)\na: [0. 0. 0.]\nShape of b: (3, 1)\nb:\n [[0.]\n [0.]\n [0.]]\nShape of c: (1, 3)\nc:\n [[0. 0. 0.]]\n\n\nIn above code, “a” is a vector (1d array) and “b” is a matrix (2d array) with 3 rows and 1 column; “c” is a 2d array with 1 row and 3 columns.\n\n\n\n\nIndexing for single elements: arr[r, c]\nSlicing for subarrays: arr[start:stop:step]\n\nRemember that slices in NumPy are views—changing a slice changes the original array.\n\n# Example array\nx = np.array([[10, 20, 30], [40, 50, 60], [70, 80, 90]])\nprint(\"Original x:\\n\", x)\n\nOriginal x:\n [[10 20 30]\n [40 50 60]\n [70 80 90]]\n\n\n\n# Accessing a single element\n# If we want to select the second element of the first row, we need to specify row and column\nprint(\"Second element of the First Row:\", x[0, 1])\n\nSecond element of the First Row: 20\n\n\n\n# Note: We can also use x[0][1] to get the same result but it is less efficient because it first creates \n# an array containing the first row and then selects the element from that row.\n\nprint(\"Second element of the First Row:\", x[0][1])\n\nSecond element of the First Row: 20\n\n\n\nprint(\"x = \", x)\n# Slicing examples\nprint(\"x[:1] =\", x[:1])  # Slices up to the first row (row index 0)\nprint(\"x[1:] =\", x[1:])  # Starts slicing from the second row (row index 1)\nprint(\"x[::2] =\", x[::2])  # Selects every second row (row indices 0 and 2 in this case)\n\nx =  [[10 20 30]\n [40 50 60]\n [70 80 90]]\nx[:1] = [[10 20 30]]\nx[1:] = [[40 50 60]\n [70 80 90]]\nx[::2] = [[10 20 30]\n [70 80 90]]\n\n\n\nprint(\"x = \", x)\n# Slicing examples\nprint(\"x[:1] =\", x[:1, :])  # Slices up to the first row (row index 0)\nprint(\"x[1:] =\", x[1:, :])  # Starts slicing from the second row (row index 1)\nprint(\"x[::2] =\", x[::2, :])  # Selects every second row (row indices 0 and 2 in this case)\n\nx =  [[10 20 30]\n [40 50 60]\n [70 80 90]]\nx[:1] = [[10 20 30]]\nx[1:] = [[40 50 60]\n [70 80 90]]\nx[::2] = [[10 20 30]\n [70 80 90]]\n\n\n\n# Changing a view changes the original array\narr2d = np.random.randint(10, size=(4,5))\nprint(\"\\narr2d:\\n\", arr2d)\n\n\narr2d:\n [[6 8 0 8 6]\n [2 2 3 1 5]\n [7 9 0 0 8]\n [5 1 8 6 5]]\n\n\n\nsub = arr2d[:2, :3]\nprint(\"\\nSubarray:\", sub)\n\n\nSubarray: [[6 8 0]\n [2 2 3]]\n\n\n\nsub[0,0] = 99\nprint(\"\\nChanged subarray =&gt; arr2d:\")\nprint(arr2d)\n\n\nChanged subarray =&gt; arr2d:\n[[99  8  0  8  6]\n [ 2  2  3  1  5]\n [ 7  9  0  0  8]\n [ 5  1  8  6  5]]\n\n\n\n# Create a copy of the array and then change the value\n\narr2d = np.random.randint(10, size=(4,5))\n\nprint(\"\\narr2d:\\n\", arr2d)\n\narr2d_copy = arr2d[:2, :3].copy()\nprint(\"\\nCopy of subarray:\", arr2d_copy)\n\narr2d_copy[0,0] = 99\n\nprint(\"\\nChanged copy of subarray \")\nprint(arr2d_copy)\n\nprint(\"\\nSame original array =&gt; arr2d:\")\nprint(arr2d)\n\n\narr2d:\n [[5 1 8 0 0]\n [0 1 4 6 3]\n [3 8 6 9 6]\n [3 5 2 8 3]]\n\nCopy of subarray: [[5 1 8]\n [0 1 4]]\n\nChanged copy of subarray \n[[99  1  8]\n [ 0  1  4]]\n\nSame original array =&gt; arr2d:\n[[5 1 8 0 0]\n [0 1 4 6 3]\n [3 8 6 9 6]\n [3 5 2 8 3]]\n\n\n\nprint(audio_arr), print(audio_arr.shape)\nAudio(audio_arr, rate=audio.frame_rate)\n\n[0 0 0 ... 0 0 0]\n(82368,)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# Get last 2 seconds of audio\nlast_2_seconds = audio_arr[-2 * audio.frame_rate:]\nAudio(last_2_seconds, rate=audio.frame_rate)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\nUse reshape to change the shape without altering data.\n\ngrid = np.arange(1, 10)\nprint(\"Array, shape, dimensions:\")\nprint(grid, grid.shape, grid.ndim)\n\nArray, shape, dimensions:\n[1 2 3 4 5 6 7 8 9] (9,) 1\n\n\n\ngrid_3x3 = grid.reshape((3,3))\nprint(\"\\nArray, shape, dimensions:\")\nprint(grid_3x3, grid_3x3.shape, grid_3x3.ndim)\n\n\nArray, shape, dimensions:\n[[1 2 3]\n [4 5 6]\n [7 8 9]] (3, 3) 2\n\n\n\ngrid_temp = grid.reshape((1, 3,3))\nprint(\"\\nArray, shape, dimensions:\")\nprint(grid_temp, grid_temp.shape, grid_temp.ndim)\n\n\nArray, shape, dimensions:\n[[[1 2 3]\n  [4 5 6]\n  [7 8 9]]] (1, 3, 3) 3\n\n\n\ngrid.reshape((2, 5))\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[60], line 1\n----&gt; 1 grid.reshape((2, 5))\n\nValueError: cannot reshape array of size 9 into shape (2,5)\n\n\n\n\n# Example usage \n\nrandom_2d_img = np.random.randint(0, 255, size=(28, 28))\nplt.imshow(random_2d_img, cmap='gray')\nprint(random_2d_img.shape)\n\n(28, 28)\n\n\n\n\n\n\n\n\n\n\n# Flatten the 2D image to 1D\nflattened_img = random_2d_img.flatten()\nprint(\"Flattened image shape:\", flattened_img.shape)\n\nFlattened image shape: (784,)\n\n\n\nN = flattened_img.size\nflattened_img_using_reshape = random_2d_img.reshape(N)\n\nprint(\"Flattened image using reshape:\", flattened_img_using_reshape.shape)\n\nFlattened image using reshape: (784,)\n\n\n\n# Using -1 in reshape\nflattened_img_using_reshape = random_2d_img.reshape(-1)\n\nprint(\"Flattened image using reshape with -1:\", flattened_img_using_reshape.shape)\n\nFlattened image using reshape with -1: (784,)\n\n\n\nflattened_img.shape\n\n(784,)\n\n\n\n# Using -1 in reshape in one dimension\n\ntwo_d_img_1= flattened_img.reshape(28, -1)\nprint(\"2D image shape:\", two_d_img_1.shape)\n\ntwo_d_img_2 = flattened_img.reshape(-1, 28)\nprint(\"2D image shape:\", two_d_img_2.shape)\n\n# Check if two arrays are equal\nnp.all(two_d_img_1 == two_d_img_2)\n\n2D image shape: (28, 28)\n2D image shape: (28, 28)\n\n\nnp.True_\n\n\n\n\n\nnp.concatenate, np.vstack, and np.hstack can help combine arrays.\n\narrA = np.array([1, 2, 3])\narrB = np.array([4, 5, 6])\nprint(\"Concatenate:\", np.concatenate([arrA, arrB]))\n\ngridA = np.array([[1,2],[3,4]])\ngridB = np.array([[5,6],[7,8]])\nprint(\"\\nVStack:\\n\", np.vstack([gridA, gridB]))\nprint(\"\\nHStack:\\n\", np.hstack([gridA, gridB]))\n\nConcatenate: [1 2 3 4 5 6]\n\nVStack:\n [[1 2]\n [3 4]\n [5 6]\n [7 8]]\n\nHStack:\n [[1 2 5 6]\n [3 4 7 8]]\n\n\n\n\n\nUfuncs are vectorized, element-by-element functions that allow fast operations on entire arrays without explicit Python loops. Each arithmetic operator (+, -, *, /, etc.) in NumPy is backed by a ufunc, and there are many more specialized ufuncs for math, stats, etc.\n\n# Create a simple array\nx = np.arange(5)\nprint(\"x:\", x)\n\n# Perform elementwise operations via ufuncs\ny = x * 2      # multiplication\nz = np.exp(x)  # exponential\nprint(\"y = x * 2:\", y)\nprint(\"z = np.exp(x):\", z)\n\nx: [0 1 2 3 4]\ny = x * 2: [0 2 4 6 8]\nz = np.exp(x): [ 1.          2.71828183  7.3890561  20.08553692 54.59815003]\n\n\n\nx_list = range(5)\nmul_two = [x*2 for x in x_list]\nprint(mul_two)\n\n[0, 2, 4, 6, 8]\n\n\n\n\n\nAggregations summarize array values into a single numeric result (or one result per axis). Common examples include minimum, maximum, sum, mean, median, standard deviation, etc.\n\ndata = np.random.randint(1, 100, size=10)\nprint(\"data:\", data)\n\n# Basic aggregations\nprint(\"Sum:\", np.sum(data))\nprint(\"Min:\", np.min(data))\nprint(\"Max:\", np.max(data))\nprint(\"Mean:\", np.mean(data))\nprint(\"Standard Deviation:\", np.std(data))\n\n\nmatrix = np.random.randint(0, 10, size=(3,4))\nprint(\"matrix:\\n\", matrix)\n\nprint(\"Min of each column:\", np.min(matrix, axis=0))\nprint(\"Max of each row:\", np.max(matrix, axis=1))\n\ndata: [38 61  9 74  1  5 60 77 71 94]\nSum: 490\nMin: 1\nMax: 94\nMean: 49.0\nStandard Deviation: 31.849646779831012\nmatrix:\n [[7 1 2 5]\n [7 3 5 5]\n [9 6 1 8]]\nMin of each column: [7 1 1 5]\nMax of each row: [7 7 9]\n\n\n\n\n\nAllows operations on arrays of different shapes by stretching dimensions when possible.\nSee this nice video\n\na = np.array([1.0, 2.0, 3.0])\nb = np.array([2.0, 2.0, 2.0])\n\nc = a*b\nprint(\"c = a*b:\", c)\nprint(c.shape)\n\nc = a*b: [2. 4. 6.]\n(3,)\n\n\n\nscalar = 2.0\nd = a * scalar\n\nprint(\"d = a * scalar:\", d)\nprint(d.shape)\n\nd = a * scalar: [2. 4. 6.]\n(3,)\n\n\n\nX = np.array([[2, 6, 8], [4, 5, 3]])\nprint(X.shape)\n\nY = np.array([[2], [1]])\nprint(Y.shape)\n\nZ = X + Y\nprint(Z.shape)\n\n(2, 3)\n(2, 1)\n(2, 3)\n\n\n\nReference: https://numpy.org/doc/stable/user/basics.broadcasting.html\n\na = np.array([[ 0.0,  0.0,  0.0],\n              [10.0, 10.0, 10.0],\n              [20.0, 20.0, 20.0],\n              [30.0, 30.0, 30.0]])\nb = np.array([1.0, 2.0, 3.0])\nprint(a)\nprint(b)\n\n# Broadcasting \nprint(\"a + b:\\n\", a + b)    \n\n[[ 0.  0.  0.]\n [10. 10. 10.]\n [20. 20. 20.]\n [30. 30. 30.]]\n[1. 2. 3.]\na + b:\n [[ 1.  2.  3.]\n [11. 12. 13.]\n [21. 22. 23.]\n [31. 32. 33.]]\n\n\n\n\n\nCreate a mask to select certain elements.\n\ndata = np.random.randint(1, 20, size=10)\nmask = data &gt; 10\nprint(\"data:\", data)\nprint(\"mask:\", mask)\nprint(\"Values &gt; 10:\", data[mask])\n\ndata: [12  8 14  5 10 13  4 14  2  1]\nmask: [ True False  True False False  True False  True False False]\nValues &gt; 10: [12 14 13 14]\n\n\n\n\n\n\nnp.sort(arr) returns a sorted copy.\narr.sort() sorts in-place.\nnp.argsort returns the indices.\n\n\nunsorted_arr = np.array([2,1,4,3,5])\nprint(\"Sorted copy:\", np.sort(unsorted_arr))\nprint(\"Original:\", unsorted_arr)\n\nunsorted_arr.sort()\nprint(\"In-place sort:\", unsorted_arr)\n\nSorted copy: [1 2 3 4 5]\nOriginal: [2 1 4 3 5]\nIn-place sort: [1 2 3 4 5]\n\n\n\n\n\nShreyans Jain, BTech IIT Gandhinagar for creating the first version of this notebook.",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to NumPy"
    ]
  },
  {
    "objectID": "notebooks/intro-numpy.html#introduction-to-numerical-computing-with-numpy",
    "href": "notebooks/intro-numpy.html#introduction-to-numerical-computing-with-numpy",
    "title": "Introduction to NumPy",
    "section": "",
    "text": "import numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# config retina mode\n%config InlineBackend.figure_format = 'retina'\n\n\nimg_path = \"../datasets/images/street.jpg\"\nimg = Image.open(img_path)\n\n\nimg_array = np.array(img).astype(np.uint8)\n\n\nplt.imshow(img_array)\n# remove axis\n_ = plt.axis('off')\n\n\n\n\n\n\n\n\n\nimg_array\n\narray([[[ 48,  51,  56],\n        [ 41,  44,  49],\n        [ 31,  34,  39],\n        ...,\n        [153, 154, 149],\n        [129, 125, 116],\n        [120, 112, 101]],\n\n       [[ 47,  50,  55],\n        [ 41,  44,  49],\n        [ 30,  33,  38],\n        ...,\n        [151, 152, 147],\n        [126, 122, 113],\n        [117, 109,  98]],\n\n       [[ 46,  49,  54],\n        [ 39,  42,  47],\n        [ 29,  32,  37],\n        ...,\n        [147, 148, 143],\n        [121, 117, 108],\n        [113, 105,  94]],\n\n       ...,\n\n       [[ 80,  93,  99],\n        [ 85,  98, 104],\n        [ 91, 104, 110],\n        ...,\n        [ 83,  88,  92],\n        [ 85,  90,  94],\n        [ 88,  93,  97]],\n\n       [[ 69,  80,  86],\n        [ 75,  86,  92],\n        [ 82,  93,  99],\n        ...,\n        [ 86,  94,  97],\n        [ 86,  94,  97],\n        [ 87,  95,  98]],\n\n       [[ 58,  65,  75],\n        [ 64,  71,  81],\n        [ 73,  80,  90],\n        ...,\n        [ 90,  98, 101],\n        [ 88,  96,  99],\n        [ 88,  96,  99]]], dtype=uint8)\n\n\n\nimg_array.shape\n\n(2000, 3000, 3)\n\n\n\nimg_array.dtype\n\ndtype('uint8')\n\n\n\n# rotate image by 90 degrees\nrotated_img_array = np.rot90(img_array)\n\nplt.imshow(rotated_img_array.astype(np.uint8))\nplt.axis('off')\n\n\n\n\n\n\n\n\n\n# 0, 0 th pixel\nimg_array[0, 0]\n\narray([48, 51, 56], dtype=uint8)\n\n\n\n# Increase R value of first quarter to max\nnew_img = img_array.copy()\nnew_img[:new_img.shape[0]//2, :new_img.shape[1]//2, 0] = 255\n\nplt.imshow(new_img)\nplt.axis('off')\n\n\n\n\n\n\n\n\n\n%pip install pydub -q\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n# load audio\nfrom email.mime import audio\nfrom pydub import AudioSegment\naudio_path = \"../datasets/audio/pm-answer.mp3\"\n\n\naudio = AudioSegment.from_file(audio_path)\n\n\naudio\n\n\n                    \n                        \n                        Your browser does not support the audio element.\n                    \n                  \n\n\n\naudio_arr = np.array(audio.get_array_of_samples())\n\n\naudio_arr\n\narray([0, 0, 0, ..., 0, 0, 0], dtype=int16)\n\n\n\nplt.plot(audio_arr)\nplt.xlabel('Sample')\nplt.ylabel('Amplitude')\n\nText(0, 0.5, 'Amplitude')\n\n\n\n\n\n\n\n\n\n\naudio_arr.shape\n\n(82368,)\n\n\n\naudio.frame_rate\n\n24000\n\n\n\n# Convert plot to time as x-axis\ntime = np.linspace(0, len(audio_arr) / audio.frame_rate, num=len(audio_arr))\n\nplt.plot(time, audio_arr)\nplt.xlabel('Time (s)')\nplt.ylabel('Amplitude')\n\nText(0, 0.5, 'Amplitude')\n\n\n\n\n\n\n\n\n\n\n# Add a smoothing effect\nfrom scipy.signal import savgol_filter\n\nsmoothed_audio_arr = savgol_filter(audio_arr, 51, 3)\n\nplt.plot(time, smoothed_audio_arr)\n\n\n\n\n\n\n\n\n\nfrom IPython.display import Audio\nAudio(audio_arr, rate=audio.frame_rate)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nAudio(smoothed_audio_arr, rate=audio.frame_rate)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Sample text data\ndocuments = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"Never jump over the lazy dog quickly\"\n]\n\n# Convert text to a bag-of-words representation\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(documents)\n\nprint(\"Feature names:\", vectorizer.get_feature_names_out())\nprint(\"Bag-of-words representation:\\n\", X.toarray())\n\nFeature names: ['brown' 'dog' 'fox' 'jump' 'jumps' 'lazy' 'never' 'over' 'quick'\n 'quickly' 'the']\nBag-of-words representation:\n [[1 1 1 0 1 1 0 1 1 0 2]\n [0 1 0 1 0 1 1 1 0 1 1]]\n\n\nWhy not use Python lists instead of NumPy arrays?\n\nimport time\n\nn_nums = 10000000\n# Using a Python list\nlst = list(range(n_nums))\nstart = time.time()\nlst_squared = [x**2 for x in lst]\nend = time.time()\nprint(f\"Python list computation time: {end - start: .2f} seconds\")\n\n# Using a NumPy array\narr = np.arange(n_nums)\nstart = time.time()\narr_squared = arr ** 2\nend = time.time()\nprint(f\"NumPy array computation time: {end - start: .2f} seconds\")\n\nPython list computation time:  0.21 seconds\nNumPy array computation time:  0.01 seconds\n\n\n\nImport & Version Check\n\nimport numpy as np\nprint(\"Using NumPy version:\", np.__version__)\n\nUsing NumPy version: 2.1.2\n\n\n\n\n\nNumPy arrays can come from Python lists or built-in functions.\n\n# From a Python list\npy_list = [1, 2, 3, 4]\narr_from_list = np.array(py_list)\nprint(\"Array from list:\", arr_from_list)\n\nArray from list: [1 2 3 4]\n\n\n\nprint(py_list)\n\n[1, 2, 3, 4]\n\n\n\nprint(arr_from_list)\n\n[1 2 3 4]\n\n\n\ntype(py_list), type(arr_from_list)\n\n(list, numpy.ndarray)\n\n\n\npy_list = [0, 0, 0, 0, 0, 0, 0]\nnp.array(py_list)\n\nzeros_arr = np.zeros(7, dtype=np.int32)\nzeros_arr, py_list\n\n(array([0, 0, 0, 0, 0, 0, 0], dtype=int32), [0, 0, 0, 0, 0, 0, 0])\n\n\n\n# Using built-in functions\nzeros_arr = np.zeros((2, 3))\nprint(\"Zeros array:\\n\", zeros_arr)\n\nzeros_1d = np.zeros(3)\nprint(\"1D Zeros array:\", zeros_1d)\n\nZeros array:\n [[0. 0. 0.]\n [0. 0. 0.]]\n1D Zeros array: [0. 0. 0.]\n\n\n\nones_arr = np.ones((3, 2))\nprint(\"Ones array:\\n\", ones_arr)\n\nOnes array:\n [[1. 1.]\n [1. 1.]\n [1. 1.]]\n\n\n\nlist(range(0, 10, 2))\n\n[0, 2, 4, 6, 8]\n\n\n\nrange_arr = np.arange(0, 10, 2)\nprint(\"range_arr =\", range_arr)\n\nrange_arr = [0 2 4 6 8]\n\n\n\nnp.arange(0, 10, 2.5)\n\narray([0. , 2.5, 5. , 7.5])\n\n\n\ndef f(x):\n    return np.sin(x)\n\nx_range = np.arange(0, 2*np.pi, 0.001)\ny = f(x_range)\n\n\nx_range\n\narray([0.000e+00, 1.000e-03, 2.000e-03, ..., 6.281e+00, 6.282e+00,\n       6.283e+00])\n\n\n\nplt.plot(x_range, y)\n\n\n\n\n\n\n\n\n\nlinspace_arr = np.linspace(0, 1, 5)\nprint(\"linspace_arr =\", linspace_arr)\n\nlinspace_arr = [0.   0.25 0.5  0.75 1.  ]\n\n\n\nidentity_mat_arr = np.eye(3)\nprint(\"Identity matrix array:\\n\", identity_mat_arr)\n\nIdentity matrix array:\n [[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\n\n\n\nshape, size, ndim, and dtype are particularly important.\n\nrandom_arr = np.random.randint(1, 10, size=(3,4))\n\nprint(\"Array:\\n\", random_arr)\nprint(\"Shape:\", random_arr.shape)\nprint(\"Size:\", random_arr.size)\nprint(\"Dimensions:\", random_arr.ndim)\nprint(\"Data Type:\", random_arr.dtype)\n\nArray:\n [[1 1 2 8]\n [8 4 5 2]\n [3 2 9 8]]\nShape: (3, 4)\nSize: 12\nDimensions: 2\nData Type: int64\n\n\n\n\n\n? and . tab completion are useful for exploring the API.\n\nnp.zeros?\n\n\nDocstring:\nzeros(shape, dtype=float, order='C', *, like=None)\nReturn a new array of given shape and type, filled with zeros.\nParameters\n----------\nshape : int or tuple of ints\n    Shape of the new array, e.g., ``(2, 3)`` or ``2``.\ndtype : data-type, optional\n    The desired data-type for the array, e.g., `numpy.int8`.  Default is\n    `numpy.float64`.\norder : {'C', 'F'}, optional, default: 'C'\n    Whether to store multi-dimensional data in row-major\n    (C-style) or column-major (Fortran-style) order in\n    memory.\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n    .. versionadded:: 1.20.0\nReturns\n-------\nout : ndarray\n    Array of zeros with the given shape, dtype, and order.\nSee Also\n--------\nzeros_like : Return an array of zeros with shape and type of input.\nempty : Return a new uninitialized array.\nones : Return a new array setting values to one.\nfull : Return a new array of given shape filled with value.\nExamples\n--------\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; np.zeros(5)\narray([ 0.,  0.,  0.,  0.,  0.])\n&gt;&gt;&gt; np.zeros((5,), dtype=int)\narray([0, 0, 0, 0, 0])\n&gt;&gt;&gt; np.zeros((2, 1))\narray([[ 0.],\n       [ 0.]])\n&gt;&gt;&gt; s = (2,2)\n&gt;&gt;&gt; np.zeros(s)\narray([[ 0.,  0.],\n       [ 0.,  0.]])\n&gt;&gt;&gt; np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype\narray([(0, 0), (0, 0)],\n      dtype=[('x', '&lt;i4'), ('y', '&lt;i4')])\nType:      builtin_function_or_method\n\n\n\n\nhelp(np.zeros)\n\nHelp on built-in function zeros in module numpy:\n\nzeros(...)\n    zeros(shape, dtype=float, order='C', *, like=None)\n\n    Return a new array of given shape and type, filled with zeros.\n\n    Parameters\n    ----------\n    shape : int or tuple of ints\n        Shape of the new array, e.g., ``(2, 3)`` or ``2``.\n    dtype : data-type, optional\n        The desired data-type for the array, e.g., `numpy.int8`.  Default is\n        `numpy.float64`.\n    order : {'C', 'F'}, optional, default: 'C'\n        Whether to store multi-dimensional data in row-major\n        (C-style) or column-major (Fortran-style) order in\n        memory.\n    like : array_like, optional\n        Reference object to allow the creation of arrays which are not\n        NumPy arrays. If an array-like passed in as ``like`` supports\n        the ``__array_function__`` protocol, the result will be defined\n        by it. In this case, it ensures the creation of an array object\n        compatible with that passed in via this argument.\n\n        .. versionadded:: 1.20.0\n\n    Returns\n    -------\n    out : ndarray\n        Array of zeros with the given shape, dtype, and order.\n\n    See Also\n    --------\n    zeros_like : Return an array of zeros with shape and type of input.\n    empty : Return a new uninitialized array.\n    ones : Return a new array setting values to one.\n    full : Return a new array of given shape filled with value.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; np.zeros(5)\n    array([ 0.,  0.,  0.,  0.,  0.])\n\n    &gt;&gt;&gt; np.zeros((5,), dtype=int)\n    array([0, 0, 0, 0, 0])\n\n    &gt;&gt;&gt; np.zeros((2, 1))\n    array([[ 0.],\n           [ 0.]])\n\n    &gt;&gt;&gt; s = (2,2)\n    &gt;&gt;&gt; np.zeros(s)\n    array([[ 0.,  0.],\n           [ 0.,  0.]])\n\n    &gt;&gt;&gt; np.zeros((2,), dtype=[('x', 'i4'), ('y', 'i4')]) # custom dtype\n    array([(0, 0), (0, 0)],\n          dtype=[('x', '&lt;i4'), ('y', '&lt;i4')])\n\n\n\n\na = np.zeros((2, 3))\na.size\n\n6\n\n\n\n# Gotcha\n# Shape of (N,) v/s (N, 1)\n\na = np.zeros(3)\nprint(\"Shape of a:\", a.shape)\nprint(\"a:\", a)\n\nb = np.zeros((3, 1))\nprint(\"Shape of b:\", b.shape)\nprint(\"b:\\n\", b)\n\nc = np.zeros((1, 3))\nprint(\"Shape of c:\", c.shape)\nprint(\"c:\\n\", c)\n\nShape of a: (3,)\na: [0. 0. 0.]\nShape of b: (3, 1)\nb:\n [[0.]\n [0.]\n [0.]]\nShape of c: (1, 3)\nc:\n [[0. 0. 0.]]\n\n\nIn above code, “a” is a vector (1d array) and “b” is a matrix (2d array) with 3 rows and 1 column; “c” is a 2d array with 1 row and 3 columns.\n\n\n\n\nIndexing for single elements: arr[r, c]\nSlicing for subarrays: arr[start:stop:step]\n\nRemember that slices in NumPy are views—changing a slice changes the original array.\n\n# Example array\nx = np.array([[10, 20, 30], [40, 50, 60], [70, 80, 90]])\nprint(\"Original x:\\n\", x)\n\nOriginal x:\n [[10 20 30]\n [40 50 60]\n [70 80 90]]\n\n\n\n# Accessing a single element\n# If we want to select the second element of the first row, we need to specify row and column\nprint(\"Second element of the First Row:\", x[0, 1])\n\nSecond element of the First Row: 20\n\n\n\n# Note: We can also use x[0][1] to get the same result but it is less efficient because it first creates \n# an array containing the first row and then selects the element from that row.\n\nprint(\"Second element of the First Row:\", x[0][1])\n\nSecond element of the First Row: 20\n\n\n\nprint(\"x = \", x)\n# Slicing examples\nprint(\"x[:1] =\", x[:1])  # Slices up to the first row (row index 0)\nprint(\"x[1:] =\", x[1:])  # Starts slicing from the second row (row index 1)\nprint(\"x[::2] =\", x[::2])  # Selects every second row (row indices 0 and 2 in this case)\n\nx =  [[10 20 30]\n [40 50 60]\n [70 80 90]]\nx[:1] = [[10 20 30]]\nx[1:] = [[40 50 60]\n [70 80 90]]\nx[::2] = [[10 20 30]\n [70 80 90]]\n\n\n\nprint(\"x = \", x)\n# Slicing examples\nprint(\"x[:1] =\", x[:1, :])  # Slices up to the first row (row index 0)\nprint(\"x[1:] =\", x[1:, :])  # Starts slicing from the second row (row index 1)\nprint(\"x[::2] =\", x[::2, :])  # Selects every second row (row indices 0 and 2 in this case)\n\nx =  [[10 20 30]\n [40 50 60]\n [70 80 90]]\nx[:1] = [[10 20 30]]\nx[1:] = [[40 50 60]\n [70 80 90]]\nx[::2] = [[10 20 30]\n [70 80 90]]\n\n\n\n# Changing a view changes the original array\narr2d = np.random.randint(10, size=(4,5))\nprint(\"\\narr2d:\\n\", arr2d)\n\n\narr2d:\n [[6 8 0 8 6]\n [2 2 3 1 5]\n [7 9 0 0 8]\n [5 1 8 6 5]]\n\n\n\nsub = arr2d[:2, :3]\nprint(\"\\nSubarray:\", sub)\n\n\nSubarray: [[6 8 0]\n [2 2 3]]\n\n\n\nsub[0,0] = 99\nprint(\"\\nChanged subarray =&gt; arr2d:\")\nprint(arr2d)\n\n\nChanged subarray =&gt; arr2d:\n[[99  8  0  8  6]\n [ 2  2  3  1  5]\n [ 7  9  0  0  8]\n [ 5  1  8  6  5]]\n\n\n\n# Create a copy of the array and then change the value\n\narr2d = np.random.randint(10, size=(4,5))\n\nprint(\"\\narr2d:\\n\", arr2d)\n\narr2d_copy = arr2d[:2, :3].copy()\nprint(\"\\nCopy of subarray:\", arr2d_copy)\n\narr2d_copy[0,0] = 99\n\nprint(\"\\nChanged copy of subarray \")\nprint(arr2d_copy)\n\nprint(\"\\nSame original array =&gt; arr2d:\")\nprint(arr2d)\n\n\narr2d:\n [[5 1 8 0 0]\n [0 1 4 6 3]\n [3 8 6 9 6]\n [3 5 2 8 3]]\n\nCopy of subarray: [[5 1 8]\n [0 1 4]]\n\nChanged copy of subarray \n[[99  1  8]\n [ 0  1  4]]\n\nSame original array =&gt; arr2d:\n[[5 1 8 0 0]\n [0 1 4 6 3]\n [3 8 6 9 6]\n [3 5 2 8 3]]\n\n\n\nprint(audio_arr), print(audio_arr.shape)\nAudio(audio_arr, rate=audio.frame_rate)\n\n[0 0 0 ... 0 0 0]\n(82368,)\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# Get last 2 seconds of audio\nlast_2_seconds = audio_arr[-2 * audio.frame_rate:]\nAudio(last_2_seconds, rate=audio.frame_rate)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\nUse reshape to change the shape without altering data.\n\ngrid = np.arange(1, 10)\nprint(\"Array, shape, dimensions:\")\nprint(grid, grid.shape, grid.ndim)\n\nArray, shape, dimensions:\n[1 2 3 4 5 6 7 8 9] (9,) 1\n\n\n\ngrid_3x3 = grid.reshape((3,3))\nprint(\"\\nArray, shape, dimensions:\")\nprint(grid_3x3, grid_3x3.shape, grid_3x3.ndim)\n\n\nArray, shape, dimensions:\n[[1 2 3]\n [4 5 6]\n [7 8 9]] (3, 3) 2\n\n\n\ngrid_temp = grid.reshape((1, 3,3))\nprint(\"\\nArray, shape, dimensions:\")\nprint(grid_temp, grid_temp.shape, grid_temp.ndim)\n\n\nArray, shape, dimensions:\n[[[1 2 3]\n  [4 5 6]\n  [7 8 9]]] (1, 3, 3) 3\n\n\n\ngrid.reshape((2, 5))\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[60], line 1\n----&gt; 1 grid.reshape((2, 5))\n\nValueError: cannot reshape array of size 9 into shape (2,5)\n\n\n\n\n# Example usage \n\nrandom_2d_img = np.random.randint(0, 255, size=(28, 28))\nplt.imshow(random_2d_img, cmap='gray')\nprint(random_2d_img.shape)\n\n(28, 28)\n\n\n\n\n\n\n\n\n\n\n# Flatten the 2D image to 1D\nflattened_img = random_2d_img.flatten()\nprint(\"Flattened image shape:\", flattened_img.shape)\n\nFlattened image shape: (784,)\n\n\n\nN = flattened_img.size\nflattened_img_using_reshape = random_2d_img.reshape(N)\n\nprint(\"Flattened image using reshape:\", flattened_img_using_reshape.shape)\n\nFlattened image using reshape: (784,)\n\n\n\n# Using -1 in reshape\nflattened_img_using_reshape = random_2d_img.reshape(-1)\n\nprint(\"Flattened image using reshape with -1:\", flattened_img_using_reshape.shape)\n\nFlattened image using reshape with -1: (784,)\n\n\n\nflattened_img.shape\n\n(784,)\n\n\n\n# Using -1 in reshape in one dimension\n\ntwo_d_img_1= flattened_img.reshape(28, -1)\nprint(\"2D image shape:\", two_d_img_1.shape)\n\ntwo_d_img_2 = flattened_img.reshape(-1, 28)\nprint(\"2D image shape:\", two_d_img_2.shape)\n\n# Check if two arrays are equal\nnp.all(two_d_img_1 == two_d_img_2)\n\n2D image shape: (28, 28)\n2D image shape: (28, 28)\n\n\nnp.True_\n\n\n\n\n\nnp.concatenate, np.vstack, and np.hstack can help combine arrays.\n\narrA = np.array([1, 2, 3])\narrB = np.array([4, 5, 6])\nprint(\"Concatenate:\", np.concatenate([arrA, arrB]))\n\ngridA = np.array([[1,2],[3,4]])\ngridB = np.array([[5,6],[7,8]])\nprint(\"\\nVStack:\\n\", np.vstack([gridA, gridB]))\nprint(\"\\nHStack:\\n\", np.hstack([gridA, gridB]))\n\nConcatenate: [1 2 3 4 5 6]\n\nVStack:\n [[1 2]\n [3 4]\n [5 6]\n [7 8]]\n\nHStack:\n [[1 2 5 6]\n [3 4 7 8]]\n\n\n\n\n\nUfuncs are vectorized, element-by-element functions that allow fast operations on entire arrays without explicit Python loops. Each arithmetic operator (+, -, *, /, etc.) in NumPy is backed by a ufunc, and there are many more specialized ufuncs for math, stats, etc.\n\n# Create a simple array\nx = np.arange(5)\nprint(\"x:\", x)\n\n# Perform elementwise operations via ufuncs\ny = x * 2      # multiplication\nz = np.exp(x)  # exponential\nprint(\"y = x * 2:\", y)\nprint(\"z = np.exp(x):\", z)\n\nx: [0 1 2 3 4]\ny = x * 2: [0 2 4 6 8]\nz = np.exp(x): [ 1.          2.71828183  7.3890561  20.08553692 54.59815003]\n\n\n\nx_list = range(5)\nmul_two = [x*2 for x in x_list]\nprint(mul_two)\n\n[0, 2, 4, 6, 8]\n\n\n\n\n\nAggregations summarize array values into a single numeric result (or one result per axis). Common examples include minimum, maximum, sum, mean, median, standard deviation, etc.\n\ndata = np.random.randint(1, 100, size=10)\nprint(\"data:\", data)\n\n# Basic aggregations\nprint(\"Sum:\", np.sum(data))\nprint(\"Min:\", np.min(data))\nprint(\"Max:\", np.max(data))\nprint(\"Mean:\", np.mean(data))\nprint(\"Standard Deviation:\", np.std(data))\n\n\nmatrix = np.random.randint(0, 10, size=(3,4))\nprint(\"matrix:\\n\", matrix)\n\nprint(\"Min of each column:\", np.min(matrix, axis=0))\nprint(\"Max of each row:\", np.max(matrix, axis=1))\n\ndata: [38 61  9 74  1  5 60 77 71 94]\nSum: 490\nMin: 1\nMax: 94\nMean: 49.0\nStandard Deviation: 31.849646779831012\nmatrix:\n [[7 1 2 5]\n [7 3 5 5]\n [9 6 1 8]]\nMin of each column: [7 1 1 5]\nMax of each row: [7 7 9]\n\n\n\n\n\nAllows operations on arrays of different shapes by stretching dimensions when possible.\nSee this nice video\n\na = np.array([1.0, 2.0, 3.0])\nb = np.array([2.0, 2.0, 2.0])\n\nc = a*b\nprint(\"c = a*b:\", c)\nprint(c.shape)\n\nc = a*b: [2. 4. 6.]\n(3,)\n\n\n\nscalar = 2.0\nd = a * scalar\n\nprint(\"d = a * scalar:\", d)\nprint(d.shape)\n\nd = a * scalar: [2. 4. 6.]\n(3,)\n\n\n\nX = np.array([[2, 6, 8], [4, 5, 3]])\nprint(X.shape)\n\nY = np.array([[2], [1]])\nprint(Y.shape)\n\nZ = X + Y\nprint(Z.shape)\n\n(2, 3)\n(2, 1)\n(2, 3)\n\n\n\nReference: https://numpy.org/doc/stable/user/basics.broadcasting.html\n\na = np.array([[ 0.0,  0.0,  0.0],\n              [10.0, 10.0, 10.0],\n              [20.0, 20.0, 20.0],\n              [30.0, 30.0, 30.0]])\nb = np.array([1.0, 2.0, 3.0])\nprint(a)\nprint(b)\n\n# Broadcasting \nprint(\"a + b:\\n\", a + b)    \n\n[[ 0.  0.  0.]\n [10. 10. 10.]\n [20. 20. 20.]\n [30. 30. 30.]]\n[1. 2. 3.]\na + b:\n [[ 1.  2.  3.]\n [11. 12. 13.]\n [21. 22. 23.]\n [31. 32. 33.]]\n\n\n\n\n\nCreate a mask to select certain elements.\n\ndata = np.random.randint(1, 20, size=10)\nmask = data &gt; 10\nprint(\"data:\", data)\nprint(\"mask:\", mask)\nprint(\"Values &gt; 10:\", data[mask])\n\ndata: [12  8 14  5 10 13  4 14  2  1]\nmask: [ True False  True False False  True False  True False False]\nValues &gt; 10: [12 14 13 14]\n\n\n\n\n\n\nnp.sort(arr) returns a sorted copy.\narr.sort() sorts in-place.\nnp.argsort returns the indices.\n\n\nunsorted_arr = np.array([2,1,4,3,5])\nprint(\"Sorted copy:\", np.sort(unsorted_arr))\nprint(\"Original:\", unsorted_arr)\n\nunsorted_arr.sort()\nprint(\"In-place sort:\", unsorted_arr)\n\nSorted copy: [1 2 3 4 5]\nOriginal: [2 1 4 3 5]\nIn-place sort: [1 2 3 4 5]\n\n\n\n\n\nShreyans Jain, BTech IIT Gandhinagar for creating the first version of this notebook.",
    "crumbs": [
      "Home",
      "Data Science Tools",
      "Introduction to NumPy"
    ]
  },
  {
    "objectID": "notebooks/random-variables.html",
    "href": "notebooks/random-variables.html",
    "title": "Introduction to Random Variables",
    "section": "",
    "text": "By the end of this notebook, you will understand:\n\nMathematical Definition: What random variables are and how they map outcomes to numerical values\nSample Spaces: Understanding the domain of random variables and possible outcomes\nDiscrete Random Variables: Working with countable outcomes and their probabilities\nProbability Mass Functions (PMF): The mathematical description of discrete random variable distributions\nPractical Applications: Real-world examples including dice games and two-dice scenarios\nSimulation vs Theory: Understanding the relationship between histograms and theoretical PMFs\n\n\n\n\nA random variable is one of the most fundamental concepts in probability theory. Despite its name, a random variable is not actually a variable in the traditional sense, nor is it random. Instead, it’s a mathematical function that assigns numerical values to the outcomes of a random experiment.\n\n\nMathematically, a random variable \\(X\\) is a function that maps each outcome in a sample space \\(\\Omega\\) to a real number:\n\\[X: \\Omega \\rightarrow \\mathbb{R}\\]\nThis mapping allows us to: - Convert non-numerical outcomes (like coin flips) into numbers - Apply mathematical operations and statistical analysis - Calculate probabilities for ranges of values - Build predictive models\n\n\n\nRandom variables are essential because they: 1. Bridge the gap between abstract probability theory and practical applications 2. Enable mathematical analysis of uncertain events 3. Provide a framework for statistical inference 4. Allow us to model real-world phenomena quantitatively\nLet’s explore these concepts through hands-on examples and interactive simulations!\n\n\n\n\nWe’ll use standard Python libraries for our probability and statistical computations:\n\n\n\nLet’s start with a fundamental example - flipping a coin twice. This will illustrate how random variables work in practice.\n\n\nFor two coin flips, our sample space consists of all possible outcomes:\n\n\n\nNow let’s define a random variable \\(X\\) that counts the number of heads in each outcome:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch \nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n\n\nLet’s create a systematic mapping from outcomes to random variable values:\n\n# Sample space for flipping a coin 2 times\nsample_space = [\"HH\", \"HT\", \"TH\", \"TT\"]\n\nThis table clearly shows how our random variable \\(X\\) maps each outcome to a numerical value representing the count of heads.\n\n\n\nWe can use our random variable to find which outcomes correspond to specific values. For example, let’s find all outcomes where \\(X = 1\\) (exactly one head):\n\n# Define a random variable X as the number of Heads in the outcomes\ndef random_variable_X(outcome):\n    return outcome.count(\"H\")\n\n\n\n\nNow we can calculate the probability distribution of our random variable. For a fair coin, each outcome is equally likely with probability 1/4:\n\nrandom_variable_X(\"HT\"), random_variable_X(\"TT\"), random_variable_X(\"HH\"), random_variable_X(\"TH\")\n\n(1, 0, 2, 1)\n\n\nThis creates an inverse mapping that will be useful for probability calculations. Notice how multiple outcomes can map to the same random variable value.\nThese probabilities form the Probability Mass Function (PMF) of our random variable: - \\(P(X = 0) = 0.25\\) (no heads) - \\(P(X = 1) = 0.50\\) (one head) - \\(P(X = 2) = 0.25\\) (two heads)\nNotice that these probabilities sum to 1, as required for any probability distribution.\n\n\n\n\nLet’s explore a more complex example with two six-sided dice. This will introduce multiple random variables defined on the same sample space.\n\n\nFrom the same sample space, we can define several different random variables: - \\(X_1\\): Sum of the two dice - \\(X_2\\): Product of the two dice\n- \\(X_3\\): Maximum of the two dice\nEach provides a different numerical perspective on the same outcomes:\n\n# Mapping of outcomes to the random variable values\nmapping = {outcome: random_variable_X(outcome) for outcome in sample_space}\n\n\n\n\nLet’s create a comprehensive dataset and analyze specific values:\nThese are all the ways to get a sum of 10 with two dice. Notice there are exactly 3 ways out of 36 possible outcomes, giving a probability of 3/36 = 1/12 ≈ 0.083.\n\n\n\nThis interactive widget allows you to explore how many ways different sums can occur:\n\n\n\n\nA crucial concept in probability is understanding the difference between: - Probability Mass Function (PMF): The theoretical probability distribution - Histogram: The empirical distribution from actual data\n\n\nFor a fair six-sided die, the theoretical PMF assigns equal probability to each outcome:\n\n\n\nNow let’s see how histograms from actual die rolls compare to the theoretical PMF. As we increase the number of rolls, the histogram should converge to the PMF:\n\nmapping\n\n{'HH': 2, 'HT': 1, 'TH': 1, 'TT': 0}\n\n\n\ndf = pd.DataFrame(mapping, index=[\"X\"]).T\ndf.index.name = \"Outcome\"\ndf\n\n\n\n\n\n\n\n\nX\n\n\nOutcome\n\n\n\n\n\nHH\n2\n\n\nHT\n1\n\n\nTH\n1\n\n\nTT\n0\n\n\n\n\n\n\n\n\n# Find records/samples where X = 1\ndf[\"X\"] == 1\n\nOutcome\nHH    False\nHT     True\nTH     True\nTT    False\nName: X, dtype: bool\n\n\n\ndf[df[\"X\"] == 1]\n\n\n\n\n\n\n\n\nX\n\n\nOutcome\n\n\n\n\n\nHT\n1\n\n\nTH\n1\n\n\n\n\n\n\n\n\n# Calculate probabilities for X = 0, 1, 2\n\ndef calculate_probability_X(x, df):\n    subset = df[df[\"X\"] == x]\n    len_subset = len(subset)\n    len_df = len(df)\n    return len_subset / len_df\n\n\n\n\n\nIn this notebook, we’ve explored the fundamental concept of random variables through:\n\n\n\nMathematical Foundation: Random variables as functions mapping outcomes to numbers\nSample Spaces: Understanding the domain of random experiments\n\nProbability Calculations: Computing PMFs from inverse mappings\nMultiple Variables: Different random variables on the same sample space\nTheory vs Practice: Comparing theoretical PMFs with empirical histograms\n\n\n\n\n\nRandom variables transform qualitative outcomes into quantitative analysis\nThe same sample space can support multiple random variables\nPMFs provide the theoretical foundation for probability calculations\nHistograms converge to PMFs as sample size increases (Law of Large Numbers)\nInteractive tools help build intuition about probability distributions\n\n\n\n\nFor any discrete random variable \\(X\\): - \\(\\sum_{x} P(X = x) = 1\\) (probabilities sum to 1) - \\(P(X = x) \\geq 0\\) for all \\(x\\) (probabilities are non-negative) - \\(P(X \\in A) = \\sum_{x \\in A} P(X = x)\\) (probability of sets)\n\n\n\nNow that you understand random variables, you’re ready to explore: - Continuous random variables and probability density functions - Cumulative distribution functions for both discrete and continuous variables - Common probability distributions and their applications - Joint distributions and relationships between multiple random variables\nRandom variables are the building blocks of probability theory - master them, and you’ll have a solid foundation for all of statistics and data science!\n\ncalculate_probability_X(0, df), calculate_probability_X(1, df), calculate_probability_X(2, df)\n\n(0.25, 0.5, 0.25)\n\n\n\n# Store inverse mapping\n\ninverse_mapping = {x: [] for x in range(3)}\nfor outcome, value in mapping.items():\n    inverse_mapping[value].append(outcome)\n    \nprint(inverse_mapping)\n\n{0: ['TT'], 1: ['HT', 'TH'], 2: ['HH']}\n\n\n\ndef calculate_probability_X(x, inverse_mapping):\n    outcomes = inverse_mapping[x]\n    len_outcomes = len(outcomes)\n    len_sample_space = len(sample_space)\n    return len_outcomes / len_sample_space\n\ncalculate_probability_X(0, inverse_mapping), calculate_probability_X(1, inverse_mapping), calculate_probability_X(2, inverse_mapping)\n\n(0.25, 0.5, 0.25)\n\n\n\n### Two dice example\n\n# Construct the sample space\n\nsample_space = [(i, j) for i in range(1, 7) for j in range(1, 7)]\nprint(sample_space)\n\n[(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6)]\n\n\n\n# Define a random variable X1 as the sum of the outcomes\n# Define a random variable X2 as the product of the outcomes\n# Define a random variable X3 as the maximum of the outcomes\n\ndef random_variable_X1(outcome):\n    return sum(outcome)\n\ndef random_variable_X2(outcome):\n    return outcome[0] * outcome[1]\n\ndef random_variable_X3(outcome):\n    return max(outcome)\n\n\nrandom_variable_X1([1, 2])\n\n3\n\n\n\n# Create a heatmap for the sum of the outcomes\ndf = pd.DataFrame(sample_space, columns=[\"D1\", \"D2\"])\n\ndf[\"X1\"] = df.apply(lambda row: random_variable_X1(row), axis=1)\ndf.index.name = \"Serial No.\"\n\n\ndf[df[\"X1\"] == 10]\n\n\n\n\n\n\n\n\nD1\nD2\nX1\n\n\nSerial No.\n\n\n\n\n\n\n\n23\n4\n6\n10\n\n\n28\n5\n5\n10\n\n\n33\n6\n4\n10\n\n\n\n\n\n\n\n\n# Create interactive ipywidgets for the sum of the outcomes\nimport ipywidgets as widgets\nfrom ipywidgets import interact\n\n@interact(x=widgets.IntSlider(min=2, max=12, step=1, value=7))\ndef show_samples(x):\n    return df[df[\"X1\"] == x]\n\n\n\n\n\n\n# Difference between histrogram and PMF\n\ndie_pmf = pd.Series([1/6]*6, index=[1,2,3,4,5,6])\ndie_pmf.plot(kind='bar', rot = 0)\nplt.xlabel('Face')\nplt.ylabel('Probability')\nplt.title('PMF of a fair die')\n\nText(0.5, 1.0, 'PMF of a fair die')\n\n\n\n\n\n\n\n\n\n\n# Now, histogram over \"N\" rolls of the die\ndef die_hist(N):\n    rolls = np.random.randint(1, 7, N)\n    fig, ax = plt.subplots()\n    hist = pd.Series(rolls).value_counts(normalize=True).sort_index()\n    hist.plot(kind='bar', rot=0, ax=ax, label='Histogram', alpha=0.5, color='C0')\n    ax.set_xlabel('Face')\n    ax.set_ylabel('Frequency')\n    ax.set_title(f'Histogram of {N} rolls of a fair die')\n\n    # Plot ideal \n    die_pmf.plot(kind='bar', rot = 0, ax=ax, alpha=0.5, label='PMF', color='C1')\n    ax.legend()\n    \n\n\ndie_hist(10)\n\n\n\n\n\n\n\n\n\ndie_hist(100)\n\n\n\n\n\n\n\n\n\ndie_hist(50000)\n\n\n\n\n\n\n\n\n\ndie_hist(500000)\n\n\n\n\n\n\n\n\n\ndie_hist(500000)",
    "crumbs": [
      "Home",
      "Foundations",
      "Introduction to Random Variables"
    ]
  },
  {
    "objectID": "notebooks/random-variables.html#learning-objectives",
    "href": "notebooks/random-variables.html#learning-objectives",
    "title": "Introduction to Random Variables",
    "section": "",
    "text": "By the end of this notebook, you will understand:\n\nMathematical Definition: What random variables are and how they map outcomes to numerical values\nSample Spaces: Understanding the domain of random variables and possible outcomes\nDiscrete Random Variables: Working with countable outcomes and their probabilities\nProbability Mass Functions (PMF): The mathematical description of discrete random variable distributions\nPractical Applications: Real-world examples including dice games and two-dice scenarios\nSimulation vs Theory: Understanding the relationship between histograms and theoretical PMFs",
    "crumbs": [
      "Home",
      "Foundations",
      "Introduction to Random Variables"
    ]
  },
  {
    "objectID": "notebooks/random-variables.html#introduction",
    "href": "notebooks/random-variables.html#introduction",
    "title": "Introduction to Random Variables",
    "section": "",
    "text": "A random variable is one of the most fundamental concepts in probability theory. Despite its name, a random variable is not actually a variable in the traditional sense, nor is it random. Instead, it’s a mathematical function that assigns numerical values to the outcomes of a random experiment.\n\n\nMathematically, a random variable \\(X\\) is a function that maps each outcome in a sample space \\(\\Omega\\) to a real number:\n\\[X: \\Omega \\rightarrow \\mathbb{R}\\]\nThis mapping allows us to: - Convert non-numerical outcomes (like coin flips) into numbers - Apply mathematical operations and statistical analysis - Calculate probabilities for ranges of values - Build predictive models\n\n\n\nRandom variables are essential because they: 1. Bridge the gap between abstract probability theory and practical applications 2. Enable mathematical analysis of uncertain events 3. Provide a framework for statistical inference 4. Allow us to model real-world phenomena quantitatively\nLet’s explore these concepts through hands-on examples and interactive simulations!",
    "crumbs": [
      "Home",
      "Foundations",
      "Introduction to Random Variables"
    ]
  },
  {
    "objectID": "notebooks/random-variables.html#setting-up-the-environment",
    "href": "notebooks/random-variables.html#setting-up-the-environment",
    "title": "Introduction to Random Variables",
    "section": "",
    "text": "We’ll use standard Python libraries for our probability and statistical computations:",
    "crumbs": [
      "Home",
      "Foundations",
      "Introduction to Random Variables"
    ]
  },
  {
    "objectID": "notebooks/random-variables.html#example-1-coin-flipping-experiment",
    "href": "notebooks/random-variables.html#example-1-coin-flipping-experiment",
    "title": "Introduction to Random Variables",
    "section": "",
    "text": "Let’s start with a fundamental example - flipping a coin twice. This will illustrate how random variables work in practice.\n\n\nFor two coin flips, our sample space consists of all possible outcomes:\n\n\n\nNow let’s define a random variable \\(X\\) that counts the number of heads in each outcome:\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch \nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n\n\nLet’s create a systematic mapping from outcomes to random variable values:\n\n# Sample space for flipping a coin 2 times\nsample_space = [\"HH\", \"HT\", \"TH\", \"TT\"]\n\nThis table clearly shows how our random variable \\(X\\) maps each outcome to a numerical value representing the count of heads.\n\n\n\nWe can use our random variable to find which outcomes correspond to specific values. For example, let’s find all outcomes where \\(X = 1\\) (exactly one head):\n\n# Define a random variable X as the number of Heads in the outcomes\ndef random_variable_X(outcome):\n    return outcome.count(\"H\")\n\n\n\n\nNow we can calculate the probability distribution of our random variable. For a fair coin, each outcome is equally likely with probability 1/4:\n\nrandom_variable_X(\"HT\"), random_variable_X(\"TT\"), random_variable_X(\"HH\"), random_variable_X(\"TH\")\n\n(1, 0, 2, 1)\n\n\nThis creates an inverse mapping that will be useful for probability calculations. Notice how multiple outcomes can map to the same random variable value.\nThese probabilities form the Probability Mass Function (PMF) of our random variable: - \\(P(X = 0) = 0.25\\) (no heads) - \\(P(X = 1) = 0.50\\) (one head) - \\(P(X = 2) = 0.25\\) (two heads)\nNotice that these probabilities sum to 1, as required for any probability distribution.",
    "crumbs": [
      "Home",
      "Foundations",
      "Introduction to Random Variables"
    ]
  },
  {
    "objectID": "notebooks/random-variables.html#example-2-two-dice-roll",
    "href": "notebooks/random-variables.html#example-2-two-dice-roll",
    "title": "Introduction to Random Variables",
    "section": "",
    "text": "Let’s explore a more complex example with two six-sided dice. This will introduce multiple random variables defined on the same sample space.\n\n\nFrom the same sample space, we can define several different random variables: - \\(X_1\\): Sum of the two dice - \\(X_2\\): Product of the two dice\n- \\(X_3\\): Maximum of the two dice\nEach provides a different numerical perspective on the same outcomes:\n\n# Mapping of outcomes to the random variable values\nmapping = {outcome: random_variable_X(outcome) for outcome in sample_space}\n\n\n\n\nLet’s create a comprehensive dataset and analyze specific values:\nThese are all the ways to get a sum of 10 with two dice. Notice there are exactly 3 ways out of 36 possible outcomes, giving a probability of 3/36 = 1/12 ≈ 0.083.\n\n\n\nThis interactive widget allows you to explore how many ways different sums can occur:",
    "crumbs": [
      "Home",
      "Foundations",
      "Introduction to Random Variables"
    ]
  },
  {
    "objectID": "notebooks/random-variables.html#understanding-pmf-vs-histogram",
    "href": "notebooks/random-variables.html#understanding-pmf-vs-histogram",
    "title": "Introduction to Random Variables",
    "section": "",
    "text": "A crucial concept in probability is understanding the difference between: - Probability Mass Function (PMF): The theoretical probability distribution - Histogram: The empirical distribution from actual data\n\n\nFor a fair six-sided die, the theoretical PMF assigns equal probability to each outcome:\n\n\n\nNow let’s see how histograms from actual die rolls compare to the theoretical PMF. As we increase the number of rolls, the histogram should converge to the PMF:\n\nmapping\n\n{'HH': 2, 'HT': 1, 'TH': 1, 'TT': 0}\n\n\n\ndf = pd.DataFrame(mapping, index=[\"X\"]).T\ndf.index.name = \"Outcome\"\ndf\n\n\n\n\n\n\n\n\nX\n\n\nOutcome\n\n\n\n\n\nHH\n2\n\n\nHT\n1\n\n\nTH\n1\n\n\nTT\n0\n\n\n\n\n\n\n\n\n# Find records/samples where X = 1\ndf[\"X\"] == 1\n\nOutcome\nHH    False\nHT     True\nTH     True\nTT    False\nName: X, dtype: bool\n\n\n\ndf[df[\"X\"] == 1]\n\n\n\n\n\n\n\n\nX\n\n\nOutcome\n\n\n\n\n\nHT\n1\n\n\nTH\n1\n\n\n\n\n\n\n\n\n# Calculate probabilities for X = 0, 1, 2\n\ndef calculate_probability_X(x, df):\n    subset = df[df[\"X\"] == x]\n    len_subset = len(subset)\n    len_df = len(df)\n    return len_subset / len_df",
    "crumbs": [
      "Home",
      "Foundations",
      "Introduction to Random Variables"
    ]
  },
  {
    "objectID": "notebooks/random-variables.html#summary",
    "href": "notebooks/random-variables.html#summary",
    "title": "Introduction to Random Variables",
    "section": "",
    "text": "In this notebook, we’ve explored the fundamental concept of random variables through:\n\n\n\nMathematical Foundation: Random variables as functions mapping outcomes to numbers\nSample Spaces: Understanding the domain of random experiments\n\nProbability Calculations: Computing PMFs from inverse mappings\nMultiple Variables: Different random variables on the same sample space\nTheory vs Practice: Comparing theoretical PMFs with empirical histograms\n\n\n\n\n\nRandom variables transform qualitative outcomes into quantitative analysis\nThe same sample space can support multiple random variables\nPMFs provide the theoretical foundation for probability calculations\nHistograms converge to PMFs as sample size increases (Law of Large Numbers)\nInteractive tools help build intuition about probability distributions\n\n\n\n\nFor any discrete random variable \\(X\\): - \\(\\sum_{x} P(X = x) = 1\\) (probabilities sum to 1) - \\(P(X = x) \\geq 0\\) for all \\(x\\) (probabilities are non-negative) - \\(P(X \\in A) = \\sum_{x \\in A} P(X = x)\\) (probability of sets)\n\n\n\nNow that you understand random variables, you’re ready to explore: - Continuous random variables and probability density functions - Cumulative distribution functions for both discrete and continuous variables - Common probability distributions and their applications - Joint distributions and relationships between multiple random variables\nRandom variables are the building blocks of probability theory - master them, and you’ll have a solid foundation for all of statistics and data science!\n\ncalculate_probability_X(0, df), calculate_probability_X(1, df), calculate_probability_X(2, df)\n\n(0.25, 0.5, 0.25)\n\n\n\n# Store inverse mapping\n\ninverse_mapping = {x: [] for x in range(3)}\nfor outcome, value in mapping.items():\n    inverse_mapping[value].append(outcome)\n    \nprint(inverse_mapping)\n\n{0: ['TT'], 1: ['HT', 'TH'], 2: ['HH']}\n\n\n\ndef calculate_probability_X(x, inverse_mapping):\n    outcomes = inverse_mapping[x]\n    len_outcomes = len(outcomes)\n    len_sample_space = len(sample_space)\n    return len_outcomes / len_sample_space\n\ncalculate_probability_X(0, inverse_mapping), calculate_probability_X(1, inverse_mapping), calculate_probability_X(2, inverse_mapping)\n\n(0.25, 0.5, 0.25)\n\n\n\n### Two dice example\n\n# Construct the sample space\n\nsample_space = [(i, j) for i in range(1, 7) for j in range(1, 7)]\nprint(sample_space)\n\n[(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (4, 6), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5), (5, 6), (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6)]\n\n\n\n# Define a random variable X1 as the sum of the outcomes\n# Define a random variable X2 as the product of the outcomes\n# Define a random variable X3 as the maximum of the outcomes\n\ndef random_variable_X1(outcome):\n    return sum(outcome)\n\ndef random_variable_X2(outcome):\n    return outcome[0] * outcome[1]\n\ndef random_variable_X3(outcome):\n    return max(outcome)\n\n\nrandom_variable_X1([1, 2])\n\n3\n\n\n\n# Create a heatmap for the sum of the outcomes\ndf = pd.DataFrame(sample_space, columns=[\"D1\", \"D2\"])\n\ndf[\"X1\"] = df.apply(lambda row: random_variable_X1(row), axis=1)\ndf.index.name = \"Serial No.\"\n\n\ndf[df[\"X1\"] == 10]\n\n\n\n\n\n\n\n\nD1\nD2\nX1\n\n\nSerial No.\n\n\n\n\n\n\n\n23\n4\n6\n10\n\n\n28\n5\n5\n10\n\n\n33\n6\n4\n10\n\n\n\n\n\n\n\n\n# Create interactive ipywidgets for the sum of the outcomes\nimport ipywidgets as widgets\nfrom ipywidgets import interact\n\n@interact(x=widgets.IntSlider(min=2, max=12, step=1, value=7))\ndef show_samples(x):\n    return df[df[\"X1\"] == x]\n\n\n\n\n\n\n# Difference between histrogram and PMF\n\ndie_pmf = pd.Series([1/6]*6, index=[1,2,3,4,5,6])\ndie_pmf.plot(kind='bar', rot = 0)\nplt.xlabel('Face')\nplt.ylabel('Probability')\nplt.title('PMF of a fair die')\n\nText(0.5, 1.0, 'PMF of a fair die')\n\n\n\n\n\n\n\n\n\n\n# Now, histogram over \"N\" rolls of the die\ndef die_hist(N):\n    rolls = np.random.randint(1, 7, N)\n    fig, ax = plt.subplots()\n    hist = pd.Series(rolls).value_counts(normalize=True).sort_index()\n    hist.plot(kind='bar', rot=0, ax=ax, label='Histogram', alpha=0.5, color='C0')\n    ax.set_xlabel('Face')\n    ax.set_ylabel('Frequency')\n    ax.set_title(f'Histogram of {N} rolls of a fair die')\n\n    # Plot ideal \n    die_pmf.plot(kind='bar', rot = 0, ax=ax, alpha=0.5, label='PMF', color='C1')\n    ax.legend()\n    \n\n\ndie_hist(10)\n\n\n\n\n\n\n\n\n\ndie_hist(100)\n\n\n\n\n\n\n\n\n\ndie_hist(50000)\n\n\n\n\n\n\n\n\n\ndie_hist(500000)\n\n\n\n\n\n\n\n\n\ndie_hist(500000)",
    "crumbs": [
      "Home",
      "Foundations",
      "Introduction to Random Variables"
    ]
  },
  {
    "objectID": "notebooks/sum-random-vars.html",
    "href": "notebooks/sum-random-vars.html",
    "title": "Sum of Random Variables",
    "section": "",
    "text": "import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n%config InlineBackend.figure_format = 'retina'",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "Sum of Random Variables"
    ]
  },
  {
    "objectID": "notebooks/sum-random-vars.html#learning-objectives",
    "href": "notebooks/sum-random-vars.html#learning-objectives",
    "title": "Sum of Random Variables",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand how sums of random variables behave\nCompare empirical distributions with theoretical results\nExplore the convolution property for independent random variables\nVisualize distributions using kernel density estimation\n\nLet’s start by importing the necessary libraries:",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "Sum of Random Variables"
    ]
  },
  {
    "objectID": "notebooks/sum-random-vars.html#case-1-sum-of-normal-random-variables",
    "href": "notebooks/sum-random-vars.html#case-1-sum-of-normal-random-variables",
    "title": "Sum of Random Variables",
    "section": "Case 1: Sum of Normal Random Variables",
    "text": "Case 1: Sum of Normal Random Variables\nWhen we add independent normal random variables, the result is also normally distributed. Let’s verify this through simulation.\nWe have: - X ~ N(0, 1) (standard normal) - Y ~ N(0, 2) (normal with standard deviation 2)\nTheory tells us that X + Y ~ N(0, √(1² + 2²)) = N(0, √5):\nLet’s sample from these distributions and visualize them:\nNow let’s compute the sum S = X + Y and visualize all three distributions:\n\nX = torch.distributions.Normal(0, 1)\nY = torch.distributions.Normal(0, 2)\n\nx_range = torch.linspace(-5, 5, 1000)\nX_pdf = X.log_prob(x_range).exp()\nY_pdf = Y.log_prob(x_range).exp()\n\nplt.figure(figsize=(10, 5))\nplt.plot(x_range.numpy(), X_pdf.numpy(), label='X ~ N(0, 1)', color='blue')\nplt.plot(x_range.numpy(), Y_pdf.numpy(), label='Y ~ N(0, 2)', color='orange')\n\nplt.title('Probability Density Functions')\nplt.legend()\n\n\n\n\n\n\n\n\n\nx_samples = X.sample((10000,))\ny_samples = Y.sample((10000,))\nfig, ax = plt.subplots(2, 1, figsize=(6, 6), sharex=True)\nax[0].hist(x_samples.numpy(), bins=30, density=True, alpha=0.5, color='blue')\nax[1]. hist(y_samples.numpy(), bins=30, density=True, alpha=0.5, color='orange')\nax[0].set_title('X ~ N(0, 1)')\nax[1].set_title('Y ~ N(0, 2)')\nplt.xlabel('x')\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\ns_samples = x_samples + y_samples\nfig, ax = plt.subplots(3, 1, figsize=(6, 9), sharex=True)\nax[0].hist(x_samples.numpy(), bins=30, density=True, alpha=0.5, color='blue')\nax[1].hist(y_samples.numpy(), bins=30, density=True, alpha=0.5, color='orange')\nax[2].hist(s_samples.numpy(), bins=30, density=True, alpha=0.5, color='green')\nax[0].set_title('X ~ N(0, 1)')\nax[1].set_title('Y ~ N(0, 2)')\nax[2].set_title('S = X + Y')\nplt.xlabel('x')\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\ntorch.distributions.Normal?\n\n\nInit signature: torch.distributions.Normal(loc, scale, validate_args=None)\n\nDocstring:     \n\nCreates a normal (also called Gaussian) distribution parameterized by\n\n:attr:`loc` and :attr:`scale`.\n\n\n\nExample::\n\n\n\n    &gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n\n    &gt;&gt;&gt; m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n\n    &gt;&gt;&gt; m.sample()  # normally distributed with loc=0 and scale=1\n\n    tensor([ 0.1046])\n\n\n\nArgs:\n\n    loc (float or Tensor): mean of the distribution (often referred to as mu)\n\n    scale (float or Tensor): standard deviation of the distribution\n\n        (often referred to as sigma)\n\nFile:           ~/base/lib/python3.12/site-packages/torch/distributions/normal.py\n\nType:           type\n\nSubclasses:     \n\n\n\n\nsns.kdeplot(x_samples.numpy(), label='X ~ N(0, 1)', color='blue', bw_adjust=2)\nsns.kdeplot(y_samples.numpy(), label='Y ~ N(0, 2)', color='orange', bw_adjust=2)\nsns.kdeplot(s_samples.numpy(), label='Z = X + Y', color='green', bw_adjust=2)\nplt.title('Kernel Density Estimation')\n\nplt.legend()\n\n\n\n\n\n\n\n\n\ns_samples.std()\n\ntensor(2.2586)\n\n\n\ns_samples.mean()\n\ntensor(-0.0113)\n\n\n\ns_analytic = torch.distributions.Normal(0, np.sqrt(1**2 + 2**2))\ns_analytic.mean, s_analytic.stddev\n\n(tensor(0.), tensor(2.2361))\n\n\n\nsns.kdeplot(s_samples.numpy(), label='Z = X + Y', color='green', bw_adjust=2)\ns_range = torch.linspace(-10, 10, 1000)\ns_pdf = s_analytic.log_prob(s_range).exp()\nplt.plot(s_range.numpy(), s_pdf.numpy(), label='Z ~ N(0, sqrt(5))', color='red')\nplt.legend()\n\n\n\n\n\n\n\n\n\n### Example 2\n\nX = torch.distributions.Categorical(torch.tensor([0.25, 0.25, 0.25, 0.25]))\nY = torch.distributions.Categorical(torch.tensor([0.25, 0.25, 0.25, 0.25]))\n\nx_samples = X.sample((10000,))\ny_samples = Y.sample((10000,))\n\n\nfig, ax = plt.subplots(2, 1, figsize=(6, 6), sharex=True)\n\nx_samples_normed = pd.Series(x_samples.numpy()).value_counts(normalize=True).sort_index()\ny_samples_normed = pd.Series(y_samples.numpy()).value_counts(normalize=True).sort_index()\n\nax[0].bar(x_samples_normed.index, x_samples_normed.values, alpha=0.5, color='blue')\nax[1].bar(y_samples_normed.index, y_samples_normed.values, alpha=0.5, color='orange')\nax[0].set_title('X ~ Categorical([0.25, 0.25, 0.25, 0.25])')\nax[1].set_title('Y ~ Categorical([0.25, 0.25, 0.25, 0.25])')\nplt.xlabel('x')\n\nText(0.5, 0, 'x')\n\n\n\n\n\n\n\n\n\n\ns_samples = x_samples + y_samples\nfig, ax = plt.subplots(3, 1, figsize=(6, 9), sharex=True)\ns_samples_normed = pd.Series(s_samples.numpy()).value_counts(normalize=True).sort_index()\nax[0].bar(x_samples_normed.index, x_samples_normed.values, alpha=0.5, color='blue')\nax[1].bar(y_samples_normed.index, y_samples_normed.values, alpha=0.5, color='orange')\nax[2].bar(s_samples_normed.index, s_samples_normed.values, alpha=0.5, color='green')\nax[0].set_title('X ~ Categorical([0.25, 0.25, 0.25, 0.25])')\nax[1].set_title('Y ~ Categorical([0.25, 0.25, 0.25, 0.25])')\nax[2].set_title('S = X + Y')\nplt.xlabel('x')\n\n\nText(0.5, 0, 'x')",
    "crumbs": [
      "Home",
      "Advanced Topics",
      "Sum of Random Variables"
    ]
  },
  {
    "objectID": "notebooks/cdf.html",
    "href": "notebooks/cdf.html",
    "title": "Cumulative Distribution Functions and Inverse Sampling",
    "section": "",
    "text": "By the end of this notebook, you will understand:\n\nCumulative Distribution Functions (CDFs): Mathematical definition and properties\nInverse Transform Sampling: Using CDFs to generate random numbers\nDiscrete CDFs: Working with categorical and discrete distributions\nContinuous CDFs: Applications to exponential, normal, and other continuous distributions\nPractical Implementation: Building random number generators from scratch\nVerification Methods: Validating generated samples against theoretical distributions\n\n\n\n\nThe Cumulative Distribution Function (CDF) is one of the most important concepts in probability theory. While the Probability Mass Function (PMF) tells us the probability of specific values, the CDF tells us the probability of getting values up to a certain point.\n\n\nFor a random variable \\(X\\), the CDF is defined as:\n\\[F_X(x) = P(X \\leq x)\\]\nThis simple definition has profound implications: - Monotonic: CDFs are always non-decreasing - Bounded: CDFs range from 0 to 1 - Right-continuous: CDFs are continuous from the right - Universal: Every random variable has a CDF\n\n\n\nCDFs are crucial because they: 1. Provide complete probabilistic information about a random variable 2. Enable random number generation through inverse sampling 3. Allow easy probability calculations for ranges of values 4. Form the basis for statistical tests and confidence intervals 5. Bridge discrete and continuous probability theory\n\n\n\nOne of the most elegant applications of CDFs is the inverse transform sampling method. The key insight is beautifully simple:\n\nIf \\(U\\) is uniform on \\([0,1]\\) and \\(F\\) is a CDF, then \\(F^{-1}(U)\\) has distribution \\(F\\).\n\nThis allows us to generate samples from any distribution using just uniform random numbers and the inverse CDF!\nLet’s explore these concepts through practical implementations and visualizations.\n\n\n\n\nWe’ll use PyTorch for distributions, NumPy for numerical operations, and Matplotlib for visualizations:\n\n\n\nLet’s implement inverse transform sampling for a categorical distribution. This will demonstrate the core concepts before moving to continuous distributions.\n\n\nWe have a categorical distribution with 4 outcomes and probabilities: - Outcome 1: \\(P(X = 1) = 0.1\\) - Outcome 2: \\(P(X = 2) = 0.2\\) - Outcome 3: \\(P(X = 3) = 0.3\\) - Outcome 4: \\(P(X = 4) = 0.4\\)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nprint(np.__version__)\nimport torch \nimport torch.nn as nn\n\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n2.2.4\n\n\n\n\n\nThe CDF is the cumulative sum of probabilities:\nThe CDF values are [0.1, 0.3, 0.6, 1.0], representing cumulative probabilities up to each outcome.\n\n\n\nThe inverse transform method works as follows: 1. Generate a uniform random number \\(u \\in [0,1]\\) 2. Find the smallest \\(x\\) such that \\(F(x) \\geq u\\) 3. Return \\(x\\) as the sample\nLet’s implement this step by step:",
    "crumbs": [
      "Home",
      "Foundations",
      "Cumulative Distribution Functions and Inverse Sampling"
    ]
  },
  {
    "objectID": "notebooks/cdf.html#learning-objectives",
    "href": "notebooks/cdf.html#learning-objectives",
    "title": "Cumulative Distribution Functions and Inverse Sampling",
    "section": "",
    "text": "By the end of this notebook, you will understand:\n\nCumulative Distribution Functions (CDFs): Mathematical definition and properties\nInverse Transform Sampling: Using CDFs to generate random numbers\nDiscrete CDFs: Working with categorical and discrete distributions\nContinuous CDFs: Applications to exponential, normal, and other continuous distributions\nPractical Implementation: Building random number generators from scratch\nVerification Methods: Validating generated samples against theoretical distributions",
    "crumbs": [
      "Home",
      "Foundations",
      "Cumulative Distribution Functions and Inverse Sampling"
    ]
  },
  {
    "objectID": "notebooks/cdf.html#introduction",
    "href": "notebooks/cdf.html#introduction",
    "title": "Cumulative Distribution Functions and Inverse Sampling",
    "section": "",
    "text": "The Cumulative Distribution Function (CDF) is one of the most important concepts in probability theory. While the Probability Mass Function (PMF) tells us the probability of specific values, the CDF tells us the probability of getting values up to a certain point.\n\n\nFor a random variable \\(X\\), the CDF is defined as:\n\\[F_X(x) = P(X \\leq x)\\]\nThis simple definition has profound implications: - Monotonic: CDFs are always non-decreasing - Bounded: CDFs range from 0 to 1 - Right-continuous: CDFs are continuous from the right - Universal: Every random variable has a CDF\n\n\n\nCDFs are crucial because they: 1. Provide complete probabilistic information about a random variable 2. Enable random number generation through inverse sampling 3. Allow easy probability calculations for ranges of values 4. Form the basis for statistical tests and confidence intervals 5. Bridge discrete and continuous probability theory\n\n\n\nOne of the most elegant applications of CDFs is the inverse transform sampling method. The key insight is beautifully simple:\n\nIf \\(U\\) is uniform on \\([0,1]\\) and \\(F\\) is a CDF, then \\(F^{-1}(U)\\) has distribution \\(F\\).\n\nThis allows us to generate samples from any distribution using just uniform random numbers and the inverse CDF!\nLet’s explore these concepts through practical implementations and visualizations.",
    "crumbs": [
      "Home",
      "Foundations",
      "Cumulative Distribution Functions and Inverse Sampling"
    ]
  },
  {
    "objectID": "notebooks/cdf.html#setting-up-the-environment",
    "href": "notebooks/cdf.html#setting-up-the-environment",
    "title": "Cumulative Distribution Functions and Inverse Sampling",
    "section": "",
    "text": "We’ll use PyTorch for distributions, NumPy for numerical operations, and Matplotlib for visualizations:",
    "crumbs": [
      "Home",
      "Foundations",
      "Cumulative Distribution Functions and Inverse Sampling"
    ]
  },
  {
    "objectID": "notebooks/cdf.html#example-1-categorical-distribution-from-scratch",
    "href": "notebooks/cdf.html#example-1-categorical-distribution-from-scratch",
    "title": "Cumulative Distribution Functions and Inverse Sampling",
    "section": "",
    "text": "Let’s implement inverse transform sampling for a categorical distribution. This will demonstrate the core concepts before moving to continuous distributions.\n\n\nWe have a categorical distribution with 4 outcomes and probabilities: - Outcome 1: \\(P(X = 1) = 0.1\\) - Outcome 2: \\(P(X = 2) = 0.2\\) - Outcome 3: \\(P(X = 3) = 0.3\\) - Outcome 4: \\(P(X = 4) = 0.4\\)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nprint(np.__version__)\nimport torch \nimport torch.nn as nn\n\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n2.2.4\n\n\n\n\n\nThe CDF is the cumulative sum of probabilities:\nThe CDF values are [0.1, 0.3, 0.6, 1.0], representing cumulative probabilities up to each outcome.\n\n\n\nThe inverse transform method works as follows: 1. Generate a uniform random number \\(u \\in [0,1]\\) 2. Find the smallest \\(x\\) such that \\(F(x) \\geq u\\) 3. Return \\(x\\) as the sample\nLet’s implement this step by step:",
    "crumbs": [
      "Home",
      "Foundations",
      "Cumulative Distribution Functions and Inverse Sampling"
    ]
  },
  {
    "objectID": "notebooks/cdf.html#example-2-continuous-distributions---exponential",
    "href": "notebooks/cdf.html#example-2-continuous-distributions---exponential",
    "title": "Cumulative Distribution Functions and Inverse Sampling",
    "section": "Example 2: Continuous Distributions - Exponential",
    "text": "Example 2: Continuous Distributions - Exponential\nNow let’s apply inverse transform sampling to a continuous distribution. The exponential distribution is perfect because its inverse CDF has a closed form.\n\nMathematical Background\nFor an exponential distribution with rate parameter \\(\\lambda\\): - PDF: \\(f(x) = \\lambda e^{-\\lambda x}\\) for \\(x \\geq 0\\) - CDF: \\(F(x) = 1 - e^{-\\lambda x}\\) for \\(x \\geq 0\\) - Inverse CDF: \\(F^{-1}(u) = -\\frac{\\ln(1-u)}{\\lambda}\\)\n\n\nImplementation\n\nprobs = torch.tensor([0.1, 0.2, 0.3, 0.4])\nprint(probs)\n\ntensor([0.1000, 0.2000, 0.3000, 0.4000])\n\n\nThe histogram shows our generated samples closely match the theoretical exponential PDF (orange line)!",
    "crumbs": [
      "Home",
      "Foundations",
      "Cumulative Distribution Functions and Inverse Sampling"
    ]
  },
  {
    "objectID": "notebooks/cdf.html#general-framework-using-built-in-inverse-cdfs",
    "href": "notebooks/cdf.html#general-framework-using-built-in-inverse-cdfs",
    "title": "Cumulative Distribution Functions and Inverse Sampling",
    "section": "General Framework: Using Built-in Inverse CDFs",
    "text": "General Framework: Using Built-in Inverse CDFs\nMany probability distributions in PyTorch provide an icdf (inverse CDF) method. Let’s create a general framework:\n\nExample: Normal Distribution\nLet’s apply our framework to generate samples from a standard normal distribution:\n\nunif = torch.distributions.uniform.Uniform(0, 1)\nprint(unif.sample())\n\ntensor(0.3224)\n\n\nPerfect! Our inverse transform sampling generates samples that match the standard normal distribution.\n\n\nExploring the Inverse CDF Method\nLet’s examine the icdf method more closely to understand what we’re using:\n\ncum_sum_prob = torch.cumsum(probs, dim=0)\nprint(cum_sum_prob)\n\ntensor([0.1000, 0.3000, 0.6000, 1.0000])\n\n\nThe icdf method is exactly what we need for inverse transform sampling - it takes probabilities and returns the corresponding quantiles.",
    "crumbs": [
      "Home",
      "Foundations",
      "Cumulative Distribution Functions and Inverse Sampling"
    ]
  },
  {
    "objectID": "notebooks/cdf.html#summary",
    "href": "notebooks/cdf.html#summary",
    "title": "Cumulative Distribution Functions and Inverse Sampling",
    "section": "Summary",
    "text": "Summary\nIn this notebook, we’ve explored the powerful connection between CDFs and random number generation:\n\nKey Concepts Covered\n\nCDF Definition: \\(F_X(x) = P(X \\leq x)\\) - the probability of values up to \\(x\\)\nInverse Transform Sampling: Using \\(F^{-1}(U)\\) where \\(U \\sim \\text{Uniform}(0,1)\\)\nDiscrete Implementation: Step-by-step categorical distribution sampling\nContinuous Applications: Exponential and normal distribution examples\nValidation Methods: Comparing empirical results with theory\n\n\n\nMathematical Foundations\nThe inverse transform method works because: - If \\(U \\sim \\text{Uniform}(0,1)\\) - And \\(F\\) is any CDF - Then \\(X = F^{-1}(U)\\) has distribution \\(F\\)\nThis is a fundamental result in probability theory!\n\n\nPractical Applications\nInverse transform sampling is used in: - Monte Carlo simulations for complex probability models - Bayesian inference for sampling from posterior distributions - Computer graphics for realistic random phenomena - Financial modeling for risk assessment and pricing - Machine learning for generative models and data augmentation\n\n\nAdvantages and Limitations\nAdvantages: - Mathematically exact (no approximation errors) - Works for any distribution with known inverse CDF - Efficient for single samples - Preserves correlations in multivariate extensions\nLimitations: - Requires analytical or numerical inverse CDF - Can be computationally expensive for complex distributions - Some distributions don’t have closed-form inverse CDFs\n\n\nNext Steps\nNow that you understand CDFs and inverse sampling, you’re ready to explore: - Joint distributions and multivariate CDFs - Conditional distributions and their CDFs - Order statistics and their relationship to CDFs - Advanced sampling methods like rejection sampling and MCMC\nThe CDF is truly one of the most versatile tools in probability and statistics!\n\nsymbols  = torch.tensor([1, 2, 3, 4])\nprint(symbols)\nsample = unif.sample()\nprint(sample)\nif cum_sum_prob[0] &gt; sample:\n    print(symbols[0])\nelif cum_sum_prob[1] &gt; sample:\n    print(symbols[1])\nelif cum_sum_prob[2] &gt; sample:\n    print(symbols[2])\nelse:\n    print(symbols[3])\n\ntensor([1, 2, 3, 4])\ntensor(0.6947)\ntensor(4)\n\n\n\nsample &lt;= cum_sum_prob\n\ntensor([False, False, False,  True])\n\n\n\nsymbols[sample &lt; cum_sum_prob][0]\n\ntensor(4)\n\n\n\n### Even more efficient\nindex = torch.searchsorted(cum_sum_prob, sample)\nprint(symbols[index])\n\ntensor(4)\n\n\n\n### Vectorized\nnum_samples = 100000\nunif_samples = unif.sample((num_samples,))\n\nindex = torch.searchsorted(cum_sum_prob, unif_samples)\nour_samples = symbols[index]\nprint(our_samples)\n\ntensor([4, 4, 4,  ..., 3, 4, 2])\n\n\n\nsamples_series = pd.Series(our_samples)\nsamples_series_norm = samples_series.value_counts(normalize=True)\nsamples_series_norm.sort_index(inplace=True)\nsamples_series_norm.plot(kind='bar', rot=0)\nfor i in range(4):\n    plt.axhline(probs[i].item(), color='r', linestyle='--')\nplt.ylim(0, 0.5)\n\n\n\n\n\n\n\n\n\n### Generating samples from exponential distribution\nrate = 1\nexp = torch.distributions.exponential.Exponential(rate)\n\nx_range = torch.linspace(0, 6, 1000)\ny = exp.log_prob(x_range).exp()\nplt.plot(x_range, y)\n\n\n\n\n\n\n\n\n\ndef inv_cdf_exp(rate, u):\n    return -torch.log(1-u)/rate\n\n\nU = torch.distributions.uniform.Uniform(0, 1)\nu_vec = U.sample((num_samples,))\nx_vec = inv_cdf_exp(rate, u_vec)\n\n\nplt.hist(x_vec.numpy(), bins=100, density=True, label='Empirical')\nplt.plot(x_range, y, label='True')\nplt.legend()\n\n\n\n\n\n\n\n\n\n## Generalised implementation when .icdf() is available\n\ndef inverse_cdf_sampling(distribution, sample_size=10000):\n    \"\"\"Performs inverse CDF sampling for a given torch distribution.\"\"\"\n    U = torch.rand(sample_size)  # Generate uniform samples\n    X = distribution.icdf(U)     # Apply inverse CDF (quantile function)\n    return X\n\n\nX = torch.distributions.Normal(0, 1)\nsamples = inverse_cdf_sampling(X, 1000)\n\n\n### Use CDF function\nour_dist = torch.distributions.Normal(0, 1)\nunif_samples = inverse_cdf_sampling(our_dist, 1000)\n\n\n\n\nplt.hist(samples.numpy(), bins=50, density=True)\n\n(array([0.00844681, 0.        , 0.01689361, 0.00844682, 0.02534042,\n        0.02534042, 0.01689361, 0.02534042, 0.04223408, 0.11825541,\n        0.07602126, 0.15204252, 0.17738312, 0.15204252, 0.18582993,\n        0.22806378, 0.261851  , 0.21117016, 0.287192  , 0.31253184,\n        0.38010629, 0.41389352, 0.45612755, 0.40544671, 0.37166023,\n        0.43078713, 0.28719142, 0.3885531 , 0.32942545, 0.33787294,\n        0.34631907, 0.23651058, 0.20272336, 0.25340419, 0.18583012,\n        0.19427616, 0.143596  , 0.10136188, 0.11825505, 0.10980871,\n        0.08446789, 0.11825553, 0.05068094, 0.06757432, 0.00844682,\n        0.02534037, 0.01689365, 0.01689358, 0.01689365, 0.01689361]),\n array([-2.97191405, -2.85352612, -2.73513818, -2.61675024, -2.49836254,\n        -2.3799746 , -2.26158667, -2.14319873, -2.02481079, -1.90642297,\n        -1.78803515, -1.66964722, -1.55125928, -1.43287146, -1.31448352,\n        -1.19609571, -1.07770777, -0.95931983, -0.84093189, -0.72254419,\n        -0.60415626, -0.48576832, -0.36738038, -0.24899244, -0.13060451,\n        -0.01221681,  0.10617113,  0.22455907,  0.34294701,  0.46133494,\n         0.57972264,  0.69811058,  0.81649852,  0.93488646,  1.05327439,\n         1.17166209,  1.29005027,  1.40843797,  1.52682567,  1.64521384,\n         1.76360154,  1.88198972,  2.00037742,  2.11876512,  2.23715329,\n         2.35554099,  2.47392917,  2.59231687,  2.71070504,  2.82909274,\n         2.94748068]),\n &lt;BarContainer object of 50 artists&gt;)\n\n\n\n\n\n\n\n\n\n\nexp.icdf?\n\n\nSignature: exp.icdf(value)\n\nDocstring:\n\nReturns the inverse cumulative density/mass function evaluated at\n\n`value`.\n\n\n\nArgs:\n\n    value (Tensor):\n\nFile:      ~/base/lib/python3.12/site-packages/torch/distributions/exponential.py\n\nType:      method",
    "crumbs": [
      "Home",
      "Foundations",
      "Cumulative Distribution Functions and Inverse Sampling"
    ]
  },
  {
    "objectID": "notebooks/pca.html",
    "href": "notebooks/pca.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nprint(np.__version__)\nimport torch \nimport torch.nn as nn\n\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom sklearn.datasets import make_blobs\nfrom torchvision import datasets, transforms\n\n1.26.4",
    "crumbs": [
      "Home",
      "Machine Learning",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "notebooks/pca.html#introduction",
    "href": "notebooks/pca.html#introduction",
    "title": "Principal Component Analysis",
    "section": "Introduction",
    "text": "Introduction\nPrincipal Component Analysis (PCA) is one of the most fundamental techniques in data science and machine learning. It serves as a cornerstone for dimensionality reduction, data visualization, and understanding the structure of high-dimensional datasets. PCA transforms data to a lower-dimensional space while preserving as much variance (information) as possible.\nAt its core, PCA answers a crucial question: “What are the most important directions of variation in my data?” This question is essential when dealing with high-dimensional data where visualization is challenging, storage is expensive, or computational complexity is prohibitive.\nPCA has applications across numerous fields: - Data Visualization: Reducing high-dimensional data to 2D/3D for plotting - Data Compression: Storing data more efficiently with minimal information loss - Noise Reduction: Filtering out noise by keeping only major components - Feature Engineering: Creating new features that capture data structure - Exploratory Data Analysis: Understanding patterns and relationships in data",
    "crumbs": [
      "Home",
      "Machine Learning",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "notebooks/pca.html#learning-objectives",
    "href": "notebooks/pca.html#learning-objectives",
    "title": "Principal Component Analysis",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this notebook, you will be able to:\n\nUnderstand the mathematical foundations of PCA and eigenvalue decomposition\nDerive the PCA algorithm from variance maximization principles\nImplement PCA from scratch using eigendecomposition\nInterpret principal components as directions of maximum variance\nAnalyze the trade-off between dimensionality reduction and information retention\nApply PCA to real-world datasets (synthetic and MNIST)\nEvaluate PCA results using reconstruction error and explained variance\nConnect PCA to linear algebra concepts (covariance, eigenvectors, projections)",
    "crumbs": [
      "Home",
      "Machine Learning",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "notebooks/pca.html#theoretical-background",
    "href": "notebooks/pca.html#theoretical-background",
    "title": "Principal Component Analysis",
    "section": "Theoretical Background",
    "text": "Theoretical Background\n\nThe Variance Maximization Problem\nPCA finds the directions in which data varies the most. Mathematically, given data matrix \\(X \\in \\mathbb{R}^{n \\times p}\\) (n samples, p features), we want to find a unit vector \\(v \\in \\mathbb{R}^p\\) such that the variance of the projected data \\(Xv\\) is maximized.\nOptimization Problem: \\[\\max_{v} \\text{Var}(Xv) \\quad \\text{subject to} \\quad ||v|| = 1\\]\n\n\nMathematical Derivation\nFor centered data (mean-subtracted), the variance of projected data is: \\[\\text{Var}(Xv) = \\frac{1}{n-1} ||Xv||^2 = \\frac{1}{n-1} v^T X^T X v = v^T \\Sigma v\\]\nwhere \\(\\Sigma = \\frac{1}{n-1} X^T X\\) is the sample covariance matrix.\nLagrangian Solution: \\[L = v^T \\Sigma v - \\lambda (v^T v - 1)\\]\nTaking derivatives and setting to zero: \\[\\frac{\\partial L}{\\partial v} = 2\\Sigma v - 2\\lambda v = 0\\]\nThis gives us the eigenvalue equation: \\[\\Sigma v = \\lambda v\\]\n\n\nKey Results\n\nPrincipal Components: Eigenvectors of the covariance matrix\nExplained Variance: Eigenvalues represent variance along each principal component\nOptimal Projection: PCA provides the best linear dimensionality reduction in terms of preserved variance\n\n\n\nProperties of PCA\n\nOrthogonal Components: Principal components are mutually orthogonal\nDecreasing Variance: Components are ordered by decreasing eigenvalues\nLinear Transformation: PCA is a linear transformation of the original data\nReversible: Can reconstruct original data (with some loss if dimensions are reduced)",
    "crumbs": [
      "Home",
      "Machine Learning",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "notebooks/pca.html#implementation-from-first-principles",
    "href": "notebooks/pca.html#implementation-from-first-principles",
    "title": "Principal Component Analysis",
    "section": "Implementation from First Principles",
    "text": "Implementation from First Principles",
    "crumbs": [
      "Home",
      "Machine Learning",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "notebooks/pca.html#example-1-understanding-pca-with-2d-correlated-data",
    "href": "notebooks/pca.html#example-1-understanding-pca-with-2d-correlated-data",
    "title": "Principal Component Analysis",
    "section": "Example 1: Understanding PCA with 2D Correlated Data",
    "text": "Example 1: Understanding PCA with 2D Correlated Data\nLet’s start with a simple 2D example to build intuition about what PCA does.\n\nUnderstanding the Data Generation Process\nWe’re generating data from a multivariate normal distribution with: - Mean: \\(\\mu = [5, -2]\\) - Covariance: \\(\\Sigma = \\begin{bmatrix} 1.0 & 0.7 \\\\ 0.7 & 1.0 \\end{bmatrix}\\)\nThe correlation coefficient is 0.7, meaning the variables are positively correlated. This creates an elliptical data cloud tilted along the correlation direction.\n\n\nStep 1: Data Centering\nWhy center the data? PCA finds directions of maximum variance from the origin. If data isn’t centered, the first principal component might just point toward the data mean rather than capturing the true variance structure.\nInterpretation: - Original data: Centered around [5, -2] with elliptical spread - Centered data: Now centered at origin [0, 0], preserving the variance structure - Red point: Original mean, Blue point: Centered mean (at origin)\nCentering doesn’t change the relative positions of data points, just shifts the entire dataset.\n\n\nStep 2: Computing the Covariance Matrix\nThe covariance matrix captures how variables co-vary. For centered data:\n\\[\\Sigma = \\frac{1}{n-1} X^T X\\]\nwhere each element \\(\\Sigma_{ij} = \\text{Cov}(X_i, X_j)\\).\nUnderstanding the Covariance Matrix: - Diagonal elements: Variances of individual variables - Off-diagonal elements: Covariances between variables - Positive covariance (0.729): Variables tend to increase/decrease together - Nearly symmetric: \\(\\text{Cov}(X_1, X_2) = \\text{Cov}(X_2, X_1)\\)\n\n\nStep 3: Eigenvalue Decomposition\nThe heart of PCA lies in decomposing the covariance matrix:\n\\[\\Sigma v = \\lambda v\\]\nPhysical Interpretation: - Eigenvectors (v): Directions of principal axes - Eigenvalues (λ): Amount of variance along each principal axis\nKey Insights:\n\nFirst Principal Component (PC1):\n\nDirection: [0.695, 0.719] (roughly 45° angle, pointing up-right)\nVariance: 1.78 (captures most variation)\nThis aligns with the major axis of the elliptical data cloud\n\nSecond Principal Component (PC2):\n\nDirection: [-0.719, 0.695] (perpendicular to PC1)\nVariance: 0.32 (captures remaining variation)\nThis aligns with the minor axis of the ellipse\n\nOrthogonality: PC1 and PC2 are perpendicular (dot product ≈ 0)\n\nThe eigenvectors show us the natural coordinate system of our data!\n\n\nStep 4: Projection and Reconstruction\nProjection: Transform data to principal component space \\[Y = X_{\\text{centered}} V\\]\nReconstruction: Transform back to original space \\[X_{\\text{reconstructed}} = Y V^T\\]\nFor 1D PCA, we only use the first principal component:\nUnderstanding the Results:\n\nProjection Formula: \\(y_i = \\mathbf{x}_i^T \\mathbf{v}_1\\) (dot product of data point with first eigenvector)\nReconstruction Formula: \\(\\hat{\\mathbf{x}}_i = y_i \\mathbf{v}_1\\) (scale the eigenvector by the projection)\nGeometric Interpretation: Each point is projected onto the line defined by the first principal component, then reconstructed back to 2D space\nInformation Loss: The distance between original and reconstructed points represents lost information (captured by PC2)\n\nThe reconstruction shows how well a 1D representation captures the 2D data structure!\n\n# Simple 2D blobs with say 0.7 correlation\nX = torch.distributions.multivariate_normal.MultivariateNormal(\n    torch.tensor([5.0, -2.0]), torch.tensor([[1.0, 0.7], [0.7, 1.0]])\n).sample((1000,))\n\n\n# Comprehensive PCA visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# 1. Original vs Centered Data\naxes[0, 0].scatter(X[:, 0], X[:, 1], alpha=0.6, s=30, label='Original Data')\naxes[0, 0].scatter(X_mean[0, 0], X_mean[0, 1], color='red', s=100, marker='x', \n                  linewidth=3, label='Original Mean')\naxes[0, 0].scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.6, s=30, \n                  color='orange', label='Centered Data')\naxes[0, 0].scatter(0, 0, color='blue', s=100, marker='x', linewidth=3, \n                  label='Centered Mean')\naxes[0, 0].set_title('Data Centering')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].axis('equal')\n\n# 2. Covariance Matrix Visualization\nim = axes[0, 1].imshow(cov.numpy(), cmap='coolwarm', vmin=-1, vmax=1)\naxes[0, 1].set_title('Covariance Matrix')\nfor i in range(2):\n    for j in range(2):\n        axes[0, 1].text(j, i, f'{cov[i,j]:.3f}', ha='center', va='center', \n                       fontsize=12, fontweight='bold')\naxes[0, 1].set_xticks([0, 1])\naxes[0, 1].set_yticks([0, 1])\naxes[0, 1].set_xticklabels(['X1', 'X2'])\naxes[0, 1].set_yticklabels(['X1', 'X2'])\nplt.colorbar(im, ax=axes[0, 1])\n\n# 3. Eigendecomposition Results\naxes[0, 2].bar(['PC1', 'PC2'], eigvals.numpy(), alpha=0.7, color=['blue', 'red'])\naxes[0, 2].set_title('Eigenvalues (Explained Variance)')\naxes[0, 2].set_ylabel('Variance')\nfor i, val in enumerate(eigvals.numpy()):\n    axes[0, 2].text(i, val + 0.02, f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\naxes[0, 2].grid(True, alpha=0.3)\n\n# 4. Principal Components Visualization\naxes[1, 0].scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.6, s=30, color='lightblue')\n\n# Plot eigenvectors as arrows from origin\nscale = 2  # Scale for visibility\nfor i in range(2):\n    vec = eigvecs[:, i] * scale\n    axes[1, 0].arrow(0, 0, vec[0], vec[1], head_width=0.1, head_length=0.1, \n                    fc=f'C{i}', ec=f'C{i}', linewidth=3, \n                    label=f'PC{i+1} (λ={eigvals[i]:.3f})')\n\naxes[1, 0].set_title('Principal Components\\n(Eigenvectors)')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].axis('equal')\n\n# 5. 1D Projection\naxes[1, 1].scatter(X_proj_1d[:, 0], X_proj_1d[:, 1], alpha=0.8, s=30, \n                  color='red', label='1D Projection')\naxes[1, 1].scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.3, s=20, \n                  color='lightblue', label='Original Data')\n\n# Draw projection lines\nfor i in range(0, len(X_centered), 20):  # Show every 20th line for clarity\n    x_orig = X_centered[i]\n    x_proj = X_proj_1d[i]\n    axes[1, 1].plot([x_orig[0], x_proj[0]], [x_orig[1], x_proj[1]], \n                   'k--', alpha=0.3, linewidth=0.5)\n\naxes[1, 1].set_title('1D PCA Reconstruction')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\naxes[1, 1].axis('equal')\n\n# 6. Explained Variance Analysis\nexplained_var_ratio = eigvals / torch.sum(eigvals)\ncumulative_var = torch.cumsum(explained_var_ratio, dim=0)\n\nx_pos = np.arange(len(eigvals))\nbars = axes[1, 2].bar(x_pos, explained_var_ratio.numpy(), alpha=0.7, label='Individual')\naxes[1, 2].plot(x_pos, cumulative_var.numpy(), 'ro-', linewidth=2, markersize=8, label='Cumulative')\n\naxes[1, 2].set_title('Explained Variance Ratio')\naxes[1, 2].set_xlabel('Principal Component')\naxes[1, 2].set_ylabel('Proportion of Variance')\naxes[1, 2].set_xticks(x_pos)\naxes[1, 2].set_xticklabels(['PC1', 'PC2'])\naxes[1, 2].legend()\naxes[1, 2].grid(True, alpha=0.3)\n\n# Add percentage labels\nfor i, (ind, cum) in enumerate(zip(explained_var_ratio.numpy(), cumulative_var.numpy())):\n    axes[1, 2].text(i, ind + 0.02, f'{ind:.1%}', ha='center', va='bottom', fontweight='bold')\n    axes[1, 2].text(i, cum + 0.02, f'{cum:.1%}', ha='center', va='bottom', fontweight='bold', color='red')\n\nplt.tight_layout()\nplt.show()\n\n# Detailed Analysis\nprint(\"COMPREHENSIVE PCA ANALYSIS:\")\nprint(\"=\"*50)\n\nprint(f\"\\n1. DATA CHARACTERISTICS:\")\nprint(f\"   - Original mean: [{X_mean[0, 0]:.3f}, {X_mean[0, 1]:.3f}]\")\nprint(f\"   - Data shape: {X.shape}\")\nprint(f\"   - Correlation: {cov[0,1] / torch.sqrt(cov[0,0] * cov[1,1]):.3f}\")\n\nprint(f\"\\n2. COVARIANCE MATRIX:\")\nprint(f\"   - Var(X1): {cov[0,0]:.3f}\")\nprint(f\"   - Var(X2): {cov[1,1]:.3f}\")\nprint(f\"   - Cov(X1,X2): {cov[0,1]:.3f}\")\nprint(f\"   - Total variance: {torch.trace(cov):.3f}\")\n\nprint(f\"\\n3. EIGENDECOMPOSITION:\")\nprint(f\"   - Eigenvalues: {eigvals.numpy()}\")\nprint(f\"   - PC1 direction: [{eigvecs[0,0]:.3f}, {eigvecs[1,0]:.3f}]\")\nprint(f\"   - PC2 direction: [{eigvecs[0,1]:.3f}, {eigvecs[1,1]:.3f}]\")\nprint(f\"   - Orthogonality check: {torch.dot(eigvecs[:,0], eigvecs[:,1]):.6f}\")\n\nprint(f\"\\n4. VARIANCE EXPLANATION:\")\nprint(f\"   - PC1 explains: {explained_var_ratio[0]:.1%} of variance\")\nprint(f\"   - PC2 explains: {explained_var_ratio[1]:.1%} of variance\")\nprint(f\"   - Total explained: {cumulative_var[-1]:.1%}\")\n\nprint(f\"\\n5. RECONSTRUCTION QUALITY:\")\nreconstruction_error = torch.mean((X_centered - X_proj_1d)**2)\nprint(f\"   - Mean Squared Error (1D): {reconstruction_error:.6f}\")\nprint(f\"   - Variance retained (1D): {explained_var_ratio[0]:.1%}\")\nprint(f\"   - Information lost (1D): {explained_var_ratio[1]:.1%}\")",
    "crumbs": [
      "Home",
      "Machine Learning",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "notebooks/pca.html#example-2-pca-on-high-dimensional-data-mnist",
    "href": "notebooks/pca.html#example-2-pca-on-high-dimensional-data-mnist",
    "title": "Principal Component Analysis",
    "section": "Example 2: PCA on High-Dimensional Data (MNIST)",
    "text": "Example 2: PCA on High-Dimensional Data (MNIST)\nNow let’s apply PCA to a real-world high-dimensional dataset: handwritten digits from MNIST. This demonstrates PCA’s power in reducing dimensionality while preserving essential information.\nUnderstanding MNIST in PCA Context:\n\nOriginal Dimensionality: 784 dimensions (28×28 pixels)\nSample Size: 1000 images (subset for computational efficiency)\nChallenge: How can we capture the essence of digit shapes in far fewer dimensions?\nPCA Goal: Find the most important ‘pixel patterns’ that distinguish different digits\n\n\nData Preprocessing and Centering\n\n\nUnderstanding the Covariance Structure\nInterpreting the Covariance Matrix:\n\nSize: 784×784 matrix showing how each pixel correlates with every other pixel\nPatterns:\n\nBright regions show high correlation (pixels that tend to be bright/dark together)\nBlock-like structure suggests spatial correlations (nearby pixels are related)\nThe pattern reveals the underlying structure of how digit pixels co-vary\n\n\n\n\nComprehensive PCA Analysis with Visualizations\nLet’s create a complete visualization showing all aspects of PCA:\n\n### Plot the data\nplt.figure(figsize=(4, 4))\nplt.scatter(X[:, 0], X[:, 1],  alpha=0.5)\n\n\n\n\n\n\n\n\n\n\nPrincipal Components as ‘Eigendigits’\nThe principal components can be interpreted as fundamental ‘building blocks’ or ‘eigendigits’ - basic patterns that combine to form all digit images.\nUnderstanding Principal Components:\nEach principal component represents a different pattern of pixel variations:\n\nPC1: Captures the most common variation (average brightness vs. background)\nPC2-PC4: Capture shape variations (edges, curves, strokes)\nHigher PCs: Capture more subtle details and noise\n\nThese components are like ‘visual features’ that the human visual system might use to recognize digits!\n\n\nDimensionality Reduction and Reconstruction Quality\nKey Observations:\n\nDramatic Dimensionality Reduction: Even 10 components (1.3% of original dimensions) capture recognizable digit structure\nQuality vs. Compression Trade-off:\n\n2 components: ~25% variance, basic shape visible\n10 components: ~60% variance, clearly recognizable digits\n50 components: ~85% variance, high-quality reconstruction\n\nDiminishing Returns: Adding more components improves quality, but with decreasing benefit\nStorage Efficiency: Instead of storing 784 values per image, we can store just 10-50 principal component coefficients!",
    "crumbs": [
      "Home",
      "Machine Learning",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "notebooks/pca.html#summary-and-key-takeaways",
    "href": "notebooks/pca.html#summary-and-key-takeaways",
    "title": "Principal Component Analysis",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nMathematical Foundations:\n\nCore Principle: PCA finds orthogonal directions of maximum variance through eigendecomposition of the covariance matrix\nOptimization: Solves \\(\\max_v v^T \\Sigma v\\) subject to \\(||v|| = 1\\), yielding \\(\\Sigma v = \\lambda v\\)\nGeometric Interpretation: Rotates coordinate system to align with natural axes of data variation\n\n\n\nPractical Insights:\n\nDimensionality Reduction: Often 90%+ of variance captured by small fraction of dimensions\nData Compression: Store only principal component coefficients instead of raw features\n\nNoise Reduction: Lower components often represent noise; keeping top components filters this out\nFeature Engineering: PC scores can serve as new features for machine learning\n\n\n\nKey Properties:\n\nLinear Transformation: \\(Y = XW\\) where W contains eigenvectors\nOrthogonal Components: Principal components are uncorrelated\nVariance Ordering: Components ordered by decreasing explained variance\nReconstruction: \\(\\hat{X} = YW^T\\) (perfect if all components kept)\n\n\n\nApplications and Extensions:\nDirect Applications: - Data visualization (reduce to 2D/3D) - Image compression and denoising - Exploratory data analysis - Preprocessing for machine learning\nConnections to Other Methods: - Factor Analysis: PCA without noise assumptions - Independent Component Analysis (ICA): Non-orthogonal components - t-SNE/UMAP: Nonlinear dimensionality reduction - Autoencoders: Neural network-based dimensionality reduction\n\n\nWhen to Use PCA:\nGood Cases: - High-dimensional data with linear correlations - Need for interpretable dimensions - Data visualization requirements - Computational efficiency important\nLimitations: - Assumes linear relationships - Components may not be interpretable - Sensitive to scaling of features - May not preserve local structure\n\n\nBest Practices:\n\nAlways center data (subtract mean)\nConsider scaling features if different units\nChoose components based on explained variance and domain knowledge\nValidate reconstruction quality for your specific use case\nCompare with other dimensionality reduction methods\n\nUnderstanding PCA provides a solid foundation for advanced topics in machine learning, computer vision, and data science. It bridges linear algebra theory with practical data analysis, making it an essential tool in the data scientist’s toolkit.\n\n\nCenter the data\n\nX_mean = X.mean(0, keepdim=True)\nX_centered = X - X_mean\n\nplt.scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.5, label='Centered')\nplt.scatter(X[:, 0], X[:, 1], alpha=0.5, label='Original')\nplt.scatter(X_mean[:, 0], X_mean[:, 1], color='red', label='Original Mean')\nplt.scatter(X_centered.mean(0, keepdim=True)[:, 0], X_centered.mean(0, keepdim=True)[:, 1], color='blue', label='Centered Mean')\nplt.title('Centered vs Original')\nplt.xlabel('X1')\nplt.ylabel('X2')\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nFinding covariance\n\ncov = X_centered.T @ X_centered / (X.shape[0] - 1)\nprint('Covariance matrix:')\nprint(cov)\n\nCovariance matrix:\ntensor([[1.0229, 0.7291],\n        [0.7291, 1.0733]])\n\n\n\n### Finding the eigenvalues and eigenvectors\neigvals, eigvecs = torch.linalg.eigh(cov)\nprint('Eigenvalues:')\nprint(eigvals)\nprint('Eigenvectors:')\nprint(eigvecs)\n\nEigenvalues:\ntensor([0.3186, 1.7776])\nEigenvectors:\ntensor([[-0.7192,  0.6948],\n        [ 0.6948,  0.7192]])\n\n\n\n# Plot centered data with eigenvectors\nplt.scatter(X_centered[:, 0], X_centered[:, 1], alpha=0.3)\n\n# Plot eigenvectors starting from the mean\nfor i in range(2):\n    vec = eigvecs[:, i]\n    plt.quiver(0, 0, vec[0], vec[1], scale=3, color=f\"C{i}\", label=f\"u{i+1} ({eigvals[i]:.2f})\")\n\nplt.axis('equal')\nplt.legend()\nplt.title(\"Centered Data with Principal Directions\")\nplt.show()\n\n\n\n\n\n\n\n\n\ntop_vec = eigvecs[:, -1]  # Last column of eigvecs (the top eigenvector)\nprint('Top eigenvector:')\nprint(top_vec)\n\nTop eigenvector:\ntensor([0.6948, 0.7192])\n\n\n\n# Project centered data onto the top eigenvector using dot product\nX_proj = torch.zeros_like(X_centered)  # Initialize an empty tensor to store projections\n\n# Loop through each data point and project it onto the top eigenvector\nfor i in range(X_centered.shape[0]):\n    # Calculate the projection of the i-th data point onto the top eigenvector\n    X_proj[i] = torch.dot(X_centered[i], top_vec) * top_vec  # Scalar projection * eigenvector\n\n# Reconstruct the data by adding the mean back\nX_recon = X_proj + X_mean\n\n\nX_recon\n\ntensor([[ 2.8117, -4.2273],\n        [ 4.4202, -2.5621],\n        [ 5.5191, -1.4246],\n        ...,\n        [ 4.1834, -2.8073],\n        [ 3.9091, -3.0913],\n        [ 5.5841, -1.3573]])\n\n\n\nplt.scatter(X[:, 0], X[:, 1], alpha=0.2, label='Original')\nplt.scatter(X_recon[:, 0], X_recon[:, 1], alpha=0.8, label='PCA-1D')\nplt.axis('equal')\nplt.legend()\nplt.title(\"PCA projection onto top component\")\n\nText(0.5, 1.0, 'PCA projection onto top component')\n\n\n\n\n\n\n\n\n\n\n# Load MNIST dataset\nimport torchvision\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\ntrain_data = torchvision.datasets.MNIST(root='~/.data', train=True, download=True, transform=transform)\n\n# Take a small subset of data for simplicity\nX = train_data.data[:1000].float()  # Take the first 1000 images (28x28 pixels)\ny = train_data.targets[:1000]  # Corresponding labels\n\n\n# View the first image\nplt.imshow(X[0].reshape(28, 28), cmap='gray')\n\n\n\n\n\n\n\n\n\nX = X / 255.0  # Normalize to [0, 1]\nX_centered = X - X.mean(dim=0)  # Center the data by subtracting the mean\n\n\nX_vecs = X_centered.reshape(X_centered.shape[0], -1)  # Reshape to (n_samples, n_features)\n\n\n# Compute covariance matrix\ncov_matrix = torch.cov(X_vecs.T)  # Transpose to get (n_features, n_samples)\n\n\nprint('Covariance matrix shape:', cov_matrix.shape)\n\nCovariance matrix shape: torch.Size([784, 784])\n\n\n\nplt.imshow(cov_matrix, cmap='hot', interpolation='nearest')\nplt.colorbar()\nplt.title('Covariance Matrix')\n\nText(0.5, 1.0, 'Covariance Matrix')\n\n\n\n\n\n\n\n\n\n\n# Eigenvalue decomposition\neigvals, eigvecs = torch.linalg.eigh(cov_matrix)\nprint('Eigenvalues shape:', eigvals.shape)\n\nEigenvalues shape: torch.Size([784])\n\n\n\n# Top K eigenvalues and eigenvectors\nK = 10\ntop_k_eigvals, top_k_indices = torch.topk(eigvals, K)\ntop_k_eigvecs = eigvecs[:, top_k_indices]\nprint('Top K eigenvalues:', top_k_eigvals)\nprint('Top K eigenvectors shape:', top_k_eigvecs.shape)\n# Plot the top K eigenvalues\nplt.figure(figsize=(8, 4))\nplt.bar(range(K), top_k_eigvals.numpy())\nplt.xlabel('Eigenvalue Index')\nplt.ylabel('Eigenvalue')\nplt.title('Top K Eigenvalues')\nplt.show()\n\nTop K eigenvalues: tensor([5.1288, 4.0052, 3.5313, 2.8018, 2.5156, 2.3427, 1.8130, 1.5647, 1.4760,\n        1.1167])\nTop K eigenvectors shape: torch.Size([784, 10])\n\n\n\n\n\n\n\n\n\n\n# Project data onto the top K eigenvectors\nX_proj = torch.matmul(X_vecs, top_k_eigvecs)\nprint('Projected data shape:', X_proj.shape)\n# Reconstruct the data from the top K components\n\nProjected data shape: torch.Size([1000, 10])\n\n\n\nimport matplotlib.cm as cm\n\nplt.figure(figsize=(5, 5))\n\ncolors = cm.tab10(np.arange(10))  # 10 distinct colors\nfor i in range(10):\n    idx = (y == i)\n    plt.scatter(X_proj[idx, 0], X_proj[idx, 1], \n                alpha=0.6, color=colors[i], label=f\"{i}\", s=10)\n\nplt.title('PCA Projection onto Top 2 Components (MNIST)')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.axis('equal')\nplt.legend(title='Digit', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.tight_layout()",
    "crumbs": [
      "Home",
      "Machine Learning",
      "Principal Component Analysis"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html",
    "href": "notebooks/pdf-continuous.html",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "",
    "text": "By the end of this notebook, you will understand:\n\nProbability Density Functions (PDF): Mathematical definition and key properties\nMajor Continuous Distributions: Normal, Exponential, Uniform, Gamma, Beta, and more\nReal-World Applications: From heights and weights to financial modeling and signal processing\nParameter Estimation: Fitting distributions to real data using maximum likelihood\nDistribution Selection: Choosing appropriate models for different data types\nPractical Implementation: Using PyTorch for continuous probability modeling\n\n\n\n\nContinuous random variables can take on any value within an interval (like real numbers), and their behavior is described by their Probability Density Function (PDF). Unlike discrete variables where we can calculate exact probabilities, continuous variables require integration to find probabilities over intervals.\n\n\nFor a continuous random variable \\(X\\), the PDF \\(f_X(x)\\) satisfies:\n\nNon-negativity: \\(f_X(x) \\geq 0\\) for all \\(x\\)\nNormalization: \\(\\int_{-\\infty}^{\\infty} f_X(x) dx = 1\\)\nProbability calculation: \\(P(a \\leq X \\leq b) = \\int_a^b f_X(x) dx\\)\n\n\n\n\n⚠️ Important: \\(f_X(x)\\) is not a probability! It’s a density that can exceed 1. Probabilities come from integrating the PDF over intervals.\n\n\n\nContinuous distributions are ubiquitous in data science and statistics: - Measurement data (heights, weights, temperatures) - Financial modeling (stock prices, returns) - Signal processing (noise, quantization errors) - Machine learning (neural network weights, latent variables) - Quality control (manufacturing tolerances) - Natural phenomena (radioactive decay, waiting times)\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nParameters\nUse Case\nExample Applications\n\n\n\n\nNormal\n\\(\\mu, \\sigma^2\\)\nSymmetric, bell-shaped\nHeights, measurement errors\n\n\nExponential\n\\(\\lambda\\)\nWaiting times, reliability\nTime between events\n\n\nUniform\n\\(a, b\\)\nEqual likelihood over interval\nRandom number generation\n\n\nGamma\n\\(\\alpha, \\beta\\)\nPositive skewed data\nWaiting times, income\n\n\nBeta\n\\(\\alpha, \\beta\\)\nBounded between 0 and 1\nProportions, probabilities\n\n\nLog-Normal\n\\(\\mu, \\sigma\\)\nMultiplicative processes\nStock prices, file sizes\n\n\nLaplace\n\\(\\mu, b\\)\nHeavy-tailed, symmetric\nRobust statistics\n\n\n\nLet’s explore each distribution with mathematical rigor and practical applications!\n\n\n\n\nWe’ll use PyTorch for probability distributions, NumPy for numerical operations, and Matplotlib for visualizations:\n\n\n\nThe Normal (Gaussian) distribution is arguably the most important continuous distribution in statistics and data science. It appears everywhere due to the Central Limit Theorem and is fundamental to many statistical methods.\n\n\nLet \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) be a normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\). The PDF is:\n\\[f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]\n\n\n\n\nMean: \\(E[X] = \\mu\\)\nVariance: \\(\\text{Var}(X) = \\sigma^2\\)\nMode: \\(\\mu\\) (also median)\nSupport: \\((-\\infty, \\infty)\\)\nSymmetry: Symmetric around \\(\\mu\\)\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch \nimport torch.nn as nn\n\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html#learning-objectives",
    "href": "notebooks/pdf-continuous.html#learning-objectives",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "",
    "text": "By the end of this notebook, you will understand:\n\nProbability Density Functions (PDF): Mathematical definition and key properties\nMajor Continuous Distributions: Normal, Exponential, Uniform, Gamma, Beta, and more\nReal-World Applications: From heights and weights to financial modeling and signal processing\nParameter Estimation: Fitting distributions to real data using maximum likelihood\nDistribution Selection: Choosing appropriate models for different data types\nPractical Implementation: Using PyTorch for continuous probability modeling",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html#introduction",
    "href": "notebooks/pdf-continuous.html#introduction",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "",
    "text": "Continuous random variables can take on any value within an interval (like real numbers), and their behavior is described by their Probability Density Function (PDF). Unlike discrete variables where we can calculate exact probabilities, continuous variables require integration to find probabilities over intervals.\n\n\nFor a continuous random variable \\(X\\), the PDF \\(f_X(x)\\) satisfies:\n\nNon-negativity: \\(f_X(x) \\geq 0\\) for all \\(x\\)\nNormalization: \\(\\int_{-\\infty}^{\\infty} f_X(x) dx = 1\\)\nProbability calculation: \\(P(a \\leq X \\leq b) = \\int_a^b f_X(x) dx\\)\n\n\n\n\n⚠️ Important: \\(f_X(x)\\) is not a probability! It’s a density that can exceed 1. Probabilities come from integrating the PDF over intervals.\n\n\n\nContinuous distributions are ubiquitous in data science and statistics: - Measurement data (heights, weights, temperatures) - Financial modeling (stock prices, returns) - Signal processing (noise, quantization errors) - Machine learning (neural network weights, latent variables) - Quality control (manufacturing tolerances) - Natural phenomena (radioactive decay, waiting times)\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nParameters\nUse Case\nExample Applications\n\n\n\n\nNormal\n\\(\\mu, \\sigma^2\\)\nSymmetric, bell-shaped\nHeights, measurement errors\n\n\nExponential\n\\(\\lambda\\)\nWaiting times, reliability\nTime between events\n\n\nUniform\n\\(a, b\\)\nEqual likelihood over interval\nRandom number generation\n\n\nGamma\n\\(\\alpha, \\beta\\)\nPositive skewed data\nWaiting times, income\n\n\nBeta\n\\(\\alpha, \\beta\\)\nBounded between 0 and 1\nProportions, probabilities\n\n\nLog-Normal\n\\(\\mu, \\sigma\\)\nMultiplicative processes\nStock prices, file sizes\n\n\nLaplace\n\\(\\mu, b\\)\nHeavy-tailed, symmetric\nRobust statistics\n\n\n\nLet’s explore each distribution with mathematical rigor and practical applications!",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html#setting-up-the-environment",
    "href": "notebooks/pdf-continuous.html#setting-up-the-environment",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "",
    "text": "We’ll use PyTorch for probability distributions, NumPy for numerical operations, and Matplotlib for visualizations:",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html#normal-distribution",
    "href": "notebooks/pdf-continuous.html#normal-distribution",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "",
    "text": "The Normal (Gaussian) distribution is arguably the most important continuous distribution in statistics and data science. It appears everywhere due to the Central Limit Theorem and is fundamental to many statistical methods.\n\n\nLet \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) be a normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\). The PDF is:\n\\[f_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\]\n\n\n\n\nMean: \\(E[X] = \\mu\\)\nVariance: \\(\\text{Var}(X) = \\sigma^2\\)\nMode: \\(\\mu\\) (also median)\nSupport: \\((-\\infty, \\infty)\\)\nSymmetry: Symmetric around \\(\\mu\\)\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch \nimport torch.nn as nn\n\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html#laplace-distribution",
    "href": "notebooks/pdf-continuous.html#laplace-distribution",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "2. Laplace Distribution",
    "text": "2. Laplace Distribution\nThe Laplace distribution is a continuous probability distribution that’s symmetric like the normal distribution but has heavier tails, making it more robust to outliers.\n\nMathematical Definition\nLet \\(X \\sim \\text{Laplace}(\\mu, b)\\) where \\(\\mu\\) is the location parameter and \\(b &gt; 0\\) is the scale parameter. The PDF is:\n\\[f_X(x) = \\frac{1}{2b} \\exp\\left(-\\frac{|x-\\mu|}{b}\\right)\\]\n\n\nProperties\n\nMean: \\(E[X] = \\mu\\)\nVariance: \\(\\text{Var}(X) = 2b^2\\)\nSupport: \\((-\\infty, \\infty)\\)\nHeavier tails than normal distribution\nUsed in robust statistics and L1 regularization\n\n\n\nComparison with Normal Distribution\nNotice how the Laplace distribution has a sharper peak and heavier tails compared to the normal distribution.",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html#half-normal-distribution",
    "href": "notebooks/pdf-continuous.html#half-normal-distribution",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "3. Half-Normal Distribution",
    "text": "3. Half-Normal Distribution\nThe Half-Normal distribution is derived from the normal distribution by taking the absolute value, making it useful for modeling positive quantities.\n\nMathematical Definition\nIf \\(Y \\sim \\mathcal{N}(0, \\sigma^2)\\), then \\(X = |Y|\\) follows a half-normal distribution with PDF:\n\\[f_X(x) = \\sqrt{\\frac{2}{\\pi\\sigma^2}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right) \\quad \\text{for } x \\geq 0\\]\n\n\nProperties\n\nMean: \\(E[X] = \\sigma\\sqrt{\\frac{2}{\\pi}}\\)\nVariance: \\(\\text{Var}(X) = \\sigma^2\\left(1-\\frac{2}{\\pi}\\right)\\)\nSupport: \\([0, \\infty)\\)\nApplications: Error magnitudes, positive measurements\n\n\n\nVisualization\n\n# Fit a normal distribution to the data\nmu = store_df[\"Height(Inches)\"].mean().item()\nsigma = store_df[\"Height(Inches)\"].std().item()\n\ndist = torch.distributions.Normal(mu, sigma)\nx = torch.linspace(50, 80, 1000)\ny = dist.log_prob(x).exp()\nplt.plot(x, y, label=\"Fitted PDF\")\n\nstore_df[\"Height(Inches)\"].plot(kind='hist', label=\"Histogram\", density=True, bins=30)\nplt.legend()\n\n\n\n\n\n\n\n\n\nstore_df[\"Weight(Pounds)\"].plot(kind='density')\n\n\n\n\n\n\n\n\n\n\nUnderstanding Log vs Linear Scale\nWhen working with very small probabilities, it’s often useful to work in log space to avoid numerical underflow:\n\n\nGrading\n\n\nNote: I DO NOT FOLLOW or endorse using a normal distribution to model grades in a class. This is just an exercise to practice the PDF of a normal distribution and show how to use percentiles.",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html#log-normal-distribution",
    "href": "notebooks/pdf-continuous.html#log-normal-distribution",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "4. Log-Normal Distribution",
    "text": "4. Log-Normal Distribution\nThe Log-Normal distribution arises when the logarithm of a random variable follows a normal distribution. It’s crucial for modeling positive quantities that result from multiplicative processes.\n\nMathematical Definition\nIf \\(Y \\sim \\mathcal{N}(\\mu, \\sigma^2)\\), then \\(X = e^Y\\) follows a log-normal distribution with PDF:\n\\[f_X(x) = \\frac{1}{x\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(\\ln(x)-\\mu)^2}{2\\sigma^2}\\right) \\quad \\text{for } x &gt; 0\\]\n\n\nProperties\n\nMean: \\(E[X] = e^{\\mu + \\sigma^2/2}\\)\nVariance: \\(\\text{Var}(X) = e^{2\\mu + \\sigma^2}(e^{\\sigma^2} - 1)\\)\nSupport: \\((0, \\infty)\\)\nRight-skewed: Always positively skewed\n\n\n\nApplications\n\nStock prices and financial returns\nFile sizes and internet traffic\nIncome distributions\nProduct lifetimes\nParticle sizes\n\n\n\nComparing Log-Normal with Normal\n\nmarks = torch.distributions.Normal(70, 8).sample((400,))\n\n\n_ = plt.hist(marks, bins=20, density=True)\n\n\n\n\n\n\n\n\n\nmu_marks, sigma_marks = marks.mean().item(), marks.std().item()\ndist = torch.distributions.Normal(mu_marks, sigma_marks)\nx = torch.linspace(30, 110, 1000)\ny = dist.log_prob(x).exp()\nplt.plot(x, y, label=\"Fitted PDF\", color='gray', lw=2)\n\n# 99% percentile and above get A+\nmarks_99_per = dist.icdf(torch.tensor(0.99))\nnum_students_getting_A_plus = marks[marks&gt;marks_99_per].shape[0]\nplt.fill_between(x, y, where=x&gt;marks_99_per, alpha=0.5, label=f\"A+ ({num_students_getting_A_plus})\")\n\n# 90th percntile to 99th percentile get A\nmarks_90_per = dist.icdf(torch.tensor(0.90))\nnum_students_getting_A = marks[(marks&gt;marks_90_per) & (marks&lt;marks_99_per)].shape[0]\nplt.fill_between(x, y, where=(x&gt;marks_90_per) & (x&lt;marks_99_per), alpha=0.5, label=f\"A ({num_students_getting_A})\")\n\n# 75th percentile to 90th percentile get A-\nmarks_75_per = dist.icdf(torch.tensor(0.75))\nnum_students_getting_B = marks[(marks&gt;marks_75_per) & (marks&lt;marks_90_per)].shape[0]\nplt.fill_between(x, y, where=(x&gt;marks_75_per) & (x&lt;marks_90_per), alpha=0.5, label=f\"B ({num_students_getting_B})\")\n\n# 60th percentile to 75th percentile get B\nmarks_60_per = dist.icdf(torch.tensor(0.60))\nnum_students_getting_B = marks[(marks&gt;marks_60_per) & (marks&lt;marks_75_per)].shape[0]\nplt.fill_between(x, y, where=(x&gt;marks_60_per) & (x&lt;marks_75_per), alpha=0.5, label=f\"B- ({num_students_getting_B})\")\n\n# 45th percentile to 60th percentile get C\nmarks_45_per = dist.icdf(torch.tensor(0.45))\nnum_students_getting_B_minus = marks[(marks&gt;marks_45_per) & (marks&lt;marks_60_per)].shape[0]\nplt.fill_between(x, y, where=(x&gt;marks_45_per) & (x&lt;marks_60_per), alpha=0.5, label=f\"C ({num_students_getting_B_minus})\")\n\n#35th percentile to 45th percentile get C-\nmarks_35_per = dist.icdf(torch.tensor(0.35))\nnum_students_getting_C = marks[(marks&gt;marks_35_per) & (marks&lt;marks_45_per)].shape[0]\nplt.fill_between(x, y, where=(x&gt;marks_35_per) & (x&lt;marks_45_per), alpha=0.5, label=f\"C- ({num_students_getting_C})\")\n\n# 20th percentile to 35th percentile get D\nmarks_20_per = dist.icdf(torch.tensor(0.20))\nnum_students_getting_C_minus = marks[(marks&gt;marks_20_per) & (marks&lt;marks_35_per)].shape[0]\nplt.fill_between(x, y, where=(x&gt;marks_20_per) & (x&lt;marks_35_per), alpha=0.5, label=f\"D ({num_students_getting_C_minus})\")\n\n# 3rd percentile to 20th percentile get E\nmarks_3_per = dist.icdf(torch.tensor(0.03))\nnum_students_getting_D = marks[(marks&gt;marks_3_per) & (marks&lt;marks_20_per)].shape[0]\nplt.fill_between(x, y, where=(x&gt;marks_3_per) & (x&lt;marks_20_per), alpha=0.5, label=f\"E ({num_students_getting_D})\")\n\n# 3rd percentile and below get F\nnum_students_getting_F = marks[marks&lt;marks_3_per].shape[0]\nplt.fill_between(x, y, where=x&lt;marks_3_per, alpha=0.5, label=f\"F ({num_students_getting_F})\")\n\nplt.legend()",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html#log-normal-distribution-1",
    "href": "notebooks/pdf-continuous.html#log-normal-distribution-1",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "Log Normal Distribution",
    "text": "Log Normal Distribution\nLet \\(Y \\sim \\mathcal{N}(\\mu, \\sigma^2)\\) be a normally distributed random variable.\nLet us define a new random variable \\(X = \\exp(Y)\\).\nWe can say that log of \\(X\\) is normally distributed, i.e., \\(\\log(X) \\sim \\mathcal{N}(\\mu, \\sigma^2)\\).\nWe can also say that \\(X\\) is log-normally distributed.\nThe probability density function (PDF) of \\(X\\) is given by:\n\\[\nf_X(x) = \\frac{1}{x\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(\\log(x)-\\mu)^2}{2\\sigma^2}\\right).\n\\]\nWe can derive the PDF of \\(X\\) using the change of variables formula. (will be covered later in the course)\nThe log-normal distribution provides a reasonable fit to the chess game length data, supporting the hypothesis that game lengths follow a log-normal distribution.",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html#gamma-distribution",
    "href": "notebooks/pdf-continuous.html#gamma-distribution",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "5. Gamma Distribution",
    "text": "5. Gamma Distribution\nThe Gamma distribution is a versatile continuous distribution that generalizes the exponential distribution and is widely used for modeling waiting times and positive-valued data.\n\nMathematical Definition\nLet \\(X \\sim \\text{Gamma}(\\alpha, \\beta)\\) where \\(\\alpha &gt; 0\\) is the shape parameter and \\(\\beta &gt; 0\\) is the rate parameter. The PDF is:\n\\[f_X(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x} \\quad \\text{for } x &gt; 0\\]\nwhere \\(\\Gamma(\\alpha) = \\int_0^\\infty t^{\\alpha-1} e^{-t} dt\\) is the gamma function.\n\n\nProperties\n\nMean: \\(E[X] = \\frac{\\alpha}{\\beta}\\)\nVariance: \\(\\text{Var}(X) = \\frac{\\alpha}{\\beta^2}\\)\nSupport: \\((0, \\infty)\\)\nSpecial cases: Exponential(\\(\\lambda\\)) = Gamma(1, \\(\\lambda\\))\n\n\n\nApplications\n\nWaiting times for multiple events\nInsurance claims modeling\nRainfall amounts\nQueue lengths\nReliability engineering\n\n\n\nParameter Estimation Example\nLet’s fit a gamma distribution to chess game length data:\n\nmu = 1.0\nsigma = 1.0\n\nlog_normal = torch.distributions.LogNormal(mu, sigma)\n\n\nlog_normal.support\n\nGreaterThan(lower_bound=0.0)",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html#uniform-distribution",
    "href": "notebooks/pdf-continuous.html#uniform-distribution",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "6. Uniform Distribution",
    "text": "6. Uniform Distribution\nThe Uniform distribution assigns equal probability density to all values within a specified interval, making it fundamental for random number generation and modeling complete uncertainty.\n\nMathematical Definition\nLet \\(X \\sim \\text{Uniform}(a, b)\\) where \\(a &lt; b\\). The PDF is:\n\\[f_X(x) = \\begin{cases}\n\\frac{1}{b-a} & \\text{if } a \\leq x \\leq b \\\\\n0 & \\text{otherwise}\n\\end{cases}\\]\n\n\nProperties\n\nMean: \\(E[X] = \\frac{a+b}{2}\\)\nVariance: \\(\\text{Var}(X) = \\frac{(b-a)^2}{12}\\)\nSupport: \\([a, b]\\)\nMaximum entropy among distributions with bounded support\n\n\n\nApplications\n\nRandom number generation\nMonte Carlo simulations\nModeling complete uncertainty\nQuality control (tolerance intervals)\nSignal processing (quantization error)\n\n\n\nPractical Application: Quantization Error Modeling\nDigital signal processing often involves quantizing continuous signals into discrete levels, introducing quantization error that can be modeled as uniform:\n\nlog_normal.mean\n\ntensor(4.4817)\n\n\n\nlog_normal.mode\n\ntensor(1.)\n\n\n\nlog_normal.variance\n\ntensor(34.5126)\n\n\n\n\nx = torch.linspace(-10, 10, 1000)\nx_non_neg_mask = x &gt; 0.001\n\ny = torch.zeros_like(x)\ny[x_non_neg_mask] = log_normal.log_prob(x[x_non_neg_mask]).exp()\nplt.plot(x, y, label=\"PDF LogNormal(1, 1)\")\n\nnormal = torch.distributions.Normal(mu, sigma)\nplt.plot(x, normal.log_prob(x).exp(), label=\"PDF Normal(1, 1)\")\n\nplt.legend()\n\n\n\n\n\n\n\n\n\nApplications\nSee: https://en.wikipedia.org/wiki/Log-normal_distribution\nSee https://chess.stackexchange.com/questions/2506/what-is-the-average-length-of-a-game-of-chess/4899#4899\n\nimport kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"datasnaek/chess\")\n\nprint(\"Path to dataset files:\", path)\n\nPath to dataset files: /Users/nipun/.cache/kagglehub/datasets/datasnaek/chess/versions/1\n\n\n\nimport os\ndf = pd.read_csv(os.path.join(path, \"games.csv\"))\n\n\ndf.head()\n\n\n\n\n\n\n\n\nid\nrated\ncreated_at\nlast_move_at\nturns\nvictory_status\nwinner\nincrement_code\nwhite_id\nwhite_rating\nblack_id\nblack_rating\nmoves\nopening_eco\nopening_name\nopening_ply\n\n\n\n\n0\nTZJHLljE\nFalse\n1.504210e+12\n1.504210e+12\n13\noutoftime\nwhite\n15+2\nbourgris\n1500\na-00\n1191\nd4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5...\nD10\nSlav Defense: Exchange Variation\n5\n\n\n1\nl1NXvwaE\nTrue\n1.504130e+12\n1.504130e+12\n16\nresign\nblack\n5+10\na-00\n1322\nskinnerua\n1261\nd4 Nc6 e4 e5 f4 f6 dxe5 fxe5 fxe5 Nxe5 Qd4 Nc6...\nB00\nNimzowitsch Defense: Kennedy Variation\n4\n\n\n2\nmIICvQHh\nTrue\n1.504130e+12\n1.504130e+12\n61\nmate\nwhite\n5+10\nischia\n1496\na-00\n1500\ne4 e5 d3 d6 Be3 c6 Be2 b5 Nd2 a5 a4 c5 axb5 Nc...\nC20\nKing's Pawn Game: Leonardis Variation\n3\n\n\n3\nkWKvrqYL\nTrue\n1.504110e+12\n1.504110e+12\n61\nmate\nwhite\n20+0\ndaniamurashov\n1439\nadivanov2009\n1454\nd4 d5 Nf3 Bf5 Nc3 Nf6 Bf4 Ng4 e3 Nc6 Be2 Qd7 O...\nD02\nQueen's Pawn Game: Zukertort Variation\n3\n\n\n4\n9tXo1AUZ\nTrue\n1.504030e+12\n1.504030e+12\n95\nmate\nwhite\n30+3\nnik221107\n1523\nadivanov2009\n1469\ne4 e5 Nf3 d6 d4 Nc6 d5 Nb4 a3 Na6 Nc3 Be7 b4 N...\nC41\nPhilidor Defense\n5\n\n\n\n\n\n\n\n\n# Distribution of the number of turns in the games\ndf[\"turns\"].plot(kind='hist', bins=50)\n\n\n\n\n\n\n\n\n\n# Logarithm of the number of turns\ndf[\"turns\"].apply(np.log).plot(kind='hist', bins=50)\n\n# Log of turns seems to be normally distributed\n\n\n\n\n\n\n\n\n\nmu, sigma = df[\"turns\"].apply(np.log).mean(), df[\"turns\"].apply(np.log1p).std()\nprint(mu, sigma)\n\n3.9070571274448245 0.6822030192719669\n\n\n\n# Plot PDF of the fitted log-normal distribution\n\nx = torch.linspace(0.001, 300, 1000)\n\nwith torch.no_grad():\n    log_normal = torch.distributions.LogNormal(mu, sigma)\ny = log_normal.log_prob(x).exp()\n\nplt.plot(x, y, label=\"Fitted PDF\")\nplt.hist(df[\"turns\"], bins=50, density=True, alpha=0.5, label=\"KDE\")\nplt.legend()",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html#uniform-distribution-1",
    "href": "notebooks/pdf-continuous.html#uniform-distribution-1",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nLet \\(X\\) be a random variable that follows a uniform distribution on the interval \\([a, b]\\). The probability density function (PDF) of \\(X\\) is given by:\n$$ f_X(x) =\n\\[\\begin{cases}\n\\frac{1}{b-a} & \\text{if } x \\in [a, b], \\\\\n0 & \\text{otherwise}.\n\\end{cases}\\]\n$$\nWe can say that \\(X \\sim \\text{Uniform}(a, b)\\).\n\na = 0.0\nb = 2.0\ndist = torch.distributions.Uniform(a, b)\n\n\ndist.support\n\nInterval(lower_bound=0.0, upper_bound=2.0)\n\n\n\ndist.high, dist.low\n\n(tensor(2.), tensor(0.))\n\n\n\ndist.mean\n\ntensor(1.)",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html#beta-distribution",
    "href": "notebooks/pdf-continuous.html#beta-distribution",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "7. Beta Distribution",
    "text": "7. Beta Distribution\nThe Beta distribution is defined on the interval [0,1] and is extremely versatile for modeling proportions, probabilities, and bounded quantities.\n\nMathematical Definition\nLet \\(X \\sim \\text{Beta}(\\alpha, \\beta)\\) where \\(\\alpha, \\beta &gt; 0\\). The PDF is:\n\\[f_X(x) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha-1} (1-x)^{\\beta-1} \\quad \\text{for } 0 &lt; x &lt; 1\\]\n\n\nProperties\n\nMean: \\(E[X] = \\frac{\\alpha}{\\alpha + \\beta}\\)\nVariance: \\(\\text{Var}(X) = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\)\nSupport: \\((0, 1)\\)\nConjugate prior for Bernoulli/Binomial likelihood\n\n\n\nShape Flexibility\nThe Beta distribution can take many shapes depending on its parameters:\n\ndist.variance\n\ntensor(0.3333)\n\n\n\nx_range = torch.linspace(-1, 3, 1000)\ntry:\n    y = dist.log_prob(x_range).exp()\nexcept Exception as e:\n    print(e)\n\nExpected value argument (Tensor of shape (1000,)) to be within the support (Interval(lower_bound=0.0, upper_bound=2.0)) of the distribution Uniform(low: 0.0, high: 2.0), but found invalid values:\ntensor([-1.0000e+00, -9.9600e-01, -9.9199e-01, -9.8799e-01, -9.8398e-01,\n        -9.7998e-01, -9.7598e-01, -9.7197e-01, -9.6797e-01, -9.6396e-01,\n        -9.5996e-01, -9.5596e-01, -9.5195e-01, -9.4795e-01, -9.4394e-01,\n        -9.3994e-01, -9.3594e-01, -9.3193e-01, -9.2793e-01, -9.2392e-01,\n        -9.1992e-01, -9.1592e-01, -9.1191e-01, -9.0791e-01, -9.0390e-01,\n        -8.9990e-01, -8.9590e-01, -8.9189e-01, -8.8789e-01, -8.8388e-01,\n        -8.7988e-01, -8.7588e-01, -8.7187e-01, -8.6787e-01, -8.6386e-01,\n        -8.5986e-01, -8.5586e-01, -8.5185e-01, -8.4785e-01, -8.4384e-01,\n        -8.3984e-01, -8.3584e-01, -8.3183e-01, -8.2783e-01, -8.2382e-01,\n        -8.1982e-01, -8.1582e-01, -8.1181e-01, -8.0781e-01, -8.0380e-01,\n        -7.9980e-01, -7.9580e-01, -7.9179e-01, -7.8779e-01, -7.8378e-01,\n        -7.7978e-01, -7.7578e-01, -7.7177e-01, -7.6777e-01, -7.6376e-01,\n        -7.5976e-01, -7.5576e-01, -7.5175e-01, -7.4775e-01, -7.4374e-01,\n        -7.3974e-01, -7.3574e-01, -7.3173e-01, -7.2773e-01, -7.2372e-01,\n        -7.1972e-01, -7.1572e-01, -7.1171e-01, -7.0771e-01, -7.0370e-01,\n        -6.9970e-01, -6.9570e-01, -6.9169e-01, -6.8769e-01, -6.8368e-01,\n        -6.7968e-01, -6.7568e-01, -6.7167e-01, -6.6767e-01, -6.6366e-01,\n        -6.5966e-01, -6.5566e-01, -6.5165e-01, -6.4765e-01, -6.4364e-01,\n        -6.3964e-01, -6.3564e-01, -6.3163e-01, -6.2763e-01, -6.2362e-01,\n        -6.1962e-01, -6.1562e-01, -6.1161e-01, -6.0761e-01, -6.0360e-01,\n        -5.9960e-01, -5.9560e-01, -5.9159e-01, -5.8759e-01, -5.8358e-01,\n        -5.7958e-01, -5.7558e-01, -5.7157e-01, -5.6757e-01, -5.6356e-01,\n        -5.5956e-01, -5.5556e-01, -5.5155e-01, -5.4755e-01, -5.4354e-01,\n        -5.3954e-01, -5.3554e-01, -5.3153e-01, -5.2753e-01, -5.2352e-01,\n        -5.1952e-01, -5.1552e-01, -5.1151e-01, -5.0751e-01, -5.0350e-01,\n        -4.9950e-01, -4.9550e-01, -4.9149e-01, -4.8749e-01, -4.8348e-01,\n        -4.7948e-01, -4.7548e-01, -4.7147e-01, -4.6747e-01, -4.6346e-01,\n        -4.5946e-01, -4.5546e-01, -4.5145e-01, -4.4745e-01, -4.4344e-01,\n        -4.3944e-01, -4.3544e-01, -4.3143e-01, -4.2743e-01, -4.2342e-01,\n        -4.1942e-01, -4.1542e-01, -4.1141e-01, -4.0741e-01, -4.0340e-01,\n        -3.9940e-01, -3.9540e-01, -3.9139e-01, -3.8739e-01, -3.8338e-01,\n        -3.7938e-01, -3.7538e-01, -3.7137e-01, -3.6737e-01, -3.6336e-01,\n        -3.5936e-01, -3.5536e-01, -3.5135e-01, -3.4735e-01, -3.4334e-01,\n        -3.3934e-01, -3.3534e-01, -3.3133e-01, -3.2733e-01, -3.2332e-01,\n        -3.1932e-01, -3.1532e-01, -3.1131e-01, -3.0731e-01, -3.0330e-01,\n        -2.9930e-01, -2.9530e-01, -2.9129e-01, -2.8729e-01, -2.8328e-01,\n        -2.7928e-01, -2.7528e-01, -2.7127e-01, -2.6727e-01, -2.6326e-01,\n        -2.5926e-01, -2.5526e-01, -2.5125e-01, -2.4725e-01, -2.4324e-01,\n        -2.3924e-01, -2.3524e-01, -2.3123e-01, -2.2723e-01, -2.2322e-01,\n        -2.1922e-01, -2.1522e-01, -2.1121e-01, -2.0721e-01, -2.0320e-01,\n        -1.9920e-01, -1.9520e-01, -1.9119e-01, -1.8719e-01, -1.8318e-01,\n        -1.7918e-01, -1.7518e-01, -1.7117e-01, -1.6717e-01, -1.6316e-01,\n        -1.5916e-01, -1.5516e-01, -1.5115e-01, -1.4715e-01, -1.4314e-01,\n        -1.3914e-01, -1.3514e-01, -1.3113e-01, -1.2713e-01, -1.2312e-01,\n        -1.1912e-01, -1.1512e-01, -1.1111e-01, -1.0711e-01, -1.0310e-01,\n        -9.9099e-02, -9.5095e-02, -9.1091e-02, -8.7087e-02, -8.3083e-02,\n        -7.9079e-02, -7.5075e-02, -7.1071e-02, -6.7067e-02, -6.3063e-02,\n        -5.9059e-02, -5.5055e-02, -5.1051e-02, -4.7047e-02, -4.3043e-02,\n        -3.9039e-02, -3.5035e-02, -3.1031e-02, -2.7027e-02, -2.3023e-02,\n        -1.9019e-02, -1.5015e-02, -1.1011e-02, -7.0070e-03, -3.0030e-03,\n         1.0010e-03,  5.0050e-03,  9.0090e-03,  1.3013e-02,  1.7017e-02,\n         2.1021e-02,  2.5025e-02,  2.9029e-02,  3.3033e-02,  3.7037e-02,\n         4.1041e-02,  4.5045e-02,  4.9049e-02,  5.3053e-02,  5.7057e-02,\n         6.1061e-02,  6.5065e-02,  6.9069e-02,  7.3073e-02,  7.7077e-02,\n         8.1081e-02,  8.5085e-02,  8.9089e-02,  9.3093e-02,  9.7097e-02,\n         1.0110e-01,  1.0511e-01,  1.0911e-01,  1.1311e-01,  1.1712e-01,\n         1.2112e-01,  1.2513e-01,  1.2913e-01,  1.3313e-01,  1.3714e-01,\n         1.4114e-01,  1.4515e-01,  1.4915e-01,  1.5315e-01,  1.5716e-01,\n         1.6116e-01,  1.6517e-01,  1.6917e-01,  1.7317e-01,  1.7718e-01,\n         1.8118e-01,  1.8519e-01,  1.8919e-01,  1.9319e-01,  1.9720e-01,\n         2.0120e-01,  2.0521e-01,  2.0921e-01,  2.1321e-01,  2.1722e-01,\n         2.2122e-01,  2.2523e-01,  2.2923e-01,  2.3323e-01,  2.3724e-01,\n         2.4124e-01,  2.4525e-01,  2.4925e-01,  2.5325e-01,  2.5726e-01,\n         2.6126e-01,  2.6527e-01,  2.6927e-01,  2.7327e-01,  2.7728e-01,\n         2.8128e-01,  2.8529e-01,  2.8929e-01,  2.9329e-01,  2.9730e-01,\n         3.0130e-01,  3.0531e-01,  3.0931e-01,  3.1331e-01,  3.1732e-01,\n         3.2132e-01,  3.2533e-01,  3.2933e-01,  3.3333e-01,  3.3734e-01,\n         3.4134e-01,  3.4535e-01,  3.4935e-01,  3.5335e-01,  3.5736e-01,\n         3.6136e-01,  3.6537e-01,  3.6937e-01,  3.7337e-01,  3.7738e-01,\n         3.8138e-01,  3.8539e-01,  3.8939e-01,  3.9339e-01,  3.9740e-01,\n         4.0140e-01,  4.0541e-01,  4.0941e-01,  4.1341e-01,  4.1742e-01,\n         4.2142e-01,  4.2543e-01,  4.2943e-01,  4.3343e-01,  4.3744e-01,\n         4.4144e-01,  4.4545e-01,  4.4945e-01,  4.5345e-01,  4.5746e-01,\n         4.6146e-01,  4.6547e-01,  4.6947e-01,  4.7347e-01,  4.7748e-01,\n         4.8148e-01,  4.8549e-01,  4.8949e-01,  4.9349e-01,  4.9750e-01,\n         5.0150e-01,  5.0551e-01,  5.0951e-01,  5.1351e-01,  5.1752e-01,\n         5.2152e-01,  5.2553e-01,  5.2953e-01,  5.3353e-01,  5.3754e-01,\n         5.4154e-01,  5.4555e-01,  5.4955e-01,  5.5355e-01,  5.5756e-01,\n         5.6156e-01,  5.6557e-01,  5.6957e-01,  5.7357e-01,  5.7758e-01,\n         5.8158e-01,  5.8559e-01,  5.8959e-01,  5.9359e-01,  5.9760e-01,\n         6.0160e-01,  6.0561e-01,  6.0961e-01,  6.1361e-01,  6.1762e-01,\n         6.2162e-01,  6.2563e-01,  6.2963e-01,  6.3363e-01,  6.3764e-01,\n         6.4164e-01,  6.4565e-01,  6.4965e-01,  6.5365e-01,  6.5766e-01,\n         6.6166e-01,  6.6567e-01,  6.6967e-01,  6.7367e-01,  6.7768e-01,\n         6.8168e-01,  6.8569e-01,  6.8969e-01,  6.9369e-01,  6.9770e-01,\n         7.0170e-01,  7.0571e-01,  7.0971e-01,  7.1371e-01,  7.1772e-01,\n         7.2172e-01,  7.2573e-01,  7.2973e-01,  7.3373e-01,  7.3774e-01,\n         7.4174e-01,  7.4575e-01,  7.4975e-01,  7.5375e-01,  7.5776e-01,\n         7.6176e-01,  7.6577e-01,  7.6977e-01,  7.7377e-01,  7.7778e-01,\n         7.8178e-01,  7.8579e-01,  7.8979e-01,  7.9379e-01,  7.9780e-01,\n         8.0180e-01,  8.0581e-01,  8.0981e-01,  8.1381e-01,  8.1782e-01,\n         8.2182e-01,  8.2583e-01,  8.2983e-01,  8.3383e-01,  8.3784e-01,\n         8.4184e-01,  8.4585e-01,  8.4985e-01,  8.5385e-01,  8.5786e-01,\n         8.6186e-01,  8.6587e-01,  8.6987e-01,  8.7387e-01,  8.7788e-01,\n         8.8188e-01,  8.8589e-01,  8.8989e-01,  8.9389e-01,  8.9790e-01,\n         9.0190e-01,  9.0591e-01,  9.0991e-01,  9.1391e-01,  9.1792e-01,\n         9.2192e-01,  9.2593e-01,  9.2993e-01,  9.3393e-01,  9.3794e-01,\n         9.4194e-01,  9.4595e-01,  9.4995e-01,  9.5395e-01,  9.5796e-01,\n         9.6196e-01,  9.6597e-01,  9.6997e-01,  9.7397e-01,  9.7798e-01,\n         9.8198e-01,  9.8599e-01,  9.8999e-01,  9.9399e-01,  9.9800e-01,\n         1.0020e+00,  1.0060e+00,  1.0100e+00,  1.0140e+00,  1.0180e+00,\n         1.0220e+00,  1.0260e+00,  1.0300e+00,  1.0340e+00,  1.0380e+00,\n         1.0420e+00,  1.0460e+00,  1.0501e+00,  1.0541e+00,  1.0581e+00,\n         1.0621e+00,  1.0661e+00,  1.0701e+00,  1.0741e+00,  1.0781e+00,\n         1.0821e+00,  1.0861e+00,  1.0901e+00,  1.0941e+00,  1.0981e+00,\n         1.1021e+00,  1.1061e+00,  1.1101e+00,  1.1141e+00,  1.1181e+00,\n         1.1221e+00,  1.1261e+00,  1.1301e+00,  1.1341e+00,  1.1381e+00,\n         1.1421e+00,  1.1461e+00,  1.1502e+00,  1.1542e+00,  1.1582e+00,\n         1.1622e+00,  1.1662e+00,  1.1702e+00,  1.1742e+00,  1.1782e+00,\n         1.1822e+00,  1.1862e+00,  1.1902e+00,  1.1942e+00,  1.1982e+00,\n         1.2022e+00,  1.2062e+00,  1.2102e+00,  1.2142e+00,  1.2182e+00,\n         1.2222e+00,  1.2262e+00,  1.2302e+00,  1.2342e+00,  1.2382e+00,\n         1.2422e+00,  1.2462e+00,  1.2503e+00,  1.2543e+00,  1.2583e+00,\n         1.2623e+00,  1.2663e+00,  1.2703e+00,  1.2743e+00,  1.2783e+00,\n         1.2823e+00,  1.2863e+00,  1.2903e+00,  1.2943e+00,  1.2983e+00,\n         1.3023e+00,  1.3063e+00,  1.3103e+00,  1.3143e+00,  1.3183e+00,\n         1.3223e+00,  1.3263e+00,  1.3303e+00,  1.3343e+00,  1.3383e+00,\n         1.3423e+00,  1.3463e+00,  1.3504e+00,  1.3544e+00,  1.3584e+00,\n         1.3624e+00,  1.3664e+00,  1.3704e+00,  1.3744e+00,  1.3784e+00,\n         1.3824e+00,  1.3864e+00,  1.3904e+00,  1.3944e+00,  1.3984e+00,\n         1.4024e+00,  1.4064e+00,  1.4104e+00,  1.4144e+00,  1.4184e+00,\n         1.4224e+00,  1.4264e+00,  1.4304e+00,  1.4344e+00,  1.4384e+00,\n         1.4424e+00,  1.4464e+00,  1.4505e+00,  1.4545e+00,  1.4585e+00,\n         1.4625e+00,  1.4665e+00,  1.4705e+00,  1.4745e+00,  1.4785e+00,\n         1.4825e+00,  1.4865e+00,  1.4905e+00,  1.4945e+00,  1.4985e+00,\n         1.5025e+00,  1.5065e+00,  1.5105e+00,  1.5145e+00,  1.5185e+00,\n         1.5225e+00,  1.5265e+00,  1.5305e+00,  1.5345e+00,  1.5385e+00,\n         1.5425e+00,  1.5465e+00,  1.5506e+00,  1.5546e+00,  1.5586e+00,\n         1.5626e+00,  1.5666e+00,  1.5706e+00,  1.5746e+00,  1.5786e+00,\n         1.5826e+00,  1.5866e+00,  1.5906e+00,  1.5946e+00,  1.5986e+00,\n         1.6026e+00,  1.6066e+00,  1.6106e+00,  1.6146e+00,  1.6186e+00,\n         1.6226e+00,  1.6266e+00,  1.6306e+00,  1.6346e+00,  1.6386e+00,\n         1.6426e+00,  1.6466e+00,  1.6507e+00,  1.6547e+00,  1.6587e+00,\n         1.6627e+00,  1.6667e+00,  1.6707e+00,  1.6747e+00,  1.6787e+00,\n         1.6827e+00,  1.6867e+00,  1.6907e+00,  1.6947e+00,  1.6987e+00,\n         1.7027e+00,  1.7067e+00,  1.7107e+00,  1.7147e+00,  1.7187e+00,\n         1.7227e+00,  1.7267e+00,  1.7307e+00,  1.7347e+00,  1.7387e+00,\n         1.7427e+00,  1.7467e+00,  1.7508e+00,  1.7548e+00,  1.7588e+00,\n         1.7628e+00,  1.7668e+00,  1.7708e+00,  1.7748e+00,  1.7788e+00,\n         1.7828e+00,  1.7868e+00,  1.7908e+00,  1.7948e+00,  1.7988e+00,\n         1.8028e+00,  1.8068e+00,  1.8108e+00,  1.8148e+00,  1.8188e+00,\n         1.8228e+00,  1.8268e+00,  1.8308e+00,  1.8348e+00,  1.8388e+00,\n         1.8428e+00,  1.8468e+00,  1.8509e+00,  1.8549e+00,  1.8589e+00,\n         1.8629e+00,  1.8669e+00,  1.8709e+00,  1.8749e+00,  1.8789e+00,\n         1.8829e+00,  1.8869e+00,  1.8909e+00,  1.8949e+00,  1.8989e+00,\n         1.9029e+00,  1.9069e+00,  1.9109e+00,  1.9149e+00,  1.9189e+00,\n         1.9229e+00,  1.9269e+00,  1.9309e+00,  1.9349e+00,  1.9389e+00,\n         1.9429e+00,  1.9469e+00,  1.9510e+00,  1.9550e+00,  1.9590e+00,\n         1.9630e+00,  1.9670e+00,  1.9710e+00,  1.9750e+00,  1.9790e+00,\n         1.9830e+00,  1.9870e+00,  1.9910e+00,  1.9950e+00,  1.9990e+00,\n         2.0030e+00,  2.0070e+00,  2.0110e+00,  2.0150e+00,  2.0190e+00,\n         2.0230e+00,  2.0270e+00,  2.0310e+00,  2.0350e+00,  2.0390e+00,\n         2.0430e+00,  2.0470e+00,  2.0511e+00,  2.0551e+00,  2.0591e+00,\n         2.0631e+00,  2.0671e+00,  2.0711e+00,  2.0751e+00,  2.0791e+00,\n         2.0831e+00,  2.0871e+00,  2.0911e+00,  2.0951e+00,  2.0991e+00,\n         2.1031e+00,  2.1071e+00,  2.1111e+00,  2.1151e+00,  2.1191e+00,\n         2.1231e+00,  2.1271e+00,  2.1311e+00,  2.1351e+00,  2.1391e+00,\n         2.1431e+00,  2.1471e+00,  2.1512e+00,  2.1552e+00,  2.1592e+00,\n         2.1632e+00,  2.1672e+00,  2.1712e+00,  2.1752e+00,  2.1792e+00,\n         2.1832e+00,  2.1872e+00,  2.1912e+00,  2.1952e+00,  2.1992e+00,\n         2.2032e+00,  2.2072e+00,  2.2112e+00,  2.2152e+00,  2.2192e+00,\n         2.2232e+00,  2.2272e+00,  2.2312e+00,  2.2352e+00,  2.2392e+00,\n         2.2432e+00,  2.2472e+00,  2.2513e+00,  2.2553e+00,  2.2593e+00,\n         2.2633e+00,  2.2673e+00,  2.2713e+00,  2.2753e+00,  2.2793e+00,\n         2.2833e+00,  2.2873e+00,  2.2913e+00,  2.2953e+00,  2.2993e+00,\n         2.3033e+00,  2.3073e+00,  2.3113e+00,  2.3153e+00,  2.3193e+00,\n         2.3233e+00,  2.3273e+00,  2.3313e+00,  2.3353e+00,  2.3393e+00,\n         2.3433e+00,  2.3473e+00,  2.3514e+00,  2.3554e+00,  2.3594e+00,\n         2.3634e+00,  2.3674e+00,  2.3714e+00,  2.3754e+00,  2.3794e+00,\n         2.3834e+00,  2.3874e+00,  2.3914e+00,  2.3954e+00,  2.3994e+00,\n         2.4034e+00,  2.4074e+00,  2.4114e+00,  2.4154e+00,  2.4194e+00,\n         2.4234e+00,  2.4274e+00,  2.4314e+00,  2.4354e+00,  2.4394e+00,\n         2.4434e+00,  2.4474e+00,  2.4515e+00,  2.4555e+00,  2.4595e+00,\n         2.4635e+00,  2.4675e+00,  2.4715e+00,  2.4755e+00,  2.4795e+00,\n         2.4835e+00,  2.4875e+00,  2.4915e+00,  2.4955e+00,  2.4995e+00,\n         2.5035e+00,  2.5075e+00,  2.5115e+00,  2.5155e+00,  2.5195e+00,\n         2.5235e+00,  2.5275e+00,  2.5315e+00,  2.5355e+00,  2.5395e+00,\n         2.5435e+00,  2.5475e+00,  2.5516e+00,  2.5556e+00,  2.5596e+00,\n         2.5636e+00,  2.5676e+00,  2.5716e+00,  2.5756e+00,  2.5796e+00,\n         2.5836e+00,  2.5876e+00,  2.5916e+00,  2.5956e+00,  2.5996e+00,\n         2.6036e+00,  2.6076e+00,  2.6116e+00,  2.6156e+00,  2.6196e+00,\n         2.6236e+00,  2.6276e+00,  2.6316e+00,  2.6356e+00,  2.6396e+00,\n         2.6436e+00,  2.6476e+00,  2.6517e+00,  2.6557e+00,  2.6597e+00,\n         2.6637e+00,  2.6677e+00,  2.6717e+00,  2.6757e+00,  2.6797e+00,\n         2.6837e+00,  2.6877e+00,  2.6917e+00,  2.6957e+00,  2.6997e+00,\n         2.7037e+00,  2.7077e+00,  2.7117e+00,  2.7157e+00,  2.7197e+00,\n         2.7237e+00,  2.7277e+00,  2.7317e+00,  2.7357e+00,  2.7397e+00,\n         2.7437e+00,  2.7477e+00,  2.7518e+00,  2.7558e+00,  2.7598e+00,\n         2.7638e+00,  2.7678e+00,  2.7718e+00,  2.7758e+00,  2.7798e+00,\n         2.7838e+00,  2.7878e+00,  2.7918e+00,  2.7958e+00,  2.7998e+00,\n         2.8038e+00,  2.8078e+00,  2.8118e+00,  2.8158e+00,  2.8198e+00,\n         2.8238e+00,  2.8278e+00,  2.8318e+00,  2.8358e+00,  2.8398e+00,\n         2.8438e+00,  2.8478e+00,  2.8519e+00,  2.8559e+00,  2.8599e+00,\n         2.8639e+00,  2.8679e+00,  2.8719e+00,  2.8759e+00,  2.8799e+00,\n         2.8839e+00,  2.8879e+00,  2.8919e+00,  2.8959e+00,  2.8999e+00,\n         2.9039e+00,  2.9079e+00,  2.9119e+00,  2.9159e+00,  2.9199e+00,\n         2.9239e+00,  2.9279e+00,  2.9319e+00,  2.9359e+00,  2.9399e+00,\n         2.9439e+00,  2.9479e+00,  2.9520e+00,  2.9560e+00,  2.9600e+00,\n         2.9640e+00,  2.9680e+00,  2.9720e+00,  2.9760e+00,  2.9800e+00,\n         2.9840e+00,  2.9880e+00,  2.9920e+00,  2.9960e+00,  3.0000e+00])\n\n\n\nx_range_mask = (x_range &gt;= a) & (x_range &lt;= b)\ny = torch.zeros_like(x_range)\ny[x_range_mask] = dist.log_prob(x_range[x_range_mask]).exp()\n\nplt.plot(x_range, y)\n\n\n\n\n\n\n\n\n\n\nModeling quantisation error using uniform distribution\nQuantization error is the error that arises when representing continuous signals with discrete signals.\nNOTE: I am using a simplified convention here. Study DSP for a rigorous treatment (with N and T used in the equations).\nLet original signal represented in computer be \\(y(t)\\). We will quantize the signal to \\(x(t)\\). The quantization error is given by:\n\\[\ne(t) = y(t) - x(t).\n\\]\nWe will quantize the signal to \\(x(t)\\) such that \\(x(t)\\) can take on \\(N\\) discrete values. Let \\(\\Delta\\) be the quantization step size. The quantization error is given by:\nIf the quantization error is uniformly distributed between \\(-\\Delta/2\\) and \\(\\Delta/2\\), then we can model the quantization error as a uniform random variable.\n\\[\ne(t) \\sim \\text{Uniform}(-\\Delta/2, \\Delta/2).\n\\]",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html#exponential-distribution",
    "href": "notebooks/pdf-continuous.html#exponential-distribution",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "8. Exponential Distribution",
    "text": "8. Exponential Distribution\nThe Exponential distribution is the continuous analog of the geometric distribution, modeling the time between events in a Poisson process.\n\nMathematical Definition\nLet \\(X \\sim \\text{Exponential}(\\lambda)\\) where \\(\\lambda &gt; 0\\) is the rate parameter. The PDF is:\n\\[f_X(x) = \\begin{cases}\n\\lambda e^{-\\lambda x} & \\text{if } x \\geq 0 \\\\\n0 & \\text{otherwise}\n\\end{cases}\\]\n\n\nProperties\n\nMean: \\(E[X] = \\frac{1}{\\lambda}\\)\nVariance: \\(\\text{Var}(X) = \\frac{1}{\\lambda^2}\\)\nSupport: \\([0, \\infty)\\)\nMemoryless property: \\(P(X &gt; s+t | X &gt; s) = P(X &gt; t)\\)\nMaximum entropy among distributions with fixed mean on \\([0, \\infty)\\)\n\n\n\nApplications\n\nReliability engineering (time to failure)\nQueueing theory (service times)\nRadioactive decay\nPhone call durations\nTime between arrivals\n\n\n\nKey Insight: Memoryless Property\nThe exponential distribution is the only continuous distribution with the memoryless property, making it ideal for modeling “fresh start” scenarios.\n\n\n\nx_t = torch.linspace(-2, 8, 5000)\ny_t = torch.sin(x_t)\nplt.plot(x_t, y_t, label=\"y_t\")\n\nmax_y_t = y_t.max()\nmin_y_t = y_t.min()\n\n# Divide the range of y_t into N (=10) equal parts\nN = 10\ny_bins = torch.linspace(min_y_t, max_y_t, N+1)\n\n# Draw the N levels as horizontal lines\nfor y_level in y_bins:\n    plt.axhline(y_level, color='gray', linestyle='--')\n    plt.text(3, y_level+.01, f\"{y_level:.2f}\")\n    \ndelta = (max_y_t - min_y_t)/N\nprint(delta)\n\n\ntensor(0.2000)\n\n\n\n\n\n\n\n\n\n\n# For x = 3, find the bin in which y_t falls\ny_t_x_3 = torch.sin(torch.tensor(3))\nprint(y_t_x_3)\n\n\nplt.plot(x_t, y_t, label=\"y_t\")\nplt.axvline(3, color='red', linestyle='--')\nplt.axhline(y_t_x_3, color='red', linestyle='--')\n\n# Draw the N levels as horizontal lines\nfor y_level in y_bins:\n    plt.axhline(y_level, color='gray', linestyle='--', alpha=0.2)\n    plt.text(3, y_level+.01, f\"{y_level:.2f}\")\n    \n# Find the bin in which y_t falls\nbin_idx = torch.searchsorted(y_bins, y_t_x_3)\nplt.axhline(y_bins[bin_idx], color='green', linestyle='--', label=\"Closest level\")\nplt.legend()\n\ntensor(0.1411)\n\n\n\n\n\n\n\n\n\n\ny_t.shape, y_bins.shape\n\n(torch.Size([5000]), torch.Size([11]))\n\n\n\n# Find closest bin for each y_t\nbins = (y_t - y_bins.reshape(-1, 1)).abs().argmin(dim=0)\n\ny_binned = y_bins[bins]\nplt.plot(x_t, y_t, label=\"y_t\")\nplt.plot(x_t, y_binned, label=\"y_binned\")\nplt.legend()\n\n\n\n\n\n\n\n\n\nplt.plot(y_t - y_binned)",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html#summary-and-practical-guidance",
    "href": "notebooks/pdf-continuous.html#summary-and-practical-guidance",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "Summary and Practical Guidance",
    "text": "Summary and Practical Guidance\nIn this comprehensive notebook, we’ve explored the rich world of continuous probability distributions. Each distribution has its unique characteristics and applications:\n\nDistribution Selection Guide\n\n\n\n\n\n\n\n\nData Characteristics\nRecommended Distribution\nKey Indicators\n\n\n\n\nSymmetric, bell-shaped\nNormal\nCentral Limit Theorem applies\n\n\nPositive, right-skewed\nLog-Normal, Gamma\nMultiplicative processes\n\n\nBounded between 0 and 1\nBeta\nProportions, probabilities\n\n\nConstant over interval\nUniform\nComplete uncertainty\n\n\nWaiting times, reliability\nExponential\nMemoryless events\n\n\nHeavy tails, robust\nLaplace\nOutlier resistance needed\n\n\nPositive measurements\nHalf-Normal\nMagnitude of errors\n\n\n\n\n\nKey Mathematical Insights\n\nPDF ≠ Probability: Density functions can exceed 1\nIntegration for Probabilities: \\(P(a \\leq X \\leq b) = \\int_a^b f(x)dx\\)\nParameter Interpretation: Location, scale, and shape parameters\nSupport Matters: Domain restrictions affect model choice\nMaximum Likelihood: Optimal parameter estimation method\n\n\n\nPractical Implementation Tips\n\nAlways validate assumptions with data visualization\nUse log-space for numerical stability with small probabilities\nConsider parameter constraints when fitting distributions\nLeverage PyTorch’s automatic differentiation for parameter optimization\nCompare multiple distributions using information criteria (AIC, BIC)\n\n\n\nAdvanced Topics for Further Study\n\nMixture distributions for complex, multi-modal data\nTruncated distributions for bounded domains\nTransformation methods for derived distributions\nCopulas for modeling dependence structures\nNon-parametric methods when distribution assumptions fail\n\n\n\nMachine Learning Connections\nContinuous distributions are fundamental to: - Variational Autoencoders (VAEs) for latent variable modeling - Generative Adversarial Networks (GANs) for data generation - Bayesian Neural Networks for uncertainty quantification - Gaussian Processes for non-parametric regression - Normalizing Flows for flexible density modeling\n\n\nReal-World Impact\nUnderstanding continuous distributions enables: - Risk assessment in finance and insurance - Quality control in manufacturing - Signal processing and communications - Epidemiological modeling and public health - Climate science and environmental monitoring\nThe journey through continuous distributions reveals the mathematical beauty underlying much of our uncertain world. These tools provide the foundation for sophisticated statistical modeling and machine learning applications that impact every aspect of modern data science!\n\n_ = plt.hist(y_t - y_binned, density=True)\nplt.xlabel(\"Error in binning\")\nplt.ylabel(\"Density\")\n\ntheoretical_uniform = torch.distributions.Uniform(-delta/2, delta/2)\nx = torch.linspace(-0.1, 0.1, 1000)\nx_mask = (x &gt;= -delta/2) & (x &lt;= delta/2)\ny = torch.zeros_like(x)\ny[x_mask] = theoretical_uniform.log_prob(x[x_mask]).exp()\nplt.plot(x, y, label=\"Theoretical PDF\")\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nAmount of bits saved\nOriginally, each sample of the signal was represented using \\(B=32\\) bits. After quantization, each sample is represented using \\(B_q = \\log_2(10)\\) bits. The amount of bits saved is given by:\n\\[\n(B - B_q) \\times \\text{number of samples}.\n\\]\n\nfrom pydub import AudioSegment\nimport numpy as np\n\n# Load MP3\naudio = AudioSegment.from_mp3(\"vlog-music.mp3\")\n\n# Convert to NumPy array\nsamples = np.array(audio.get_array_of_samples(), dtype=np.float32)\nsample_rate = audio.frame_rate\n\nprint(f\"Samples shape: {samples.shape}\")\nprint(f\"Sample rate: {sample_rate}\")\n\nSamples shape: (5430528,)\nSample rate: 44100\n\n\n\nfrom IPython.display import Audio\nAudio(\"vlog-music.mp3\")\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nplt.plot(samples)\n\n\n\n\n\n\n\n\n\n# Plot 2nd second to 5th second\nfiltered_audio  = samples[sample_rate*2:sample_rate*5]\nfig, ax = plt.subplots(figsize=(20, 5))\n\nax.plot(filtered_audio)\n\n# Quantize to 10 levels\nmin_audio = filtered_audio.min()\nmax_audio = filtered_audio.max()\n\nN = 10\n\naudio_bins = torch.linspace(min_audio, max_audio, N+1)\n# Plotting audio bins\nfor audio_bin in audio_bins:\n    plt.axhline(audio_bin, color='gray', linestyle='--', alpha=0.5)\n    \n    \n\n\n\n\n\n\n\n\n\n# Quantize the audio \n\naudio_bins = np.linspace(min_audio, max_audio, N+1)\n\n# Find closest bin for each audio sample\nbins = np.abs(filtered_audio - audio_bins.reshape(-1, 1)).argmin(0)\n\naudio_binned = audio_bins[bins]\n\nfig, ax = plt.subplots(figsize=(20, 5))\nax.plot(filtered_audio, label=\"Original audio\", alpha=0.2)\nax.plot(audio_binned, label=\"Quantized audio\")\nax.legend()\n\n\n\n\n\n\n\n\n\nAudio(filtered_audio, rate=sample_rate)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n# Play the quantized audio\nAudio(audio_binned, rate=sample_rate)\n\n\n                \n                    \n                    Your browser does not support the audio element.",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/pdf-continuous.html#exponential-distribution-1",
    "href": "notebooks/pdf-continuous.html#exponential-distribution-1",
    "title": "Probability Density Functions and Continuous Distributions",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\nLet \\(X\\) be a random variable that follows an exponential distribution with rate parameter \\(\\lambda\\). The probability density function (PDF) of \\(X\\) is given by:\n$$ f_X(x) =\n\\[\\begin{cases}\n\\lambda \\exp(-\\lambda x) & \\text{if } x \\geq 0, \\\\\n0 & \\text{otherwise}.\n\n\\end{cases}\\]\n$$\nWe can say that \\(X \\sim \\text{Exponential}(\\lambda)\\).\n\nThe exponential distribution may be viewed as a continuous counterpart of the geometric distribution, which describes the number of Bernoulli trials necessary for a discrete process to change state. In contrast, the exponential distribution describes the time for a continuous process to change state.\n\n\nl = 5.0\ndist = torch.distributions.Exponential(l)\n\n\ndist.support\n\nGreaterThanEq(lower_bound=0.0)\n\n\n\ndist.rate\n\ntensor(5.)\n\n\n\ndist.mean\n\ntensor(0.2000)\n\n\n\ndist.mode\n\ntensor(0.)\n\n\n\n# Plotting the PDF\nx_range = torch.linspace(0.001, 10, 1000)\ny = dist.log_prob(x_range).exp()\nplt.plot(x_range, y)",
    "crumbs": [
      "Home",
      "Foundations",
      "Probability Density Functions and Continuous Distributions"
    ]
  },
  {
    "objectID": "notebooks/images-joint-distribution.html",
    "href": "notebooks/images-joint-distribution.html",
    "title": "Images and Joint Distributions",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nprint(np.__version__)\nimport torch \nimport torch.nn as nn\n\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n2.2.4",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Images and Joint Distributions"
    ]
  },
  {
    "objectID": "notebooks/images-joint-distribution.html#introduction",
    "href": "notebooks/images-joint-distribution.html#introduction",
    "title": "Images and Joint Distributions",
    "section": "Introduction",
    "text": "Introduction\nJoint distributions describe the probability behavior of multiple random variables simultaneously. In this notebook, we explore joint distributions through the lens of high-dimensional image data, demonstrating how techniques like Principal Component Analysis (PCA) help us understand and visualize joint probability structures in complex datasets.\nWhen dealing with image data, each pixel can be thought of as a random variable, and the entire image represents a realization from a high-dimensional joint distribution. Understanding these joint structures is crucial for:\n\nImage compression and denoising\nPattern recognition and classification\nDimensionality reduction for visualization\nFeature extraction for machine learning\nUnderstanding data dependencies and correlations",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Images and Joint Distributions"
    ]
  },
  {
    "objectID": "notebooks/images-joint-distribution.html#learning-objectives",
    "href": "notebooks/images-joint-distribution.html#learning-objectives",
    "title": "Images and Joint Distributions",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this notebook, you will be able to:\n\nUnderstand joint distributions in high-dimensional spaces\nApply PCA to discover the structure of joint distributions\nInterpret principal components as directions of maximum variance\nVisualize high-dimensional joint distributions in lower dimensions\nAnalyze the relationship between data correlation and joint distribution structure\nConnect dimensionality reduction to probabilistic concepts",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Images and Joint Distributions"
    ]
  },
  {
    "objectID": "notebooks/images-joint-distribution.html#theoretical-background",
    "href": "notebooks/images-joint-distribution.html#theoretical-background",
    "title": "Images and Joint Distributions",
    "section": "Theoretical Background",
    "text": "Theoretical Background\n\nJoint Distributions\nFor random variables \\(X_1, X_2, \\ldots, X_p\\), the joint distribution describes their collective probabilistic behavior. The joint probability density function (PDF) is:\n\\[f_{X_1,\\ldots,X_p}(x_1, \\ldots, x_p)\\]\nThis function gives the probability density at any point \\((x_1, \\ldots, x_p)\\) in the \\(p\\)-dimensional space.\n\n\nKey Properties:\n\nMarginal Distributions: \\(f_{X_i}(x_i) = \\int \\cdots \\int f_{X_1,\\ldots,X_p}(x_1, \\ldots, x_p) dx_1 \\cdots dx_{i-1} dx_{i+1} \\cdots dx_p\\)\nIndependence: Variables are independent if \\(f_{X_1,\\ldots,X_p}(x_1, \\ldots, x_p) = \\prod_{i=1}^p f_{X_i}(x_i)\\)\nCovariance Structure: \\(\\text{Cov}(X_i, X_j) = E[(X_i - \\mu_i)(X_j - \\mu_j)]\\)\n\n\n\nPrincipal Component Analysis (PCA) and Joint Distributions\nPCA finds the directions of maximum variance in a joint distribution. For a multivariate dataset with covariance matrix \\(\\Sigma\\):\n\nEigenvalue Decomposition: \\(\\Sigma = V\\Lambda V^T\\)\nPrincipal Components: Columns of \\(V\\) (eigenvectors)\nExplained Variance: Diagonal elements of \\(\\Lambda\\) (eigenvalues)\n\nPCA essentially rotates the coordinate system to align with the natural axes of the joint distribution.",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Images and Joint Distributions"
    ]
  },
  {
    "objectID": "notebooks/images-joint-distribution.html#practical-implementation-analyzing-handwritten-digits",
    "href": "notebooks/images-joint-distribution.html#practical-implementation-analyzing-handwritten-digits",
    "title": "Images and Joint Distributions",
    "section": "Practical Implementation: Analyzing Handwritten Digits",
    "text": "Practical Implementation: Analyzing Handwritten Digits",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Images and Joint Distributions"
    ]
  },
  {
    "objectID": "notebooks/images-joint-distribution.html#loading-and-understanding-the-dataset",
    "href": "notebooks/images-joint-distribution.html#loading-and-understanding-the-dataset",
    "title": "Images and Joint Distributions",
    "section": "Loading and Understanding the Dataset",
    "text": "Loading and Understanding the Dataset\nWe’ll use the digits dataset, where each image is an 8×8 pixel grid. This gives us 64-dimensional data points, representing samples from a 64-dimensional joint distribution.\n\nfrom sklearn.datasets import load_digits\n\nData Interpretation: - X.shape = (1797, 64): We have 1,797 samples from a 64-dimensional joint distribution - Each row represents one realization from this joint distribution\n- Each column represents one random variable (pixel intensity) - The joint distribution captures how all 64 pixels co-vary across different digit images\n\n# Visualize several samples from our joint distribution\nfig, axes = plt.subplots(2, 5, figsize=(12, 6))\naxes = axes.ravel()\n\nfor i in range(10):\n    # Each image is a sample from the 64-dimensional joint distribution\n    sample_idx = i * 180  # Spread out the examples\n    image = X[sample_idx].reshape(8, 8)\n    axes[i].imshow(image, cmap='gray')\n    axes[i].set_title(f'Digit: {y[sample_idx]}\\nSample #{sample_idx}')\n    axes[i].axis('off')\n\nplt.suptitle('Individual Samples from the 64-Dimensional Joint Distribution', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n# Show the actual 64-dimensional vector for one sample\nprint(f\"Sample {sample_idx} as 64-dimensional vector:\")\nprint(f\"First 10 values: {X[sample_idx][:10]}\")\nprint(f\"Range: [{X[sample_idx].min():.1f}, {X[sample_idx].max():.1f}]\")\nprint(f\"Mean: {X[sample_idx].mean():.2f}, Std: {X[sample_idx].std():.2f}\")\n\n\n# Analyze marginal distributions of pixels\nfig, axes = plt.subplots(2, 3, figsize=(15, 8))\n\n# 1. Mean image (expected value of the joint distribution)\nmean_image = X.mean(axis=0).reshape(8, 8)\naxes[0, 0].imshow(mean_image, cmap='gray')\naxes[0, 0].set_title('Mean Image\\n(Expected Value)')\naxes[0, 0].axis('off')\n\n# 2. Standard deviation image (marginal variances)\nstd_image = X.std(axis=0).reshape(8, 8)\naxes[0, 1].imshow(std_image, cmap='hot')\naxes[0, 1].set_title('Standard Deviation per Pixel\\n(Marginal Variances)')\naxes[0, 1].axis('off')\n\n# 3. Sample of marginal distributions\npixel_indices = [10, 28, 35, 50]  # Different pixel positions\nfor i, px_idx in enumerate(pixel_indices):\n    if i &lt; 2:\n        ax = axes[0, 2]\n    else:\n        ax = axes[1, 2]\n    \n    ax.hist(X[:, px_idx], bins=30, alpha=0.7, density=True, \n            label=f'Pixel {px_idx}')\n\naxes[0, 2].set_title('Marginal Distributions\\nof Selected Pixels')\naxes[0, 2].set_xlabel('Pixel Intensity')\naxes[0, 2].set_ylabel('Density')\naxes[0, 2].legend()\n\naxes[1, 2].set_xlabel('Pixel Intensity')\naxes[1, 2].set_ylabel('Density')\naxes[1, 2].legend()\n\n# 4. Pixel correlation analysis\n# Sample a few pixels to show correlation\nsample_pixels = [20, 21, 28, 29]  # Adjacent pixels\npixel_data = X[:, sample_pixels]\ncorrelation_matrix = np.corrcoef(pixel_data.T)\n\nim = axes[1, 0].imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)\naxes[1, 0].set_title('Correlation Matrix\\n(Sample Adjacent Pixels)')\naxes[1, 0].set_xticks(range(len(sample_pixels)))\naxes[1, 0].set_yticks(range(len(sample_pixels)))\naxes[1, 0].set_xticklabels([f'Px{i}' for i in sample_pixels])\naxes[1, 0].set_yticklabels([f'Px{i}' for i in sample_pixels])\nplt.colorbar(im, ax=axes[1, 0])\n\n# 5. Scatter plot showing dependence\naxes[1, 1].scatter(X[:, 20], X[:, 21], alpha=0.3, s=10)\naxes[1, 1].set_title('Pixel 20 vs Pixel 21\\n(Adjacent Pixels)')\naxes[1, 1].set_xlabel('Pixel 20 Intensity')\naxes[1, 1].set_ylabel('Pixel 21 Intensity')\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Joint Distribution Properties:\")\nprint(f\"- Dimensionality: {X.shape[1]} (each sample is 64-dimensional)\")\nprint(f\"- Sample size: {X.shape[0]} realizations\")\nprint(f\"- Mean pixel intensity: {X.mean():.2f}\")\nprint(f\"- Overall variance: {X.var():.2f}\")\nprint(f\"- Range: [{X.min():.1f}, {X.max():.1f}]\")\nprint(f\"- Correlation between adjacent pixels (20,21): {np.corrcoef(X[:, 20], X[:, 21])[0,1]:.3f}\")\n\n\nStep 2: Principal Component Analysis - Finding Joint Distribution Structure\nPCA helps us understand the joint distribution by finding the directions of maximum variance. These directions reveal the underlying structure of how the 64 pixel intensities co-vary.",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Images and Joint Distributions"
    ]
  },
  {
    "objectID": "notebooks/images-joint-distribution.html#summary-and-key-takeaways",
    "href": "notebooks/images-joint-distribution.html#summary-and-key-takeaways",
    "title": "Images and Joint Distributions",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\n\nWhat We’ve Learned About Joint Distributions:\n\nHigh-Dimensional Reality: Real data often lives in high-dimensional spaces (64D for 8×8 images), but the effective dimensionality can be much lower\nPCA Reveals Structure: Principal Component Analysis uncovers the underlying structure of joint distributions by finding directions of maximum variance\nCovariance is Key: The joint distribution’s covariance matrix completely determines the PCA transformation - they are mathematically equivalent\nDimensionality Reduction: Most of the information in a 64-dimensional joint distribution can be captured in just a few principal components\nVisual Interpretation: We can visualize complex joint distributions by projecting to 2D or 3D spaces\n\n\n\nMathematical Connections:\n\nJoint Distribution → Covariance Matrix → Eigendecomposition → Principal Components\nPCA components are eigenvectors of the covariance matrix\nPCA eigenvalues represent variance along each principal direction\nReconstruction quality depends on how many components we retain\n\n\n\nPractical Applications:\n\nData Compression: Store images using fewer principal components\nNoise Reduction: Reconstruct data using only major components\nVisualization: Plot high-dimensional data in 2D/3D\nFeature Extraction: Use PC scores as features for machine learning\nAnomaly Detection: Identify samples that don’t fit the joint distribution pattern\n\n\n\nKey Insights for Data Science:\n\nCurse of Dimensionality: High-dimensional spaces are mostly empty\nIntrinsic Dimensionality: Data often has lower effective dimensionality than the ambient space\nCorrelation Structure: Understanding dependencies between variables is crucial\nTrade-offs: Dimensionality reduction involves balancing information retention vs. simplicity\n\nThis analysis demonstrates how abstract concepts like joint distributions become concrete and actionable through computational tools like PCA, bridging probability theory with practical data analysis.\n\n# Explore the relationship between PCA and covariance structure\nfrom sklearn.covariance import EmpiricalCovariance\n\n# Compute sample covariance matrix\ncov_estimator = EmpiricalCovariance()\ncov_matrix = cov_estimator.fit(X).covariance_\n\n# Compare with PCA eigendecomposition\neigenvals, eigenvecs = np.linalg.eigh(cov_matrix)\n# Sort in descending order (like PCA)\nidx = np.argsort(eigenvals)[::-1]\neigenvals = eigenvals[idx]\neigenvecs = eigenvecs[:, idx]\n\n# Create visualization\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# 1. Covariance matrix\nim1 = axes[0, 0].imshow(cov_matrix, cmap='coolwarm')\naxes[0, 0].set_title('Sample Covariance Matrix\\n(64×64)')\naxes[0, 0].set_xlabel('Pixel Index')\naxes[0, 0].set_ylabel('Pixel Index')\nplt.colorbar(im1, ax=axes[0, 0])\n\n# 2. Eigenvalues comparison\naxes[0, 1].plot(range(1, 11), eigenvals[:10], 'bo-', label='Covariance Eigenvalues')\naxes[0, 1].plot(range(1, 11), pca_detailed.explained_variance_, 'ro-', label='PCA Eigenvalues')\naxes[0, 1].set_title('Eigenvalues Comparison')\naxes[0, 1].set_xlabel('Component')\naxes[0, 1].set_ylabel('Eigenvalue')\naxes[0, 1].legend()\naxes[0, 1].grid(True)\n\n# 3. Eigenvector comparison (first PC)\naxes[0, 2].plot(eigenvecs[:, 0], 'b-', label='Cov. Eigenvector 1')\naxes[0, 2].plot(pca_detailed.components_[0], 'r--', label='PCA Component 1')\naxes[0, 2].set_title('First Principal Component\\n(Eigenvector Comparison)')\naxes[0, 2].set_xlabel('Pixel Index')\naxes[0, 2].set_ylabel('Component Weight')\naxes[0, 2].legend()\naxes[0, 2].grid(True)\n\n# 4. Correlation matrix (easier to interpret)\ncorrelation_matrix = np.corrcoef(X.T)\nim2 = axes[1, 0].imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)\naxes[1, 0].set_title('Sample Correlation Matrix\\n(64×64)')\naxes[1, 0].set_xlabel('Pixel Index')\naxes[1, 0].set_ylabel('Pixel Index')\nplt.colorbar(im2, ax=axes[1, 0])\n\n# 5. Local correlation structure (show 8x8 spatial structure)\n# Reshape correlation matrix to show spatial structure\nspatial_corr = np.zeros((8, 8))\ncenter_pixel = 28  # Middle-ish pixel\nfor i in range(64):\n    row, col = i // 8, i % 8\n    spatial_corr[row, col] = correlation_matrix[center_pixel, i]\n\nim3 = axes[1, 1].imshow(spatial_corr, cmap='coolwarm', vmin=-1, vmax=1)\naxes[1, 1].set_title(f'Correlation with Pixel {center_pixel}\\n(Spatial Layout)')\nplt.colorbar(im3, ax=axes[1, 1])\n\n# 6. Distribution of correlations\naxes[1, 2].hist(correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)], \n                bins=50, alpha=0.7, density=True)\naxes[1, 2].set_title('Distribution of Pairwise\\nPixel Correlations')\naxes[1, 2].set_xlabel('Correlation Coefficient')\naxes[1, 2].set_ylabel('Density')\naxes[1, 2].axvline(0, color='red', linestyle='--', alpha=0.7)\naxes[1, 2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Statistical summary\nprint(\"COVARIANCE STRUCTURE ANALYSIS:\")\nprint(\"=\"*40)\nprint(f\"Covariance matrix shape: {cov_matrix.shape}\")\nprint(f\"Covariance matrix rank: {np.linalg.matrix_rank(cov_matrix)}\")\nprint(f\"Trace (total variance): {np.trace(cov_matrix):.2f}\")\nprint(f\"Maximum correlation: {correlation_matrix[correlation_matrix &lt; 1].max():.3f}\")\nprint(f\"Minimum correlation: {correlation_matrix.min():.3f}\")\nprint(f\"Mean absolute correlation: {np.abs(correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)]).mean():.3f}\")\n\nprint(\"\\nPROOF THAT PCA = COVARIANCE EIGENDECOMPOSITION:\")\nprint(\"=\"*50)\nprint(f\"Eigenvalue difference (should be ~0): {np.max(np.abs(eigenvals[:10] - pca_detailed.explained_variance_)):.2e}\")\nprint(f\"Eigenvector difference (should be ~0): {np.max(np.abs(np.abs(eigenvecs[:, 0]) - np.abs(pca_detailed.components_[0]))):.2e}\")\nprint(\"\\n✓ PCA components are eigenvectors of the covariance matrix!\")\nprint(\"✓ PCA eigenvalues are eigenvalues of the covariance matrix!\")",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Images and Joint Distributions"
    ]
  },
  {
    "objectID": "notebooks/images-joint-distribution.html#connecting-pca-to-probability-theory",
    "href": "notebooks/images-joint-distribution.html#connecting-pca-to-probability-theory",
    "title": "Images and Joint Distributions",
    "section": "Connecting PCA to Probability Theory",
    "text": "Connecting PCA to Probability Theory\n\nCovariance Matrix and Eigenstructure\nThe principal components are directly related to the covariance structure of our joint distribution:\n\n# Detailed PCA Analysis\npca_detailed = PCA(n_components=10)  # Get more components for analysis\nX_reduced_detailed = pca_detailed.fit_transform(X)\n\n# Create comprehensive visualization\nfig = plt.figure(figsize=(16, 12))\n\n# 1. Explained variance ratio\nax1 = plt.subplot(3, 4, 1)\nplt.bar(range(10), pca_detailed.explained_variance_ratio_)\nplt.title('Explained Variance Ratio\\nby Principal Component')\nplt.xlabel('Component')\nplt.ylabel('Variance Ratio')\nplt.xticks(range(10))\n\n# 2. Cumulative explained variance\nax2 = plt.subplot(3, 4, 2)\ncumsum_var = np.cumsum(pca_detailed.explained_variance_ratio_)\nplt.plot(range(10), cumsum_var, 'bo-')\nplt.title('Cumulative Explained Variance')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Variance Ratio')\nplt.grid(True)\n\n# 3-6. First 4 principal components as images\nfor i in range(4):\n    ax = plt.subplot(3, 4, 3 + i)\n    component_image = pca_detailed.components_[i].reshape(8, 8)\n    plt.imshow(component_image, cmap='RdBu_r')\n    plt.title(f'PC{i+1}\\n(Var: {pca_detailed.explained_variance_ratio_[i]:.3f})')\n    plt.axis('off')\n    plt.colorbar(shrink=0.6)\n\n# 7. 2D projection colored by digit\nax7 = plt.subplot(3, 4, 7)\nscatter = plt.scatter(X_reduced_detailed[:, 0], X_reduced_detailed[:, 1], \n                     c=y, cmap='tab10', alpha=0.6, s=10)\nplt.title('Joint Distribution Projection\\n(First 2 PCs)')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.colorbar(scatter, shrink=0.6)\n\n# 8. Different PC pairs\nax8 = plt.subplot(3, 4, 8)\nplt.scatter(X_reduced_detailed[:, 2], X_reduced_detailed[:, 3], \n           c=y, cmap='tab10', alpha=0.6, s=10)\nplt.title('Joint Distribution Projection\\n(PC3 vs PC4)')\nplt.xlabel('PC3')\nplt.ylabel('PC4')\n\n# 9. Reconstruction demonstration\nax9 = plt.subplot(3, 4, 9)\n# Original image\noriginal_idx = 100\noriginal_image = X[original_idx].reshape(8, 8)\nplt.imshow(original_image, cmap='gray')\nplt.title(f'Original\\n(Digit {y[original_idx]})')\nplt.axis('off')\n\n# 10. Reconstruction with 2 components\nax10 = plt.subplot(3, 4, 10)\npca_2 = PCA(n_components=2)\nX_2d = pca_2.fit_transform(X)\nX_reconstructed_2 = pca_2.inverse_transform(X_2d)\nreconstructed_image_2 = X_reconstructed_2[original_idx].reshape(8, 8)\nplt.imshow(reconstructed_image_2, cmap='gray')\nplt.title('2-PC Reconstruction')\nplt.axis('off')\n\n# 11. Reconstruction with 5 components\nax11 = plt.subplot(3, 4, 11)\npca_5 = PCA(n_components=5)\nX_5d = pca_5.fit_transform(X)\nX_reconstructed_5 = pca_5.inverse_transform(X_5d)\nreconstructed_image_5 = X_reconstructed_5[original_idx].reshape(8, 8)\nplt.imshow(reconstructed_image_5, cmap='gray')\nplt.title('5-PC Reconstruction')\nplt.axis('off')\n\n# 12. Reconstruction error analysis\nax12 = plt.subplot(3, 4, 12)\nn_components_range = range(1, 21)\nreconstruction_errors = []\n\nfor n_comp in n_components_range:\n    pca_temp = PCA(n_components=n_comp)\n    X_temp = pca_temp.fit_transform(X)\n    X_recon_temp = pca_temp.inverse_transform(X_temp)\n    error = np.mean((X - X_recon_temp) ** 2)\n    reconstruction_errors.append(error)\n\nplt.plot(n_components_range, reconstruction_errors, 'ro-')\nplt.title('Reconstruction Error\\nvs Number of Components')\nplt.xlabel('Number of Components')\nplt.ylabel('Mean Squared Error')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Print summary statistics\nprint(\"JOINT DISTRIBUTION ANALYSIS SUMMARY:\")\nprint(\"=\"*50)\nprint(f\"Original dimensionality: {X.shape[1]}\")\nprint(f\"Number of samples: {X.shape[0]}\")\nprint(f\"First 2 PCs explain {cumsum_var[1]:.1%} of total variance\")\nprint(f\"First 5 PCs explain {cumsum_var[4]:.1%} of total variance\")\nprint(f\"First 10 PCs explain {cumsum_var[9]:.1%} of total variance\")\nprint(f\"\\nThis means the 64-dimensional joint distribution has most of its\")\nprint(f\"structure captured in just a few principal directions!\")\n\nprint(f\"\\nReconstruction Quality:\")\nprint(f\"- 2 components: MSE = {reconstruction_errors[1]:.3f}\")\nprint(f\"- 5 components: MSE = {reconstruction_errors[4]:.3f}\")\nprint(f\"- 10 components: MSE = {reconstruction_errors[9]:.3f}\")\n\nLet’s explore this in more detail by analyzing the principal components and their relationship to the joint distribution:\nUnderstanding the PCA Results:\n\nOriginal space: 64-dimensional joint distribution (64 pixel intensities)\nReduced space: 2-dimensional projection (captures main patterns of co-variation)\nInformation loss: We visualize the most important 2 directions out of 64 possible\n\nThe scatter plot shows how the 64-dimensional samples project onto the first two principal components, revealing the underlying structure of the joint distribution.",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Images and Joint Distributions"
    ]
  },
  {
    "objectID": "notebooks/images-joint-distribution.html#analyzing-the-joint-distribution-structure",
    "href": "notebooks/images-joint-distribution.html#analyzing-the-joint-distribution-structure",
    "title": "Images and Joint Distributions",
    "section": "Analyzing the Joint Distribution Structure",
    "text": "Analyzing the Joint Distribution Structure\n\nStep 1: Understanding Marginal Distributions\nBefore applying PCA, let’s examine the marginal distributions - how individual pixels (random variables) behave across all samples.\n\n\nVisualizing Individual Samples\nLet’s examine individual realizations from our 64-dimensional joint distribution:\n\nX, y = load_digits(return_X_y=True)\nX = X.astype(np.float32)\ny = y.astype(np.int64)\nprint(X.shape, y.shape)\n\n(1797, 64) (1797,)\n\n\n\nX[0], y[0]\n\n(array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n        15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n        12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n         0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n        10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.],\n       dtype=float32),\n np.int64(0))\n\n\n\ndig = 10\nplt.imshow(X[dig].reshape(8, 8), cmap='gray')\nplt.title(f'Target: {y[dig]}')\n\nText(0.5, 1.0, 'Target: 0')\n\n\n\n\n\n\n\n\n\n\n# Using PCA to reduce the dimensionality of the data to 2d\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_reduced = pca.fit_transform(X)\nprint(X_reduced.shape)\n\n(1797, 2)\n\n\n\n# Plotting the reduced data\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, cmap='tab10')\nplt.colorbar()",
    "crumbs": [
      "Home",
      "Interactive & Quizzes",
      "Images and Joint Distributions"
    ]
  }
]
[
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Slides",
    "section": "",
    "text": "1-introduction-ml.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\n1d-cnn.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nCNN.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nKNN-approx.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nLogistic-Hessian.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nLogistic-vectorisation.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nMovieRecommendation.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nSGD.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nWeighted-least.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\naccuracy_convention.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nautograd.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nbias-variance.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nconstrained-1.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nconstrained-2.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nconvexity.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\ncross-validation.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\ndecision-trees.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nensemble.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nfind-widths.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\ngradient-descent.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nguest-lecture.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nknn.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nlasso-regression.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nlinear-regression.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nlogistic-1.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nmisc.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nml-maths-2-contour.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nml-maths.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nmlp.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nmock-quiz-2-solution.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nmock-quiz-2.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nmock-quiz.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nnext-token.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nridge-regression.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nrl.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nsvm-intro.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nsvm-soft-margin.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\ntime_complexity.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\n\n\n\n\n\nunsupervised.pdf\n\n\n9/25/24, 8:45:35 AM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cnn/convolution-operation-stride.html",
    "href": "cnn/convolution-operation-stride.html",
    "title": "CIFAR",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom matplotlib import patches\n\n\ninp = np.random.choice(range(10), (5, 5))\nfilter_conv = np.array([\n    [1, 0, -1],\n    [1, 0, -1],\n    [1, 0, -1]\n])\n\n\nplt.imshow(inp, cmap='Greys')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(ncols=3, figsize=(12, 4))\n\nsns.heatmap(inp, annot=True, cbar=None, ax=ax[0], cmap='Purples')\nsns.heatmap(filter_conv, annot=True, cbar=None, ax=ax[1], cmap='Purples')\ng = ax[0]\nrect = patches.Rectangle((0,0),3,3,linewidth=5,edgecolor='grey',facecolor='black', alpha=0.5)\n\n# Add the patch to the Axes\ng.add_patch(rect)\n\nax[0].set_title(\"Input\")\nax[1].set_title(\"Filter\")\n\nText(0.5, 1.0, 'Filter')\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport matplotlib.animation\n\n#####################\n# Array preparation\n#####################\n\n#input array\nn = 6\np = 0\ns = 2\nf = 3\na = np.random.randint(0, 5, size=(n, n))\n# kernel\nkernel = np.tile([1, 0, -1], f).reshape(f, f)\n#f = kernel.shape[0]\n\ndef create_animation(a, kernel, p, s, fname, frate, figsize=(8, 4)):\n\n    if p:\n        # visualization array (2 bigger in each direction)\n        va = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p), dtype=int)\n        va[p:-p,p:-p] = a\n        va_color = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p)) \n        va_color[p:-p,p:-p] = 0.5\n    else:\n        va = a\n        va_color = np.zeros_like(a)\n    n = a.shape[0]\n    o_shape = np.floor_divide(n+2*p-f, s)+1\n    #output array\n    res = np.zeros((o_shape, o_shape))\n\n\n\n    #####################\n    # Create inital plot\n    #####################\n    fig = plt.figure(figsize=figsize)\n\n    def add_axes_inches(fig, rect):\n        w,h = fig.get_size_inches()\n        return fig.add_axes([rect[0]/w, rect[1]/h, rect[2]/w, rect[3]/h])\n\n    axwidth = 3.\n    cellsize = axwidth/va.shape[1]\n    axheight = cellsize*va.shape[0]\n\n    ax_va  = add_axes_inches(fig, [cellsize, cellsize, axwidth, axheight])\n    ax_kernel  = add_axes_inches(fig, [cellsize*2+axwidth,\n                                       (2+res.shape[0])*cellsize-kernel.shape[0]*cellsize,\n                                       kernel.shape[1]*cellsize,  \n                                       kernel.shape[0]*cellsize])\n    ax_res = add_axes_inches(fig, [cellsize*3+axwidth+kernel.shape[1]*cellsize,\n                                   2*cellsize, \n                                   res.shape[1]*cellsize,  \n                                   res.shape[0]*cellsize])\n    ax_kernel.set_title(\"Kernel\", size=12)\n\n    im_va = ax_va.imshow(va_color, vmin=0., vmax=1.3, cmap=\"Blues\")\n    ax_va.set_title(\"Image size: {}X{}\\n Padding: {} and Strides: {}\".format(n, n, p, s))\n    for i in range(va.shape[0]):\n        for j in range(va.shape[1]):\n            ax_va.text(j,i, va[i,j], va=\"center\", ha=\"center\")\n\n    ax_kernel.imshow(np.zeros_like(kernel), vmin=-1, vmax=1, cmap=\"Pastel1\")\n    for i in range(kernel.shape[0]):\n        for j in range(kernel.shape[1]):\n            ax_kernel.text(j,i, kernel[i,j], va=\"center\", ha=\"center\")\n\n\n    im_res = ax_res.imshow(res, vmin=0, vmax=1.3, cmap=\"Greens\")\n    res_texts = []\n    for i in range(res.shape[0]):\n        row = []\n        for j in range(res.shape[1]):\n            row.append(ax_res.text(j,i, \"\", va=\"center\", ha=\"center\"))\n        res_texts.append(row)    \n\n    ax_res.set_title(\"Output size: {}X{}\".format(n+2*p-f+1, n+2*p-f+1))\n\n    for ax  in [ax_va, ax_kernel, ax_res]:\n        ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n        ax.yaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.xaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.grid(color=\"k\")\n\n    ###############\n    # Animation\n    ###############\n    def init():\n        for row in res_texts:\n            for text in row:\n                text.set_text(\"\")\n\n    def animate(ij):\n        i,j=ij\n        o = kernel.shape[1]//2\n        # calculate result\n\n        res_ij = (kernel*va[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1]).sum()\n        res_texts[i][j].set_text(res_ij)\n        # make colors\n        c = va_color.copy()\n        c[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1] = 1.\n        im_va.set_array(c)\n\n        r = res.copy()\n        r[i,j] = 1\n        im_res.set_array(r)\n\n\n\n    i,j = np.indices(res.shape)\n    ani = matplotlib.animation.FuncAnimation(fig, animate, init_func=init, \n                                             frames=zip(i.flat, j.flat), interval=frate)\n    ani.save(fname, writer=\"imagemagick\")\n\n\ncreate_animation(a, kernel, p, s, 'demo.gif', 400)\n\n\n\n\n\n\n\n\n\nfrom keras.datasets import mnist\n\nUsing TensorFlow backend.\n\n\n\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n\ncreate_animation(x_train[0], kernel, 0, 1, 'mnist.gif', 2, (20, 4))\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport matplotlib.animation\n\n#####################\n# Array preparation\n#####################\n\n#input array\nn = 6\np = 0\ns = 2\nf = 3\na = np.random.randint(0, 5, size=(n, n))\n# kernel\nkernel = np.tile([1, 0, -1], f).reshape(f, f)\n#f = kernel.shape[0]\n\ndef create_static(a, kernel, p, s, fname, frate, figsize=(8, 4)):\n\n    if p:\n        # visualization array (2 bigger in each direction)\n        va = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p), dtype=int)\n        va[p:-p,p:-p] = a\n        va_color = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p)) \n        va_color[p:-p,p:-p] = 0.5\n    else:\n        va = a\n        va_color = np.zeros_like(a)\n    n = a.shape[0]\n    o_shape = np.floor_divide(n+2*p-f, s)+1\n    #output array\n    res = np.zeros((o_shape, o_shape))\n\n\n\n    #####################\n    # Create inital plot\n    #####################\n    fig = plt.figure(figsize=figsize)\n\n    def add_axes_inches(fig, rect):\n        w,h = fig.get_size_inches()\n        return fig.add_axes([rect[0]/w, rect[1]/h, rect[2]/w, rect[3]/h])\n\n    axwidth = 3.\n    cellsize = axwidth/va.shape[1]\n    axheight = cellsize*va.shape[0]\n\n    ax_va  = add_axes_inches(fig, [cellsize, cellsize, axwidth, axheight])\n    ax_kernel  = add_axes_inches(fig, [cellsize*2+axwidth,\n                                       (2+res.shape[0])*cellsize-kernel.shape[0]*cellsize,\n                                       kernel.shape[1]*cellsize,  \n                                       kernel.shape[0]*cellsize])\n    ax_res = add_axes_inches(fig, [cellsize*3+axwidth+kernel.shape[1]*cellsize,\n                                   2*cellsize, \n                                   res.shape[1]*cellsize,  \n                                   res.shape[0]*cellsize])\n    ax_kernel.set_title(\"Kernel\", size=12)\n\n    im_va = ax_va.imshow(va_color, vmin=0., vmax=1.3, cmap=\"Blues\")\n    ax_va.set_title(\"Image size: {}X{}\\n Padding: {} and Strides: {}\".format(n, n, p, s))\n    for i in range(va.shape[0]):\n        for j in range(va.shape[1]):\n            ax_va.text(j,i, va[i,j], va=\"center\", ha=\"center\")\n\n    ax_kernel.imshow(np.zeros_like(kernel), vmin=-1, vmax=1, cmap=\"Pastel1\")\n    for i in range(kernel.shape[0]):\n        for j in range(kernel.shape[1]):\n            ax_kernel.text(j,i, kernel[i,j], va=\"center\", ha=\"center\")\n\n\n    im_res = ax_res.imshow(res, vmin=0, vmax=1.3, cmap=\"Greens\")\n    res_texts = []\n    for i in range(res.shape[0]):\n        row = []\n        for j in range(res.shape[1]):\n            row.append(ax_res.text(j,i, \"\", va=\"center\", ha=\"center\"))\n        res_texts.append(row)    \n\n    ax_res.set_title(\"Output size: {}X{}\".format(n+2*p-f+1, n+2*p-f+1))\n\n    for ax  in [ax_va, ax_kernel, ax_res]:\n        ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n        ax.yaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.xaxis.set_major_locator(mticker.IndexLocator(1,0))\n        ax.grid(color=\"k\")\n\n    ###############\n    # Animation\n    ###############\n    def init():\n        for row in res_texts:\n            for text in row:\n                text.set_text(\"\")\n\n    def animate(ij):\n        i,j=ij\n        o = kernel.shape[1]//2\n        # calculate result\n\n        res_ij = (kernel*va[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1]).sum()\n        res_texts[i][j].set_text(res_ij)\n        # make colors\n        c = va_color.copy()\n        c[1+s*i-o:1+s*i+o+1, 1+s*j-o:1+s*j+o+1] = 1.\n        im_va.set_array(c)\n\n        r = res.copy()\n        r[i,j] = 1\n        im_res.set_array(r)\n\n\n\n    i,j = np.indices(res.shape)\n     \n    frames=zip(i.flat, j.flat)\n    animate(frames)\n    fig.savefig(fname)\n\n\nfrom keras import backend as K\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\n\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\nmodel_vertical_edge = keras.Sequential()\nmodel_vertical_edge.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='linear', input_shape=(28, 28, 1)))\n\n\nmodel_vertical_edge_relu = keras.Sequential()\nmodel_vertical_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n\n\nT = model_vertical_edge.layers[0].get_weights()\nfilter_conv = filter_conv\nT[0] = filter_conv.reshape(T[0].shape)\nmodel_vertical_edge.layers[0].set_weights(T)\n\n\nsns.heatmap(model_vertical_edge.predict(x_train[2:3]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"4-vertical-edge-linear.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nT = model_vertical_edge_relu.layers[0].get_weights()\nfilter_conv = filter_conv\nT[0] = filter_conv.reshape(T[0].shape)\nmodel_vertical_edge_relu.layers[0].set_weights(T)\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[2:3]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"4-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap((x_train[2:3]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-4.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nmodel_horizontal_edge_relu = keras.Sequential()\nmodel_horizontal_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n\n\nT = model_horizontal_edge_relu.layers[0].get_weights()\nT[0] = filter_conv.T.reshape(T[0].shape)\nmodel_horizontal_edge_relu.layers[0].set_weights(T)\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[2:3]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"4-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap((x_train[0:1]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-5.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[0:1]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"5-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[0:1]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"5-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap((x_train[5:6]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-2.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[5:6]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"2-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[5:6]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"2-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap((x_train[6:7]/255).reshape(28, 28),cmap='Greys')\nplt.savefig(\"mnist-1.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[6:7]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"1-horizontal-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[6:7]/255).reshape(26, 26),cmap='Greys')\nplt.savefig(\"1-vertical-edge-relu.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nfrom keras.datasets import cifar10\n\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\nDownloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170500096/170498071 [==============================] - 91s 1us/step\n\n\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel.add(Activation('relu'))\n\n\nmodel_horizontal_edge_relu = keras.Sequential()\nmodel_horizontal_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n\n\nmodel_horizontal_edge_relu.layers[0].get_weights()[0].shape\n\n(3, 3, 3, 1)\n\n\n\nfilter_3d_horizontal = np.empty((3, 3, 3))\nfilter_3d_horizontal[:] = filter_conv.T\n\n\nfilter_3d_horizontal\n\narray([[[ 1.,  1.,  1.],\n        [ 0.,  0.,  0.],\n        [-1., -1., -1.]],\n\n       [[ 1.,  1.,  1.],\n        [ 0.,  0.,  0.],\n        [-1., -1., -1.]],\n\n       [[ 1.,  1.,  1.],\n        [ 0.,  0.,  0.],\n        [-1., -1., -1.]]])\n\n\n\nT = model_horizontal_edge_relu.layers[0].get_weights()\nT[0] = filter_3d_horizontal.reshape(T[0].shape)\nmodel_horizontal_edge_relu.layers[0].set_weights(T)\n\n\nplt.imshow(x_train[4])\nplt.title(y_train[4])\nplt.savefig(\"cifar-10-car.pdf\", transparent=True)\n\n/home/nipunbatra-pc/anaconda3/lib/python3.7/site-packages/matplotlib/text.py:1191: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  if s != self._text:\n\n\n\n\n\n\n\n\n\n\nplt.imshow(x_train[4][:, :, 0], cmap='Reds')\nplt.savefig(\"cifar-10-car-red.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nplt.imshow(x_train[4][:, :, 1], cmap='Greens')\nplt.savefig(\"cifar-10-car-green.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nplt.imshow(x_train[4][:, :, 2], cmap='Blues')\nplt.savefig(\"cifar-10-car-blue.pdf\", transparent=True)\n\n\n\n\n\n\n\n\n\nsns.heatmap(model_horizontal_edge_relu.predict(x_train[6:7]).reshape(30, 30),cmap='Greys')\n\n\n\n\n\n\n\n\n\nx_train.shape[1:]\n\n(32, 32, 3)\n\n\n\nmodel_horizontal_edge_relu.predict(x_train[4:5]).shape\n\n(1, 30, 30, 1)\n\n\n\nmodel_vertical_edge_relu = keras.Sequential()\nmodel_vertical_edge_relu.add(layers.Conv2D(filters=1, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)))\n\n\nfilter_3d_vertical = np.empty((3, 3, 3))\nfilter_3d_vertical[:] = filter_conv\nfilter_3d_vertical = filter_3d_vertical\n\nT = model_vertical_edge_relu.layers[0].get_weights()\nT[0] = filter_3d_vertical.reshape(T[0].shape)\nmodel_vertical_edge_relu.layers[0].set_weights(T)\n\n\nplt.imshow((filter_3d_horizontal+1)/2)\n\n\n\n\n\n\n\n\n\nplt.imshow(((filter_3d_vertical+1)/2).T)\n\n\n\n\n\n\n\n\n\nfilter_3d_vertical\n\narray([[[ 1.,  0., -1.],\n        [ 1.,  0., -1.],\n        [ 1.,  0., -1.]],\n\n       [[ 1.,  0., -1.],\n        [ 1.,  0., -1.],\n        [ 1.,  0., -1.]],\n\n       [[ 1.,  0., -1.],\n        [ 1.,  0., -1.],\n        [ 1.,  0., -1.]]])\n\n\n\n(filter_3d_vertical+1)/2\n\narray([[[1. , 0.5, 0. ],\n        [1. , 0.5, 0. ],\n        [1. , 0.5, 0. ]],\n\n       [[1. , 0.5, 0. ],\n        [1. , 0.5, 0. ],\n        [1. , 0.5, 0. ]],\n\n       [[1. , 0.5, 0. ],\n        [1. , 0.5, 0. ],\n        [1. , 0.5, 0. ]]])\n\n\n\nsns.heatmap(model_vertical_edge_relu.predict(x_train[6:7]).reshape(30, 30),cmap='Greys')\n\n\n\n\n\n\n\n\n\nmodel_vertical_edge_relu.layers[0].get_weights()[0][0].shape\n\n(3, 3, 1)\n\n\n\nimport scipy\nimg = x_train[6:7].reshape(32, 32, 3)\nfrom skimage import color\nimg = color.rgb2gray(img)\nsharpen_kernel = np.array([[0,-1,0],[-1,5,-1],[0,-1,0]])\nimage_sharpen = scipy.signal.convolve2d(img, sharpen_kernel, 'valid')\n\n\nfrom skimage import io\n\n\nbeach = io.imread(\"beach.jpg\")\n\n\nbeach.shape\n\n(1704, 2272, 3)\n\n\n\nbuildings = io.imread(\"buildings.jpg\")\n\n\nbuildings.shape\n\n(1704, 2272, 3)\n\n\n\nplt.imshow(beach)\nplt.axis('OFF')\n\n\n\n\n\n\n\n\n\nplt.imshow(beach[:, :, 0], cmap='Reds')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nplt.imshow(beach[:, :, 1], cmap='Greens')\n\n\n\n\n\n\n\n\n\nplt.imshow(beach[:, :, 2], cmap='Blues')\n\n\n\n\n\n\n\n\n\nvertical_kernel = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\nimage_out_beach_red = scipy.signal.convolve2d(beach[:, :, 0], vertical_kernel, 'valid')\nplt.imshow(image_out_beach_red, cmap='Greys')\n\n\n\n\n\n\n\n\n\nvertical_kernel = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\nimage_out_beach_green = scipy.signal.convolve2d(beach[:, :, 1], vertical_kernel, 'valid')\nplt.imshow(image_out_beach_green, cmap='Greens')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nvertical_kernel = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])\nimage_out_beach_blue = scipy.signal.convolve2d(beach[:, :, 2], vertical_kernel, 'valid')\nplt.imshow(image_out_beach_blue, cmap='Blues')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nimage_out_buildings_blue = scipy.signal.convolve2d(buildings[:, :, 2], vertical_kernel, 'valid')\nplt.imshow(image_out_buildings_blue, cmap='Greys')\nplt.axis('off')\n\n\n\n\n\n\n\n\n\nhorizontal_kernel = vertical_kernel.T\nhorizontal_kernel\n\narray([[ 1,  1,  1],\n       [ 0,  0,  0],\n       [-1, -1, -1]])\n\n\n\nimage_out_buildings_blue_horizontal = scipy.signal.convolve2d(buildings[:, :, 2], horizontal_kernel, 'valid')\nplt.imshow(image_out_buildings_blue_horizontal, cmap='Greys')\nplt.axis('off')"
  },
  {
    "objectID": "cnn/vgg-minst.html",
    "href": "cnn/vgg-minst.html",
    "title": "Figure out which ones we are getting wrong",
    "section": "",
    "text": "import tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\n\nfrom tensorflow.keras.applications.vgg16 import VGG16\nmodel = VGG16()\n\nDownloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n553467904/553467096 [==============================] - 362s 1us/step\n\n\n\nmodel.summary()\n\nModel: \"vgg16\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n_________________________________________________________________\nblock1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n_________________________________________________________________\nblock1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n_________________________________________________________________\nblock1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n_________________________________________________________________\nblock2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n_________________________________________________________________\nblock2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n_________________________________________________________________\nblock2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n_________________________________________________________________\nblock3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n_________________________________________________________________\nblock3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n_________________________________________________________________\nblock3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n_________________________________________________________________\nblock4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n_________________________________________________________________\nblock4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n_________________________________________________________________\nblock4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n_________________________________________________________________\nblock5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n_________________________________________________________________\nblock5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 25088)             0         \n_________________________________________________________________\nfc1 (Dense)                  (None, 4096)              102764544 \n_________________________________________________________________\nfc2 (Dense)                  (None, 4096)              16781312  \n_________________________________________________________________\npredictions (Dense)          (None, 1000)              4097000   \n=================================================================\nTotal params: 138,357,544\nTrainable params: 138,357,544\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom keras.datasets import mnist\nfrom keras import backend as K\n\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\n\nx_train.shape\n\n(60000, 28, 28, 1)\n\n\n\ny_train.shape\n\n(60000,)\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n%matplotlib inline\nplt.imshow(x_train[0][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\nplt.title(y_train[0])\n\nText(0.5, 1.0, '5')\n\n\n\n\n\n\n\n\n\n\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n\n\nEPOCHS = 10\nBATCH_SIZE = 128\n\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\n\nnum_classes = 10\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\ny_train[0]\n\narray([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)\n\n\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/12\n60000/60000 [==============================] - 8s 138us/sample - loss: 0.3959 - accuracy: 0.8896 - val_loss: 0.1322 - val_accuracy: 0.9593\nEpoch 2/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.1192 - accuracy: 0.9637 - val_loss: 0.0776 - val_accuracy: 0.9744\nEpoch 3/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0831 - accuracy: 0.9751 - val_loss: 0.0646 - val_accuracy: 0.9789\nEpoch 4/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0668 - accuracy: 0.9804 - val_loss: 0.0557 - val_accuracy: 0.9822\nEpoch 5/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0575 - accuracy: 0.9826 - val_loss: 0.0455 - val_accuracy: 0.9853\nEpoch 6/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0501 - accuracy: 0.9849 - val_loss: 0.0484 - val_accuracy: 0.9846\nEpoch 7/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0423 - accuracy: 0.9870 - val_loss: 0.0431 - val_accuracy: 0.9854\nEpoch 8/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0371 - accuracy: 0.9886 - val_loss: 0.0365 - val_accuracy: 0.9876\nEpoch 9/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0339 - accuracy: 0.9894 - val_loss: 0.0430 - val_accuracy: 0.9868\nEpoch 10/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0320 - accuracy: 0.9902 - val_loss: 0.0406 - val_accuracy: 0.9870\nEpoch 11/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0271 - accuracy: 0.9915 - val_loss: 0.0360 - val_accuracy: 0.9891\nEpoch 12/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.0367 - val_accuracy: 0.9881\nTest loss: 0.03666715431667981\nTest accuracy: 0.9881\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 0])\n\n\n\n\n\n\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 1])\n\n\n\n\n\n\n\n\n\nlen(model.get_weights())\n\n10\n\n\n\ne = model.layers[0]\n\n\ne.get_weights()[0]\n\n(3, 3, 1, 6)\n\n\n\ne.name\n\n'conv2d_3'\n\n\n\nw, b = model.get_layer(\"conv2d_3\").get_weights()\n\n\nimport pandas as pd\n\n\npd.Series(b).plot(kind='bar')\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nfig, ax = plt.subplots(ncols=6)\nfor i in range(6):\n    sns.heatmap(w[:, :, 0, i], ax=ax[i], annot=True)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nnp.argmax(model.predict(x_test[0:1]))\n\n7\n\n\n\ntest_sample = 5\nplt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\npred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\nplt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\nText(0.5, 1.0, 'Predicted = 1, GT = 1')\n\n\n\n\n\n\n\n\n\n\nnp.argmax(model.predict(x_test[0:1])[0])\n\n7\n\n\n\npred_overall = np.argmax(model.predict(x_test), axis=1)\n\n\ngt_overall = np.argmax(y_test, axis=1)\n\n\nnp.where(np.not_equal(pred_overall, gt_overall))[0]\n\narray([ 247,  259,  321,  359,  445,  448,  449,  495,  582,  583,  625,\n        659,  684,  924,  947,  965, 1014, 1039, 1045, 1062, 1181, 1182,\n       1226, 1232, 1247, 1260, 1299, 1319, 1393, 1414, 1530, 1549, 1554,\n       1621, 1681, 1901, 1955, 1987, 2035, 2044, 2070, 2098, 2109, 2130,\n       2135, 2189, 2293, 2369, 2387, 2406, 2414, 2488, 2597, 2654, 2720,\n       2760, 2863, 2896, 2939, 2953, 2995, 3073, 3225, 3422, 3503, 3520,\n       3534, 3558, 3559, 3597, 3762, 3767, 3808, 3869, 3985, 4007, 4065,\n       4075, 4193, 4207, 4248, 4306, 4405, 4500, 4571, 4639, 4699, 4723,\n       4740, 4761, 4807, 4823, 5228, 5265, 5937, 5955, 5973, 6555, 6560,\n       6597, 6614, 6625, 6651, 6755, 6847, 7259, 7851, 7921, 8059, 8069,\n       8311, 8325, 8408, 9009, 9587, 9629, 9634, 9679, 9729])\n\n\n\npred_overall\n\narray([7, 2, 1, ..., 4, 5, 6])\n\n\n\ndef plot_prediction(test_sample):\n    plt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\n    pred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\n    plt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\n\nplot_prediction(359)\n\n\n\n\n\n\n\n\n\nplot_prediction(9729)\n\n\n\n\n\n\n\n\n\nplot_prediction(9634)\n\n\n\n\n\n\n\n\n\n### Feature map\n\n\nfm_model = keras.Model(inputs=model.inputs, outputs=model.layers[2].output)\n\n\nmodel.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.summary()\n\nModel: \"model_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3_input (InputLayer)  [(None, 28, 28, 1)]       0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n=================================================================\nTotal params: 940\nTrainable params: 940\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.predict(x_test[test_sample:test_sample+1]).shape\n\n(1, 11, 11, 16)\n\n\n\ntest_sample = 88\nfm_1 = fm_model.predict(x_test[test_sample:test_sample+1])[0, :, :, :]\n\n\nfig, ax = plt.subplots(ncols=16, figsize=(20, 4))\nfor i in range(16):\n    ax[i].imshow(fm_1[:, :, i], cmap=\"Greys\")"
  },
  {
    "objectID": "cnn/tensor-factorisation.html",
    "href": "cnn/tensor-factorisation.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import tensorly as tl\n\nUsing numpy backend.\n\n\n\nimport cvxpy as cp\nimport numpy as np\n\n# Ensure repeatably random problem data.\nnp.random.seed(0)\n\n# Generate random data matrix A.\nm = 10\nn = 10\no = 5\nk = 2\nD = 10*np.ones((m, n, o)) + np.random.randn(m, n, o)\n\n# Initialize Y randomly.\nA_init = 10*np.ones((m, k))\nB_init = 10*np.ones((n, k))\nC_init = 10*np.ones((o, k))\n\n\n\n\nPred_A = np.einsum('ir, jr, kr -&gt;ijk', A_init, B_init, C_init)\n\n\n# Ensure same initial random Y, rather than generate new one\n# when executing this cell.\nB = B_init\nC = C_init\n\n# Perform alternating minimization.\nMAX_ITERS = 100\nresidual = np.zeros(MAX_ITERS)\nfor iter_num in range(0, 1+MAX_ITERS):\n\n    if iter_num % 3 == 0:\n        A = cp.Variable(shape=(n, k))\n        constraint = [A &gt;= 0]\n        prediction = A@tl.tenalg.khatri_rao([C, B]).T\n    elif iter_num % 3 == 1:\n        B = cp.Variable(shape=(m, k))\n        constraint = [B &gt;= 0]\n        prediction = B@tl.tenalg.khatri_rao([A, C]).T\n    elif iter_num % 3 == 2:\n        C = cp.Variable(shape=(o, k))\n        constraint = [C &gt;= 0]\n        prediction = C@tl.tenalg.khatri_rao([B, A]).T\n\n    obj = cp.Minimize(cp.norm(D.reshape(prediction.shape) - prediction, 'fro')/D.size)\n    prob = cp.Problem(obj, constraint)\n    prob.solve(solver=cp.SCS, max_iters=10000)\n\n    if prob.status != cp.OPTIMAL:\n        raise Exception(\"Solver did not converge!\")\n\n    print('Iteration {}, residual norm {}'.format(iter_num, prob.value))\n    residual[iter_num-1] = prob.value\n\n    # Convert variable to NumPy array constant for next iteration.\n    if iter_num % 3 == 0:\n        A = A.value\n    elif iter_num%3 == 1:\n        B = B.value\n    else:\n        C = C.value\n\nIteration 0, residual norm 0.044229038768601577\nIteration 1, residual norm 0.04438975125966638\nIteration 2, residual norm 0.04485089174072711\nIteration 3, residual norm 0.0446730384004453\nIteration 4, residual norm 0.044526862069177754\nIteration 5, residual norm 0.04484264445045543\nIteration 6, residual norm 0.044478708695822676\nIteration 7, residual norm 0.04447482085818169\nIteration 8, residual norm 0.045090403949033964\nIteration 9, residual norm 0.04454108258260972\nIteration 10, residual norm 0.04409564845828368\nIteration 11, residual norm 0.04513476609369982\nIteration 12, residual norm 0.04452491993393663\nIteration 13, residual norm 0.04378836637021587\nIteration 14, residual norm 0.045190029913906464\nIteration 15, residual norm 0.04437455947694065\nIteration 16, residual norm 0.04376389953499031\nIteration 17, residual norm 0.04474118548369649\nIteration 18, residual norm 0.04485265629997688\nIteration 19, residual norm 0.04423721109971314\nIteration 20, residual norm 0.04463836313028598\nIteration 21, residual norm 0.04509480034427943\nIteration 22, residual norm 0.04449824189344165\nIteration 23, residual norm 0.0446297404908044\nIteration 24, residual norm 0.044916791718091605\nIteration 25, residual norm 0.04475629136811872\nIteration 26, residual norm 0.044530205009593614\nIteration 27, residual norm 0.04481551774462266\nIteration 28, residual norm 0.04450448414700286\nIteration 29, residual norm 0.04473161074066948\nIteration 30, residual norm 0.044760915844488124\nIteration 31, residual norm 0.04447831568798033\nIteration 32, residual norm 0.04458463344810684\nIteration 33, residual norm 0.04486246216584251\nIteration 34, residual norm 0.04441553594150665\nIteration 35, residual norm 0.04465894246345131\nIteration 36, residual norm 0.04504337082596328\nIteration 37, residual norm 0.044526299849176346\nIteration 38, residual norm 0.04482460772221398\nIteration 39, residual norm 0.04500800584243032\nIteration 40, residual norm 0.04450100433007423\nIteration 41, residual norm 0.04484822280377619\nIteration 42, residual norm 0.04482311584413178\nIteration 43, residual norm 0.044522803791676675\nIteration 44, residual norm 0.044772484551616504\nIteration 45, residual norm 0.04458696318356589\nIteration 46, residual norm 0.044520256516783666\nIteration 47, residual norm 0.04482402095966313\nIteration 48, residual norm 0.04426750617419428\nIteration 49, residual norm 0.044306760469399145\nIteration 50, residual norm 0.04488415609564971\nIteration 51, residual norm 0.04462703244119978\nIteration 52, residual norm 0.04393438972821902\nIteration 53, residual norm 0.04496061133340096\nIteration 54, residual norm 0.04474813896252984\nIteration 55, residual norm 0.04412129577027855\nIteration 56, residual norm 0.04476074674202858\nIteration 57, residual norm 0.04493986609489073\nIteration 58, residual norm 0.04433894515459506\nIteration 59, residual norm 0.04479359793862809\nIteration 60, residual norm 0.04507730499950036\nIteration 61, residual norm 0.04443726027910054\nIteration 62, residual norm 0.04482400811580273\nIteration 63, residual norm 0.044904850352243696\nIteration 64, residual norm 0.04455396547559336\nIteration 65, residual norm 0.04468608722592516\nIteration 66, residual norm 0.04458327810705387\nIteration 67, residual norm 0.0445511237555968\nIteration 68, residual norm 0.044703743776719096\nIteration 69, residual norm 0.04439204969221895\nIteration 70, residual norm 0.04419479288919463\nIteration 71, residual norm 0.04496926843613955\nIteration 72, residual norm 0.04456014579787714\nIteration 73, residual norm 0.04390674892508923\nIteration 74, residual norm 0.04471211816232015\nIteration 75, residual norm 0.04477670162347586\nIteration 76, residual norm 0.044259574198538376\nIteration 77, residual norm 0.044866509022554346\nIteration 78, residual norm 0.04499103987882674\nIteration 79, residual norm 0.04443576055416042\nIteration 80, residual norm 0.04494346910320409\nIteration 81, residual norm 0.04480069447165729\nIteration 82, residual norm 0.044256568121726014\nIteration 83, residual norm 0.044744496252969\nIteration 84, residual norm 0.04448314930970543\nIteration 85, residual norm 0.044155459981671225\nIteration 86, residual norm 0.04521780706935539\nIteration 87, residual norm 0.04446147094598418\nIteration 88, residual norm 0.04404940602099275\nIteration 89, residual norm 0.04516034088743775\nIteration 90, residual norm 0.04457572511910495\nIteration 91, residual norm 0.044197230653923565\nIteration 92, residual norm 0.045118201306481545\nIteration 93, residual norm 0.04421183315228502\nIteration 94, residual norm 0.04375429174860549\nIteration 95, residual norm 0.04441022241202105\nIteration 96, residual norm 0.04401508759046129\nIteration 97, residual norm 0.04393321896924147\nIteration 98, residual norm 0.04435341885307329\nIteration 99, residual norm 0.04411809866902199\nIteration 100, residual norm 0.044273830016112556\n\n\n\nA\n\narray([[ 1.23599056e-01,  4.95799879e-02],\n       [-6.88417340e-12,  4.89807637e-02],\n       [ 1.97771012e-01,  5.00354889e-02],\n       [ 2.97660174e-11,  4.86588823e-02],\n       [ 1.16488475e-01,  4.82973254e-02],\n       [-1.07464945e-11,  4.91216434e-02],\n       [ 2.41321912e-11,  4.81319502e-02],\n       [ 1.23285226e-01,  4.79485790e-02],\n       [ 8.32457971e-12,  4.89433882e-02],\n       [ 1.25287913e-11,  4.91104662e-02]])\n\n\n\nB\n\narray([[ 9.47630862e+00,  1.31432989e+01],\n       [ 2.21280935e+00,  1.29931743e+01],\n       [ 5.84494148e-10,  1.33764365e+01],\n       [ 1.08369019e+01,  1.28286679e+01],\n       [ 5.85242755e+00,  1.28260099e+01],\n       [-2.53650233e-09,  1.30457870e+01],\n       [ 2.49310302e+00,  1.27637902e+01],\n       [-9.13914054e-10,  1.27890647e+01],\n       [ 1.19440017e+00,  1.29894214e+01],\n       [-5.49265493e-10,  1.30485092e+01]])\n\n\n\nC\n\narray([[ 1.52713520e-01,  1.58149369e+01],\n       [ 4.52202894e-01,  1.58153740e+01],\n       [ 5.73957401e-10,  1.56441552e+01],\n       [-1.73226998e-12,  1.54265628e+01],\n       [-1.68743313e-10,  1.57148110e+01]])\n\n\n\nnp.einsum('ir, jr, kr -&gt;ijk', A, B, C)\n\narray([[[10.48458589, 10.83565147, 10.19442928, 10.05263637,\n         10.24047173],\n        [10.22977218, 10.31196443, 10.07798707,  9.93781374,\n         10.12350361],\n        [10.48852243, 10.4888123 , 10.37525943, 10.23095138,\n         10.42211858],\n        [10.26356388, 10.66498719,  9.95038983,  9.81199123,\n          9.99533009],\n        [10.16739655, 10.38431148,  9.94832818,  9.80995826,\n          9.99325913],\n        [10.22925871, 10.22954142, 10.11879543,  9.97805451,\n         10.16449628],\n        [10.05520169, 10.1477645 ,  9.90006825,  9.76236957,\n          9.94478124],\n        [10.02796162, 10.02823877,  9.9196721 ,  9.78170076,\n          9.96447363],\n        [10.20760682, 10.25210095, 10.07507622,  9.93494338,\n         10.12057961],\n        [10.2313932 , 10.23167597, 10.12090687,  9.98013658,\n         10.16661726]],\n\n       [[10.18116321, 10.18144459, 10.07121931,  9.93114011,\n         10.11670529],\n        [10.06487253, 10.0651507 ,  9.95618442,  9.81770523,\n         10.00115085],\n        [10.36175805, 10.36204442, 10.24986394, 10.1073    ,\n         10.29615676],\n        [ 9.93744133,  9.93771598,  9.83012932,  9.69340341,\n          9.87452643],\n        [ 9.93538237,  9.93565696,  9.82809259,  9.69139501,\n          9.87248051],\n        [10.10562779, 10.10590708,  9.99649957,  9.85745964,\n         10.04164808],\n        [ 9.88718524,  9.88745849,  9.78041593,  9.64438148,\n          9.82458851],\n        [ 9.90676358,  9.90703738,  9.79978285,  9.66347903,\n          9.84404291],\n        [10.06196547, 10.06224355,  9.95330875,  9.81486955,\n          9.99826219],\n        [10.10773648, 10.10801583,  9.99858549,  9.85951655,\n         10.04374343]],\n\n       [[10.68660524, 11.24817743, 10.28808747, 10.14499189,\n         10.33455292],\n        [10.34843597, 10.47978553, 10.17057548, 10.02911436,\n         10.2165102 ],\n        [10.58488251, 10.58517505, 10.47057894, 10.32494511,\n         10.5178686 ],\n        [10.47872824, 11.12088193, 10.04180598,  9.90213589,\n         10.08715912],\n        [10.32608231, 10.67300395, 10.0397254 ,  9.90008425,\n         10.08506914],\n        [10.32323689, 10.32352219, 10.21175876, 10.06972482,\n         10.25787948],\n        [10.17538798, 10.3233344 ,  9.99102209,  9.85205835,\n         10.03614586],\n        [10.12009045, 10.12037014, 10.01080605,  9.87156714,\n         10.05601918],\n        [10.314708  , 10.38573678, 10.16763789, 10.02621762,\n         10.21355934],\n        [10.32539099, 10.32567635, 10.2138896 , 10.07182603,\n         10.26001994]],\n\n       [[10.11425681, 10.11453634, 10.00503541,  9.86587676,\n         10.05022247],\n        [ 9.99873034,  9.99900668,  9.89075648,  9.75318732,\n          9.93542741],\n        [10.29366485, 10.29394934, 10.18250606, 10.040879  ,\n         10.22849466],\n        [ 9.87213657,  9.87240941,  9.76552976,  9.62970236,\n          9.80963512],\n        [ 9.87009114,  9.87036392,  9.76350642,  9.62770716,\n          9.80760264],\n        [10.03921777, 10.03949523,  9.9308067 ,  9.79268048,\n          9.97565852],\n        [ 9.82221073,  9.82248219,  9.71614307,  9.58100258,\n          9.76002537],\n        [ 9.84166042,  9.84193242,  9.73538272,  9.59997463,\n          9.77935192],\n        [ 9.99584238,  9.99611864,  9.88789971,  9.75037028,\n          9.93255774],\n        [10.04131261, 10.04159012,  9.93287891,  9.79472388,\n          9.97774009]],\n\n       [[10.20768094, 10.53855895,  9.93069359,  9.79256895,\n          9.97554489],\n        [ 9.96379984, 10.04127254,  9.8172638 ,  9.68071684,\n          9.86160281],\n        [10.21717838, 10.21746076, 10.10684556,  9.96627084,\n         10.15249244],\n        [ 9.99156385, 10.36990232,  9.69296758,  9.55814943,\n          9.73674521],\n        [ 9.90086298, 10.10530773,  9.69095927,  9.55616906,\n          9.73472783],\n        [ 9.96462196,  9.96489736,  9.85701643,  9.71991656,\n          9.90153498],\n        [ 9.7935781 ,  9.88082458,  9.64394785,  9.50981151,\n          9.68750408],\n        [ 9.76853255,  9.76880252,  9.66304454,  9.52864259,\n          9.70668703],\n        [ 9.94281649,  9.98475981,  9.81442826,  9.67792073,\n          9.85875446],\n        [ 9.96670123,  9.96697669,  9.85907325,  9.72194476,\n          9.90360109]],\n\n       [[10.21044653, 10.21072872, 10.1001864 ,  9.95970431,\n         10.14580321],\n        [10.09382137, 10.09410034,  9.98482065,  9.84594316,\n         10.02991642],\n        [10.3915608 , 10.39184799, 10.27934486, 10.13637088,\n         10.32577082],\n        [ 9.96602365,  9.96629909,  9.85840299,  9.72128382,\n          9.9029278 ],\n        [ 9.96395877,  9.96423415,  9.8563604 ,  9.71926965,\n          9.90087599],\n        [10.13469385, 10.13497395, 10.02525176,  9.88581192,\n         10.07053013],\n        [ 9.91562301,  9.91589705,  9.80854661,  9.67212089,\n          9.85284625],\n        [ 9.93525767,  9.93553225,  9.82796924,  9.69127337,\n          9.87235659],\n        [10.09090595, 10.09118483,  9.98193671,  9.84309933,\n         10.02701945],\n        [10.13680861, 10.13708877, 10.02734368,  9.88787475,\n         10.0726315 ]],\n\n       [[10.00472845, 10.00500495,  9.89668982,  9.75903813,\n          9.94138755],\n        [ 9.89045303,  9.89072638,  9.78364843,  9.64756902,\n          9.82783562],\n        [10.18219366, 10.18247507, 10.07223863,  9.93214525,\n         10.11772921],\n        [ 9.76523015,  9.76550004,  9.65977781,  9.5254213 ,\n          9.70340554],\n        [ 9.76320687,  9.7634767 ,  9.65777638,  9.5234477 ,\n          9.70139507],\n        [ 9.93050202,  9.93077647,  9.82326494,  9.68663451,\n          9.86763105],\n        [ 9.71584497,  9.71611349,  9.61092593,  9.47724889,\n          9.65433302],\n        [ 9.73508404,  9.73535309,  9.62995723,  9.49601549,\n          9.67345028],\n        [ 9.88759634,  9.88786961,  9.78082259,  9.64478249,\n          9.82499702],\n        [ 9.93257417,  9.93284868,  9.82531472,  9.68865578,\n          9.86969008]],\n\n       [[10.14502635, 10.4951919 ,  9.85898582,  9.72185855,\n          9.90351326],\n        [ 9.89443406,  9.97640907,  9.74637509,  9.61081411,\n          9.79039394],\n        [10.14340196, 10.1436823 , 10.03386583,  9.89430618,\n         10.0791831 ],\n        [ 9.93205681, 10.33245243,  9.62297639,  9.48913174,\n          9.66643791],\n        [ 9.83619694, 10.05255267,  9.62098258,  9.48716567,\n          9.6644351 ],\n        [ 9.8926692 ,  9.89294261,  9.78584067,  9.64973077,\n          9.83003776],\n        [ 9.7257684 ,  9.81808778,  9.57431062,  9.44114286,\n          9.61755234],\n        [ 9.69799572,  9.69826374,  9.59326942,  9.45983796,\n          9.63659677],\n        [ 9.87241434,  9.91678695,  9.74356002,  9.60803819,\n          9.78756615],\n        [ 9.89473346,  9.89500692,  9.78788264,  9.65174433,\n          9.83208895]],\n\n       [[10.17339432, 10.17367549, 10.06353431,  9.92356201,\n         10.10898558],\n        [10.05719238, 10.05747033,  9.9485872 ,  9.81021368,\n          9.99351932],\n        [10.35385135, 10.3541375 , 10.24204263, 10.09958747,\n         10.28830012],\n        [ 9.92985842,  9.93013285,  9.82262829,  9.68600671,\n          9.86699153],\n        [ 9.92780103,  9.92807541,  9.82059312,  9.68399985,\n          9.86494716],\n        [10.09791654, 10.09819562,  9.98887159,  9.84993776,\n         10.03398565],\n        [ 9.87964067,  9.87991372,  9.77295283,  9.63702219,\n          9.81709171],\n        [ 9.89920408,  9.89947766,  9.79230498,  9.65610517,\n          9.83653126],\n        [10.05428753, 10.0545654 ,  9.94571372,  9.80738017,\n          9.99063287],\n        [10.10002362, 10.10030276,  9.99095592,  9.8519931 ,\n         10.0360794 ]],\n\n       [[10.20812322, 10.20840535, 10.09788818,  9.95743805,\n         10.14349461],\n        [10.0915246 , 10.0918035 ,  9.98254868,  9.84370279,\n         10.02763418],\n        [10.38919627, 10.3894834 , 10.27700587, 10.13406442,\n         10.32342127],\n        [ 9.96375596,  9.96403133,  9.85615978,  9.71907182,\n          9.90067446],\n        [ 9.96169154,  9.96196686,  9.85411766,  9.7170581 ,\n          9.89862311],\n        [10.13238778, 10.13266781, 10.02297059,  9.88356248,\n         10.06823865],\n        [ 9.91336678,  9.91364076,  9.80631475,  9.66992007,\n          9.8506043 ],\n        [ 9.93299697,  9.93327149,  9.82573295,  9.68906819,\n          9.87011021],\n        [10.08860984, 10.08888866,  9.97966539,  9.84085961,\n         10.02473787],\n        [10.13450206, 10.13478215, 10.02506203,  9.88562483,\n         10.07033954]]])"
  },
  {
    "objectID": "notebooks/dt_weighted.html",
    "href": "notebooks/dt_weighted.html",
    "title": "Weighted Decision Trees",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom latexify import latexify, format_axes\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n!which latex\n\n/usr/local/bin/latex\n\n\n\nlatexify(columns=2)\n\n\n# Dummy Data\nx1 = np.array([1, 3, 2, 5, 7, 8])\nx2 = np.array([1.5, 3, 5, 2, 4, 4.5])\ncategory = np.array([0, 1, 1, 1, 0, 0]) # 0 -&gt; - class and 1 -&gt; + class\n\n# Separate data points for each class\nclass_0_x1 = x1[category == 0]\nclass_0_x2 = x2[category == 0]\nclass_1_x1 = x1[category == 1]\nclass_1_x2 = x2[category == 1]\n\n# Create a scatter plot for - class with blue color\nplt.scatter(class_0_x1, class_0_x2, color='blue', marker='_')\n\n# Create a scatter plot for + class with red color\nplt.scatter(class_1_x1, class_1_x2, color='red', marker='+')\n\n\nplt.xlim(0, 10)\nplt.ylim(0, 6)\nplt.xlabel('$X1$')\nplt.ylabel('$X2$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/dt_weighted/fig1.pdf\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\np = np.array([0.3, 0.1, 0.1, 0.3, 0.1, 0.1])\n\n# Create a scatter plot for - class with blue color\nplt.scatter(class_0_x1, class_0_x2, color='blue', marker='_')\n\n# Create a scatter plot for + class with red color\nplt.scatter(class_1_x1, class_1_x2, color='red', marker='+')\n\nfor i, txt in enumerate(p):\n    plt.annotate(txt, (x1[i], x2[i]), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n\n# Add labels and title\nplt.xlim(0, 10)\nplt.ylim(0, 6)\nplt.xlabel('$X1$')\nplt.ylabel('$X2$')\n\nformat_axes(plt.gca())\nplt.savefig(\"../figures/dt_weighted/fig2.pdf\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n# Create a scatter plot for - class with blue color\nplt.scatter(class_0_x1, class_0_x2, color='blue', marker='_')\n\n# Create a scatter plot for + class with red color\nplt.scatter(class_1_x1, class_1_x2, color='red', marker='+')\n\nplt.axvline(x=4, color='black', linestyle='--')\n\nfor i, txt in enumerate(p):\n    plt.annotate(txt, (x1[i], x2[i]), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n\n# Add labels and title\nplt.xlim(0, 10)\nplt.ylim(0, 6)\nplt.xlabel('$X1$')\nplt.ylabel('$X2$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/dt_weighted/fig3.pdf\")\nplt.show()\n\n\n\n\n\n\n\n\n\np = np.array([0.3, 0.1, 0.1, 0.3, 0.1, 0.1])\n\n# Create a scatter plot for - class with blue color\nplt.scatter(class_0_x1, class_0_x2, color='blue', marker='_')\n\n# Create a scatter plot for + class with red color\nplt.scatter(class_1_x1, class_1_x2, color='red', marker='+')\n\nplt.axvline(x=4, color='black', linestyle='--')\n\nfor i, txt in enumerate(p):\n    plt.annotate(txt, (x1[i], x2[i]), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n\n\nplt.fill_betweenx(y=np.arange(0, 6, 0.01), x1=0, x2= 4, color='red', alpha=0.3)\n\n# Add labels and title\nplt.xlim(0, 10)\nplt.ylim(0, 6)\nplt.xlabel('$X1$')\nplt.ylabel('$X2$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/dt_weighted/fig4.pdf\")\nplt.show()\n\n\n\n\n\n\n\n\n\np = np.array([0.3, 0.1, 0.1, 0.3, 0.1, 0.1])\n\n# Create a scatter plot for - class with blue color\nplt.scatter(class_0_x1, class_0_x2, color='blue', marker='_')\n\n# Create a scatter plot for + class with red color\nplt.scatter(class_1_x1, class_1_x2, color='red', marker='+')\n\nplt.axvline(x=4, color='black', linestyle='--')\n\nfor i, txt in enumerate(p):\n    plt.annotate(txt, (x1[i], x2[i]), textcoords=\"offset points\", xytext=(0, 5), ha='center')\n\n\nplt.fill_betweenx(y=np.arange(0, 6, 0.01), x1=4, x2= 10, color='blue', alpha=0.3)\n\n# Add labels and title\nplt.xlim(0, 10)\nplt.ylim(0, 6)\nplt.xlabel('$X1$')\nplt.ylabel('$X2$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/dt_weighted/fig5.pdf\")\nplt.show()"
  },
  {
    "objectID": "notebooks/rule-based-vs-ml.html",
    "href": "notebooks/rule-based-vs-ml.html",
    "title": "Traditional Programming vs Machine Learning",
    "section": "",
    "text": "import torch\nimport torchvision\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom latexify import latexify\nimport seaborn as sns\n%matplotlib inline\n# config retina\n%config InlineBackend.figure_format = 'retina'\n\n\n# Set device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda')\n\n\n\n# Load MNIST dataset\nmnist_train = torchvision.datasets.MNIST('../datasets', train=True, transform=torchvision.transforms.ToTensor(), download=True)\nmnist_test = torchvision.datasets.MNIST('../datasets', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n\n\n\n# Function to show a digit marking 28x28 grid with arrows pointing to random pixels\ndef show_digit_with_arrows(digit, label=None):\n    fig, ax = plt.subplots()\n    digit = digit.numpy().reshape(28, 28)\n\n    # Display the digit\n    ax.imshow(digit, cmap='gray')\n\n    # Add gridlines corresponding to 28 rows and columns\n    for i in range(1, 28):\n        ax.axhline(i, color='white', linewidth=0.5)\n        ax.axvline(i, color='white', linewidth=0.5)\n\n    # Display label if available\n    if label is not None:\n        ax.set_title(f'Label: {label}')\n    return fig, ax\n\n\nindex = 2\n# Show a random digit with arrows pointing to random 10 pixels\nfig, ax = show_digit_with_arrows(*mnist_train[index])\n# save figure\nfig.savefig(\"../figures/mnist.pdf\", bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n# Find indices of digit 4 in the training set\ndigit_4_indices_train = torch.where(torch.tensor(mnist_train.targets) == 4)[0]\ndigit_4_indices_test = torch.where(torch.tensor(mnist_test.targets) == 4)[0]\n\nprint(f\"Indices of digit 4 in Train dataset: {digit_4_indices_train}\")\nprint(f\"Number of digit 4 images in training set: {len(digit_4_indices_train)}\\n\")\n\nIndices of digit 4 in Train dataset: tensor([    2,     9,    20,  ..., 59943, 59951, 59975])\nNumber of digit 4 images in training set: 5842\n\n\n\n/tmp/ipykernel_1361527/214778730.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  digit_4_indices_train = torch.where(torch.tensor(mnist_train.targets) == 4)[0]\n/tmp/ipykernel_1361527/214778730.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  digit_4_indices_test = torch.where(torch.tensor(mnist_test.targets) == 4)[0]\n\n\n\nlatexify(fig_width=7, fig_height=5)\n\nfor i in range(15):\n    plt.subplot(3, 5, i+1)\n    plt.imshow(mnist_train.data[digit_4_indices_train[i]], cmap='gray')\n    plt.title(f\"idx: {digit_4_indices_train[i]}\")\n    plt.axis('off')\n\n\n\n\n\n\n\n\n\n# Select a sample from the training set\nsample_idx_1 = 60\nimage, label = mnist_train[sample_idx_1]\nplt.imshow(image.squeeze().numpy(), cmap='gray')\nplt.title(f\"Label: {label}\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Function to extract edges based on intensity threshold\ndef extract_edges(image, threshold=0.1):\n    '''\n    Input:\n        image: torch.tensor of shape (28, 28)\n        threshold: (float) the minimum intensity value to be considered as white pixel\n    '''\n    edges = torch.zeros_like(image)\n\n    # converting all the pixels with intensity greater than threshold to white\n    edges[image &gt; threshold] = 1.0\n    return edges\n\n\n# Creating rules based upon one image\nedges = extract_edges(image)\n\nplt.imshow(edges[0, :, :], cmap='gray')\n\n# finding areas of edges\nleft_edge_train = edges[:, 4:15, 3:12]\nupper_right_edge_train = edges[:, 4:19, 17:24]\nmiddle_edge_train = edges[:, 14:20, 5:25]\nlower_right_edge_train = edges[:, 17:24, 18:24]\n\n\n# R1 (4-15, 3-12)\nr1 = plt.Rectangle((3, 4), 9, 11, linewidth=1, edgecolor='r', facecolor='none')\nr2 = plt.Rectangle((17, 4), 7, 15, linewidth=1, edgecolor='g', facecolor='none')\nr3 = plt.Rectangle((5, 14), 20, 6, linewidth=1, edgecolor='b', facecolor='none')\nr4 = plt.Rectangle((18, 17), 6, 7, linewidth=1, edgecolor='y', facecolor='none')\nfor rect in [r1, r2, r3, r4]:\n    plt.gca().add_patch(rect)\n\n\n\n\n\n\n\n\n\n\n# creat a subplot 2 rows by 2 columns\nfig, axs = plt.subplots(2, 2, figsize=(8, 10))\n\n# plotting the images\naxs[0, 0].imshow(left_edge_train.squeeze().numpy(), cmap='gray')\naxs[0, 1].imshow(upper_right_edge_train.squeeze().numpy(), cmap='gray')\naxs[1, 0].imshow(middle_edge_train.squeeze().numpy(), cmap='gray')\naxs[1, 1].imshow(lower_right_edge_train.squeeze().numpy(), cmap='gray')\n\naxs[0, 0].set_title(f\"Left Edge\\nWhite pixels: {int(left_edge_train.sum())}/{left_edge_train.numel()}\")\naxs[0, 1].set_title(f\"Upper Right Edge\\nWhite pixels: {int(upper_right_edge_train.sum())}/{upper_right_edge_train.numel()}\")\naxs[1, 0].set_title(f\"Middle Edge\\nWhite pixels: {int(middle_edge_train.sum())}/{middle_edge_train.numel()}\")\naxs[1, 1].set_title(f\"Lower Right Edge\\nWhite pixels: {int(lower_right_edge_train.sum())}/{lower_right_edge_train.numel()}\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Rule-based digit classifier for digit 4\ndef rule_based_classifier(image):\n    # Extract edges\n    edges = extract_edges(image)\n\n    # Define rules for digit 4 based on the edges of the digit\n    left_edge = edges[:, 4:15, 3:12]\n    upper_right_edge = edges[:, 4:19, 17:24]\n    middle_edge = edges[:, 14:20, 5:25]\n    lower_right_edge = edges[:, 17:24, 18:24]\n\n    # Check if all required edges are present by checking the number of white pixels for each edge.\n    # The number of white pixels for each edge is 'sub' less than the number of pixels in the edge for the above take digit.\n    sub = 10\n    if torch.sum(left_edge) &gt; left_edge_train.sum() - sub and torch.sum(upper_right_edge) &gt; upper_right_edge_train.sum() - sub and torch.sum(middle_edge) &gt; middle_edge_train.sum() - sub and torch.sum(lower_right_edge) &gt; lower_right_edge_train.sum() - sub:\n        return 4\n    else:\n        return -1 # -1 indicates that the digit is not 4\n\n\n# Display some wrongly classified images\n\nindices = [6, 19, 25, 200]\n# define image size\nplt.figure(figsize=(14, 3))\n\nfor i in range(4):\n    plt.subplot(1, 4, i+1)\n    image, label = mnist_test[indices[i]]\n    pred = rule_based_classifier(image)\n    pred = pred if pred != -1 else \"Not 4\"\n    plt.title(f\"Label: {label}, Predicted: {pred}\")\n    plt.imshow(image.squeeze().numpy(), cmap='gray')\n\n\n\n\n\n\n\n\n\n# Evaluating the rule-based classifier\ncount = 0\ncount_4 = 0\nfor i, (image, label) in enumerate(mnist_test):\n    classification = rule_based_classifier(image)\n    if (classification == 4 and label == 4) or (classification == -1 and label != 4):\n        count += 1\n    if (classification == 4 and label == 4):\n        count_4 += 1\n\naccuracy_rule = count * 100/ len(mnist_test)\npercentage_TP_rule = count_4 * 100/ len(digit_4_indices_test)\nprint(f\"Accuracy of the rule-based classifier: {accuracy_rule} %\")\nprint(f\"Percentage of 4s actually classified as 4 (percentage of True Positives): {percentage_TP_rule:.3} %\")\n\nAccuracy of the rule-based classifier: 88.56 %\nPercentage of 4s actually classified as 4 (percentage of True Positives): 4.28 %\n\n\nNote: As per rules, it is predicting most of the digits as non-4 for most of the digits. And since the number of non-4 digits are much more compared to number of instances of the digit 4, the accuracy is high. But this is not a good model as it is not predicting the digit 4 correctly.\n\nML based approach\n\n# Flatten the images and convert the labels to 4 and -1 for binary classification problem\nX_train = mnist_train.data.numpy().reshape((len(mnist_train), -1))\ny_train = np.where(mnist_train.targets.numpy() == 4, 4, -1)\n\nX_test = mnist_test.data.numpy().reshape((len(mnist_test), -1))\ny_test = np.where(mnist_test.targets.numpy() == 4, 4, -1)\n\n\n# Create and train the MLP model\nmlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=20, random_state=42)\nmlp_model.fit(X_train, y_train)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n  warnings.warn(\n\n\nMLPClassifier(max_iter=20, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPClassifierMLPClassifier(max_iter=20, random_state=42)\n\n\n\n# Evaluate the model\ny_pred = mlp_model.predict(X_test)\naccuracy_ML = accuracy_score(y_test,( y_pred))\naccuracy_ML = accuracy_ML * 100\npercentage_TP_ML = np.sum((y_test == 4) & (y_pred == 4)) * 100 / len(digit_4_indices_test)\nprint(f'Test Accuracy: {accuracy_ML:.2f}%')\nprint(f\"Percentage of 4s actually classified as 4 (percentage of True Positives): {percentage_TP_ML:.3} %\")\n\nTest Accuracy: 99.47%\nPercentage of 4s actually classified as 4 (percentage of True Positives): 97.1 %\n\n\n\n\nComparison of Rule-based system and ML based system\n\n# Categories for the bar plot\ncategories = ['Accuracy', 'True Positive Percentage']\n\n# Values for the rule-based classifier\nrule_based_values = [accuracy_rule, percentage_TP_rule]\n\n# Values for the MLP classifier\nmlp_values = [accuracy_ML, percentage_TP_ML]\n\n# Bar width\nbar_width = 0.35\n\n# X-axis positions for the bars\nindex = range(len(categories))\n\n# Plotting the bar plot\nfig, ax = plt.subplots(figsize=(9, 5))\nbar1 = ax.bar(index, rule_based_values, bar_width, label='Rule-Based Classifier')\nbar2 = ax.bar([i + bar_width for i in index], mlp_values, bar_width, label='MLP Classifier')\n\n# Adding labels, title, and legend\nax.set_xlabel('Metrics')\nax.set_ylabel('Percentage / Accuracy')\nax.set_title('Comparison of Classifiers')\nax.set_xticks([i + bar_width / 2 for i in index])\nax.set_xticklabels(categories)\nax.legend()\n\n# Display the values on top of the bars\nfor bar in bar1 + bar2:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "notebooks/Ridge.html",
    "href": "notebooks/Ridge.html",
    "title": "Question",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib\n\n/tmp/ipykernel_2413694/3589217045.py:3: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify()\n\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(60,300,4)])\nnp.random.seed(10)  #Setting seed for reproducability\ny = 4*x + 7 + np.random.normal(0,3,len(x))\ny_true = 4*x + 7\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\ndata[\"ones\"] = 1\nplt.plot(data['x'],data['y'],'.', label='Data Points')\nplt.plot(data['x'], y_true,'g', label='True Function')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nformat_axes(plt.gca())\nplt.savefig('lin_1.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 16\n     14 plt.ylabel(\"y\")\n     15 plt.legend()\n---&gt; 16 format_axes(plt.gca())\n     17 plt.savefig('lin_1.pdf', transparent=True, bbox_inches=\"tight\")\n\nNameError: name 'format_axes' is not defined\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfor i, deg in enumerate([1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 15, 20, 25]):\n    poly = PolynomialFeatures(degree=deg, include_bias=False)\n    X_ = poly.fit_transform(x.reshape(-1, 1))\n    from sklearn.linear_model import LinearRegression\n    clf = LinearRegression()\n    clf.fit(X_, data['y'])\n    print(X_.shape)\n    y_pred = clf.predict(X_)\n    plt.figure()\n    plt.plot(data['x'], y_pred, label='Degree: {}'.format(deg))\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.scatter(x, y)\n    plt.legend()\n    train_rmse = np.sqrt(np.mean((y_pred - y)**2))\n    highest_coef = np.max([np.max(clf.coef_), clf.intercept_])\n    sqrt_sum_squares_coef = np.sqrt(np.sum(clf.coef_**2) + clf.intercept_**2)\n    plt.title('RMSE: {:.2f}\\n Highest Coef: {:.2f}\\n Sqrt Sum of Squares of Coef: {:.2f}'.format(train_rmse, highest_coef, sqrt_sum_squares_coef))\n    #format_axes(plt.gca())\n    #plt.savefig('lin_{}.pdf'.format(i+2), transparent=True, bbox_inches=\"tight\")\n\n(60, 1)\n(60, 2)\n(60, 3)\n(60, 4)\n(60, 5)\n(60, 6)\n(60, 7)\n(60, 8)\n(60, 10)\n(60, 12)\n(60, 15)\n(60, 20)\n(60, 25)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\ncoef_list = []\nfor i,deg in enumerate([1,3,6,11]):\n    predictors = ['ones','x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=False, fit_intercept=False)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n    coef_list.append(abs(max(regressor.coef_, key=abs)))\n\n    plt.scatter(data['x'],data['y'], label='Train')\n    plt.plot(data['x'], y_pred,'k', label='Prediction')\n    plt.plot(data['x'], y_true,'g.', label='True Function')\n    format_axes(plt.gca())\n    plt.legend() \n    plt.title(f\"Degree: {deg} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n    plt.savefig('lin_plot_{}.pdf'.format(deg), transparent=True, bbox_inches=\"tight\")\n    plt.clf()\n\n&lt;Figure size 244.08x150.85 with 0 Axes&gt;\n\n\n\nplt.semilogy([1,3,6,11],coef_list,'o-k')\nplt.xticks([1,3,6,11])\nplt.xlabel('Degree')\nplt.ylabel('Max Coef')\nformat_axes(plt.gca())\nplt.savefig('lin_plot_coef.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\ndef cost(theta_0, theta_1, x, y):\n    s = 0\n    for i in range(len(x)):\n        y_i_hat = x[i]*theta_1 + theta_0\n        s += (y[i]-y_i_hat)**2\n    return s/len(x)\n\nx_grid, y_grid = np.mgrid[-4:15:.2, -4:15:.2]\n\ncost_matrix = np.zeros_like(x_grid)\nfor i in range(x_grid.shape[0]):\n    for j in range(x_grid.shape[1]):\n        cost_matrix[i, j] = cost(x_grid[i, j], y_grid[i, j], data['x'], data['y'])\n\n\nfrom matplotlib.patches import Circle\n\nlevels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\n\nplt.contourf(x_grid, y_grid, cost_matrix, levels,alpha=.4)\nplt.colorbar()\nplt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\nplt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\nCS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\nplt.clabel(CS, inline=1, fontsize=8)\nplt.title(\"Least squares objective function\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\np1 = Circle((0, 0), 3, color='g', label=r'$\\theta_0^2+\\theta_1^2=3$')\nplt.scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\nplt.gca().add_patch(p1)\nplt.legend()\nplt.gca().set_aspect('equal')\nformat_axes(plt.gca())\nplt.savefig('ridge_base_contour.pdf', transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(7,7))\nax = plt.axes(projection='3d')\n\nax = plt.axes(projection='3d')\nax.plot_surface(x_grid, y_grid, cost_matrix,cmap='viridis', edgecolor='none')\nax.set_title('Least squares objective function');\nax.set_xlabel(r\"$\\theta_0$\")\nax.set_ylabel(r\"$\\theta_1$\")\nax.set_xlim([-4,15])\nax.set_ylim([-4,15])\n\nu = np.linspace(0, np.pi, 30)\nv = np.linspace(0, 2 * np.pi, 30)\n\n# x = np.outer(500*np.sin(u), np.sin(v))\n# y = np.outer(500*np.sin(u), np.cos(v))\n# z = np.outer(500*np.cos(u), np.ones_like(v))\n# ax.plot_wireframe(x, y, z)\n\nax.view_init(45, 120)\nplt.savefig('ridge_base_surface.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nlatexify(fig_width=5, fig_height=2.5)\nfrom sklearn.linear_model import Ridge\n\nfor alpha in [1, 10, 1000]:\n    fig, ax = plt.subplots(nrows=1, ncols=2)\n    \n    deg = 1\n    predictors = ['ones','x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = Ridge(alpha=alpha,normalize=True, fit_intercept=False)\n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n    format_axes(ax[0])\n    format_axes(ax[1])\n    # Plot\n    ax[0].scatter(data['x'],data['y'], label='Train')\n    ax[0].plot(data['x'], y_pred,'k', label='Prediction')\n    ax[0].plot(data['x'], y_true,'g.', label='True Function')\n    ax[0].legend() \n    ax[0].set_title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coef: {max(regressor.coef_, key=abs):.2f}\")\n\n    # Circle\n    p1 = Circle((0, 0), np.sqrt(regressor.coef_.T@regressor.coef_), alpha=0.6, color='g', label=r'$\\theta_0^2+\\theta_1^2={:.2f}$'.format(np.sqrt(regressor.coef_.T@regressor.coef_)))\n    ax[1].add_patch(p1)\n\n    # Contour\n    levels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\n    ax[1].contourf(x_grid, y_grid, cost_matrix, levels,alpha=.3)\n    #ax[1].colorbar()\n    ax[1].axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\n    ax[1].axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\n    CS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\n    ax[1].clabel(CS, inline=1, fontsize=8)\n    ax[1].set_title(\"Least squares objective function\")\n    ax[1].set_xlabel(r\"$\\theta_0$\")\n    ax[1].set_ylabel(r\"$\\theta_1$\")\n    ax[1].scatter(regressor.coef_[0],regressor.coef_[1] ,marker='x', color='r',s=25,label='Ridge Solution')\n    ax[1].scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\n    ax[1].set_xlim([-4,15])\n    ax[1].set_ylim([-4,15])\n    ax[1].legend()\n    ax[1].set_aspect('equal')\n    plt.savefig('ridge_{}.pdf'.format(alpha), transparent=True, bbox_inches=\"tight\")\n    plt.show()\n    plt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 360x180 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 360x180 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 360x180 with 0 Axes&gt;\n\n\n\nlatexify()\nfrom sklearn.linear_model import Ridge\n\nfor i,deg in enumerate([19]):\n    predictors = ['ones', 'x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for i,alpha in enumerate([1, 1e7]):\n        regressor = Ridge(alpha=alpha,normalize=False, fit_intercept=False)\n        regressor.fit(data[predictors],data['y'])\n        y_pred = regressor.predict(data[predictors])\n        plt.scatter(data['x'],data['y'], label='Train')\n        plt.plot(data['x'], y_pred,'k', label='Prediction')\n        plt.plot(data['x'], y_true,'g.', label='True Function')\n        plt.legend() \n        format_axes(plt.gca())\n        plt.title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n        plt.savefig('ridge_{}_{}.pdf'.format(alpha, deg), transparent=True, bbox_inches=\"tight\")\n        plt.show()\n        plt.clf()\n\n\n\n\n\n\n\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.62699e-21): result may not be accurate.\n  overwrite_a=True).T\n\n\n\n\n\n\n\n\n\n&lt;Figure size 244.08x150.85 with 0 Axes&gt;\n\n\n\n# from sklearn.linear_model import Ridge\n\n# for i,deg in enumerate([2,4,8,16]):\n#   predictors = ['x']\n#   if deg &gt;= 2:\n#     predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n#   fig, ax = plt.subplots(nrows=1, ncols=4, sharey=True, figsize=(20, 4))\n\n#   for i,alpha in enumerate([1e-15,1e-4,1,20]):\n#     regressor = Ridge(alpha=alpha,normalize=True)\n#     regressor.fit(data[predictors],data['y'])\n#     y_pred = regressor.predict(data[predictors])\n#     ax[i].scatter(data['x'],data['y'], label='Train')\n#     ax[i].plot(data['x'], y_pred,'k', label='Prediction')\n#     ax[i].plot(data['x'], y_true,'g.', label='True Function')\n#     ax[i].legend() \n#     ax[i].set_title(f\"Degree: {deg} | Alpha: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n\n\nimport pandas as pd\n\ndata = pd.read_excel(\"data.xlsx\")\ncols = data.columns\nalph_list = np.logspace(-2,10,num=20, endpoint=False)\ncoef_list = []\n\nfor i,alpha in enumerate(alph_list):\n    regressor = Ridge(alpha=alpha,normalize=True)\n    regressor.fit(data[cols[1:-1]],data[cols[-1]])\n    coef_list.append(regressor.coef_)\n\ncoef_list = np.abs(np.array(coef_list).T)\nfor i in range(len(cols[1:-1])):\n    plt.loglog(alph_list, coef_list[i] , label=r\"$\\theta_{}$\".format(i))\nplt.xlabel('$\\mu$ value')\nplt.ylabel('Coefficient Value')\nplt.legend() \nformat_axes(plt.gca())\n\nlim = True\nif lim:\n    plt.ylim((10e-20, 100))\n    plt.savefig('rid_reg-without-lim.pdf', transparent=True, bbox_inches=\"tight\")\nelse:\n    plt.savefig('rid_reg-with-lim.pdf', transparent=True, bbox_inches=\"tight\")\n\n# plt.set_title(f\"Degree: {deg} | Alpha: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n\n\n\n\n\n\n\n\n\nplt.style.use('seaborn-whitegrid')\n\nx = [1,2,3,4]\ny = [1,2,3,0]\ny_1 = [(2-i/5) for i in x]\ny_2 = [(0.5+0.4*i) for i in x]\nplt.ylim(-0.2,3.3)\nplt.plot(x,y,'.')\nfor i in range(len(x)):\n    plt.text(x[i]-0.1, y[i]+0.1, \"({},{})\".format(x[i], y[i]))\n#plt.plot(x,y_1, label=\"unreg\")\n#plt.plot(x,y_2, label=\"reg\")\n#plt.legend()\n#format_axes(plt.gca())\nplt.savefig('temp.pdf', transparent=True)\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\nx_new = np.vstack([np.ones_like(x), x]).T\nregressor = LinearRegression(normalize=False, fit_intercept=False)  \nregressor.fit(x_new,y)\ny_pred = regressor.predict(x_new)\n\nfor i in range(len(x)):\n    plt.text(x[i]-0.1, y[i]+0.1, \"({},{})\".format(x[i], y[i]))\nplt.plot(x,y,'.', label='Data Points')\nplt.plot(x,y_pred,'-g', label='Unregularized Fit')\nplt.legend(loc='lower left')\n#format_axes(plt.gca())\nplt.savefig('q_unreg.pdf', transparent=True)\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nregressor = Ridge(alpha=4, normalize=False, fit_intercept=False)  \nregressor.fit(x_new,y)\ny_pred_r = regressor.predict(x_new)\n\nfor i in range(len(x)):\n    plt.text(x[i]-0.1, y[i]+0.1, \"({},{})\".format(x[i], y[i]))\ndef ridge_func(x):\n    return 0.56 + 0.26*np.array(x)\nplt.plot(x,y,'.', label='Data Points')\nplt.plot(x,y_pred,'-g', label='Unregularized Fit')\nplt.plot(x,ridge_func(x),'-b', label='Regularized Fit')\nplt.legend(loc='lower left')\n#format_axes(plt.gca())\nplt.savefig('q_reg.pdf', transparent=True)\n\n\n\n\n\n\n\n\n\nregressor.coef_\n\narray([0.37209302, 0.30232558])\n\n\n\nRetrying with a better example\n\nnp.linalg.inv([[4, 10], [10, 34]])@np.array([6, 14])\n\narray([ 1.77777778, -0.11111111])\n\n\n\nnp.linalg.inv([[8, 10], [10, 34]])@np.array([6, 14])\n\narray([0.37209302, 0.30232558])\n\n\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(60,600,10)])\nnp.random.seed(10)  #Setting seed for reproducability\ny = 4*x + 7 + np.random.normal(0,5,len(x))\ny_true = 4*x + 7\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\ndata[\"ones\"] = 1\nplt.plot(data['x'],data['y'],'.', label='Data Points')\nplt.plot(data['x'], y_true,'g', label='True Function')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\n#format_axes(plt.gca())\nplt.savefig('lin_1.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\n\nfor i,deg in enumerate([17]):\n    predictors = ['ones', 'x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for i,alpha in enumerate([1e-3, 1e7]):\n        regressor = Ridge(alpha=alpha,normalize=False)\n        regressor.fit(data[predictors],data['y'])\n        y_pred = regressor.predict(data[predictors])\n        plt.scatter(data['x'],data['y'], label='Train')\n        plt.plot(data['x'], y_pred,'k', label='Prediction')\n        plt.plot(data['x'], y_true,'g.', label='True Function')\n        plt.legend() \n        #format_axes(plt.gca())\n        #print(regressor.coef_)\n        plt.ylim([0,60])\n        plt.title(f\"Degree: {deg} | $\\mu$: {alpha}\")\n        plt.savefig('ridge_new_{}_{}.pdf'.format(i, deg), transparent=True, bbox_inches=\"tight\")\n        plt.show()\n        plt.clf()\n\n\n\n\n\n\n\n\n/Users/nipun/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/ridge.py:147: LinAlgWarning: Ill-conditioned matrix (rcond=1.11479e-28): result may not be accurate.\n  overwrite_a=True).T\n\n\n\n\n\n\n\n\n\n&lt;Figure size 244.08x150.85 with 0 Axes&gt;"
  },
  {
    "objectID": "notebooks/confusion-mnist.html",
    "href": "notebooks/confusion-mnist.html",
    "title": "Notion of Confusion in ML",
    "section": "",
    "text": "import numpy as np\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom sklearn.utils.multiclass import unique_labels\nimport torchvision\nimport torchvision.transforms as transforms\nfrom latexify import latexify\n%matplotlib inline\n# Retina\n%config InlineBackend.figure_format = 'retina'\n\n\n# Set device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda')\n\n\n\n# Define transformations\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n\n# Download and load MNIST dataset using torchvision\ntrain_dataset = torchvision.datasets.MNIST(root='../datasets', train=True, download=True, transform=transform)\ntest_dataset = torchvision.datasets.MNIST(root='../datasets', train=False, download=True, transform=transform)\n\n\n# Flatten the images for sklearn MLP\nX_train = train_dataset.data.numpy().reshape((len(train_dataset), -1))\ny_train = train_dataset.targets.numpy()\nX_test = test_dataset.data.numpy().reshape((len(test_dataset), -1))\ny_test = test_dataset.targets.numpy()\n\n# Standardize features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n\n# Plot few images\nlatexify()\nfig, axs = plt.subplots(1, 7, figsize=(8, 10))\nfor i in range(7):\n    axs[i].imshow(X_train[i].reshape((28, 28)), cmap='gray')\n    axs[i].set_title(y_train[i])\n    axs[i].axis('off')\n\n\n\n\n\n\n\n\n\n# Create and train the MLP model\nmlp_model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=20, random_state=42)\nmlp_model.fit(X_train_scaled, y_train)\n\n# Predict probabilities on the test set\ny_probabilities = mlp_model.predict_proba(X_test_scaled)\n\n# Predict on the test set\ny_pred = mlp_model.predict(X_test_scaled)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n\nAccuracy: 0.9735\n\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n  warnings.warn(\n\n\n\nlatexify(fig_width=6)\ncm = confusion_matrix(y_test, y_pred)\ncm_display = ConfusionMatrixDisplay(cm).plot(values_format='d', cmap='gray', ax=plt.gca())\n\n# Save the figure with a higher resolution and without lossy compression\nplt.savefig(\"../figures/mnist-cm.png\", bbox_inches=\"tight\", dpi=400, transparent=True)\n\n# Show the plot\n\n\n\n\n\n\n\n\n\n# Display classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.98      0.99      0.98       980\n           1       0.99      0.99      0.99      1135\n           2       0.97      0.96      0.97      1032\n           3       0.96      0.98      0.97      1010\n           4       0.98      0.97      0.98       982\n           5       0.98      0.97      0.97       892\n           6       0.97      0.97      0.97       958\n           7       0.97      0.98      0.97      1028\n           8       0.96      0.96      0.96       974\n           9       0.98      0.96      0.97      1009\n\n    accuracy                           0.97     10000\n   macro avg       0.97      0.97      0.97     10000\nweighted avg       0.97      0.97      0.97     10000\n\n\n\n\n# Display the first k wrong classified images with highest probabilities\n\n# Find indices of wrongly classified samples\nwrong_indices = np.where(y_pred != y_test)[0]\n\n# Sort wrong predictions by highest class probability\nsorted_indices = np.argsort(np.max(y_probabilities[wrong_indices], axis=1))[::-1]\n\nk = 9\nlatexify(fig_width=8)\nfor i, idx in enumerate(sorted_indices[:k]):\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(test_dataset[wrong_indices[idx]][0].numpy().squeeze(), cmap='gray')\n    plt.title(f'True: {y_test[wrong_indices[idx]]}, Pred: {y_pred[wrong_indices[idx]]}\\nProb: {np.max(y_probabilities[wrong_indices[idx]]):.1f}')\n\nplt.tight_layout()"
  },
  {
    "objectID": "notebooks/taylor.html",
    "href": "notebooks/taylor.html",
    "title": "Taylor’s Series",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\nf = lambda x, y: x**2 + y**2\n\ndef f(x, y):\n    return x**2 + y**2\n\nf_dash = torch.func.grad(f, argnums=(0, 1))\n\n\nf_dash\n\n&lt;function __main__.f(x, y)&gt;\n\n\n\ndef f2(argument):\n    x, y= argument\n    return x**2 + y**2\n\n\ntorch.func.grad(f2)(torch.tensor([1.0, 1.0]))\n\ntensor([2., 2.])\n\n\n\nf_dash(torch.tensor(1.0), torch.tensor(1.0))\n\n(tensor(2.), tensor(2.))\n\n\n\nx = torch.tensor(1.0, requires_grad=True)\ny = torch.tensor(1.0, requires_grad=True)\nprint(\"Before backward: \", x.grad, y.grad)\nz = f(x, y)\nz.backward()\nprint(\"After backward: \", x.grad, y.grad)\n\nBefore backward:  None None\nAfter backward:  tensor(2.) tensor(2.)\n\n\n\nf = lambda x: torch.cos(x)\n\n\nx_range = torch.arange(-2*np.pi, 2*np.pi, 0.01)\ny_range = f(x_range)\nplt.plot(x_range, y_range)\n\n\n\n\n\n\n\n\n\ndef nth_order_appx(f, n, x0=0.0, verbose=False):\n    x0 = torch.tensor(x0)\n    derivs = {1:torch.func.grad(f)}\n    vals = {0:f(x0), 1:derivs[1](x0)}\n    if verbose:\n        print(\"f(x0) = {}\".format(vals[0]))\n        print(\"f'(x0) = {}\".format(vals[1]))\n\n    for i in range(2, n+1):\n        derivs[i] = torch.func.grad(derivs[i-1])\n        vals[i] = derivs[i](x0)\n        if verbose:\n            d = \"'\"*i\n            print(\"f{}(x0) = {}\".format(d, vals[i]))\n    \n    def g(x):\n        x_diff = x - x0\n        str_rep = \"f(x) = f(x0) + \"\n        out = vals[0].repeat(x.shape)\n        for i in range(1, n+1):\n            str_rep += f\"{vals[i]} * (x-{x0.item()})^{i} / {i}! + \"\n            out += vals[i] * x_diff**i / torch.math.factorial(i)\n        if verbose:\n            print(\"--\"*40)\n            print(str_rep)\n        return out\n\n    return g\n        \n\n\nf = lambda x: torch.cos(x)\n_ = nth_order_appx(f, 1, 0.0, verbose=True)(x_range)\n\nf(x0) = 1.0\nf'(x0) = -0.0\n--------------------------------------------------------------------------------\nf(x) = f(x0) + -0.0 * (x-0.0)^1 / 1! + \n\n\n\nplt.plot(x_range, f(x_range), label=\"f(x) = cos(x)\")\nfor i in range(16, 19, 2):\n    plt.plot(x_range, nth_order_appx(f,  i, 0.0)(x_range), label=f\"order {i} appx\")\nplt.ylim(-2, 2)\n# legend outside\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\n\n\n\n\n\n\n\n\nf = lambda x: torch.cos(x)\nx0 = 3.14\nplt.plot(x_range, f(x_range))\nplt.scatter(x0, f(torch.tensor(x0)), c='r', label='x0')\nfor i in range(1, 11, 2):\n    \n    plt.plot(x_range, nth_order_appx(f,  i, 3.14)(x_range), label=f\"order {i}\")\nplt.ylim(-2, 2)\nplt.legend()\n\n\n\n\n\n\n\n\n\nf = lambda x: x**2 + 2\nplt.plot(x_range, f(x_range))\nx0 = 2.0\nplt.scatter(x0, f(torch.tensor(x0)), c='r', label='x0')\nplt.plot(x_range, nth_order_appx(f, 1, x0)(x_range), label=f\"order 1\")\n\n\n\n\n\n\n\n\n\nplt.plot(x_range, f(x_range), label=\"f(x) = x^2 + 2\", lw=3, alpha=0.5, ls='--')\nplt.scatter(x0, f(torch.tensor(x0)), c='r', label='x0')\nplt.plot(x_range, nth_order_appx(f, 1, x0)(x_range), label=f\"order 1\")\nplt.xlim(1.5, 2.5)\nplt.legend()\n\n\n\n\n\n\n\n\n\nf = lambda x: x**2 + 2\nplt.plot(x_range, f(x_range))\nx0 = 2.0\nplt.scatter(x0, f(torch.tensor(x0)), c='r', label='x0')\nplt.plot(x_range, nth_order_appx(f, 1, x0)(x_range), label=f\"order 1\")\n\n\n\n\n\n\n\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify(columns=2)\n\n\ndef plot_gd(alpha=0.1, iter=3):\n    x0 = torch.tensor(2.0)\n\n    xi = x0\n    plt.plot(x_range, f(x_range), label=r\"$f(x) = x^2 + 2$\", lw=3, alpha=0.5, ls='--', color='k')\n    for i in range(iter):\n        plt.scatter(xi, f(xi), label=fr'$x_{i}$ = {xi.item():0.2f}', s=100, c=f\"C{i}\")\n        with torch.no_grad():\n            appx = nth_order_appx(f, 1, xi)(x_range)\n        plt.plot(x_range, appx, label=fr\"order 1 appx. at $x=x_{i}$\", c=f\"C{i}\", alpha=0.5)\n        xi = xi - alpha * torch.func.grad(f)(xi)\n\n    plt.xlim(-2.5, 2.5)\n    plt.ylim(0, 8)\n    # legend outside plot\n    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n    format_axes(plt.gca())\n\n\nplot_gd(alpha=0.1, iter=5)\nplt.savefig(\"../figures/mml/gd-lr-0.1.pdf\", bbox_inches=\"tight\")\n\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n\n\n\n\n\n\n\n\n\n\nplot_gd(alpha=0.8, iter=4)\nplt.savefig(\"../figures/mml/gd-lr-0.8.pdf\", bbox_inches=\"tight\")\n\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n\n\n\n\n\n\n\n\n\n\nplot_gd(alpha=1.01, iter=4)\nplt.savefig(\"../figures/mml/gd-lr-1.01.pdf\", bbox_inches=\"tight\")\n\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n\n\n\n\n\n\n\n\n\n\nplot_gd(alpha=0.01, iter=4)\nplt.savefig(\"../figures/mml/gd-lr-0.01.pdf\", bbox_inches=\"tight\")\n\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)\n/tmp/ipykernel_4089142/3273250015.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x0 = torch.tensor(x0)"
  },
  {
    "objectID": "notebooks/svm-primal-dual.html",
    "href": "notebooks/svm-primal-dual.html",
    "title": "SVM with RBF kernel",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom sklearn import svm\nfrom sklearn.datasets import make_blobs, make_classification\n\n# Generate some random data for demonstration\nX, y = make_blobs(n_samples=20, centers=2, random_state=6)\n\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cm.Paired)\n\n\n\n\n\n\n\n\n\n# Create an SVM model with an RBF kernel\nmodel = svm.SVC(kernel='linear', C = 1e6)\n\n# Fit the model to the data\nmodel.fit(X, y)\n\n# Access the alpha coefficients\nalphas = np.abs(model.dual_coef_.ravel())  # Absolute values of dual coefficients\nprint(alphas, model.dual_coef_)\n\n[0.14522303 0.11368673 0.0315363 ] [[-0.14522303  0.11368673  0.0315363 ]]\n\n\n\nmodel.support_vectors_ \n\narray([[ 8.98426675, -4.87449712],\n       [ 7.98907212, -8.45336239],\n       [ 7.28183008, -8.2229685 ]])\n\n\n\n# Plot the support vectors\nplt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100,\n            linewidth=1, facecolors='none', edgecolors='k')\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cm.Paired)\n\n\n\n\n\n\n\n\n\nmodel.support_vectors_.shape, model.support_vectors_\n\n((3, 2),\n array([[ 8.98426675, -4.87449712],\n        [ 7.98907212, -8.45336239],\n        [ 7.28183008, -8.2229685 ]]))\n\n\n\nmodel.dual_coef_.shape, model.dual_coef_, y[model.support_]\n\n((1, 3), array([[-0.14522303,  0.11368673,  0.0315363 ]]), array([0, 1, 1]))\n\n\n\nys_minus1_and_plus1 = np.where(y[model.support_] == 0, -1, 1)\nys_minus1_and_plus1\n\narray([-1,  1,  1])\n\n\n\nw = np.dot(alphas * ys_minus1_and_plus1, model.support_vectors_)\nprint(\"w found using dual coefficients using our calculation:\", w)\nprint(\"w found using model.coef_:\", model.coef_)\n\nw found using dual coefficients using our calculation: [-0.16682897 -0.51246787]\nw found using model.coef_: [[-0.16682897 -0.51246787]]\n\n\n\n# intercept\n\nnsv = model.support_vectors_.shape[0]\nb_sum = 0.0\nfor i in range(nsv):\n    b_sum += ys_minus1_and_plus1[i] - np.dot(w, model.support_vectors_[i])\n\nb = b_sum / nsv\n\nb_sklearn = model.intercept_\n\nprint(\"b found using dual coefficients using our calculation:\", b)\nprint(\"b found using model.intercept_:\", b_sklearn)\n\nb found using dual coefficients using our calculation: -1.9992140529976397\nb found using model.intercept_: [-1.99921422]\n\n\n\n### Method 1: Contour plot\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cm.Paired)\n\nax = plt.gca()\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\nxx = np.linspace(xlim[0], xlim[1], 30)\nyy = np.linspace(ylim[0], ylim[1], 30)\n\nYY, XX = np.meshgrid(yy, xx)\n\nxy = np.vstack([XX.ravel(), YY.ravel()]).T\n\nZ = np.dot(xy, w) + b\n\nZ = Z.reshape(XX.shape)\n\nplt.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n            linestyles=['--', '-', '--'])\n\nplt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100,\n            linewidth=1, facecolors='none', edgecolors='k')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n### Method 2: plotting the decision function directly using wx + b = 0 and wx + b = 1 and wx + b = -1\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cm.Paired)\n\nax = plt.gca()\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# decision line\n# w1x1 + w2x2 + b = 0\n# x2 = (-w1x1 - b) / w2\n# x2 is y-axis and x1 is x-axis\n\nw = model.coef_[0]\nb = model.intercept_[0]\n\nx1 = np.linspace(xlim[0], xlim[1], 30)\nx2 = (-w[0] * x1 - b) / w[1]\n\nplt.plot(x1, x2, 'k-')\n\n# Now plot the margins\n# w1x1 + w2x2 + b = 1\n# x2 = (-w1x1 - b + 1) / w2\n\nx2 = (-w[0] * x1 - b + 1) / w[1]\n\nplt.plot(x1, x2, 'k--')\n\nx2= (-w[0] * x1 - b - 1) / w[1]\nplt.plot(x1, x2, 'k--')\n\nplt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100,\n            linewidth=1, facecolors='none', edgecolors='k')\n\n\n\n\n\n\n\n\n\n### Method 3: using sklearn's decision_function\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cm.Paired)\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\nxx = np.linspace(xlim[0], xlim[1], 30)\nyy = np.linspace(ylim[0], ylim[1], 30)\n\nYY, XX = np.meshgrid(yy, xx)\n\nxy = np.vstack([XX.ravel(), YY.ravel()]).T\n\nZ = model.decision_function(xy).reshape(XX.shape)\n\nplt.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n            linestyles=['--', '-', '--'])\n\nplt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100,\n            linewidth=1, facecolors='none', edgecolors='k')\n\n\n\n\n\n\n\n\n\n\n### Method 4: using dot product of phi(xj).phi(x_test) for each support vector point xj\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cm.Paired)\n\n\nax = plt.gca()\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\nxx = np.linspace(xlim[0], xlim[1], 30)\nyy = np.linspace(ylim[0], ylim[1], 30)\n\ndef phi_linear(x):\n    return x\n\nYY, XX = np.meshgrid(yy, xx)\n\nxy = np.vstack([XX.ravel(), YY.ravel()]).T\n\nphi_linear_xy = phi_linear(xy)\n\nphi_linear_support_vectors = phi_linear(model.support_vectors_)\n\nalphas = model.dual_coef_\n\nb = model.intercept_[0]\n\nZ = np.ones(len(xy))\n\nfor index, phi_x_test in enumerate(phi_linear_xy):\n    decision = 0\n    for i in range(len(alphas[0])):\n        decision += alphas[0][i] * np.dot(phi_linear_support_vectors[i], phi_x_test)\n    decision += b\n    \n    Z[index] = decision\n    \nplt.contour(XX, YY, Z.reshape(XX.shape), colors='k', levels=[-1, 0, 1], alpha=0.5,\n            linestyles=['--', '-', '--'])\n\nplt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100,\n            linewidth=1, facecolors='none', edgecolors='k')\n\n\n\n\n\n\n\n\n\n\n\nx_test = xy[0]\n\nphi_linear_support_vectors[0]@x_test*alphas[0][0] + phi_linear_support_vectors[1]@x_test*alphas[0][1] + phi_linear_support_vectors[2]@x_test*alphas[0][2] + b\n\n3.025383870347727\n\n\n\nmodel.decision_function([x_test])\n\narray([3.02538387])\n\n\n\nmodel.decision_function(xy).shape\n\n(900,)\n\n\n\nZ.shape\n\n(900,)\n\n\n\nnp.allclose(Z, model.decision_function(xy))\n\nTrue\n\n\n\n### Method 5: using K(x_sv, x_test) for each support vector point x_sv\n\ndef linear_kernel(x1, x2):\n    return np.dot(x1, x2)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cm.Paired)\n\nax = plt.gca()\n\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\nxx = np.linspace(xlim[0], xlim[1], 30)\nyy = np.linspace(ylim[0], ylim[1], 30)\n\nYY, XX = np.meshgrid(yy, xx)\n\nxy = np.vstack([XX.ravel(), YY.ravel()]).T\n\nZ = np.ones(len(xy))\n\nfor index, x_test in enumerate(xy):\n    decision = 0\n    for i in range(len(alphas[0])):\n        decision += alphas[0][i] * linear_kernel(model.support_vectors_[i], x_test)\n    decision += b\n    \n    Z[index] = decision\n    \nplt.contour(XX, YY, Z.reshape(XX.shape), colors='k', levels=[-1, 0, 1], alpha=0.5,\n            linestyles=['--', '-', '--'])\n\nplt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100,\n            linewidth=1, facecolors='none', edgecolors='k')"
  },
  {
    "objectID": "notebooks/polynomial_features.html",
    "href": "notebooks/polynomial_features.html",
    "title": "Polynomial Regression with Basis Functions",
    "section": "",
    "text": "Reference: https://alexshtf.github.io/2024/01/21/Bernstein.html\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport numpy.polynomial.polynomial as poly\nfrom sklearn.linear_model import Ridge\n\n\n1. Simple Polynomial Regression\n\nm = number of data points\nn = degree of polynomial\nInput: \\[\\{X_i\\}_{i=0}^{m}\\]\nTarget: \\[\\{y_i\\}_{i=0}^{m}\\]\nFeature Expansion: \\[\\mathbb{E}_n = {1, x, x^2, ..., x^n}\\]\nPrediction:\n\n\\[\\hat{y}_i = \\alpha_0 \\cdot 1 + \\alpha_1 \\cdot x + \\alpha_2 \\cdot x^2 + \\cdots + \\alpha_n x^n\\]\n\nLoss: \\[L = \\sum_{i=1}^m (\\alpha_0 + \\alpha_1 x_i + \\dots + \\alpha_n x_i^n - y_i)^2\\]\nFeature expanded X:\n\\[\\begin{pmatrix}\n1 & x_1 & x_1^2 & \\dots & x_1^n \\\\\n1 & x_2 & x_2^2 & \\dots & x_2^n \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_m & x_m^2 & \\dots & x_m^n \\\\\n\\end{pmatrix}\\]\nPolynomials have different degrees, therefore, different units.\nnumpy.polynomial.polynomial.polyvander takes X and expands it to the above matrix.\n\n\nX = np.array([1, 2, 3, 4])\nn = 3\n\n\nX_poly = poly.polyvander(X, deg=n)\nprint(\"X: \\n\", X)\nprint(\"\\nX_poly:\")\nX_poly\n\nX: \n [1 2 3 4]\n\nX_poly:\n\n\narray([[ 1.,  1.,  1.,  1.],\n       [ 1.,  2.,  4.,  8.],\n       [ 1.,  3.,  9., 27.],\n       [ 1.,  4., 16., 64.]])\n\n\n\n\n\n2. Chebyshev polynomials\n\\[\\mathbb{T}_n = \\{ T_0, T_1, \\dots, T_n \\}\\]\n\\[\\begin{align*}\nT_0(x) &= 1 \\\\\nT_1(x) &= x \\\\\nT_{n+1}(x) &= 2xT_n(x) - T_{n-1}(x)\n\\end{align*}\\]\n\nFeature expansion:\n\\[\\begin{pmatrix}\nT_0(x_1) & T_1(x_1) & \\dots & T_n(x_1) \\\\\nT_0(x_2) & T_1(x_2) & \\dots & T_n(x_2) \\\\\n\\vdots & \\vdots  & \\ddots& \\vdots  \\\\\nT_0(x_m) & T_1(x_m) & \\dots & T_n(x_m) \\\\\n\\end{pmatrix}\\]\nPolynomial \\(T_k\\) are k-degree polynomials. Therefore, their units are different.\nnumpy.polynomial.chebyshev.chebvander(X, deg) takes X and expands it to the above matrix.\n\n\ndef chebyshev_polynomial(n, x):\n    if n == 0:\n        return 1\n    elif n == 1:\n        return x\n    else:\n        return 2 * x * chebyshev_polynomial(n - 1, x) - chebyshev_polynomial(n - 2, x)\n\n# Example: Generate and print Chebyshev polynomials T_0(x) to T_4(x) at x = 0.5\nx_value = 4\nchebyshev_results = []\nfor i in range(4):\n    result = chebyshev_polynomial(i, x_value)\n    chebyshev_results.append(result)\n\nprint(x_value)\nprint(chebyshev_results)\n\n4\n[1, 4, 31, 244]\n\n\n\nimport numpy.polynomial.chebyshev as cheb\n\nX_poly = cheb.chebvander(X, deg=n)\nprint(\"X: \\n\", X)\nprint(\"\\nX_poly:\")\nX_poly\n\nX: \n [1 2 3 4]\n\nX_poly:\n\n\narray([[  1.,   1.,   1.,   1.],\n       [  1.,   2.,   7.,  26.],\n       [  1.,   3.,  17.,  99.],\n       [  1.,   4.,  31., 244.]])\n\n\n\n\n\n3. Legendre polynomials\n\\[\\mathbb{P}_n = \\{ P_0, P_1, \\dots, P_n \\}\\]\n\\[\\begin{align*}\nP_0(x) &= 1 \\\\\nP_1(x) &= x \\\\\n(n+1)P_{n+1}(x) &= (2n+1)xP_n(x) - nP_{n-1}(x)\n\\end{align*}\\]\n\nFeature expansion:\n\\[\\begin{pmatrix}\nP_0(x_1) & P_1(x_1) & \\dots & P_n(x_1) \\\\\nP_0(x_2) & P_1(x_2) & \\dots & P_n(x_2) \\\\\n\\vdots & \\vdots  & \\ddots& \\vdots  \\\\\nP_0(x_m) & P_1(x_m) & \\dots & P_n(x_m) \\\\\n\\end{pmatrix}\\]\nPolynomial \\(P_k\\) are both k-degree polynomials. Therefore, their units are different.\nnumpy.polynomial.legendre.legvander(X, degn) takes X and expands it to the above matrix.\n\n\ndef legendre_polynomial(n, x):\n    if n == 0:\n        return 1\n    elif n == 1:\n        return x\n    else:\n        return ((2 * n - 1) * x * legendre_polynomial(n - 1, x) - (n - 1) * legendre_polynomial(n - 2, x)) / n\n\n# Example usage:\nx_value = 4\ndegree = 3\nlegendre_results = []\nfor i in range(degree + 1):\n    legendre_results.append(legendre_polynomial(i, x_value))\n\nprint(x_value)\nprint(legendre_results)\n\n4\n[1, 4, 23.5, 154.0]\n\n\n\nimport numpy.polynomial.legendre as leg\n\nX_poly = leg.legvander(X, deg=n)\nprint(\"X: \\n\", X)\nprint(\"\\nX_poly:\")\nX_poly\n\nX: \n [1 2 3 4]\n\nX_poly:\n\n\narray([[  1. ,   1. ,   1. ,   1. ],\n       [  1. ,   2. ,   5.5,  17. ],\n       [  1. ,   3. ,  13. ,  63. ],\n       [  1. ,   4. ,  23.5, 154. ]])\n\n\n\n\n\n4. Bernstein basis\n\\[\\mathbb{B}_n = \\{  B_{0,n}, \\dots, B_{n, n} \\}\\]\n\\[B_{i,n}(x) = \\binom{n}{i} x^i (1-x)^{n-i}\\] - here, x is probability of success, and i is the number of successes. Therefore, \\[0=&lt;x&lt;=1\\] - Feature expansion:\n\\[\\begin{pmatrix}\nB_{0,n}(x_1) & B_{1,n}(x_1) & \\dots & B_{n,n}(x_1) \\\\\nB_{0,n}(x_2) & B_{1,n}(x_2) & \\dots & B_{n,n}(x_2) \\\\\n\\vdots & \\vdots  & \\ddots& \\vdots  \\\\\nB_{0,n}(x_m) & B_{1,n}(x_m) & \\dots & B_{n,n}(x_m) \\\\\n\\end{pmatrix}\\]\n\nPolynomial \\(B_{i, n}\\) are n-degree polynomials. Therefore, their units are same.\nscipy.stats.binom.pmf(i, n, x) gives the binomial coefficient.\n\n\nfrom scipy.stats import binom\n\nX = np.array([0.0, 0.5, 0.7])\n\ndef bernvander(x, deg):\n    return binom.pmf(np.arange(1 + deg), deg, x.reshape(-1, 1))\n\nX_poly = bernvander(X, deg=n)\nprint(\"X: \\n\", X)\nprint(\"\\nX_poly:\")\nX_poly\n\nX: \n [0.  0.5 0.7]\n\nX_poly:\n\n\narray([[1.   , 0.   , 0.   , 0.   ],\n       [0.125, 0.375, 0.375, 0.125],\n       [0.027, 0.189, 0.441, 0.343]])\n\n\n\n\n\nRidge Regression\n\nLinear regression with penalty on the weights - ensures that the coefficients \\((\\alpha_0, \\alpha_1, \\alpha_2, ..., \\alpha_n)\\) do not grow too large.\nRidge coefficients, \\(\\alpha\\) determines the amount of shrinkage:\n\n\\(\\alpha = 0\\): no shrinkage\n\\(\\alpha = \\infty\\): all coefficients are zero\n\nNote: you will study this in upcoming lectures.\n\n\n\n# True function\ndef true_func(x):\n  return np.sin(8 * np.pi * x) / np.exp(x) + x\n\n\nm = 30\nsigma = 0.1\n\n# generate\nnp.random.seed(42)\nX = np.random.rand(m)\ny = true_func(X) + sigma * np.random.randn(m)\n\n\nplt_xs = np.linspace(0, 1, 1000)\nplt.scatter(X, y)\nplt.plot(plt_xs, true_func(plt_xs), 'blue')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1. Simple polynomial regression\n\nn = 50\nX_poly = poly.polyvander(X, deg=n)\nX_poly.shape\n\n(30, 51)\n\n\n\nX_poly[0,:]\n\narray([1.00000000e+00, 3.74540119e-01, 1.40280301e-01, 5.25406005e-02,\n       1.96785627e-02, 7.37041123e-03, 2.76051470e-03, 1.03392350e-03,\n       3.87245832e-04, 1.45039100e-04, 5.43229617e-05, 2.03461285e-05,\n       7.62044140e-06, 2.85416103e-06, 1.06899781e-06, 4.00382567e-07,\n       1.49959334e-07, 5.61657868e-08, 2.10363405e-08, 7.87895346e-09,\n       2.95098417e-09, 1.10526196e-09, 4.13964946e-10, 1.55046480e-10,\n       5.80711271e-11, 2.17499668e-11, 8.14623516e-12, 3.05109189e-12,\n       1.14275632e-12, 4.28008087e-13, 1.60306200e-13, 6.00411031e-14,\n       2.24878019e-14, 8.42258399e-15, 3.15459561e-15, 1.18152261e-15,\n       4.42527621e-16, 1.65744348e-16, 6.20779076e-17, 2.32506669e-17,\n       8.70830755e-18, 3.26161054e-18, 1.22160400e-18, 4.57539708e-19,\n       1.71366976e-19, 6.41838077e-20, 2.40394110e-20, 9.00372384e-21,\n       3.37225580e-21, 1.26304509e-21, 4.73061057e-22])\n\n\n\nmodel = LinearRegression(fit_intercept=False)\nmodel.fit(X_poly, y)\n\nLinearRegression(fit_intercept=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression(fit_intercept=False)\n\n\n\nplt.scatter(X, y)                                    # plot the samples\nplt.plot(plt_xs, true_func(plt_xs), 'blue', label='True')                          # plot the true function\nplt.plot(plt_xs, model.predict(poly.polyvander(plt_xs, deg=n)), 'r', label=\"Predicted\") # plot the fit model\nplt.ylim([-5, 5])\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRidge regression with polynomial basis\n\n# Fit a linear model\ndef fit_and_plot(vander, n, alpha):\n  model = Ridge(fit_intercept=False, alpha=alpha)\n  model.fit(vander(X, deg=n), y)\n\n  plt.scatter(X, y)                           # plot the samples\n  plt.plot(plt_xs, true_func(plt_xs), 'blue')                 # plot the true function\n  plt.plot(plt_xs, model.predict(vander(plt_xs, deg=n)), 'r') # plot the fit model\n  plt.ylim([-5, 5])\n  plt.show()\n\n\nfit_and_plot(poly.polyvander, n=50, alpha=1e-7)\n\n\n\n\n\n\n\n\n\n\n2. Chebyshev basis\n\ndef scaled_chebvander(x, deg):\n  return cheb.chebvander(2 * x - 1, deg=deg)\n\n\nfit_and_plot(scaled_chebvander, n=50, alpha=1)\n\n\n\n\n\n\n\n\n\nfit_and_plot(scaled_chebvander, n=50, alpha=10)\n\n\n\n\n\n\n\n\n\n\n3. Legendre basis\n\ndef scaled_legvander(x, deg):\n  return leg.legvander(2 * x - 1, deg=deg)\n\n\nfit_and_plot(scaled_legvander, n=50, alpha=0.5)\n\n\n\n\n\n\n\n\n\n\n4. Bernstein basis\n\ndef bernvander(x, deg):\n    return binom.pmf(np.arange(1 + deg), deg, x.reshape(-1, 1))\n\n\nfit_and_plot(bernvander, n=50, alpha=0)\n\nc:\\Users\\ryees\\anaconda3\\envs\\pml\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:255: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\nfit_and_plot(bernvander, n=50, alpha=5e-4)\n\n\n\n\n\n\n\n\n\nfit_and_plot(bernvander, n=100, alpha=5e-4)"
  },
  {
    "objectID": "notebooks/cnn.html",
    "href": "notebooks/cnn.html",
    "title": "CNN",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, TensorDataset\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Set random seed for reproducibility\ntorch.manual_seed(0)\n\n# Torch version\ntorch.__version__\n\n'2.0.0+cu118'\n\n\n\n# MNIST dataset\n\nfrom torchvision import datasets, transforms\nimport torchvision\n\n# Split MNIST into train, validation, and test sets\ntrain_data = datasets.MNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\ntest_data = datasets.MNIST(root='data', train=False, download=True, transform=transforms.ToTensor())\n\n# Split train_data into train and validation sets\nval_data = torch.utils.data.Subset(train_data, range(50000, 51000))\n\n# Reduce the size of the training set to 5,000\ntrain_data = torch.utils.data.Subset(train_data, range(0, 5000))\n\n\n# Create data loaders\nbatch_size = 64\n\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n\n\nimg, target = next(iter(train_loader))\nprint(img.shape)\nprint(target.shape)\n\ntorch.Size([64, 1, 28, 28])\ntorch.Size([64])\n\n\n\nplt.imshow(img[0].numpy().squeeze(), cmap='gray_r');\n\n\n\n\n\n\n\n\n\nimg[0].shape\n\ntorch.Size([1, 28, 28])\n\n\n\n# Create a simple LeNet like CNN\n\nclass LeNet5(nn.Module):\n    def __init__(self):\n        super(LeNet5, self).__init__()\n        # 1 input image channel, 6 output channels, 5x5 square convolution\n        self.conv1 = nn.Conv2d(1, 6, 5) \n        # 6 input image channel, 16 output channels, 5x5 square convolution\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n        \n    def forward(self, x):\n        x = self.conv1(x) # 28x28x1 -&gt; 24x24x6\n        x = F.max_pool2d(F.relu(x), 2) # 24x24x6 -&gt; 12x12x6\n        x = self.conv2(x) # 12x12x6 -&gt; 8x8x16\n        x = F.max_pool2d(F.relu(x), 2) # 8x8x16 -&gt; 4x4x16\n        x = x.view(-1, self.num_flat_features(x)) # 4x4x16 -&gt; 256\n        x = self.fc1(x) # 256 -&gt; 120\n        x = F.relu(x)\n        x = self.fc2(x) # 120 -&gt; 84\n        x = F.relu(x)\n        x = self.fc3(x) # 84 -&gt; 10\n        return x\n    \n    def num_flat_features(self, x):\n        size = x.size()[1:]\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\n\n# Create a model\n\nmodel = LeNet5()\nprint(model)\n\nLeNet5(\n  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n  (fc1): Linear(in_features=256, out_features=120, bias=True)\n  (fc2): Linear(in_features=120, out_features=84, bias=True)\n  (fc3): Linear(in_features=84, out_features=10, bias=True)\n)\n\n\n\n# Train the model\n\n# Define the loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\nn_epochs = 10\n\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(n_epochs):\n    train_loss = 0.0\n    val_loss = 0.0\n    \n    # Train the model\n    model.train()\n    for data, target in train_loader:\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()*data.size(0)\n        \n    # Evaluate the model\n    model.eval()\n    for data, target in val_loader:\n        output = model(data)\n        loss = criterion(output, target)\n        val_loss += loss.item()*data.size(0)\n        \n    # Calculate average losses\n    train_loss = train_loss/len(train_loader.sampler)\n    val_loss = val_loss/len(val_loader.sampler)\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    \n    # Print training/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch+1, \n        train_loss,\n        val_loss\n        ))\n\nEpoch: 1    Training Loss: 1.437300     Validation Loss: 0.653900\nEpoch: 2    Training Loss: 0.424091     Validation Loss: 0.367598\nEpoch: 3    Training Loss: 0.303504     Validation Loss: 0.308797\nEpoch: 4    Training Loss: 0.219186     Validation Loss: 0.257062\nEpoch: 5    Training Loss: 0.195089     Validation Loss: 0.214157\nEpoch: 6    Training Loss: 0.153489     Validation Loss: 0.190220\nEpoch: 7    Training Loss: 0.130065     Validation Loss: 0.189110\nEpoch: 8    Training Loss: 0.114033     Validation Loss: 0.173153\nEpoch: 9    Training Loss: 0.103402     Validation Loss: 0.167645\nEpoch: 10   Training Loss: 0.089715     Validation Loss: 0.156438\n\n\n\n# Plot the training and validation loss\n\nplt.plot(train_losses, label='Training loss')\nplt.plot(val_losses, label='Validation loss')\n\n\n\n\n\n\n\n\n\n# Test the model\n\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for data, target in test_loader:\n        output = model(data)\n        _, predicted = torch.max(output.data, 1)\n        total += target.size(0)\n        correct += (predicted == target).sum().item()\n\n    print('Test Accuracy: {}%'.format(100 * correct / total))\n\nTest Accuracy: 96.1%\n\n\n\n# Now, let us take an image and walk it through the model\n\ntest_img = train_data[1][0].unsqueeze(0)\n\n\nplt.imshow(test_img.numpy().squeeze(), cmap='gray_r');\n\n\n\n\n\n\n\n\n\n# Get weights and biases from the first convolutional layer\n\nweights = model.conv1.weight.data\nw = weights.numpy()\n\n# Plot the weights\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nax = axes.ravel()\n\nfor i in range(6):\n    sns.heatmap(w[i][0], ax=ax[i], cmap='gray', cbar=False, annot=True)\n    ax[i].set_title('Filter {}'.format(i+1))\n\n\n\n\n\n\n\n\n\n# Get output from model's first conv1 layer\n\nconv1 = F.relu(model.conv1(test_img))\n\n# For plotting bring all the images to the same scale\nc1 = conv1 - conv1.min()\nc1 = c1 / conv1.max()\n\nprint(c1.shape)\nprint(\"1 image, 6 channels, 24x24 pixels\")\n\ntorch.Size([1, 6, 24, 24])\n1 image, 6 channels, 24x24 pixels\n\n\n\n# Visualizae the output of the first convolutional layer\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nax = axes.ravel()\n\nfor i in range(6):\n    sns.heatmap(c1[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n    ax[i].set_title('Image {}'.format(i+1))\n\n\n\n\n\n\n\n\n\n# Get output from model after max pooling\n\npool1 = F.max_pool2d(conv1, 2)\n\n# For plotting bring all the images to the same scale\np1 = pool1 - pool1.min()\np1 = p1 / pool1.max()\n\nprint(p1.shape)\nprint(\"1 image, 6 channels, 12x12 pixels\")\n\n# Visualizae the output of the first convolutional layer\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\nax = axes.ravel()\n\nfor i in range(6):\n    sns.heatmap(p1[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n    ax[i].set_title('Image {}'.format(i+1))\n\ntorch.Size([1, 6, 12, 12])\n1 image, 6 channels, 12x12 pixels\n\n\n\n\n\n\n\n\n\n\n# Visualize the filters in the second convolutional layer\n\nweights = model.conv2.weight.data\nw = weights.numpy()\n\n# Plot the weights\n\nfig, axes = plt.subplots(4, 4, figsize=(16, 16))\nax = axes.ravel()\n\nfor i in range(16):\n    sns.heatmap(w[i][0], ax=ax[i], cmap='gray', cbar=False)\n    ax[i].set_title('Filter {}'.format(i+1))\n\n\n\n\n\n\n\n\n\n# Get output from model's second conv2 layer\n\nconv2 = F.relu(model.conv2(pool1))\n\n# For plotting bring all the images to the same scale\nc2 = conv2 - conv2.min()\nc2 = c2 / conv2.max()\n\nprint(c2.shape)\nprint(\"1 image, 16 channels, 8x8 pixels\")\n\n# Visualizae the output of the first convolutional layer\n\nfig, axes = plt.subplots(4, 4, figsize=(18, 18))\nax = axes.ravel()\n\nfor i in range(16):\n    sns.heatmap(c2[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n    ax[i].set_title('Image {}'.format(i+1))\n\ntorch.Size([1, 16, 8, 8])\n1 image, 16 channels, 8x8 pixels\n\n\n\n\n\n\n\n\n\n\n# Get output from model after max pooling\n\npool2 = F.max_pool2d(conv2, 2)\n\n# For plotting bring all the images to the same scale\np2 = pool2 - pool2.min()\np2 = p2 / pool2.max()\n\nprint(p2.shape)\nprint(\"1 image, 16 channels, 4x4 pixels\")\n\n# Visualizae the output of the first convolutional layer\n\nfig, axes = plt.subplots(4, 4, figsize=(18, 18))\nax = axes.ravel()\n\nfor i in range(16):\n    sns.heatmap(p2[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n    ax[i].set_title('Image {}'.format(i+1))\n\ntorch.Size([1, 16, 4, 4])\n1 image, 16 channels, 4x4 pixels\n\n\n\n\n\n\n\n\n\n\n# Flatten the output of the second convolutional layer\n\nflat = pool2.view(pool2.size(0), -1)\nprint(flat.shape)\n\ntorch.Size([1, 256])\n\n\n\n# Repeat the above process as a function to visualize the convolution outputs for any image for any layer\ndef scale_img(img):\n    \"\"\"\n    Scale the image to the same scale\n    \"\"\"\n    img = img - img.min()\n    img = img / img.max()\n    return img\n\ndef visualize_conv_output(model, img):\n    \"\"\"\n    Visualize the output of a convolutional layer\n    \"\"\"\n    # Get output from model's first conv1 layer\n    conv1 = F.relu(model.conv1(img))\n\n    # For plotting bring all the images to the same scale\n    c1 = scale_img(conv1)\n\n    # Visualizae the output of the first convolutional layer\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    ax = axes.ravel()\n\n\n    for i in range(6):\n        sns.heatmap(c1[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n        ax[i].set_title('Image {}'.format(i+1))\n    # Add title to the figure\n    fig.suptitle('Convolutional Layer 1', fontsize=16)\n    \n    # Get output from model after max pooling\n    pool1 = F.max_pool2d(conv1, 2)\n\n    # For plotting bring all the images to the same scale\n    p1 = scale_img(pool1)\n\n    # Visualizae the output of the first convolutional layer\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    ax = axes.ravel()\n\n\n    for i in range(6):\n        sns.heatmap(p1[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n        ax[i].set_title('Image {}'.format(i+1))\n    # Add title to the figure\n    fig.suptitle('Max Pooling Layer 1', fontsize=16)\n\n    # Get output from model's second conv2 layer\n    conv2 = F.relu(model.conv2(pool1))\n\n    # For plotting bring all the images to the same scale\n    c2 = scale_img(conv2)\n\n    # Visualizae the output of the first convolutional layer\n    fig, axes = plt.subplots(4, 4, figsize=(18, 18))\n    ax = axes.ravel()\n\n    \n    for i in range(16):\n        sns.heatmap(c2[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n        ax[i].set_title('Image {}'.format(i+1))\n    # Add title to the figure\n    fig.suptitle('Convolutional Layer 2', fontsize=16)\n\n    # Get output from model after max pooling\n    pool2 = F.max_pool2d(conv2, 2)\n\n    # For plotting bring all the images to the same scale\n    p2 = scale_img(pool2)\n\n    # Visualizae the output of the first convolutional layer\n    fig, axes = plt.subplots(4, 4, figsize=(18, 18))\n    ax = axes.ravel()\n\n    for i in range(16):\n        sns.heatmap(p2[0][i].detach().numpy(), ax=ax[i], cmap='gray')\n        ax[i].set_title('Image {}'.format(i+1))\n    # Add title to the figure\n    fig.suptitle('Max Pooling Layer 2', fontsize=16)\n\n\nvisualize_conv_output(model, train_data[2][0].unsqueeze(0))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvisualize_conv_output(model, train_data[4][0].unsqueeze(0))"
  },
  {
    "objectID": "notebooks/names.html",
    "href": "notebooks/names.html",
    "title": "Generating names using MLPs",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\nimport pandas as pd\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nfrom pprint import pprint\n\n\ntorch.__version__\n\n'2.0.1'\n\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndevice\n\ndevice(type='cpu')\n\n\n\n# Get some names from https://github.com/MASTREX/List-of-Indian-Names\n\n\n!wget https://raw.githubusercontent.com/balasahebgulave/Dataset-Indian-Names/master/Indian_Names.csv -O names-long.csv\n\n--2024-03-07 11:57:26--  https://raw.githubusercontent.com/balasahebgulave/Dataset-Indian-Names/master/Indian_Names.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 85538 (84K) [text/plain]\nSaving to: ‘names-long.csv’\n\nnames-long.csv      100%[===================&gt;]  83.53K  --.-KB/s    in 0.08s   \n\n2024-03-07 11:57:26 (1.08 MB/s) - ‘names-long.csv’ saved [85538/85538]\n\n\n\n\n!head names-long.csv\n\n,Name\n0,aabid\n1,aabida\n2,aachal\n3,aadesh\n4,aadil\n5,aadish\n6,aaditya\n7,aaenab\n8,aafreen\n\n\n\n!tail names-long.csv\n\n6476,zeshan\n6477,zhini\n6478,ziarul\n6479,zile\n6480,zina\n6481,zishan\n6482,ziyabul\n6483,zoya\n6484,zuhaib\n6485,zuveb\n\n\n\nwords = pd.read_csv('names-long.csv')[\"Name\"]\nwords = words.str.lower()\nwords = words.str.strip()\nwords = words.str.replace(\" \", \"\")\n\nwords = words[words.str.len() &gt; 2]\nwords = words[words.str.len() &lt; 10]\n\n# Randomly shuffle the words\nwords = words.sample(frac=1).reset_index(drop=True)\nwords = words.tolist()\n\n# Remove words having non alphabets\nwords = [word for word in words if word.isalpha()]\nwords[:10]\n\n['diti',\n 'bajrang',\n 'sanjya',\n 'nain',\n 'mahrul',\n 'shib',\n 'jashgul',\n 'zeba',\n 'jaishmin',\n 'somil']\n\n\n\nlen(words)\n\n6184\n\n\n\n# build the vocabulary of characters and mappings to/from integers\nchars = sorted(list(set(''.join(words))))\nstoi = {s:i+1 for i,s in enumerate(chars)}\nstoi['.'] = 0\nitos = {i:s for s,i in stoi.items()}\npprint(itos)\n\n{0: '.',\n 1: 'a',\n 2: 'b',\n 3: 'c',\n 4: 'd',\n 5: 'e',\n 6: 'f',\n 7: 'g',\n 8: 'h',\n 9: 'i',\n 10: 'j',\n 11: 'k',\n 12: 'l',\n 13: 'm',\n 14: 'n',\n 15: 'o',\n 16: 'p',\n 17: 'q',\n 18: 'r',\n 19: 's',\n 20: 't',\n 21: 'u',\n 22: 'v',\n 23: 'w',\n 24: 'x',\n 25: 'y',\n 26: 'z'}\n\n\n\nblock_size = 5 # context length: how many characters do we take to predict the next one?\nX, Y = [], []\nfor w in words[:]:\n  \n  #print(w)\n  context = [0] * block_size\n  for ch in w + '.':\n    ix = stoi[ch]\n    X.append(context)\n    Y.append(ix)\n    print(''.join(itos[i] for i in context), '---&gt;', itos[ix])\n    context = context[1:] + [ix] # crop and append\n  \n# Move data to GPU\n\nX = torch.tensor(X).to(device)\nY = torch.tensor(Y).to(device)\n\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; t\n..dit ---&gt; i\n.diti ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; j\n..baj ---&gt; r\n.bajr ---&gt; a\nbajra ---&gt; n\najran ---&gt; g\njrang ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; y\nsanjy ---&gt; a\nanjya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; n\n.nain ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; r\n.mahr ---&gt; u\nmahru ---&gt; l\nahrul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; b\n.shib ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; h\n.jash ---&gt; g\njashg ---&gt; u\nashgu ---&gt; l\nshgul ---&gt; .\n..... ---&gt; z\n....z ---&gt; e\n...ze ---&gt; b\n..zeb ---&gt; a\n.zeba ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; s\n.jais ---&gt; h\njaish ---&gt; m\naishm ---&gt; i\nishmi ---&gt; n\nshmin ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; i\n.somi ---&gt; l\nsomil ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; o\nhario ---&gt; m\nariom ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; u\n.faru ---&gt; k\nfaruk ---&gt; h\narukh ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; p\n.gurp ---&gt; r\ngurpr ---&gt; e\nurpre ---&gt; e\nrpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; h\n..ruh ---&gt; h\n.ruhh ---&gt; e\nruhhe ---&gt; n\nuhhen ---&gt; a\nhhena ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; s\n..mis ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; n\n..kon ---&gt; i\n.koni ---&gt; k\nkonik ---&gt; a\nonika ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; i\n.giri ---&gt; s\ngiris ---&gt; h\nirish ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; h\n.mith ---&gt; a\nmitha ---&gt; l\nithal ---&gt; e\nthale ---&gt; s\nhales ---&gt; h\nalesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; j\n.kamj ---&gt; e\nkamje ---&gt; e\namjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; y\n..piy ---&gt; a\n.piya ---&gt; r\npiyar ---&gt; i\niyari ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; m\n..dam ---&gt; a\n.dama ---&gt; n\ndaman ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; a\n.rupa ---&gt; l\nrupal ---&gt; i\nupali ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; p\n.gurp ---&gt; r\ngurpr ---&gt; i\nurpri ---&gt; t\nrprit ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; d\nsamad ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; m\n.mohm ---&gt; e\nmohme ---&gt; d\nohmed ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; u\n.beeu ---&gt; t\nbeeut ---&gt; y\neeuty ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; k\n..aak ---&gt; i\n.aaki ---&gt; b\naakib ---&gt; .\n..... ---&gt; a\n....a ---&gt; h\n...ah ---&gt; a\n..aha ---&gt; m\n.aham ---&gt; a\nahama ---&gt; d\nhamad ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; u\n.hemu ---&gt; n\nhemun ---&gt; a\nemuna ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; o\n..moo ---&gt; k\n.mook ---&gt; a\nmooka ---&gt; n\nookan ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; p\n.kalp ---&gt; e\nkalpe ---&gt; s\nalpes ---&gt; h\nlpesh ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; j\n..taj ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; u\nshabu ---&gt; d\nhabud ---&gt; d\nabudd ---&gt; i\nbuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; k\n..vak ---&gt; i\n.vaki ---&gt; l\nvakil ---&gt; a\nakila ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; h\ngulsh ---&gt; a\nulsha ---&gt; f\nlshaf ---&gt; a\nshafa ---&gt; .\n..... ---&gt; o\n....o ---&gt; n\n...on ---&gt; g\n..ong ---&gt; i\n.ongi ---&gt; n\nongin ---&gt; y\nnginy ---&gt; e\nginye ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; e\n.deve ---&gt; n\ndeven ---&gt; d\nevend ---&gt; e\nvende ---&gt; r\nender ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; e\n..tee ---&gt; n\n.teen ---&gt; u\nteenu ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; r\n.bhor ---&gt; e\nbhore ---&gt; l\nhorel ---&gt; a\norela ---&gt; l\nrelal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; a\nshana ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; s\n..sos ---&gt; a\n.sosa ---&gt; r\nsosar ---&gt; i\nosari ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; f\n.shaf ---&gt; i\nshafi ---&gt; b\nhafib ---&gt; u\nafibu ---&gt; l\nfibul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; j\n..shj ---&gt; a\n.shja ---&gt; d\nshjad ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; s\nharis ---&gt; h\narish ---&gt; e\nrishe ---&gt; n\nishen ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; n\nsaran ---&gt; t\narant ---&gt; h\nranth ---&gt; e\nanthe ---&gt; m\nnthem ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; s\n.ruks ---&gt; h\nruksh ---&gt; a\nuksha ---&gt; n\nkshan ---&gt; a\nshana ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; u\nrabhu ---&gt; .\n..... ---&gt; e\n....e ---&gt; d\n...ed ---&gt; r\n..edr ---&gt; i\n.edri ---&gt; s\nedris ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; e\n.kame ---&gt; s\nkames ---&gt; h\namesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; c\n.sakc ---&gt; h\nsakch ---&gt; a\nakcha ---&gt; n\nkchan ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; i\n.dali ---&gt; p\ndalip ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; r\n..adr ---&gt; a\n.adra ---&gt; s\nadras ---&gt; h\ndrash ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; a\nsanja ---&gt; i\nanjai ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; y\n..aay ---&gt; e\n.aaye ---&gt; s\naayes ---&gt; h\nayesh ---&gt; a\nyesha ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; a\nprama ---&gt; t\nramat ---&gt; m\namatm ---&gt; a\nmatma ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; p\n.anup ---&gt; u\nanupu ---&gt; m\nnupum ---&gt; a\nupuma ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; p\n..gop ---&gt; i\n.gopi ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; e\n.nase ---&gt; e\nnasee ---&gt; b\naseeb ---&gt; a\nseeba ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; u\nsharu ---&gt; k\nharuk ---&gt; h\narukh ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; s\n.murs ---&gt; h\nmursh ---&gt; i\nurshi ---&gt; d\nrshid ---&gt; a\nshida ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; a\n.sana ---&gt; v\nsanav ---&gt; v\nanavv ---&gt; a\nnavva ---&gt; r\navvar ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; n\n..ron ---&gt; a\n.rona ---&gt; k\nronak ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; r\n.nazr ---&gt; i\nnazri ---&gt; n\nazrin ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; y\n..joy ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; n\n.mahn ---&gt; a\nmahna ---&gt; t\nahnat ---&gt; h\nhnath ---&gt; a\nnatha ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; w\n.bhaw ---&gt; a\nbhawa ---&gt; n\nhawan ---&gt; a\nawana ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; a\n.jana ---&gt; k\njanak ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; d\n.bhad ---&gt; d\nbhadd ---&gt; a\nhadda ---&gt; l\naddal ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; g\n..beg ---&gt; a\n.bega ---&gt; m\nbegam ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; a\nparta ---&gt; b\nartab ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; i\n.mehi ---&gt; b\nmehib ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; n\n..ben ---&gt; j\n.benj ---&gt; i\nbenji ---&gt; r\nenjir ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; a\n.vasa ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; v\n..gov ---&gt; i\n.govi ---&gt; n\ngovin ---&gt; d\novind ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; d\n..jed ---&gt; u\n.jedu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; g\n..nag ---&gt; e\n.nage ---&gt; n\nnagen ---&gt; d\nagend ---&gt; r\ngendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; i\namari ---&gt; n\nmarin ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; j\n.dalj ---&gt; e\ndalje ---&gt; e\naljee ---&gt; t\nljeet ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; u\n..kau ---&gt; s\n.kaus ---&gt; h\nkaush ---&gt; a\nausha ---&gt; r\nushar ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; a\n.hema ---&gt; n\nheman ---&gt; t\nemant ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; w\n.dhaw ---&gt; a\ndhawa ---&gt; l\nhawal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; f\n.mahf ---&gt; u\nmahfu ---&gt; j\nahfuj ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; h\n.sudh ---&gt; a\nsudha ---&gt; t\nudhat ---&gt; a\ndhata ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; s\n.shus ---&gt; h\nshush ---&gt; i\nhushi ---&gt; l\nushil ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; c\n..inc ---&gt; e\n.ince ---&gt; e\nincee ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; a\n.mada ---&gt; m\nmadam ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; j\n.ramj ---&gt; i\nramji ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; j\n..roj ---&gt; m\n.rojm ---&gt; e\nrojme ---&gt; r\nojmer ---&gt; i\njmeri ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; l\n.tril ---&gt; o\ntrilo ---&gt; c\nriloc ---&gt; h\niloch ---&gt; a\nlocha ---&gt; n\nochan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; e\n.same ---&gt; r\nsamer ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; s\n..yos ---&gt; h\n.yosh ---&gt; o\nyosho ---&gt; d\noshod ---&gt; a\nshoda ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; i\n..ati ---&gt; t\n.atit ---&gt; a\natita ---&gt; j\ntitaj ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; w\n.kanw ---&gt; a\nkanwa ---&gt; r\nanwar ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; s\n..bis ---&gt; m\n.bism ---&gt; i\nbismi ---&gt; l\nismil ---&gt; l\nsmill ---&gt; a\nmilla ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; h\n..jah ---&gt; i\n.jahi ---&gt; r\njahir ---&gt; u\nahiru ---&gt; l\nhirul ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; n\ndhann ---&gt; i\nhanni ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; l\n.manl ---&gt; i\nmanli ---&gt; s\nanlis ---&gt; a\nnlisa ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; c\n..jac ---&gt; k\n.jack ---&gt; y\njacky ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; y\n..jiy ---&gt; a\n.jiya ---&gt; u\njiyau ---&gt; l\niyaul ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; b\n..rab ---&gt; i\n.rabi ---&gt; y\nrabiy ---&gt; a\nabiya ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; l\n..anl ---&gt; i\n.anli ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; m\n.karm ---&gt; b\nkarmb ---&gt; i\narmbi ---&gt; r\nrmbir ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; n\n..nen ---&gt; c\n.nenc ---&gt; y\nnency ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; n\n..avn ---&gt; i\n.avni ---&gt; s\navnis ---&gt; h\nvnish ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; u\n.sagu ---&gt; f\nsaguf ---&gt; t\naguft ---&gt; a\ngufta ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; e\n..hee ---&gt; r\n.heer ---&gt; a\nheera ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; s\nabhis ---&gt; h\nbhish ---&gt; i\nhishi ---&gt; e\nishie ---&gt; k\nshiek ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; t\n.ramt ---&gt; e\nramte ---&gt; k\namtek ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; b\n.balb ---&gt; e\nbalbe ---&gt; e\nalbee ---&gt; r\nlbeer ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; a\nbhara ---&gt; t\nharat ---&gt; l\naratl ---&gt; a\nratla ---&gt; l\natlal ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; u\n.biru ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; e\n.vine ---&gt; t\nvinet ---&gt; a\nineta ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; b\nsukhb ---&gt; e\nukhbe ---&gt; e\nkhbee ---&gt; r\nhbeer ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; u\n..kau ---&gt; s\n.kaus ---&gt; h\nkaush ---&gt; a\nausha ---&gt; l\nushal ---&gt; y\nshaly ---&gt; a\nhalya ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; t\n..ket ---&gt; a\n.keta ---&gt; n\nketan ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; w\n.pusw ---&gt; a\npuswa ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; e\n.vije ---&gt; n\nvijen ---&gt; d\nijend ---&gt; a\njenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; s\n.shas ---&gt; h\nshash ---&gt; a\nhasha ---&gt; i\nashai ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; l\n.meel ---&gt; a\nmeela ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; z\n..riz ---&gt; w\n.rizw ---&gt; a\nrizwa ---&gt; a\nizwaa ---&gt; n\nzwaan ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; i\nlakhi ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; r\n.indr ---&gt; a\nindra ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; b\nkhusb ---&gt; h\nhusbh ---&gt; u\nusbhu ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; i\n..abi ---&gt; d\n.abid ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; l\nshail ---&gt; j\nhailj ---&gt; a\nailja ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; p\n.ranp ---&gt; a\nranpa ---&gt; l\nanpal ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; p\n.ganp ---&gt; a\nganpa ---&gt; t\nanpat ---&gt; i\nnpati ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; b\nsukhb ---&gt; i\nukhbi ---&gt; r\nkhbir ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; u\n.babu ---&gt; l\nbabul ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; a\n.musa ---&gt; r\nmusar ---&gt; r\nusarr ---&gt; a\nsarra ---&gt; t\narrat ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; n\n.ratn ---&gt; e\nratne ---&gt; s\natnes ---&gt; w\ntnesw ---&gt; a\nneswa ---&gt; r\neswar ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; c\n.dalc ---&gt; h\ndalch ---&gt; a\nalcha ---&gt; n\nlchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; i\n.nazi ---&gt; r\nnazir ---&gt; .\n..... ---&gt; l\n....l ---&gt; u\n...lu ---&gt; v\n..luv ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; s\nparas ---&gt; h\narash ---&gt; u\nrashu ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; k\namrik ---&gt; .\n..... ---&gt; g\n....g ---&gt; j\n...gj ---&gt; e\n..gje ---&gt; n\n.gjen ---&gt; d\ngjend ---&gt; e\njende ---&gt; r\nender ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; j\n..bij ---&gt; a\n.bija ---&gt; n\nbijan ---&gt; d\nijand ---&gt; e\njande ---&gt; r\nander ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; g\n..asg ---&gt; a\n.asga ---&gt; r\nasgar ---&gt; i\nsgari ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; j\nsharj ---&gt; p\nharjp ---&gt; r\narjpr ---&gt; i\nrjpri ---&gt; t\njprit ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; b\n..aab ---&gt; i\n.aabi ---&gt; d\naabid ---&gt; a\nabida ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; d\n.hand ---&gt; u\nhandu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; b\n.jagb ---&gt; i\njagbi ---&gt; r\nagbir ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; l\n.raml ---&gt; i\nramli ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; m\n..vim ---&gt; a\n.vima ---&gt; l\nvimal ---&gt; a\nimala ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; e\n.shre ---&gt; e\nshree ---&gt; e\nhreee ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; k\n.devk ---&gt; i\ndevki ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; a\n.jama ---&gt; a\njamaa ---&gt; l\namaal ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; e\n.inde ---&gt; r\ninder ---&gt; p\nnderp ---&gt; a\nderpa ---&gt; l\nerpal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; l\n.bhul ---&gt; i\nbhuli ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; r\n..var ---&gt; d\n.vard ---&gt; a\nvarda ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; i\n.jani ---&gt; d\njanid ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; r\nsantr ---&gt; o\nantro ---&gt; .\n..... ---&gt; f\n....f ---&gt; u\n...fu ---&gt; r\n..fur ---&gt; k\n.furk ---&gt; h\nfurkh ---&gt; a\nurkha ---&gt; n\nrkhan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; a\nramka ---&gt; r\namkar ---&gt; a\nmkara ---&gt; n\nkaran ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; r\nlakhr ---&gt; a\nakhra ---&gt; j\nkhraj ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; p\n.jasp ---&gt; a\njaspa ---&gt; l\naspal ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; n\n..tin ---&gt; k\n.tink ---&gt; u\ntinku ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; o\n.niro ---&gt; s\nniros ---&gt; h\nirosh ---&gt; a\nrosha ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; j\n.khaj ---&gt; a\nkhaja ---&gt; n\nhajan ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; a\n.kira ---&gt; n\nkiran ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; d\n.bind ---&gt; a\nbinda ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; m\n..dum ---&gt; n\n.dumn ---&gt; i\ndumni ---&gt; k\numnik ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; f\n.shaf ---&gt; i\nshafi ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; l\n..sel ---&gt; a\n.sela ---&gt; r\nselar ---&gt; s\nelars ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; g\n..beg ---&gt; u\n.begu ---&gt; m\nbegum ---&gt; p\negump ---&gt; u\ngumpu ---&gt; r\numpur ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; a\n.kisa ---&gt; n\nkisan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; n\nrajen ---&gt; d\najend ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; t\n.kant ---&gt; a\nkanta ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; m\n.birm ---&gt; a\nbirma ---&gt; d\nirmad ---&gt; e\nrmade ---&gt; v\nmadev ---&gt; i\nadevi ---&gt; .\n..... ---&gt; j\n....j ---&gt; g\n...jg ---&gt; d\n..jgd ---&gt; i\n.jgdi ---&gt; s\njgdis ---&gt; h\ngdish ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; b\n..akb ---&gt; a\n.akba ---&gt; r\nakbar ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; j\n.dalj ---&gt; i\ndalji ---&gt; t\naljit ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; a\n.jaya ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; l\n.harl ---&gt; e\nharle ---&gt; e\narlee ---&gt; n\nrleen ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; s\nlakhs ---&gt; m\nakhsm ---&gt; i\nkhsmi ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; d\n..mud ---&gt; a\n.muda ---&gt; s\nmudas ---&gt; a\nudasa ---&gt; r\ndasar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; t\n.ratt ---&gt; a\nratta ---&gt; n\nattan ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; h\n.mith ---&gt; i\nmithi ---&gt; l\nithil ---&gt; e\nthile ---&gt; s\nhiles ---&gt; h\nilesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; r\n.mukr ---&gt; a\nmukra ---&gt; m\nukram ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; a\n.tara ---&gt; n\ntaran ---&gt; n\narann ---&gt; u\nrannu ---&gt; m\nannum ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; l\n.kosl ---&gt; y\nkosly ---&gt; a\noslya ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; t\n..sot ---&gt; a\n.sota ---&gt; j\nsotaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; d\n.sard ---&gt; a\nsarda ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; u\n.ashu ---&gt; t\nashut ---&gt; o\nshuto ---&gt; s\nhutos ---&gt; h\nutosh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; a\nsatya ---&gt; v\natyav ---&gt; a\ntyava ---&gt; t\nyavat ---&gt; i\navati ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; u\n.babu ---&gt; p\nbabup ---&gt; u\nabupu ---&gt; r\nbupur ---&gt; i\nupuri ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; l\n..tul ---&gt; s\n.tuls ---&gt; h\ntulsh ---&gt; a\nulsha ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; n\n..jun ---&gt; n\n.junn ---&gt; a\njunna ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; p\n..nup ---&gt; u\n.nupu ---&gt; r\nnupur ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; s\n.anis ---&gt; h\nanish ---&gt; a\nnisha ---&gt; .\n..... ---&gt; e\n....e ---&gt; s\n...es ---&gt; r\n..esr ---&gt; a\n.esra ---&gt; i\nesrai ---&gt; l\nsrail ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; a\n.soha ---&gt; i\nsohai ---&gt; b\nohaib ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; n\nchhan ---&gt; o\nhhano ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; d\n.maad ---&gt; h\nmaadh ---&gt; u\naadhu ---&gt; r\nadhur ---&gt; i\ndhuri ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; w\n.harw ---&gt; i\nharwi ---&gt; n\narwin ---&gt; d\nrwind ---&gt; r\nwindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; n\n..gon ---&gt; a\n.gona ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; v\n..mev ---&gt; a\n.meva ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; l\n.phol ---&gt; w\npholw ---&gt; a\nholwa ---&gt; t\nolwat ---&gt; i\nlwati ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; a\n.abha ---&gt; k\nabhak ---&gt; i\nbhaki ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; v\n..jav ---&gt; i\n.javi ---&gt; d\njavid ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; i\n.risi ---&gt; r\nrisir ---&gt; a\nisira ---&gt; j\nsiraj ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; r\n.deer ---&gt; e\ndeere ---&gt; n\neeren ---&gt; d\nerend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; n\n.navn ---&gt; e\nnavne ---&gt; e\navnee ---&gt; t\nvneet ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; u\n.ansu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; t\n.shet ---&gt; a\nsheta ---&gt; n\nhetan ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; j\n.barj ---&gt; e\nbarje ---&gt; s\narjes ---&gt; h\nrjesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; i\nshabi ---&gt; r\nhabir ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; p\n.veep ---&gt; a\nveepa ---&gt; l\neepal ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; e\n.pune ---&gt; e\npunee ---&gt; t\nuneet ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; l\n..sul ---&gt; e\n.sule ---&gt; n\nsulen ---&gt; d\nulend ---&gt; e\nlende ---&gt; r\nender ---&gt; .\n..... ---&gt; a\n....a ---&gt; w\n...aw ---&gt; e\n..awe ---&gt; d\n.awed ---&gt; h\nawedh ---&gt; e\nwedhe ---&gt; s\nedhes ---&gt; h\ndhesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; e\n..ale ---&gt; m\n.alem ---&gt; a\nalema ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; z\nshahz ---&gt; a\nhahza ---&gt; d\nahzad ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; d\n.bhud ---&gt; h\nbhudh ---&gt; i\nhudhi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; e\n.vike ---&gt; s\nvikes ---&gt; h\nikesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; k\n..amk ---&gt; i\n.amki ---&gt; t\namkit ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; e\nsidhe ---&gt; s\nidhes ---&gt; w\ndhesw ---&gt; a\nheswa ---&gt; r\neswar ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; y\nprity ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; p\n.jagp ---&gt; a\njagpa ---&gt; t\nagpat ---&gt; i\ngpati ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; d\n.sajd ---&gt; a\nsajda ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; h\n.rosh ---&gt; n\nroshn ---&gt; i\noshni ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; t\n..mat ---&gt; h\n.math ---&gt; u\nmathu ---&gt; r\nathur ---&gt; a\nthura ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; t\n..pat ---&gt; r\n.patr ---&gt; a\npatra ---&gt; s\natras ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; c\n..bac ---&gt; c\n.bacc ---&gt; h\nbacch ---&gt; u\nacchu ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; t\n.lalt ---&gt; i\nlalti ---&gt; a\naltia ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; i\n.basi ---&gt; r\nbasir ---&gt; a\nasira ---&gt; n\nsiran ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; n\nsahin ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; l\n..sel ---&gt; v\n.selv ---&gt; a\nselva ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; o\nsanto ---&gt; s\nantos ---&gt; h\nntosh ---&gt; i\ntoshi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; a\n..saa ---&gt; h\n.saah ---&gt; i\nsaahi ---&gt; l\naahil ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; t\n..vit ---&gt; h\n.vith ---&gt; a\nvitha ---&gt; l\nithal ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; g\n.gang ---&gt; o\ngango ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; i\n.muki ---&gt; s\nmukis ---&gt; h\nukish ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; p\n.ramp ---&gt; a\nrampa ---&gt; y\nampay ---&gt; a\nmpaya ---&gt; r\npayar ---&gt; i\nayari ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; a\nshama ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; t\n..git ---&gt; a\n.gita ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; d\n.devd ---&gt; h\ndevdh ---&gt; a\nevdha ---&gt; r\nvdhar ---&gt; i\ndhari ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; w\n.kulw ---&gt; a\nkulwa ---&gt; n\nulwan ---&gt; t\nlwant ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; u\n..jhu ---&gt; m\n.jhum ---&gt; k\njhumk ---&gt; i\nhumki ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; v\n..gov ---&gt; i\n.govi ---&gt; n\ngovin ---&gt; d\novind ---&gt; a\nvinda ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; a\nrabha ---&gt; s\nabhas ---&gt; h\nbhash ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; z\n.sehz ---&gt; a\nsehza ---&gt; d\nehzad ---&gt; a\nhzada ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; a\n.nira ---&gt; n\nniran ---&gt; j\niranj ---&gt; a\nranja ---&gt; n\nanjan ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; b\nveerb ---&gt; h\neerbh ---&gt; a\nerbha ---&gt; n\nrbhan ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; c\n..ric ---&gt; h\n.rich ---&gt; a\nricha ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; m\nsamim ---&gt; a\namima ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; a\n.jama ---&gt; d\njamad ---&gt; a\namada ---&gt; r\nmadar ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; a\n.fira ---&gt; s\nfiras ---&gt; a\nirasa ---&gt; t\nrasat ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; b\n.mahb ---&gt; i\nmahbi ---&gt; r\nahbir ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; b\nshabb ---&gt; i\nhabbi ---&gt; .\n..... ---&gt; a\n....a ---&gt; p\n...ap ---&gt; h\n..aph ---&gt; s\n.aphs ---&gt; a\naphsa ---&gt; n\nphsan ---&gt; a\nhsana ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; k\n..rek ---&gt; h\n.rekh ---&gt; a\nrekha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; i\nrambi ---&gt; r\nambir ---&gt; i\nmbiri ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; a\n..ada ---&gt; l\n.adal ---&gt; a\nadala ---&gt; t\ndalat ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; i\n..isi ---&gt; k\n.isik ---&gt; a\nisika ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; a\n..bea ---&gt; u\n.beau ---&gt; t\nbeaut ---&gt; y\neauty ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; c\n..ruc ---&gt; k\n.ruck ---&gt; m\nruckm ---&gt; a\nuckma ---&gt; n\nckman ---&gt; i\nkmani ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; l\n..dol ---&gt; l\n.doll ---&gt; y\ndolly ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; t\n..yat ---&gt; h\n.yath ---&gt; a\nyatha ---&gt; r\nathar ---&gt; t\nthart ---&gt; h\nharth ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; k\nmohak ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; a\nrisha ---&gt; b\nishab ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; r\n..asr ---&gt; a\n.asra ---&gt; n\nasran ---&gt; i\nsrani ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; j\n..aaj ---&gt; i\n.aaji ---&gt; v\naajiv ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; a\nramka ---&gt; l\namkal ---&gt; i\nmkali ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; n\n.deen ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; e\nramse ---&gt; m\namsem ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; p\n.dalp ---&gt; a\ndalpa ---&gt; t\nalpat ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; h\n.rakh ---&gt; e\nrakhe ---&gt; e\nakhee ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; e\n.pree ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; a\n.asha ---&gt; r\nashar ---&gt; a\nshara ---&gt; n\nharan ---&gt; i\narani ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; n\n..jen ---&gt; a\n.jena ---&gt; b\njenab ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; i\n.anji ---&gt; l\nanjil ---&gt; a\nnjila ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; s\n.nees ---&gt; a\nneesa ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; i\n.rasi ---&gt; d\nrasid ---&gt; a\nasida ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; m\n.bhim ---&gt; a\nbhima ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; g\n.jhag ---&gt; d\njhagd ---&gt; u\nhagdu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; i\n.mari ---&gt; u\nmariu ---&gt; m\narium ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; y\n.mary ---&gt; a\nmarya ---&gt; m\naryam ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; a\n..oma ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; a\n.soha ---&gt; n\nsohan ---&gt; l\nohanl ---&gt; a\nhanla ---&gt; l\nanlal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; u\nmanju ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; t\n.bant ---&gt; y\nbanty ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; r\n.nasr ---&gt; u\nnasru ---&gt; d\nasrud ---&gt; e\nsrude ---&gt; e\nrudee ---&gt; n\nudeen ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; j\n..rij ---&gt; v\n.rijv ---&gt; a\nrijva ---&gt; n\nijvan ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; a\n..bra ---&gt; m\n.bram ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; i\n.shri ---&gt; r\nshrir ---&gt; a\nhrira ---&gt; m\nriram ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; c\n..joc ---&gt; k\n.jock ---&gt; y\njocky ---&gt; i\nockyi ---&gt; p\nckyip ---&gt; a\nkyipa ---&gt; i\nyipai ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; k\n..fak ---&gt; i\n.faki ---&gt; r\nfakir ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; t\n..pat ---&gt; a\n.pata ---&gt; s\npatas ---&gt; o\nataso ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; c\n..mic ---&gt; h\n.mich ---&gt; a\nmicha ---&gt; e\nichae ---&gt; l\nchael ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; i\n.biri ---&gt; j\nbirij ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; a\n.mama ---&gt; n\nmaman ---&gt; b\namanb ---&gt; a\nmanba ---&gt; i\nanbai ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; k\n..pak ---&gt; h\n.pakh ---&gt; a\npakha ---&gt; l\nakhal ---&gt; i\nkhali ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; j\n..aaj ---&gt; a\n.aaja ---&gt; d\naajad ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; a\n.sura ---&gt; t\nsurat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; u\n.sadu ---&gt; r\nsadur ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; a\n..pha ---&gt; r\n.phar ---&gt; j\npharj ---&gt; a\nharja ---&gt; n\narjan ---&gt; a\nrjana ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; n\nranjn ---&gt; a\nanjna ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; b\n.shub ---&gt; a\nshuba ---&gt; n\nhuban ---&gt; k\nubank ---&gt; a\nbanka ---&gt; r\nankar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; m\nrahim ---&gt; u\nahimu ---&gt; n\nhimun ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; m\n.saim ---&gt; a\nsaima ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; d\n..avd ---&gt; e\n.avde ---&gt; s\navdes ---&gt; h\nvdesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; r\n.bhur ---&gt; e\nbhure ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; v\n..div ---&gt; a\n.diva ---&gt; k\ndivak ---&gt; a\nivaka ---&gt; r\nvakar ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; s\n.alis ---&gt; h\nalish ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; b\n..mab ---&gt; i\n.mabi ---&gt; y\nmabiy ---&gt; a\nabiya ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; b\n.harb ---&gt; a\nharba ---&gt; s\narbas ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; p\n..bup ---&gt; a\n.bupa ---&gt; l\nbupal ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; v\n..ruv ---&gt; i\n.ruvi ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; u\n.kalu ---&gt; s\nkalus ---&gt; i\nalusi ---&gt; n\nlusin ---&gt; g\nusing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; a\n.nira ---&gt; j\nniraj ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; d\n..lad ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; a\nramda ---&gt; s\namdas ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; a\n.bala ---&gt; k\nbalak ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; u\nshaku ---&gt; n\nhakun ---&gt; t\nakunt ---&gt; l\nkuntl ---&gt; a\nuntla ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; i\n..vai ---&gt; b\n.vaib ---&gt; h\nvaibh ---&gt; a\naibha ---&gt; v\nibhav ---&gt; .\n..... ---&gt; m\n....m ---&gt; h\n...mh ---&gt; o\n..mho ---&gt; s\n.mhos ---&gt; i\nmhosi ---&gt; n\nhosin ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; i\n.suri ---&gt; n\nsurin ---&gt; d\nurind ---&gt; e\nrinde ---&gt; r\ninder ---&gt; a\nndera ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; m\n.ashm ---&gt; i\nashmi ---&gt; n\nshmin ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; s\n.nans ---&gt; h\nnansh ---&gt; i\nanshi ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; n\n.meen ---&gt; a\nmeena ---&gt; k\neenak ---&gt; s\nenaks ---&gt; h\nnaksh ---&gt; i\nakshi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; n\n.ramn ---&gt; i\nramni ---&gt; w\namniw ---&gt; a\nmniwa ---&gt; s\nniwas ---&gt; j\niwasj ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; h\n.vash ---&gt; a\nvasha ---&gt; n\nashan ---&gt; a\nshana ---&gt; v\nhanav ---&gt; i\nanavi ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; d\n.somd ---&gt; a\nsomda ---&gt; t\nomdat ---&gt; h\nmdath ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; i\n.akhi ---&gt; l\nakhil ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; s\n..hus ---&gt; e\n.huse ---&gt; n\nhusen ---&gt; i\nuseni ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; a\n.saga ---&gt; n\nsagan ---&gt; .\n..... ---&gt; f\n....f ---&gt; u\n...fu ---&gt; l\n..ful ---&gt; o\n.fulo ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; e\n..khe ---&gt; m\n.khem ---&gt; c\nkhemc ---&gt; h\nhemch ---&gt; a\nemcha ---&gt; n\nmchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; l\n..asl ---&gt; a\n.asla ---&gt; m\naslam ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; l\n..nil ---&gt; o\n.nilo ---&gt; f\nnilof ---&gt; a\nilofa ---&gt; r\nlofar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; a\n.shya ---&gt; m\nshyam ---&gt; a\nhyama ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; m\n.mohm ---&gt; m\nmohmm ---&gt; a\nohmma ---&gt; d\nhmmad ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; s\n.pras ---&gt; a\nprasa ---&gt; n\nrasan ---&gt; t\nasant ---&gt; a\nsanta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; e\n.same ---&gt; .\n..... ---&gt; f\n....f ---&gt; u\n...fu ---&gt; l\n..ful ---&gt; i\n.fuli ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; f\n.gulf ---&gt; a\ngulfa ---&gt; s\nulfas ---&gt; a\nlfasa ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; t\nbhart ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; t\nshant ---&gt; y\nhanty ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; g\n.poog ---&gt; a\npooga ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; i\n.rohi ---&gt; l\nrohil ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; s\nabhis ---&gt; h\nbhish ---&gt; i\nhishi ---&gt; k\nishik ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; d\n..bid ---&gt; u\n.bidu ---&gt; r\nbidur ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; r\n.char ---&gt; n\ncharn ---&gt; j\nharnj ---&gt; e\narnje ---&gt; e\nrnjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; e\n.akhe ---&gt; e\nakhee ---&gt; l\nkheel ---&gt; e\nheele ---&gt; s\neeles ---&gt; h\nelesh ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; u\n.dipu ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; n\n..chn ---&gt; a\n.chna ---&gt; d\nchnad ---&gt; a\nhnada ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; u\n.mamu ---&gt; n\nmamun ---&gt; i\namuni ---&gt; .\n..... ---&gt; i\n....i ---&gt; l\n...il ---&gt; e\n..ile ---&gt; m\n.ilem ---&gt; a\nilema ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; z\n..haz ---&gt; i\n.hazi ---&gt; r\nhazir ---&gt; a\nazira ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; l\n..dol ---&gt; i\n.doli ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; n\n.meen ---&gt; a\nmeena ---&gt; z\neenaz ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; l\n..isl ---&gt; a\n.isla ---&gt; m\nislam ---&gt; u\nslamu ---&gt; d\nlamud ---&gt; i\namudi ---&gt; n\nmudin ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; s\n.vans ---&gt; h\nvansh ---&gt; i\nanshi ---&gt; k\nnshik ---&gt; a\nshika ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; b\ngulab ---&gt; s\nulabs ---&gt; a\nlabsa ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; a\nmansa ---&gt; v\nansav ---&gt; i\nnsavi ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; t\n.alit ---&gt; a\nalita ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; l\n..dol ---&gt; o\n.dolo ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; i\n.soni ---&gt; y\nsoniy ---&gt; a\noniya ---&gt; .\n..... ---&gt; p\n....p ---&gt; p\n...pp ---&gt; h\n..pph ---&gt; o\n.ppho ---&gt; l\npphol ---&gt; a\nphola ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; p\ndhanp ---&gt; a\nhanpa ---&gt; t\nanpat ---&gt; i\nnpati ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; i\npremi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; a\n.sata ---&gt; n\nsatan ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; i\n..abi ---&gt; s\n.abis ---&gt; h\nabish ---&gt; a\nbisha ---&gt; k\nishak ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; b\n.manb ---&gt; h\nmanbh ---&gt; a\nanbha ---&gt; r\nnbhar ---&gt; i\nbhari ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; n\n.basn ---&gt; t\nbasnt ---&gt; i\nasnti ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; j\n..taj ---&gt; i\n.taji ---&gt; n\ntajin ---&gt; d\najind ---&gt; e\njinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; i\n..udi ---&gt; t\n.udit ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; s\n..prs ---&gt; h\n.prsh ---&gt; a\nprsha ---&gt; n\nrshan ---&gt; t\nshant ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; u\n.babu ---&gt; d\nbabud ---&gt; d\nabudd ---&gt; e\nbudde ---&gt; n\nudden ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; b\nkushb ---&gt; u\nushbu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; m\n.narm ---&gt; a\nnarma ---&gt; d\narmad ---&gt; a\nrmada ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; e\n..ame ---&gt; n\n.amen ---&gt; a\namena ---&gt; .\n..... ---&gt; e\n....e ---&gt; m\n...em ---&gt; i\n..emi ---&gt; l\n.emil ---&gt; i\nemili ---&gt; a\nmilia ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; s\n.nees ---&gt; h\nneesh ---&gt; u\neeshu ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; t\n..tit ---&gt; u\n.titu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; h\n.kash ---&gt; i\nkashi ---&gt; r\nashir ---&gt; a\nshira ---&gt; m\nhiram ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; a\n.tara ---&gt; c\ntarac ---&gt; h\narach ---&gt; a\nracha ---&gt; n\nachan ---&gt; d\nchand ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; l\n.navl ---&gt; e\nnavle ---&gt; e\navlee ---&gt; n\nvleen ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; h\n.ramh ---&gt; e\nramhe ---&gt; t\namhet ---&gt; u\nmhetu ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; h\n..tah ---&gt; i\n.tahi ---&gt; r\ntahir ---&gt; a\nahira ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; p\n..arp ---&gt; i\n.arpi ---&gt; t\narpit ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; h\n..zah ---&gt; i\n.zahi ---&gt; r\nzahir ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; r\n.nazr ---&gt; a\nnazra ---&gt; n\nazran ---&gt; a\nzrana ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; m\n.karm ---&gt; a\nkarma ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; t\n.mant ---&gt; u\nmantu ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; i\n.lali ---&gt; y\nlaliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; d\nsahid ---&gt; a\nahida ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; i\n.sadi ---&gt; k\nsadik ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; r\n..imr ---&gt; a\n.imra ---&gt; n\nimran ---&gt; a\nmrana ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; a\n.sawa ---&gt; r\nsawar ---&gt; i\nawari ---&gt; y\nwariy ---&gt; a\nariya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; b\nsahib ---&gt; a\nahiba ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; c\n..arc ---&gt; h\n.arch ---&gt; n\narchn ---&gt; a\nrchna ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; y\n.sony ---&gt; i\nsonyi ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; o\n.phoo ---&gt; l\nphool ---&gt; a\nhoola ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; n\nchhan ---&gt; y\nhhany ---&gt; a\nhanya ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; p\n.manp ---&gt; r\nmanpr ---&gt; e\nanpre ---&gt; e\nnpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; n\n.navn ---&gt; i\nnavni ---&gt; t\navnit ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; b\nsarab ---&gt; j\narabj ---&gt; e\nrabje ---&gt; e\nabjee ---&gt; t\nbjeet ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; i\n.rami ---&gt; l\nramil ---&gt; a\namila ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; a\n..pha ---&gt; k\n.phak ---&gt; i\nphaki ---&gt; r\nhakir ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; n\nnandn ---&gt; i\nandni ---&gt; .\n..... ---&gt; z\n....z ---&gt; e\n...ze ---&gt; s\n..zes ---&gt; h\n.zesh ---&gt; a\nzesha ---&gt; n\neshan ---&gt; .\n..... ---&gt; a\n....a ---&gt; w\n...aw ---&gt; s\n..aws ---&gt; h\n.awsh ---&gt; i\nawshi ---&gt; n\nwshin ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; n\n..zan ---&gt; m\n.zanm ---&gt; i\nzanmi ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; a\n.hema ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; k\n..nak ---&gt; u\n.naku ---&gt; l\nnakul ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; e\n.sahe ---&gt; n\nsahen ---&gt; a\nahena ---&gt; j\nhenaj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; s\n..bus ---&gt; h\n.bush ---&gt; r\nbushr ---&gt; a\nushra ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; o\n..ano ---&gt; o\n.anoo ---&gt; j\nanooj ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; v\nshanv ---&gt; a\nhanva ---&gt; j\nanvaj ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; r\n.najr ---&gt; a\nnajra ---&gt; n\najran ---&gt; a\njrana ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; a\n.mana ---&gt; v\nmanav ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; m\n.neem ---&gt; a\nneema ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; e\nhande ---&gt; r\nander ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; d\n.navd ---&gt; e\nnavde ---&gt; e\navdee ---&gt; p\nvdeep ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; m\n.cham ---&gt; a\nchama ---&gt; n\nhaman ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; d\n..mud ---&gt; s\n.muds ---&gt; i\nmudsi ---&gt; r\nudsir ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; r\n..mor ---&gt; a\n.mora ---&gt; l\nmoral ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; u\npancu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; v\n.jaiv ---&gt; a\njaiva ---&gt; n\naivan ---&gt; t\nivant ---&gt; i\nvanti ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; y\n..pry ---&gt; i\n.pryi ---&gt; n\npryin ---&gt; k\nryink ---&gt; a\nyinka ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; v\n.bhav ---&gt; u\nbhavu ---&gt; k\nhavuk ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; t\nmanit ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; m\n..vim ---&gt; l\n.viml ---&gt; e\nvimle ---&gt; s\nimles ---&gt; h\nmlesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; k\n.musk ---&gt; a\nmuska ---&gt; n\nuskan ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; a\nkesha ---&gt; v\neshav ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; d\n..ved ---&gt; e\n.vede ---&gt; h\nvedeh ---&gt; i\nedehi ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; c\n..bac ---&gt; h\n.bach ---&gt; c\nbachc ---&gt; h\nachch ---&gt; a\nchcha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; g\n.narg ---&gt; i\nnargi ---&gt; s\nargis ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; a\nkesha ---&gt; n\neshan ---&gt; t\nshant ---&gt; i\nhanti ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; d\n.pand ---&gt; u\npandu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; v\nmahav ---&gt; i\nahavi ---&gt; r\nhavir ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; o\n.najo ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; a\n.rama ---&gt; i\nramai ---&gt; y\namaiy ---&gt; a\nmaiya ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; a\n.kara ---&gt; n\nkaran ---&gt; v\naranv ---&gt; e\nranve ---&gt; e\nanvee ---&gt; r\nnveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; o\n.sato ---&gt; s\nsatos ---&gt; h\natosh ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; m\n..nam ---&gt; e\n.name ---&gt; e\nnamee ---&gt; t\nameet ---&gt; a\nmeeta ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; t\n..lat ---&gt; u\n.latu ---&gt; r\nlatur ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; h\n.nath ---&gt; u\nnathu ---&gt; r\nathur ---&gt; a\nthura ---&gt; m\nhuram ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; h\n.megh ---&gt; a\nmegha ---&gt; r\neghar ---&gt; a\nghara ---&gt; m\nharam ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; s\n..bus ---&gt; h\n.bush ---&gt; a\nbusha ---&gt; r\nushar ---&gt; a\nshara ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; h\n.rukh ---&gt; s\nrukhs ---&gt; o\nukhso ---&gt; n\nkhson ---&gt; a\nhsona ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; r\n..dar ---&gt; k\n.dark ---&gt; a\ndarka ---&gt; s\narkas ---&gt; h\nrkash ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; f\n..jaf ---&gt; a\n.jafa ---&gt; r\njafar ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; w\n.balw ---&gt; a\nbalwa ---&gt; n\nalwan ---&gt; t\nlwant ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; k\n..dak ---&gt; s\n.daks ---&gt; h\ndaksh ---&gt; y\nakshy ---&gt; a\nkshya ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; l\n.amil ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; e\n.vije ---&gt; s\nvijes ---&gt; h\nijesh ---&gt; .\n..... ---&gt; i\n....i ---&gt; k\n...ik ---&gt; a\n..ika ---&gt; r\n.ikar ---&gt; a\nikara ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; b\n.sarb ---&gt; a\nsarba ---&gt; r\narbar ---&gt; i\nrbari ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; e\n.sake ---&gt; e\nsakee ---&gt; n\nakeen ---&gt; a\nkeena ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; k\n..aak ---&gt; a\n.aaka ---&gt; r\naakar ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; i\n.sumi ---&gt; t\nsumit ---&gt; r\numitr ---&gt; a\nmitra ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; r\n..per ---&gt; v\n.perv ---&gt; i\npervi ---&gt; n\nervin ---&gt; d\nrvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; d\n..mid ---&gt; d\n.midd ---&gt; a\nmidda ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; a\n..ima ---&gt; m\n.imam ---&gt; a\nimama ---&gt; n\nmaman ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; a\n.aara ---&gt; d\naarad ---&gt; h\naradh ---&gt; a\nradha ---&gt; n\nadhan ---&gt; a\ndhana ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; y\n.sury ---&gt; a\nsurya ---&gt; n\nuryan ---&gt; a\nryana ---&gt; t\nyanat ---&gt; h\nanath ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; a\n.sura ---&gt; j\nsuraj ---&gt; b\nurajb ---&gt; h\nrajbh ---&gt; a\najbha ---&gt; n\njbhan ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; s\n..aks ---&gt; h\n.aksh ---&gt; a\naksha ---&gt; y\nkshay ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; n\n.chin ---&gt; t\nchint ---&gt; a\nhinta ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; b\n.harb ---&gt; a\nharba ---&gt; i\narbai ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; b\n.shob ---&gt; h\nshobh ---&gt; a\nhobha ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; a\n..afa ---&gt; r\n.afar ---&gt; i\nafari ---&gt; n\nfarin ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; v\n.bhav ---&gt; i\nbhavi ---&gt; s\nhavis ---&gt; a\navisa ---&gt; y\nvisay ---&gt; a\nisaya ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; e\n.vire ---&gt; n\nviren ---&gt; d\nirend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; m\nparam ---&gt; v\naramv ---&gt; e\nramve ---&gt; e\namvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; h\ngulsh ---&gt; a\nulsha ---&gt; n\nlshan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; a\n..naa ---&gt; j\n.naaj ---&gt; i\nnaaji ---&gt; m\naajim ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; h\n.rosh ---&gt; a\nrosha ---&gt; n\noshan ---&gt; a\nshana ---&gt; r\nhanar ---&gt; a\nanara ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; j\n.shaj ---&gt; i\nshaji ---&gt; d\nhajid ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; a\nsanja ---&gt; n\nanjan ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; b\n..nib ---&gt; h\n.nibh ---&gt; a\nnibha ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; k\n.vikk ---&gt; i\nvikki ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; u\n.ranu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; a\n.shoa ---&gt; i\nshoai ---&gt; b\nhoaib ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; r\n.satr ---&gt; u\nsatru ---&gt; d\natrud ---&gt; a\ntruda ---&gt; n\nrudan ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; a\n..aka ---&gt; s\n.akas ---&gt; h\nakash ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; d\n..yud ---&gt; h\n.yudh ---&gt; b\nyudhb ---&gt; i\nudhbi ---&gt; r\ndhbir ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; l\n..nil ---&gt; e\n.nile ---&gt; s\nniles ---&gt; h\nilesh ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; y\n..gay ---&gt; t\n.gayt ---&gt; r\ngaytr ---&gt; i\naytri ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; g\n.ramg ---&gt; o\nramgo ---&gt; p\namgop ---&gt; a\nmgopa ---&gt; l\ngopal ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; a\n.deva ---&gt; s\ndevas ---&gt; h\nevash ---&gt; i\nvashi ---&gt; s\nashis ---&gt; h\nshish ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; u\nchotu ---&gt; r\nhotur ---&gt; a\notura ---&gt; m\nturam ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; g\n.surg ---&gt; y\nsurgy ---&gt; a\nurgya ---&gt; n\nrgyan ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; a\nrisha ---&gt; n\nishan ---&gt; a\nshana ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; l\n..tal ---&gt; i\n.tali ---&gt; m\ntalim ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; v\n.tanv ---&gt; i\ntanvi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; i\nrashi ---&gt; d\nashid ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; i\n.nari ---&gt; n\nnarin ---&gt; d\narind ---&gt; e\nrinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; t\n.pret ---&gt; i\npreti ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; e\n.mune ---&gt; e\nmunee ---&gt; r\nuneer ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; n\n.gyan ---&gt; e\ngyane ---&gt; n\nyanen ---&gt; d\nanend ---&gt; e\nnende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; a\nshana ---&gt; h\nhanah ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; t\n.chat ---&gt; a\nchata ---&gt; r\nhatar ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; v\n..omv ---&gt; e\n.omve ---&gt; e\nomvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; l\n.swal ---&gt; i\nswali ---&gt; y\nwaliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; c\n..bac ---&gt; h\n.bach ---&gt; u\nbachu ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; s\n.vans ---&gt; h\nvansh ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; e\n..swe ---&gt; e\n.swee ---&gt; t\nsweet ---&gt; y\nweety ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; n\n..dan ---&gt; v\n.danv ---&gt; e\ndanve ---&gt; e\nanvee ---&gt; r\nnveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; l\n..sul ---&gt; t\n.sult ---&gt; a\nsulta ---&gt; n\nultan ---&gt; .\n..... ---&gt; s\n....s ---&gt; t\n...st ---&gt; e\n..ste ---&gt; p\n.step ---&gt; h\nsteph ---&gt; e\ntephe ---&gt; n\nephen ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; n\nrishn ---&gt; a\nishna ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; k\n.balk ---&gt; i\nbalki ---&gt; s\nalkis ---&gt; h\nlkish ---&gt; a\nkisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; s\n..jos ---&gt; h\n.josh ---&gt; n\njoshn ---&gt; a\noshna ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; t\n.mitt ---&gt; h\nmitth ---&gt; u\nitthu ---&gt; n\ntthun ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; d\n.gurd ---&gt; a\ngurda ---&gt; y\nurday ---&gt; a\nrdaya ---&gt; l\ndayal ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; r\n..hir ---&gt; a\n.hira ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; y\n..kay ---&gt; u\n.kayu ---&gt; m\nkayum ---&gt; .\n..... ---&gt; e\n....e ---&gt; s\n...es ---&gt; h\n..esh ---&gt; a\n.esha ---&gt; v\neshav ---&gt; a\nshava ---&gt; r\nhavar ---&gt; y\navary ---&gt; a\nvarya ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; a\n.risa ---&gt; b\nrisab ---&gt; h\nisabh ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; n\n.rajn ---&gt; e\nrajne ---&gt; e\najnee ---&gt; s\njnees ---&gt; h\nneesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; h\n.push ---&gt; k\npushk ---&gt; a\nushka ---&gt; r\nshkar ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; t\n..jet ---&gt; e\n.jete ---&gt; n\njeten ---&gt; d\netend ---&gt; e\ntende ---&gt; r\nender ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; i\n.puni ---&gt; t\npunit ---&gt; a\nunita ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; m\n.cham ---&gt; a\nchama ---&gt; n\nhaman ---&gt; l\namanl ---&gt; a\nmanla ---&gt; l\nanlal ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; b\n..hab ---&gt; i\n.habi ---&gt; b\nhabib ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; p\n.rajp ---&gt; a\nrajpa ---&gt; l\najpal ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; j\n.rinj ---&gt; u\nrinju ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; m\n.harm ---&gt; a\nharma ---&gt; n\narman ---&gt; i\nrmani ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; l\n.kall ---&gt; o\nkallo ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; r\n.subr ---&gt; a\nsubra ---&gt; t\nubrat ---&gt; i\nbrati ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; n\n..sen ---&gt; t\n.sent ---&gt; h\nsenth ---&gt; i\nenthi ---&gt; a\nnthia ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; i\n.kari ---&gt; n\nkarin ---&gt; a\narina ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; t\n..hit ---&gt; l\n.hitl ---&gt; a\nhitla ---&gt; r\nitlar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; e\n.rame ---&gt; h\nrameh ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; a\n.dila ---&gt; w\ndilaw ---&gt; a\nilawa ---&gt; r\nlawar ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; u\nneelu ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; c\n..vic ---&gt; a\n.vica ---&gt; k\nvicak ---&gt; h\nicakh ---&gt; a\ncakha ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; d\n.jayd ---&gt; a\njayda ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; r\n.rajr ---&gt; a\nrajra ---&gt; n\najran ---&gt; i\njrani ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; i\n.rubi ---&gt; y\nrubiy ---&gt; a\nubiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; f\n.sarf ---&gt; r\nsarfr ---&gt; a\narfra ---&gt; j\nrfraj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; f\n..raf ---&gt; e\n.rafe ---&gt; d\nrafed ---&gt; d\nafedd ---&gt; i\nfeddi ---&gt; n\neddin ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; n\namrin ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; s\n..pas ---&gt; a\n.pasa ---&gt; n\npasan ---&gt; j\nasanj ---&gt; e\nsanje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; a\n..ima ---&gt; m\n.imam ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; u\n..tau ---&gt; s\n.taus ---&gt; i\ntausi ---&gt; n\nausin ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; s\n..ims ---&gt; a\n.imsa ---&gt; a\nimsaa ---&gt; n\nmsaan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; p\n.bhup ---&gt; e\nbhupe ---&gt; s\nhupes ---&gt; h\nupesh ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; u\n.guru ---&gt; b\ngurub ---&gt; a\nuruba ---&gt; k\nrubak ---&gt; s\nubaks ---&gt; h\nbaksh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; z\n..saz ---&gt; i\n.sazi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; l\n..hal ---&gt; k\n.halk ---&gt; i\nhalki ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; a\n.suba ---&gt; n\nsuban ---&gt; i\nubani ---&gt; .\n..... ---&gt; i\n....i ---&gt; k\n...ik ---&gt; r\n..ikr ---&gt; a\n.ikra ---&gt; r\nikrar ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; i\n.nasi ---&gt; a\nnasia ---&gt; r\nasiar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; e\n.rame ---&gt; s\nrames ---&gt; w\namesw ---&gt; a\nmeswa ---&gt; r\neswar ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; h\n.rosh ---&gt; a\nrosha ---&gt; n\noshan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; e\n.nave ---&gt; e\nnavee ---&gt; n\naveen ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; a\n..nea ---&gt; h\n.neah ---&gt; a\nneaha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; d\nravid ---&gt; u\navidu ---&gt; t\nvidut ---&gt; t\nidutt ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; d\n.sund ---&gt; a\nsunda ---&gt; r\nundar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; a\n.saya ---&gt; n\nsayan ---&gt; a\nayana ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; u\n.hanu ---&gt; m\nhanum ---&gt; a\nanuma ---&gt; n\nnuman ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; n\n.rajn ---&gt; i\nrajni ---&gt; s\najnis ---&gt; h\njnish ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; i\nsushi ---&gt; l\nushil ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; s\n.mees ---&gt; h\nmeesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; w\nahnaw ---&gt; a\nhnawa ---&gt; z\nnawaz ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; a\nparta ---&gt; p\nartap ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; u\n.niru ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; r\n.dhur ---&gt; p\ndhurp ---&gt; a\nhurpa ---&gt; t\nurpat ---&gt; i\nrpati ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; d\nhahid ---&gt; a\nahida ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; o\n.mano ---&gt; h\nmanoh ---&gt; a\nanoha ---&gt; r\nnohar ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; b\n.sehb ---&gt; o\nsehbo ---&gt; o\nehboo ---&gt; b\nhboob ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; p\n.pusp ---&gt; a\npuspa ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; y\n..nay ---&gt; a\n.naya ---&gt; k\nnayak ---&gt; a\nayaka ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; k\n.devk ---&gt; r\ndevkr ---&gt; a\nevkra ---&gt; n\nvkran ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; u\n..fau ---&gt; j\n.fauj ---&gt; i\nfauji ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; a\n..kra ---&gt; s\n.kras ---&gt; h\nkrash ---&gt; n\nrashn ---&gt; a\nashna ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; a\n.susa ---&gt; n\nsusan ---&gt; t\nusant ---&gt; o\nsanto ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; i\n.nasi ---&gt; b\nnasib ---&gt; u\nasibu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; h\n..nah ---&gt; a\n.naha ---&gt; r\nnahar ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; m\n..mem ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; i\n.shri ---&gt; s\nshris ---&gt; h\nhrish ---&gt; t\nrisht ---&gt; i\nishti ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; a\n..aja ---&gt; y\n.ajay ---&gt; p\najayp ---&gt; a\njaypa ---&gt; l\naypal ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; a\n.lila ---&gt; v\nlilav ---&gt; a\nilava ---&gt; t\nlavat ---&gt; i\navati ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; v\n..dav ---&gt; e\n.dave ---&gt; n\ndaven ---&gt; d\navend ---&gt; r\nvendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; k\n.rink ---&gt; o\nrinko ---&gt; o\ninkoo ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; n\n..ren ---&gt; n\n.renn ---&gt; u\nrennu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; r\nshabr ---&gt; e\nhabre ---&gt; e\nabree ---&gt; n\nbreen ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; f\n.gurf ---&gt; a\ngurfa ---&gt; a\nurfaa ---&gt; n\nrfaan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; s\n.sams ---&gt; u\nsamsu ---&gt; d\namsud ---&gt; d\nmsudd ---&gt; i\nsuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; r\n.ompr ---&gt; k\nomprk ---&gt; a\nmprka ---&gt; s\nprkas ---&gt; h\nrkash ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; h\n..mih ---&gt; a\n.miha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; v\n.ramv ---&gt; e\nramve ---&gt; e\namvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; m\n.susm ---&gt; a\nsusma ---&gt; t\nusmat ---&gt; a\nsmata ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; h\n.sheh ---&gt; n\nshehn ---&gt; a\nhehna ---&gt; z\nehnaz ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; r\n.musr ---&gt; a\nmusra ---&gt; t\nusrat ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; s\n..hus ---&gt; i\n.husi ---&gt; y\nhusiy ---&gt; a\nusiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; r\nsukhr ---&gt; a\nukhra ---&gt; m\nkhram ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; i\n.mohi ---&gt; n\nmohin ---&gt; i\nohini ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; w\n.tabw ---&gt; s\ntabws ---&gt; u\nabwsu ---&gt; m\nbwsum ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; v\n..div ---&gt; y\n.divy ---&gt; a\ndivya ---&gt; n\nivyan ---&gt; s\nvyans ---&gt; h\nyansh ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; p\n..nip ---&gt; u\n.nipu ---&gt; n\nnipun ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; b\n.mahb ---&gt; u\nmahbu ---&gt; b\nahbub ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; i\n.sali ---&gt; n\nsalin ---&gt; i\nalini ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; s\n..dis ---&gt; i\n.disi ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; s\n..gos ---&gt; i\n.gosi ---&gt; a\ngosia ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; i\n.niki ---&gt; t\nnikit ---&gt; a\nikita ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; a\nhusha ---&gt; b\nushab ---&gt; u\nshabu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; e\n.sahe ---&gt; e\nsahee ---&gt; n\naheen ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; g\n.gang ---&gt; a\nganga ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; y\n..piy ---&gt; u\n.piyu ---&gt; s\npiyus ---&gt; h\niyush ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; a\nparsa ---&gt; n\narsan ---&gt; t\nrsant ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; l\n..jul ---&gt; e\n.jule ---&gt; e\njulee ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; a\n.safa ---&gt; l\nsafal ---&gt; i\nafali ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; k\n.reek ---&gt; h\nreekh ---&gt; a\neekha ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; s\n..ars ---&gt; h\n.arsh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; i\n.shei ---&gt; k\nsheik ---&gt; h\nheikh ---&gt; s\neikhs ---&gt; a\nikhsa ---&gt; i\nkhsai ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; t\n.kirt ---&gt; i\nkirti ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; i\n.puni ---&gt; t\npunit ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; r\nshivr ---&gt; a\nhivra ---&gt; j\nivraj ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; h\n..joh ---&gt; o\n.joho ---&gt; l\njohol ---&gt; a\nohola ---&gt; l\nholal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; y\nbhagy ---&gt; a\nhagya ---&gt; .\n..... ---&gt; m\n....m ---&gt; h\n...mh ---&gt; e\n..mhe ---&gt; g\n.mheg ---&gt; a\nmhega ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; r\n..nur ---&gt; j\n.nurj ---&gt; a\nnurja ---&gt; h\nurjah ---&gt; a\nrjaha ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; a\nharma ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; e\nsande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; h\nrambh ---&gt; a\nambha ---&gt; j\nmbhaj ---&gt; a\nbhaja ---&gt; n\nhajan ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; e\n.deve ---&gt; n\ndeven ---&gt; d\nevend ---&gt; r\nvendr ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; w\n..kaw ---&gt; a\n.kawa ---&gt; l\nkawal ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; i\n.somi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; l\n.shil ---&gt; l\nshill ---&gt; y\nhilly ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; o\n.saro ---&gt; o\nsaroo ---&gt; p\naroop ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; e\n..ale ---&gt; e\n.alee ---&gt; s\nalees ---&gt; h\nleesh ---&gt; a\neesha ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; p\n..tap ---&gt; e\n.tape ---&gt; n\ntapen ---&gt; d\napend ---&gt; e\npende ---&gt; r\nender ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; r\nnandr ---&gt; a\nandra ---&gt; m\nndram ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; f\n..hif ---&gt; j\n.hifj ---&gt; u\nhifju ---&gt; l\nifjul ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; n\n..yun ---&gt; i\n.yuni ---&gt; s\nyunis ---&gt; h\nunish ---&gt; s\nnishs ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; w\n..suw ---&gt; a\n.suwa ---&gt; d\nsuwad ---&gt; h\nuwadh ---&gt; i\nwadhi ---&gt; n\nadhin ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; k\n.rink ---&gt; i\nrinki ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; h\n..zah ---&gt; e\n.zahe ---&gt; e\nzahee ---&gt; r\naheer ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; v\n.manv ---&gt; e\nmanve ---&gt; n\nanven ---&gt; d\nnvend ---&gt; r\nvendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; a\n.suka ---&gt; n\nsukan ---&gt; y\nukany ---&gt; a\nkanya ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; a\nranja ---&gt; n\nanjan ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; n\n..jen ---&gt; i\n.jeni ---&gt; f\njenif ---&gt; e\nenife ---&gt; r\nnifer ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; i\nrashi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; p\nubhap ---&gt; a\nbhapa ---&gt; l\nhapal ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; k\neepak ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; i\ndhani ---&gt; y\nhaniy ---&gt; a\naniya ---&gt; .\n..... ---&gt; u\n....u ---&gt; r\n...ur ---&gt; v\n..urv ---&gt; a\n.urva ---&gt; s\nurvas ---&gt; h\nrvash ---&gt; i\nvashi ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; s\n..mos ---&gt; h\n.mosh ---&gt; a\nmosha ---&gt; d\noshad ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; n\n..ann ---&gt; i\n.anni ---&gt; e\nannie ---&gt; l\nnniel ---&gt; a\nniela ---&gt; l\nielal ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; a\n.soha ---&gt; n\nsohan ---&gt; p\nohanp ---&gt; a\nhanpa ---&gt; l\nanpal ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; t\n..akt ---&gt; h\n.akth ---&gt; a\naktha ---&gt; r\nkthar ---&gt; i\nthari ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; s\n.shas ---&gt; h\nshash ---&gt; i\nhashi ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; s\n.tabs ---&gt; u\ntabsu ---&gt; m\nabsum ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; z\n..faz ---&gt; i\n.fazi ---&gt; n\nfazin ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; k\n..jak ---&gt; i\n.jaki ---&gt; r\njakir ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; j\n..bij ---&gt; e\n.bije ---&gt; n\nbijen ---&gt; d\nijend ---&gt; e\njende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; d\n.shid ---&gt; h\nshidh ---&gt; a\nhidha ---&gt; r\nidhar ---&gt; t\ndhart ---&gt; h\nharth ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; d\n.vand ---&gt; h\nvandh ---&gt; a\nandha ---&gt; n\nndhan ---&gt; a\ndhana ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; m\nrashm ---&gt; a\nashma ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; m\nlakhm ---&gt; i\nakhmi ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; j\n.nirj ---&gt; l\nnirjl ---&gt; a\nirjla ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; a\n.kala ---&gt; w\nkalaw ---&gt; a\nalawa ---&gt; t\nlawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; e\n.nave ---&gt; e\nnavee ---&gt; l\naveel ---&gt; a\nveela ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; m\n.rahm ---&gt; a\nrahma ---&gt; t\nahmat ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; h\n..suh ---&gt; e\n.suhe ---&gt; l\nsuhel ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; m\n..dam ---&gt; i\n.dami ---&gt; n\ndamin ---&gt; i\namini ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; t\n.meet ---&gt; u\nmeetu ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; t\n.chit ---&gt; r\nchitr ---&gt; a\nhitra ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; v\n.mahv ---&gt; i\nmahvi ---&gt; s\nahvis ---&gt; h\nhvish ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; d\n.prad ---&gt; e\nprade ---&gt; e\nradee ---&gt; p\nadeep ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; a\n..ala ---&gt; p\n.alap ---&gt; n\nalapn ---&gt; a\nlapna ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; a\n..kaa ---&gt; m\n.kaam ---&gt; i\nkaami ---&gt; n\naamin ---&gt; i\namini ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; z\n..saz ---&gt; m\n.sazm ---&gt; i\nsazmi ---&gt; n\nazmin ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; t\n.kart ---&gt; i\nkarti ---&gt; k\nartik ---&gt; a\nrtika ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; a\n.kesa ---&gt; v\nkesav ---&gt; i\nesavi ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; l\n.dhul ---&gt; i\ndhuli ---&gt; n\nhulin ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; i\ndeepi ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; n\n..pin ---&gt; t\n.pint ---&gt; t\npintt ---&gt; u\ninttu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; i\nramki ---&gt; s\namkis ---&gt; h\nmkish ---&gt; a\nkisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; t\n..net ---&gt; r\n.netr ---&gt; a\nnetra ---&gt; p\netrap ---&gt; a\ntrapa ---&gt; l\nrapal ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; i\n.yasi ---&gt; n\nyasin ---&gt; .\n..... ---&gt; t\n....t ---&gt; w\n...tw ---&gt; i\n..twi ---&gt; n\n.twin ---&gt; k\ntwink ---&gt; i\nwinki ---&gt; l\ninkil ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; n\n..ren ---&gt; u\n.renu ---&gt; k\nrenuk ---&gt; a\nenuka ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; n\n.karn ---&gt; e\nkarne ---&gt; s\narnes ---&gt; h\nrnesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; h\nsushh ---&gt; m\nushhm ---&gt; i\nshhmi ---&gt; t\nhhmit ---&gt; a\nhmita ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; s\n..sis ---&gt; p\n.sisp ---&gt; a\nsispa ---&gt; l\nispal ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; e\n..sne ---&gt; h\n.sneh ---&gt; a\nsneha ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; d\n..rud ---&gt; h\n.rudh ---&gt; r\nrudhr ---&gt; a\nudhra ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; j\n.girj ---&gt; a\ngirja ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; r\nmohar ---&gt; d\nohard ---&gt; i\nhardi ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; s\n.dils ---&gt; h\ndilsh ---&gt; a\nilsha ---&gt; d\nlshad ---&gt; i\nshadi ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; u\nmangu ---&gt; b\nangub ---&gt; a\nnguba ---&gt; i\ngubai ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; r\n..gor ---&gt; d\n.gord ---&gt; h\ngordh ---&gt; a\nordha ---&gt; n\nrdhan ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; s\n..res ---&gt; h\n.resh ---&gt; a\nresha ---&gt; m\nesham ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; m\n.rukm ---&gt; a\nrukma ---&gt; n\nukman ---&gt; i\nkmani ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; u\n.aaru ---&gt; s\naarus ---&gt; h\narush ---&gt; i\nrushi ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; m\n.dham ---&gt; e\ndhame ---&gt; n\nhamen ---&gt; d\namend ---&gt; e\nmende ---&gt; r\nender ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; i\n..bri ---&gt; j\n.brij ---&gt; m\nbrijm ---&gt; o\nrijmo ---&gt; h\nijmoh ---&gt; a\njmoha ---&gt; n\nmohan ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; b\n.dilb ---&gt; a\ndilba ---&gt; r\nilbar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; r\n.beer ---&gt; a\nbeera ---&gt; m\neeram ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; d\n.kund ---&gt; e\nkunde ---&gt; n\nunden ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; n\n.sann ---&gt; a\nsanna ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; h\n..fah ---&gt; i\n.fahi ---&gt; j\nfahij ---&gt; a\nahija ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; m\n.jaim ---&gt; a\njaima ---&gt; l\naimal ---&gt; a\nimala ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; f\n..kif ---&gt; a\n.kifa ---&gt; y\nkifay ---&gt; a\nifaya ---&gt; t\nfayat ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; k\n.bhak ---&gt; u\nbhaku ---&gt; n\nhakun ---&gt; i\nakuni ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; a\n.ompa ---&gt; l\nompal ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; i\n..hai ---&gt; l\n.hail ---&gt; e\nhaile ---&gt; n\nailen ---&gt; a\nilena ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; d\n..nid ---&gt; h\n.nidh ---&gt; i\nnidhi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; i\n.lali ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; l\n..kel ---&gt; i\n.keli ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; n\n..bun ---&gt; t\n.bunt ---&gt; y\nbunty ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; s\nshams ---&gt; h\nhamsh ---&gt; a\namsha ---&gt; d\nmshad ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; m\n.karm ---&gt; v\nkarmv ---&gt; e\narmve ---&gt; e\nrmvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; f\n.sarf ---&gt; a\nsarfa ---&gt; r\narfar ---&gt; a\nrfara ---&gt; j\nfaraj ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; a\nveera ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; m\n.anam ---&gt; i\nanami ---&gt; k\nnamik ---&gt; a\namika ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; i\n..asi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; i\n.sabi ---&gt; r\nsabir ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; i\n..mai ---&gt; r\n.mair ---&gt; y\nmairy ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; a\n.rosa ---&gt; n\nrosan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; a\n.nara ---&gt; y\nnaray ---&gt; a\naraya ---&gt; n\nrayan ---&gt; i\nayani ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; i\n.vini ---&gt; t\nvinit ---&gt; a\ninita ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; i\n..moi ---&gt; n\n.moin ---&gt; u\nmoinu ---&gt; d\noinud ---&gt; d\ninudd ---&gt; i\nnuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; l\n.mahl ---&gt; i\nmahli ---&gt; k\nahlik ---&gt; k\nhlikk ---&gt; a\nlikka ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; i\nramdi ---&gt; n\namdin ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; a\n.ompa ---&gt; r\nompar ---&gt; k\nmpark ---&gt; e\nparke ---&gt; s\narkes ---&gt; h\nrkesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; a\n..ara ---&gt; d\n.arad ---&gt; h\naradh ---&gt; a\nradha ---&gt; n\nadhan ---&gt; a\ndhana ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; i\n.rani ---&gt; y\nraniy ---&gt; a\naniya ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; a\n.dura ---&gt; g\ndurag ---&gt; a\nuraga ---&gt; .\n..... ---&gt; p\n....p ---&gt; y\n...py ---&gt; a\n..pya ---&gt; r\n.pyar ---&gt; i\npyari ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; j\nabhij ---&gt; e\nbhije ---&gt; e\nhijee ---&gt; t\nijeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; a\nsanja ---&gt; y\nanjay ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; e\n.kale ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; h\n.kish ---&gt; o\nkisho ---&gt; r\nishor ---&gt; e\nshore ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; e\n..ade ---&gt; r\n.ader ---&gt; s\naders ---&gt; e\nderse ---&gt; n\nersen ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; l\n.rupl ---&gt; a\nrupla ---&gt; l\nuplal ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; o\n.subo ---&gt; d\nsubod ---&gt; h\nubodh ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; g\n..jog ---&gt; i\n.jogi ---&gt; n\njogin ---&gt; d\nogind ---&gt; e\nginde ---&gt; r\ninder ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; s\n.shas ---&gt; h\nshash ---&gt; a\nhasha ---&gt; n\nashan ---&gt; k\nshank ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; s\n.anas ---&gt; h\nanash ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; r\n.samr ---&gt; i\nsamri ---&gt; n\namrin ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; e\n..swe ---&gt; e\n.swee ---&gt; t\nsweet ---&gt; i\nweeti ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; s\n..bis ---&gt; h\n.bish ---&gt; a\nbisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; a\n..saa ---&gt; l\n.saal ---&gt; u\nsaalu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; e\n.sude ---&gt; s\nsudes ---&gt; h\nudesh ---&gt; w\ndeshw ---&gt; e\neshwe ---&gt; r\nshwer ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; s\nravis ---&gt; h\navish ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; h\n..wah ---&gt; i\n.wahi ---&gt; d\nwahid ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; h\n.ansh ---&gt; i\nanshi ---&gt; y\nnshiy ---&gt; a\nshiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; e\nshale ---&gt; n\nhalen ---&gt; d\nalend ---&gt; e\nlende ---&gt; r\nender ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; s\n..dis ---&gt; h\n.dish ---&gt; a\ndisha ---&gt; n\nishan ---&gt; t\nshant ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; r\n..gor ---&gt; u\n.goru ---&gt; a\ngorua ---&gt; v\noruav ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; m\n.bhum ---&gt; i\nbhumi ---&gt; k\nhumik ---&gt; a\numika ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; y\n..nay ---&gt; n\n.nayn ---&gt; a\nnayna ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; j\n.sajj ---&gt; a\nsajja ---&gt; n\najjan ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; t\n.mont ---&gt; u\nmontu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; h\nramdh ---&gt; i\namdhi ---&gt; n\nmdhin ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; c\n..kac ---&gt; h\n.kach ---&gt; r\nkachr ---&gt; i\nachri ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; k\n..tek ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; e\n.rahe ---&gt; e\nrahee ---&gt; s\nahees ---&gt; h\nheesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; i\nsubhi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; i\nrambi ---&gt; r\nambir ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; i\n.yasi ---&gt; b\nyasib ---&gt; .\n..... ---&gt; e\n....e ---&gt; s\n...es ---&gt; h\n..esh ---&gt; w\n.eshw ---&gt; a\neshwa ---&gt; r\nshwar ---&gt; i\nhwari ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; a\n..ara ---&gt; d\n.arad ---&gt; d\naradd ---&gt; h\nraddh ---&gt; n\naddhn ---&gt; a\nddhna ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; r\n.char ---&gt; a\nchara ---&gt; n\nharan ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; k\n..ajk ---&gt; a\n.ajka ---&gt; t\najkat ---&gt; i\njkati ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; h\nrambh ---&gt; o\nambho ---&gt; o\nmbhoo ---&gt; l\nbhool ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; m\n..mom ---&gt; i\n.momi ---&gt; n\nmomin ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; i\n.saki ---&gt; r\nsakir ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; a\nshara ---&gt; d\nharad ---&gt; h\naradh ---&gt; h\nradhh ---&gt; a\nadhha ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; n\nhiman ---&gt; s\nimans ---&gt; i\nmansi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; e\n.shee ---&gt; l\nsheel ---&gt; u\nheelu ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; a\n.bada ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; e\n..ake ---&gt; e\n.akee ---&gt; l\nakeel ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; m\n..rim ---&gt; p\n.rimp ---&gt; i\nrimpi ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; v\n.seev ---&gt; a\nseeva ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; m\n.bhim ---&gt; s\nbhims ---&gt; e\nhimse ---&gt; n\nimsen ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; t\n.adit ---&gt; i\naditi ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; k\n..bik ---&gt; k\n.bikk ---&gt; i\nbikki ---&gt; .\n..... ---&gt; t\n....t ---&gt; h\n...th ---&gt; a\n..tha ---&gt; r\n.thar ---&gt; u\ntharu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; k\n.rakk ---&gt; i\nrakki ---&gt; b\nakkib ---&gt; h\nkkibh ---&gt; u\nkibhu ---&gt; l\nibhul ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; l\nanjal ---&gt; y\nnjaly ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; w\n.rajw ---&gt; a\nrajwa ---&gt; n\najwan ---&gt; t\njwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; j\n..arj ---&gt; u\n.arju ---&gt; n\narjun ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; e\n.save ---&gt; t\nsavet ---&gt; a\naveta ---&gt; .\n..... ---&gt; z\n....z ---&gt; i\n...zi ---&gt; y\n..ziy ---&gt; a\n.ziya ---&gt; b\nziyab ---&gt; u\niyabu ---&gt; l\nyabul ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; c\n..suc ---&gt; h\n.such ---&gt; i\nsuchi ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; t\n..pit ---&gt; i\n.piti ---&gt; k\npitik ---&gt; a\nitika ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; t\n.jeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; o\n.sano ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; u\n..kau ---&gt; v\n.kauv ---&gt; a\nkauva ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; a\n..asa ---&gt; n\n.asan ---&gt; t\nasant ---&gt; i\nsanti ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; i\n.riti ---&gt; k\nritik ---&gt; a\nitika ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; b\n.ranb ---&gt; i\nranbi ---&gt; r\nanbir ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; n\n.chin ---&gt; t\nchint ---&gt; u\nhintu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; c\n..sac ---&gt; h\n.sach ---&gt; i\nsachi ---&gt; n\nachin ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; n\nsanjn ---&gt; a\nanjna ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; i\n.fari ---&gt; d\nfarid ---&gt; a\narida ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; k\n.rajk ---&gt; u\nrajku ---&gt; m\najkum ---&gt; a\njkuma ---&gt; r\nkumar ---&gt; .\n..... ---&gt; a\n....a ---&gt; p\n...ap ---&gt; s\n..aps ---&gt; a\n.apsa ---&gt; n\napsan ---&gt; a\npsana ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; t\n.munt ---&gt; a\nmunta ---&gt; j\nuntaj ---&gt; a\nntaja ---&gt; r\ntajar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; m\n.salm ---&gt; a\nsalma ---&gt; a\nalmaa ---&gt; n\nlmaan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; e\nhande ---&gt; s\nandes ---&gt; h\nndesh ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; a\n.rata ---&gt; n\nratan ---&gt; i\natani ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; w\n..isw ---&gt; a\n.iswa ---&gt; r\niswar ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; r\n..her ---&gt; a\n.hera ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; l\n..til ---&gt; k\n.tilk ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; y\n..piy ---&gt; u\n.piyu ---&gt; s\npiyus ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; h\n.maah ---&gt; i\nmaahi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; o\n.salo ---&gt; n\nsalon ---&gt; i\naloni ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; i\n.bani ---&gt; t\nbanit ---&gt; a\nanita ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; l\n..mol ---&gt; u\n.molu ---&gt; .\n..... ---&gt; i\n....i ---&gt; r\n...ir ---&gt; t\n..irt ---&gt; u\n.irtu ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; h\n..udh ---&gt; a\n.udha ---&gt; m\nudham ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; b\n.sarb ---&gt; j\nsarbj ---&gt; e\narbje ---&gt; e\nrbjee ---&gt; t\nbjeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; i\n..asi ---&gt; s\n.asis ---&gt; h\nasish ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; l\n..sil ---&gt; e\n.sile ---&gt; n\nsilen ---&gt; d\nilend ---&gt; e\nlende ---&gt; r\nender ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; u\n..chu ---&gt; n\n.chun ---&gt; n\nchunn ---&gt; i\nhunni ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; d\n..had ---&gt; a\n.hada ---&gt; r\nhadar ---&gt; a\nadara ---&gt; l\ndaral ---&gt; i\narali ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; s\nubhas ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; e\n.same ---&gt; e\nsamee ---&gt; r\nameer ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; s\n..dis ---&gt; h\n.dish ---&gt; a\ndisha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; n\n.rajn ---&gt; i\nrajni ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; m\n.farm ---&gt; a\nfarma ---&gt; a\narmaa ---&gt; n\nrmaan ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; h\ngulsh ---&gt; a\nulsha ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; z\n..waz ---&gt; i\n.wazi ---&gt; r\nwazir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; r\n.sair ---&gt; a\nsaira ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; s\n.saks ---&gt; i\nsaksi ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; h\n..azh ---&gt; a\n.azha ---&gt; r\nazhar ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; k\n..brk ---&gt; h\n.brkh ---&gt; a\nbrkha ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; l\n.jhal ---&gt; l\njhall ---&gt; a\nhalla ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; u\n.bhau ---&gt; k\nbhauk ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; a\nsusha ---&gt; n\nushan ---&gt; t\nshant ---&gt; .\n..... ---&gt; i\n....i ---&gt; b\n...ib ---&gt; a\n..iba ---&gt; r\n.ibar ---&gt; h\nibarh ---&gt; i\nbarhi ---&gt; m\narhim ---&gt; .\n..... ---&gt; y\n....y ---&gt; i\n...yi ---&gt; n\n..yin ---&gt; i\n.yini ---&gt; t\nyinit ---&gt; a\ninita ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; w\n..new ---&gt; a\n.newa ---&gt; l\nnewal ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; v\n.rajv ---&gt; i\nrajvi ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; a\n.kama ---&gt; n\nkaman ---&gt; a\namana ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; h\n.rukh ---&gt; s\nrukhs ---&gt; a\nukhsa ---&gt; r\nkhsar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; c\n..rac ---&gt; h\n.rach ---&gt; n\nrachn ---&gt; a\nachna ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; a\n.dipa ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; e\n.saye ---&gt; r\nsayer ---&gt; i\nayeri ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; d\n.lald ---&gt; h\nlaldh ---&gt; a\naldha ---&gt; r\nldhar ---&gt; i\ndhari ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; i\n..bri ---&gt; j\n.brij ---&gt; p\nbrijp ---&gt; a\nrijpa ---&gt; l\nijpal ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; s\n.aans ---&gt; u\naansu ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; a\n.meha ---&gt; r\nmehar ---&gt; b\neharb ---&gt; a\nharba ---&gt; n\narban ---&gt; .\n..... ---&gt; l\n....l ---&gt; u\n...lu ---&gt; c\n..luc ---&gt; k\n.luck ---&gt; i\nlucki ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; t\n.akht ---&gt; a\nakhta ---&gt; r\nkhtar ---&gt; i\nhtari ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; r\n.sadr ---&gt; e\nsadre ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; n\n.ratn ---&gt; i\nratni ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; k\n..dak ---&gt; c\n.dakc ---&gt; h\ndakch ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; v\n..lov ---&gt; e\n.love ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; t\n.mint ---&gt; u\nmintu ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; h\n.bash ---&gt; a\nbasha ---&gt; n\nashan ---&gt; t\nshant ---&gt; i\nhanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; e\n.save ---&gt; r\nsaver ---&gt; o\navero ---&gt; o\nveroo ---&gt; n\neroon ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; z\n..raz ---&gt; n\n.razn ---&gt; i\nrazni ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; h\n.kanh ---&gt; e\nkanhe ---&gt; y\nanhey ---&gt; a\nnheya ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; f\n.praf ---&gt; h\nprafh ---&gt; o\nrafho ---&gt; o\nafhoo ---&gt; l\nfhool ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; u\n..piu ---&gt; e\n.piue ---&gt; s\npiues ---&gt; h\niuesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; h\n.bhah ---&gt; a\nbhaha ---&gt; d\nhahad ---&gt; u\nahadu ---&gt; r\nhadur ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; l\nanjal ---&gt; i\nnjali ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; l\n..sul ---&gt; e\n.sule ---&gt; m\nsulem ---&gt; a\nulema ---&gt; n\nleman ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; i\n.viri ---&gt; y\nviriy ---&gt; a\niriya ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; w\n..tiw ---&gt; a\n.tiwa ---&gt; n\ntiwan ---&gt; k\niwank ---&gt; l\nwankl ---&gt; e\nankle ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; s\n.dils ---&gt; a\ndilsa ---&gt; h\nilsah ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; h\n.mush ---&gt; i\nmushi ---&gt; r\nushir ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; i\nrashi ---&gt; k\nashik ---&gt; a\nshika ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; m\n..kim ---&gt; a\n.kima ---&gt; t\nkimat ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; i\n..mai ---&gt; n\n.main ---&gt; a\nmaina ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; i\nprati ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; a\ncheta ---&gt; n\nhetan ---&gt; r\netanr ---&gt; a\ntanra ---&gt; m\nanram ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; m\nparam ---&gt; d\naramd ---&gt; e\nramde ---&gt; e\namdee ---&gt; p\nmdeep ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; n\n..isn ---&gt; e\n.isne ---&gt; s\nisnes ---&gt; h\nsnesh ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; c\n..ric ---&gt; h\n.rich ---&gt; a\nricha ---&gt; r\nichar ---&gt; a\nchara ---&gt; j\nharaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; e\n.safe ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; y\n.aany ---&gt; a\naanya ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; t\n..got ---&gt; a\n.gota ---&gt; m\ngotam ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; p\n..bip ---&gt; i\n.bipi ---&gt; n\nbipin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; b\n..rab ---&gt; i\n.rabi ---&gt; n\nrabin ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; h\n..neh ---&gt; a\n.neha ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; p\n.chap ---&gt; p\nchapp ---&gt; l\nhappl ---&gt; a\nappla ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; r\n..nur ---&gt; a\n.nura ---&gt; i\nnurai ---&gt; s\nurais ---&gt; h\nraish ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; i\n.jagi ---&gt; r\njagir ---&gt; .\n..... ---&gt; f\n....f ---&gt; u\n...fu ---&gt; r\n..fur ---&gt; k\n.furk ---&gt; a\nfurka ---&gt; n\nurkan ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; b\n..bob ---&gt; y\n.boby ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; g\n..jug ---&gt; a\n.juga ---&gt; n\njugan ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; b\n..rob ---&gt; e\n.robe ---&gt; r\nrober ---&gt; t\nobert ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; e\n.suke ---&gt; s\nsukes ---&gt; h\nukesh ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; u\ndhanu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; a\n.kana ---&gt; h\nkanah ---&gt; i\nanahi ---&gt; y\nnahiy ---&gt; a\nahiya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; e\n.naze ---&gt; e\nnazee ---&gt; m\nazeem ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; a\nshiva ---&gt; m\nhivam ---&gt; .\n..... ---&gt; g\n....g ---&gt; r\n...gr ---&gt; p\n..grp ---&gt; r\n.grpr ---&gt; e\ngrpre ---&gt; e\nrpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; n\nriyan ---&gt; s\niyans ---&gt; h\nyansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; o\n.sugo ---&gt; d\nsugod ---&gt; h\nugodh ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; f\n..kaf ---&gt; i\n.kafi ---&gt; a\nkafia ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; k\n.shek ---&gt; i\nsheki ---&gt; b\nhekib ---&gt; a\nekiba ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; a\n.mina ---&gt; x\nminax ---&gt; i\ninaxi ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; h\n.parh ---&gt; a\nparha ---&gt; l\narhal ---&gt; a\nrhala ---&gt; d\nhalad ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; u\n..gau ---&gt; t\n.gaut ---&gt; a\ngauta ---&gt; m\nautam ---&gt; .\n..... ---&gt; w\n....w ---&gt; i\n...wi ---&gt; r\n..wir ---&gt; e\n.wire ---&gt; s\nwires ---&gt; h\niresh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; a\nmanda ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; a\n.suna ---&gt; y\nsunay ---&gt; n\nunayn ---&gt; a\nnayna ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; m\n.dhrm ---&gt; v\ndhrmv ---&gt; e\nhrmve ---&gt; e\nrmvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; t\n..aat ---&gt; i\n.aati ---&gt; k\naatik ---&gt; u\natiku ---&gt; n\ntikun ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; s\n..ars ---&gt; h\n.arsh ---&gt; l\narshl ---&gt; a\nrshla ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; s\n.shas ---&gt; w\nshasw ---&gt; a\nhaswa ---&gt; t\naswat ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; t\n..att ---&gt; a\n.atta ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; r\n.megr ---&gt; a\nmegra ---&gt; a\negraa ---&gt; j\ngraaj ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; j\nnoorj ---&gt; h\noorjh ---&gt; a\norjha ---&gt; n\nrjhan ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; o\n..foo ---&gt; l\n.fool ---&gt; w\nfoolw ---&gt; a\noolwa ---&gt; t\nolwat ---&gt; i\nlwati ---&gt; .\n..... ---&gt; s\n....s ---&gt; r\n...sr ---&gt; h\n..srh ---&gt; e\n.srhe ---&gt; e\nsrhee ---&gt; r\nrheer ---&gt; a\nheera ---&gt; m\neeram ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; s\n.amis ---&gt; h\namish ---&gt; a\nmisha ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; j\n.anuj ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; r\n..isr ---&gt; a\n.isra ---&gt; r\nisrar ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; m\n..dim ---&gt; p\n.dimp ---&gt; y\ndimpy ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; b\n..vib ---&gt; h\n.vibh ---&gt; a\nvibha ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; m\n..asm ---&gt; a\n.asma ---&gt; t\nasmat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; d\nsamad ---&gt; h\namadh ---&gt; a\nmadha ---&gt; n\nadhan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; f\n..raf ---&gt; i\n.rafi ---&gt; q\nrafiq ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; u\nramdu ---&gt; t\namdut ---&gt; t\nmdutt ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; l\n.tasl ---&gt; i\ntasli ---&gt; m\naslim ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; k\n.rajk ---&gt; u\nrajku ---&gt; m\najkum ---&gt; a\njkuma ---&gt; r\nkumar ---&gt; i\numari ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; w\nbhanw ---&gt; a\nhanwa ---&gt; r\nanwar ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; o\npramo ---&gt; d\nramod ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; d\n..jad ---&gt; u\n.jadu ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; m\n..jum ---&gt; m\n.jumm ---&gt; a\njumma ---&gt; n\numman ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; e\n..ome ---&gt; n\n.omen ---&gt; d\nomend ---&gt; e\nmende ---&gt; r\nender ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; j\n..haj ---&gt; a\n.haja ---&gt; r\nhajar ---&gt; i\najari ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; r\n.jair ---&gt; a\njaira ---&gt; m\nairam ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; n\nparsn ---&gt; a\narsna ---&gt; t\nrsnat ---&gt; h\nsnath ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; a\nveera ---&gt; n\neeran ---&gt; d\nerand ---&gt; e\nrande ---&gt; r\nander ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; i\n.ragi ---&gt; n\nragin ---&gt; i\nagini ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; k\n.fark ---&gt; u\nfarku ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; t\n..pat ---&gt; a\n.pata ---&gt; v\npatav ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; m\n.nazm ---&gt; u\nnazmu ---&gt; s\nazmus ---&gt; l\nzmusl ---&gt; a\nmusla ---&gt; m\nuslam ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; e\n.suje ---&gt; n\nsujen ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; y\n.anay ---&gt; a\nanaya ---&gt; t\nnayat ---&gt; h\nayath ---&gt; a\nyatha ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; a\n.fara ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; e\n.vije ---&gt; n\nvijen ---&gt; d\nijend ---&gt; r\njendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; m\nahnam ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; b\n..reb ---&gt; e\n.rebe ---&gt; c\nrebec ---&gt; c\nebecc ---&gt; a\nbecca ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; f\ngulaf ---&gt; s\nulafs ---&gt; h\nlafsh ---&gt; a\nafsha ---&gt; n\nfshan ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; j\n..puj ---&gt; j\n.pujj ---&gt; a\npujja ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; e\nhande ---&gt; n\nanden ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; a\n.ruba ---&gt; l\nrubal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; m\n.mahm ---&gt; u\nmahmu ---&gt; n\nahmun ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; e\n.tane ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; n\neepan ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; o\nyasho ---&gt; d\nashod ---&gt; h\nshodh ---&gt; a\nhodha ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; i\n.sari ---&gt; q\nsariq ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; w\n.ishw ---&gt; a\nishwa ---&gt; r\nshwar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; t\n.malt ---&gt; i\nmalti ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; i\n..abi ---&gt; d\n.abid ---&gt; a\nabida ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; j\n..baj ---&gt; r\n.bajr ---&gt; a\nbajra ---&gt; n\najran ---&gt; g\njrang ---&gt; i\nrangi ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; s\n..nos ---&gt; a\n.nosa ---&gt; r\nnosar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; m\nparam ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; w\n.ragw ---&gt; e\nragwe ---&gt; n\nagwen ---&gt; d\ngwend ---&gt; e\nwende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; d\n.sadd ---&gt; a\nsadda ---&gt; m\naddam ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; p\n.rupp ---&gt; a\nruppa ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; e\n..swe ---&gt; t\n.swet ---&gt; a\nsweta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; a\n.saya ---&gt; d\nsayad ---&gt; a\nayada ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; m\n.gurm ---&gt; e\ngurme ---&gt; e\nurmee ---&gt; t\nrmeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; i\naashi ---&gt; k\nashik ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; r\n.sabr ---&gt; e\nsabre ---&gt; e\nabree ---&gt; n\nbreen ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; p\n.papp ---&gt; y\npappy ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; o\n..pro ---&gt; m\n.prom ---&gt; i\npromi ---&gt; l\nromil ---&gt; a\nomila ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; g\n..ang ---&gt; u\n.angu ---&gt; r\nangur ---&gt; i\nnguri ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; t\n..alt ---&gt; a\n.alta ---&gt; b\naltab ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; w\nbhagw ---&gt; a\nhagwa ---&gt; n\nagwan ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; j\n..sej ---&gt; a\n.seja ---&gt; n\nsejan ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; s\nparas ---&gt; h\narash ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; e\n.jame ---&gt; e\njamee ---&gt; r\nameer ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; b\n.rajb ---&gt; e\nrajbe ---&gt; e\najbee ---&gt; r\njbeer ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; g\n..ang ---&gt; r\n.angr ---&gt; e\nangre ---&gt; g\nngreg ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; i\n..koi ---&gt; l\n.koil ---&gt; u\nkoilu ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; r\n..afr ---&gt; o\n.afro ---&gt; z\nafroz ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; a\n.kama ---&gt; l\nkamal ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; o\n..joo ---&gt; h\n.jooh ---&gt; i\njoohi ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; a\n.mura ---&gt; r\nmurar ---&gt; i\nurari ---&gt; l\nraril ---&gt; a\narila ---&gt; l\nrilal ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; n\n..adn ---&gt; a\n.adna ---&gt; n\nadnan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; n\n.sabn ---&gt; u\nsabnu ---&gt; r\nabnur ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; n\n..pin ---&gt; k\n.pink ---&gt; i\npinki ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; r\nsamar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; t\n.shit ---&gt; a\nshita ---&gt; l\nhital ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; t\n..bit ---&gt; t\n.bitt ---&gt; u\nbittu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; a\n.raha ---&gt; t\nrahat ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; j\n.marj ---&gt; i\nmarji ---&gt; n\narjin ---&gt; a\nrjina ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; s\n..des ---&gt; h\n.desh ---&gt; r\ndeshr ---&gt; a\neshra ---&gt; j\nshraj ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; w\n.jaiw ---&gt; a\njaiwa ---&gt; n\naiwan ---&gt; t\niwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; t\n.firt ---&gt; u\nfirtu ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; b\n..rob ---&gt; i\n.robi ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; t\n..met ---&gt; i\n.meti ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; a\n..ima ---&gt; m\n.imam ---&gt; u\nimamu ---&gt; d\nmamud ---&gt; e\namude ---&gt; e\nmudee ---&gt; n\nudeen ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; y\n..diy ---&gt; a\n.diya ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; u\n.manu ---&gt; v\nmanuv ---&gt; a\nanuva ---&gt; r\nnuvar ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; s\n..afs ---&gt; h\n.afsh ---&gt; a\nafsha ---&gt; r\nfshar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; n\n.sabn ---&gt; a\nsabna ---&gt; m\nabnam ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; n\n..amn ---&gt; a\n.amna ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; n\n..tin ---&gt; a\n.tina ---&gt; .\n..... ---&gt; e\n....e ---&gt; b\n...eb ---&gt; a\n..eba ---&gt; n\n.eban ---&gt; e\nebane ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; n\n.hann ---&gt; y\nhanny ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; d\n..sed ---&gt; a\n.seda ---&gt; r\nsedar ---&gt; a\nedara ---&gt; t\ndarat ---&gt; h\narath ---&gt; .\n..... ---&gt; w\n....w ---&gt; i\n...wi ---&gt; l\n..wil ---&gt; i\n.wili ---&gt; y\nwiliy ---&gt; a\niliya ---&gt; m\nliyam ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; v\n..siv ---&gt; a\n.siva ---&gt; m\nsivam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; j\nsahaj ---&gt; a\nahaja ---&gt; d\nhajad ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; w\n.nirw ---&gt; a\nnirwa ---&gt; t\nirwat ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; i\nparvi ---&gt; n\narvin ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; k\n..tak ---&gt; s\n.taks ---&gt; h\ntaksh ---&gt; i\nakshi ---&gt; l\nkshil ---&gt; a\nshila ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; t\n.satt ---&gt; a\nsatta ---&gt; r\nattar ---&gt; a\nttara ---&gt; m\ntaram ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; i\n.mami ---&gt; n\nmamin ---&gt; i\namini ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; k\nbhark ---&gt; h\nharkh ---&gt; a\narkha ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; h\n.kish ---&gt; n\nkishn ---&gt; a\nishna ---&gt; r\nshnar ---&gt; a\nhnara ---&gt; m\nnaram ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; e\n.vine ---&gt; y\nviney ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; w\n..kaw ---&gt; a\n.kawa ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; i\n..rai ---&gt; s\n.rais ---&gt; h\nraish ---&gt; m\naishm ---&gt; a\nishma ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; s\n.dils ---&gt; h\ndilsh ---&gt; a\nilsha ---&gt; n\nlshan ---&gt; a\nshana ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; y\n..riy ---&gt; a\n.riya ---&gt; j\nriyaj ---&gt; u\niyaju ---&gt; d\nyajud ---&gt; i\najudi ---&gt; n\njudin ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; m\n..bim ---&gt; l\n.biml ---&gt; e\nbimle ---&gt; s\nimles ---&gt; h\nmlesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; n\n.kesn ---&gt; a\nkesna ---&gt; t\nesnat ---&gt; a\nsnata ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; s\nmanis ---&gt; h\nanish ---&gt; a\nnisha ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; o\n..ano ---&gt; k\n.anok ---&gt; h\nanokh ---&gt; a\nnokha ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; a\n.bada ---&gt; n\nbadan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; u\nshaku ---&gt; t\nhakut ---&gt; a\nakuta ---&gt; l\nkutal ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; j\n.birj ---&gt; u\nbirju ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; i\n.lali ---&gt; t\nlalit ---&gt; a\nalita ---&gt; l\nlital ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; l\n..dul ---&gt; a\n.dula ---&gt; r\ndular ---&gt; i\nulari ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; k\n..sik ---&gt; h\n.sikh ---&gt; a\nsikha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; k\n.rajk ---&gt; u\nrajku ---&gt; m\najkum ---&gt; a\njkuma ---&gt; r\nkumar ---&gt; m\numarm ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; n\n.ratn ---&gt; e\nratne ---&gt; s\natnes ---&gt; h\ntnesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; a\n.suba ---&gt; t\nsubat ---&gt; o\nubato ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; r\n.mehr ---&gt; a\nmehra ---&gt; j\nehraj ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; y\n.rosy ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; n\n.ratn ---&gt; .\n..... ---&gt; z\n....z ---&gt; u\n...zu ---&gt; h\n..zuh ---&gt; a\n.zuha ---&gt; i\nzuhai ---&gt; b\nuhaib ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; h\n.ansh ---&gt; u\nanshu ---&gt; l\nnshul ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; i\n.devi ---&gt; d\ndevid ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; r\n.shor ---&gt; a\nshora ---&gt; v\nhorav ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; n\n..dan ---&gt; a\n.dana ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; i\npriti ---&gt; b\nritib ---&gt; h\nitibh ---&gt; a\ntibha ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; a\n.shra ---&gt; w\nshraw ---&gt; a\nhrawa ---&gt; n\nrawan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; o\nmango ---&gt; l\nangol ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; s\n.roos ---&gt; h\nroosh ---&gt; i\nooshi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; r\n.sabr ---&gt; a\nsabra ---&gt; j\nabraj ---&gt; e\nbraje ---&gt; e\nrajee ---&gt; t\najeet ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; a\nrasha ---&gt; b\nashab ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; b\nkhusb ---&gt; u\nhusbu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; i\nrashi ---&gt; m\nashim ---&gt; i\nshimi ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; t\n.bhot ---&gt; r\nbhotr ---&gt; a\nhotra ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; t\n..mot ---&gt; i\n.moti ---&gt; l\nmotil ---&gt; a\notila ---&gt; l\ntilal ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; e\n.rame ---&gt; s\nrames ---&gt; h\namesh ---&gt; w\nmeshw ---&gt; e\neshwe ---&gt; r\nshwer ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; h\n..zah ---&gt; i\n.zahi ---&gt; d\nzahid ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; p\n.harp ---&gt; r\nharpr ---&gt; i\narpri ---&gt; t\nrprit ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; y\n..may ---&gt; a\n.maya ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; o\n..moo ---&gt; l\n.mool ---&gt; c\nmoolc ---&gt; h\noolch ---&gt; a\nolcha ---&gt; n\nlchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; e\n.sure ---&gt; n\nsuren ---&gt; d\nurend ---&gt; e\nrende ---&gt; r\nender ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; u\n.babu ---&gt; r\nbabur ---&gt; a\nabura ---&gt; m\nburam ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; l\n.bhol ---&gt; a\nbhola ---&gt; r\nholar ---&gt; a\nolara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; h\n..keh ---&gt; k\n.kehk ---&gt; a\nkehka ---&gt; s\nehkas ---&gt; a\nhkasa ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; s\n..als ---&gt; e\n.alse ---&gt; e\nalsee ---&gt; p\nlseep ---&gt; a\nseepa ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; a\n.shya ---&gt; m\nshyam ---&gt; v\nhyamv ---&gt; e\nyamve ---&gt; e\namvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; f\n.mehf ---&gt; o\nmehfo ---&gt; o\nehfoo ---&gt; j\nhfooj ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; e\n.amie ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; j\n..amj ---&gt; a\n.amja ---&gt; t\namjat ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; n\n.reen ---&gt; a\nreena ---&gt; .\n..... ---&gt; i\n....i ---&gt; r\n...ir ---&gt; p\n..irp ---&gt; h\n.irph ---&gt; a\nirpha ---&gt; n\nrphan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; a\n.sawa ---&gt; t\nsawat ---&gt; r\nawatr ---&gt; i\nwatri ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; a\nprama ---&gt; l\nramal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; e\n.shee ---&gt; t\nsheet ---&gt; a\nheeta ---&gt; l\neetal ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; h\n.vash ---&gt; i\nvashi ---&gt; l\nashil ---&gt; a\nshila ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; m\n..asm ---&gt; i\n.asmi ---&gt; t\nasmit ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; a\n..aka ---&gt; n\n.akan ---&gt; s\nakans ---&gt; h\nkansh ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; v\n..dav ---&gt; i\n.davi ---&gt; n\ndavin ---&gt; d\navind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; i\n.sati ---&gt; s\nsatis ---&gt; h\natish ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; p\n..sup ---&gt; n\n.supn ---&gt; a\nsupna ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; r\n.khur ---&gt; s\nkhurs ---&gt; h\nhursh ---&gt; e\nurshe ---&gt; e\nrshee ---&gt; d\nsheed ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; a\nfarha ---&gt; n\narhan ---&gt; a\nrhana ---&gt; z\nhanaz ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; a\n.isha ---&gt; n\nishan ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; a\n..ala ---&gt; u\n.alau ---&gt; d\nalaud ---&gt; i\nlaudi ---&gt; n\naudin ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; y\n..joy ---&gt; t\n.joyt ---&gt; i\njoyti ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; c\npremc ---&gt; h\nremch ---&gt; a\nemcha ---&gt; n\nmchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; a\n.vija ---&gt; y\nvijay ---&gt; a\nijaya ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; l\n.tasl ---&gt; e\ntasle ---&gt; e\naslee ---&gt; m\nsleem ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; n\n.gyan ---&gt; e\ngyane ---&gt; n\nyanen ---&gt; d\nanend ---&gt; r\nnendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; z\n..raz ---&gt; i\n.razi ---&gt; d\nrazid ---&gt; a\nazida ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; r\n..ver ---&gt; e\n.vere ---&gt; n\nveren ---&gt; d\nerend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; s\n.ashs ---&gt; i\nashsi ---&gt; s\nshsis ---&gt; h\nhsish ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; s\n..jis ---&gt; m\n.jism ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; y\n..dey ---&gt; j\n.deyj ---&gt; i\ndeyji ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; a\nkhusa ---&gt; b\nhusab ---&gt; o\nusabo ---&gt; o\nsaboo ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; c\n..ruc ---&gt; k\n.ruck ---&gt; s\nrucks ---&gt; a\nucksa ---&gt; n\ncksan ---&gt; a\nksana ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; o\n..koo ---&gt; k\n.kook ---&gt; i\nkooki ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; k\n.beek ---&gt; a\nbeeka ---&gt; r\neekar ---&gt; .\n..... ---&gt; f\n....f ---&gt; r\n...fr ---&gt; r\n..frr ---&gt; a\n.frra ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; a\n.basa ---&gt; n\nbasan ---&gt; t\nasant ---&gt; i\nsanti ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; h\n.ansh ---&gt; i\nanshi ---&gt; k\nnshik ---&gt; a\nshika ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; n\n.mehn ---&gt; a\nmehna ---&gt; j\nehnaj ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; l\n.anil ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; a\n.rupa ---&gt; l\nrupal ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; i\n.babi ---&gt; t\nbabit ---&gt; a\nabita ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; u\n.rinu ---&gt; k\nrinuk ---&gt; a\ninuka ---&gt; n\nnukan ---&gt; w\nukanw ---&gt; r\nkanwr ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; m\nparam ---&gt; j\naramj ---&gt; i\nramji ---&gt; t\namjit ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; u\n.suku ---&gt; m\nsukum ---&gt; a\nukuma ---&gt; r\nkumar ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; m\n.farm ---&gt; a\nfarma ---&gt; n\narman ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; l\n.ball ---&gt; u\nballu ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; w\n..gaw ---&gt; r\n.gawr ---&gt; i\ngawri ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; a\n.bala ---&gt; m\nbalam ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; t\n.shat ---&gt; i\nshati ---&gt; s\nhatis ---&gt; h\natish ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; v\n..kav ---&gt; i\n.kavi ---&gt; l\nkavil ---&gt; a\navila ---&gt; s\nvilas ---&gt; h\nilash ---&gt; .\n..... ---&gt; t\n....t ---&gt; w\n...tw ---&gt; e\n..twe ---&gt; n\n.twen ---&gt; k\ntwenk ---&gt; l\nwenkl ---&gt; e\nenkle ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; a\n.amra ---&gt; t\namrat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; e\n.sabe ---&gt; n\nsaben ---&gt; o\nabeno ---&gt; o\nbenoo ---&gt; r\nenoor ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; s\n.bans ---&gt; i\nbansi ---&gt; l\nansil ---&gt; a\nnsila ---&gt; l\nsilal ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; e\n.hase ---&gt; e\nhasee ---&gt; n\naseen ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; i\nsandi ---&gt; p\nandip ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; i\n.sagi ---&gt; r\nsagir ---&gt; a\nagira ---&gt; n\ngiran ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; p\n.papp ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; e\n.yoge ---&gt; n\nyogen ---&gt; d\nogend ---&gt; r\ngendr ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; b\n.roob ---&gt; i\nroobi ---&gt; .\n..... ---&gt; a\n....a ---&gt; e\n...ae ---&gt; s\n..aes ---&gt; h\n.aesh ---&gt; a\naesha ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; b\n..arb ---&gt; a\n.arba ---&gt; n\narban ---&gt; a\nrbana ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; h\n.radh ---&gt; a\nradha ---&gt; b\nadhab ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; i\nchoti ---&gt; b\nhotib ---&gt; a\notiba ---&gt; i\ntibai ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; y\n.kany ---&gt; a\nkanya ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; d\n.kund ---&gt; a\nkunda ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; p\n..arp ---&gt; n\n.arpn ---&gt; a\narpna ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; j\n..vaj ---&gt; i\n.vaji ---&gt; d\nvajid ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; b\nkhusb ---&gt; h\nhusbh ---&gt; o\nusbho ---&gt; o\nsbhoo ---&gt; .\n..... ---&gt; p\n....p ---&gt; y\n...py ---&gt; a\n..pya ---&gt; r\n.pyar ---&gt; e\npyare ---&gt; l\nyarel ---&gt; a\narela ---&gt; l\nrelal ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; n\n.chin ---&gt; t\nchint ---&gt; a\nhinta ---&gt; n\nintan ---&gt; .\n..... ---&gt; a\n....a ---&gt; y\n...ay ---&gt; a\n..aya ---&gt; s\n.ayas ---&gt; h\nayash ---&gt; a\nyasha ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; t\n..akt ---&gt; r\n.aktr ---&gt; i\naktri ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; t\n.dipt ---&gt; i\ndipti ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; s\n..jes ---&gt; m\n.jesm ---&gt; i\njesmi ---&gt; n\nesmin ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; i\n.sehi ---&gt; n\nsehin ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; v\n.satv ---&gt; i\nsatvi ---&gt; d\natvid ---&gt; a\ntvida ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; o\n.chho ---&gt; t\nchhot ---&gt; e\nhhote ---&gt; l\nhotel ---&gt; a\notela ---&gt; l\ntelal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; t\n..mat ---&gt; u\n.matu ---&gt; l\nmatul ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; e\n.amre ---&gt; e\namree ---&gt; k\nmreek ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; h\n.sunh ---&gt; a\nsunha ---&gt; r\nunhar ---&gt; a\nnhara ---&gt; .\n..... ---&gt; t\n....t ---&gt; y\n...ty ---&gt; a\n..tya ---&gt; r\n.tyar ---&gt; a\ntyara ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; p\n.lavp ---&gt; r\nlavpr ---&gt; e\navpre ---&gt; e\nvpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; e\n.rupe ---&gt; s\nrupes ---&gt; h\nupesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; a\n.ansa ---&gt; l\nansal ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; w\n..naw ---&gt; a\n.nawa ---&gt; z\nnawaz ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; a\n.sala ---&gt; u\nsalau ---&gt; d\nalaud ---&gt; d\nlaudd ---&gt; i\nauddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; a\n.dipa ---&gt; k\ndipak ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; e\n.gane ---&gt; s\nganes ---&gt; h\nanesh ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; a\n.lala ---&gt; n\nlalan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; p\n..sap ---&gt; n\n.sapn ---&gt; a\nsapna ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; m\n.harm ---&gt; i\nharmi ---&gt; t\narmit ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; i\nparti ---&gt; b\nartib ---&gt; h\nrtibh ---&gt; a\ntibha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; r\n.nasr ---&gt; i\nnasri ---&gt; n\nasrin ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; f\n..suf ---&gt; i\n.sufi ---&gt; y\nsufiy ---&gt; a\nufiya ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; k\n.anik ---&gt; e\nanike ---&gt; t\nniket ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; o\nshalo ---&gt; o\nhaloo ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; l\n.heml ---&gt; a\nhemla ---&gt; t\nemlat ---&gt; a\nmlata ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; u\n..abu ---&gt; l\n.abul ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; i\n.ishi ---&gt; t\nishit ---&gt; a\nshita ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; g\n.birg ---&gt; e\nbirge ---&gt; s\nirges ---&gt; h\nrgesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; a\n..pha ---&gt; l\n.phal ---&gt; g\nphalg ---&gt; u\nhalgu ---&gt; n\nalgun ---&gt; i\nlguni ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; r\n..mir ---&gt; a\n.mira ---&gt; z\nmiraz ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; i\nakshi ---&gt; t\nkshit ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; u\n..mau ---&gt; s\n.maus ---&gt; a\nmausa ---&gt; m\nausam ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; i\n..kai ---&gt; l\n.kail ---&gt; a\nkaila ---&gt; s\nailas ---&gt; h\nilash ---&gt; .\n..... ---&gt; o\n....o ---&gt; s\n...os ---&gt; i\n..osi ---&gt; e\n.osie ---&gt; r\nosier ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; i\n..ari ---&gt; f\n.arif ---&gt; a\narifa ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; f\n..aff ---&gt; a\n.affa ---&gt; n\naffan ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; t\n..fat ---&gt; i\n.fati ---&gt; m\nfatim ---&gt; a\natima ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; h\n.barh ---&gt; a\nbarha ---&gt; m\narham ---&gt; .\n..... ---&gt; k\n....k ---&gt; l\n...kl ---&gt; u\n..klu ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; h\n..neh ---&gt; a\n.neha ---&gt; l\nnehal ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; a\n.anka ---&gt; l\nankal ---&gt; a\nnkala ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; a\nnanda ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; a\n.kama ---&gt; l\nkamal ---&gt; j\namalj ---&gt; e\nmalje ---&gt; e\naljee ---&gt; t\nljeet ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; e\n..bhe ---&gt; e\n.bhee ---&gt; m\nbheem ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; v\nharmv ---&gt; e\narmve ---&gt; e\nrmvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; z\nahnaz ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; f\n.gulf ---&gt; a\ngulfa ---&gt; r\nulfar ---&gt; a\nlfara ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; t\n.mont ---&gt; i\nmonti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; b\n.sabb ---&gt; o\nsabbo ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; i\n.safi ---&gt; n\nsafin ---&gt; a\nafina ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; v\n.neev ---&gt; e\nneeve ---&gt; n\neeven ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; s\n..sis ---&gt; h\n.sish ---&gt; p\nsishp ---&gt; a\nishpa ---&gt; l\nshpal ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; s\n..nus ---&gt; r\n.nusr ---&gt; a\nnusra ---&gt; t\nusrat ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; n\n..ron ---&gt; y\n.rony ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; k\n..shk ---&gt; u\n.shku ---&gt; n\nshkun ---&gt; d\nhkund ---&gt; l\nkundl ---&gt; a\nundla ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; t\n.mant ---&gt; o\nmanto ---&gt; s\nantos ---&gt; h\nntosh ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; d\n.kuld ---&gt; e\nkulde ---&gt; e\nuldee ---&gt; p\nldeep ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; s\nsaras ---&gt; w\narasw ---&gt; a\nraswa ---&gt; t\naswat ---&gt; i\nswati ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; h\n..tah ---&gt; i\n.tahi ---&gt; r\ntahir ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; j\n.ramj ---&gt; e\nramje ---&gt; e\namjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; m\n.jasm ---&gt; i\njasmi ---&gt; n\nasmin ---&gt; a\nsmina ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; a\n.dhra ---&gt; m\ndhram ---&gt; a\nhrama ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; e\n..dhe ---&gt; e\n.dhee ---&gt; r\ndheer ---&gt; a\nheera ---&gt; j\neeraj ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; j\n..roj ---&gt; m\n.rojm ---&gt; e\nrojme ---&gt; n\nojmen ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; y\n.ruby ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; o\n.kano ---&gt; k\nkanok ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; a\n.jaga ---&gt; t\njagat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; i\n.savi ---&gt; t\nsavit ---&gt; a\navita ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; a\nramsa ---&gt; g\namsag ---&gt; a\nmsaga ---&gt; r\nsagar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; d\n.said ---&gt; a\nsaida ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; n\n.sann ---&gt; y\nsanny ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; g\nshahg ---&gt; u\nhahgu ---&gt; j\nahguj ---&gt; t\nhgujt ---&gt; a\ngujta ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; n\nhahin ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; v\n.jasv ---&gt; e\njasve ---&gt; e\nasvee ---&gt; n\nsveen ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; o\n.choo ---&gt; t\nchoot ---&gt; u\nhootu ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; a\n..aja ---&gt; h\n.ajah ---&gt; a\najaha ---&gt; r\njahar ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; h\n.megh ---&gt; a\nmegha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; a\n.naza ---&gt; n\nnazan ---&gt; i\nazani ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; t\n..bit ---&gt; t\n.bitt ---&gt; o\nbitto ---&gt; o\nittoo ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; d\nmahad ---&gt; e\nahade ---&gt; v\nhadev ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; a\nbhara ---&gt; t\nharat ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; c\n.prac ---&gt; h\nprach ---&gt; i\nrachi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; a\naksha ---&gt; y\nkshay ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; u\n.taru ---&gt; n\ntarun ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; y\nsandy ---&gt; a\nandya ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; y\n.gudy ---&gt; a\ngudya ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; s\n..dis ---&gt; h\n.dish ---&gt; a\ndisha ---&gt; d\nishad ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; n\n..bun ---&gt; d\n.bund ---&gt; a\nbunda ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; a\n.sona ---&gt; m\nsonam ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; l\n..tal ---&gt; e\n.tale ---&gt; e\ntalee ---&gt; m\naleem ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; a\n.lila ---&gt; r\nlilar ---&gt; a\nilara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; a\n.kama ---&gt; k\nkamak ---&gt; s\namaks ---&gt; h\nmaksh ---&gt; y\nakshy ---&gt; a\nkshya ---&gt; .\n..... ---&gt; i\n....i ---&gt; r\n...ir ---&gt; a\n..ira ---&gt; m\n.iram ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; m\n..pam ---&gt; j\n.pamj ---&gt; e\npamje ---&gt; e\namjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; j\n.bhoj ---&gt; p\nbhojp ---&gt; a\nhojpa ---&gt; l\nojpal ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; z\n..siz ---&gt; a\n.siza ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; l\n.shil ---&gt; a\nshila ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; h\n..bah ---&gt; w\n.bahw ---&gt; a\nbahwa ---&gt; n\nahwan ---&gt; a\nhwana ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; u\n.ritu ---&gt; n\nritun ---&gt; j\nitunj ---&gt; a\ntunja ---&gt; y\nunjay ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; v\n..div ---&gt; y\n.divy ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; t\n.kart ---&gt; i\nkarti ---&gt; k\nartik ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; n\n.uman ---&gt; g\numang ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; e\nranje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; a\njeeta ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; n\n.ratn ---&gt; a\nratna ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; m\n..asm ---&gt; a\n.asma ---&gt; n\nasman ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; h\n.nash ---&gt; i\nnashi ---&gt; m\nashim ---&gt; a\nshima ---&gt; .\n..... ---&gt; w\n....w ---&gt; i\n...wi ---&gt; l\n..wil ---&gt; k\n.wilk ---&gt; i\nwilki ---&gt; s\nilkis ---&gt; h\nlkish ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; i\n.muni ---&gt; a\nmunia ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; s\n.tabs ---&gt; s\ntabss ---&gt; u\nabssu ---&gt; m\nbssum ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; n\n.amin ---&gt; a\namina ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; m\nrishm ---&gt; a\nishma ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; a\nprata ---&gt; p\nratap ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; n\neepan ---&gt; s\nepans ---&gt; h\npansh ---&gt; i\nanshi ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; b\n.adib ---&gt; a\nadiba ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; i\n..kai ---&gt; f\n.kaif ---&gt; i\nkaifi ---&gt; y\naifiy ---&gt; a\nifiya ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; s\n.adis ---&gt; a\nadisa ---&gt; n\ndisan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; w\n..shw ---&gt; e\n.shwe ---&gt; t\nshwet ---&gt; a\nhweta ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; a\n.nara ---&gt; i\nnarai ---&gt; n\narain ---&gt; i\nraini ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; o\n.vino ---&gt; d\nvinod ---&gt; a\ninoda ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; t\n..het ---&gt; a\n.heta ---&gt; l\nhetal ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; u\n.guru ---&gt; d\ngurud ---&gt; u\nurudu ---&gt; t\nrudut ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; u\n..fau ---&gt; j\n.fauj ---&gt; d\nfaujd ---&gt; a\naujda ---&gt; r\nujdar ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; u\n.muku ---&gt; n\nmukun ---&gt; d\nukund ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; z\n..raz ---&gt; i\n.razi ---&gt; a\nrazia ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; a\n.suma ---&gt; n\nsuman ---&gt; t\numant ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; f\nsaraf ---&gt; r\narafr ---&gt; a\nrafra ---&gt; j\nafraj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; f\n..raf ---&gt; a\n.rafa ---&gt; t\nrafat ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; d\n..aad ---&gt; i\n.aadi ---&gt; t\naadit ---&gt; y\nadity ---&gt; a\nditya ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; s\n.raks ---&gt; h\nraksh ---&gt; i\nakshi ---&gt; t\nkshit ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; m\n..mum ---&gt; t\n.mumt ---&gt; a\nmumta ---&gt; z\numtaz ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; w\nbhanw ---&gt; e\nhanwe ---&gt; r\nanwer ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; j\n.binj ---&gt; a\nbinja ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; b\n..dab ---&gt; u\n.dabu ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; s\n..bis ---&gt; t\n.bist ---&gt; o\nbisto ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; c\n..bac ---&gt; c\n.bacc ---&gt; h\nbacch ---&gt; e\nacche ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; b\n..dib ---&gt; a\n.diba ---&gt; k\ndibak ---&gt; a\nibaka ---&gt; r\nbakar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; i\n.safi ---&gt; l\nsafil ---&gt; a\nafila ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; e\n..ate ---&gt; e\n.atee ---&gt; k\nateek ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; s\nprems ---&gt; i\nremsi ---&gt; n\nemsin ---&gt; g\nmsing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; t\n.mant ---&gt; a\nmanta ---&gt; s\nantas ---&gt; h\nntash ---&gt; a\ntasha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; n\nrahin ---&gt; a\nahina ---&gt; .\n..... ---&gt; s\n....s ---&gt; r\n...sr ---&gt; i\n..sri ---&gt; t\n.srit ---&gt; a\nsrita ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; h\n.sheh ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; a\n.dhra ---&gt; m\ndhram ---&gt; p\nhramp ---&gt; a\nrampa ---&gt; l\nampal ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; a\nkusha ---&gt; g\nushag ---&gt; r\nshagr ---&gt; a\nhagra ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; r\n.devr ---&gt; a\ndevra ---&gt; j\nevraj ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; h\n.rukh ---&gt; s\nrukhs ---&gt; a\nukhsa ---&gt; n\nkhsan ---&gt; a\nhsana ---&gt; .\n..... ---&gt; a\n....a ---&gt; i\n...ai ---&gt; j\n..aij ---&gt; a\n.aija ---&gt; y\naijay ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; j\n.munj ---&gt; i\nmunji ---&gt; r\nunjir ---&gt; a\nnjira ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; s\nabhis ---&gt; h\nbhish ---&gt; a\nhisha ---&gt; k\nishak ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; c\nharic ---&gt; h\narich ---&gt; a\nricha ---&gt; n\nichan ---&gt; d\nchand ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; w\n.sanw ---&gt; a\nsanwa ---&gt; r\nanwar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; u\n..sau ---&gt; d\n.saud ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; a\n..ara ---&gt; j\n.araj ---&gt; u\naraju ---&gt; n\nrajun ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; i\n..zai ---&gt; n\n.zain ---&gt; a\nzaina ---&gt; b\nainab ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; n\n.sann ---&gt; i\nsanni ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; d\navind ---&gt; a\nvinda ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; s\n..yus ---&gt; u\n.yusu ---&gt; f\nyusuf ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; c\n.nisc ---&gt; h\nnisch ---&gt; a\nischa ---&gt; y\nschay ---&gt; a\nchaya ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; s\n.khas ---&gt; b\nkhasb ---&gt; h\nhasbh ---&gt; o\nasbho ---&gt; o\nsbhoo ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; k\n..hak ---&gt; a\n.haka ---&gt; m\nhakam ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; r\n.jhar ---&gt; n\njharn ---&gt; a\nharna ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; k\n.sukk ---&gt; h\nsukkh ---&gt; u\nukkhu ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; h\n..bhh ---&gt; a\n.bhha ---&gt; t\nbhhat ---&gt; u\nhhatu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; d\n.jayd ---&gt; u\njaydu ---&gt; r\naydur ---&gt; g\nydurg ---&gt; a\ndurga ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; n\nrajen ---&gt; d\najend ---&gt; e\njende ---&gt; r\nender ---&gt; i\nnderi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; a\nsatya ---&gt; d\natyad ---&gt; e\ntyade ---&gt; v\nyadev ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; h\n.nish ---&gt; a\nnisha ---&gt; l\nishal ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; p\n.jaip ---&gt; a\njaipa ---&gt; l\naipal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; s\n.bhas ---&gt; k\nbhask ---&gt; e\nhaske ---&gt; r\nasker ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; j\n.faij ---&gt; i\nfaiji ---&gt; n\naijin ---&gt; a\nijina ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; s\n..ves ---&gt; a\n.vesa ---&gt; l\nvesal ---&gt; i\nesali ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; i\n..kai ---&gt; s\n.kais ---&gt; h\nkaish ---&gt; a\naisha ---&gt; v\nishav ---&gt; .\n..... ---&gt; a\n....a ---&gt; w\n...aw ---&gt; d\n..awd ---&gt; e\n.awde ---&gt; s\nawdes ---&gt; h\nwdesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; t\n.sart ---&gt; a\nsarta ---&gt; j\nartaj ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; t\n.kast ---&gt; o\nkasto ---&gt; o\nastoo ---&gt; r\nstoor ---&gt; i\ntoori ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; d\n.sayd ---&gt; a\nsayda ---&gt; .\n..... ---&gt; i\n....i ---&gt; q\n...iq ---&gt; r\n..iqr ---&gt; a\n.iqra ---&gt; m\niqram ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; n\n..chn ---&gt; d\n.chnd ---&gt; r\nchndr ---&gt; a\nhndra ---&gt; k\nndrak ---&gt; l\ndrakl ---&gt; a\nrakla ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; o\n.shyo ---&gt; r\nshyor ---&gt; a\nhyora ---&gt; m\nyoram ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; t\n.must ---&gt; a\nmusta ---&gt; q\nustaq ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; h\n.nath ---&gt; i\nnathi ---&gt; y\nathiy ---&gt; a\nthiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; a\n..sna ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; b\n..nab ---&gt; i\n.nabi ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; r\n.meer ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; u\n.jayu ---&gt; t\njayut ---&gt; i\nayuti ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; m\n..pom ---&gt; a\n.poma ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; m\n.rasm ---&gt; i\nrasmi ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; h\n.kanh ---&gt; y\nkanhy ---&gt; a\nanhya ---&gt; l\nnhyal ---&gt; a\nhyala ---&gt; l\nyalal ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; v\n..rev ---&gt; t\n.revt ---&gt; i\nrevti ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; d\n.chid ---&gt; d\nchidd ---&gt; u\nhiddu ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; i\n.dali ---&gt; m\ndalim ---&gt; a\nalima ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; l\n..mul ---&gt; i\n.muli ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; l\n.tasl ---&gt; i\ntasli ---&gt; m\naslim ---&gt; a\nslima ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; t\n.shit ---&gt; t\nshitt ---&gt; a\nhitta ---&gt; l\nittal ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; r\n.jamr ---&gt; u\njamru ---&gt; j\namruj ---&gt; a\nmruja ---&gt; h\nrujah ---&gt; a\nujaha ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; p\n.birp ---&gt; a\nbirpa ---&gt; l\nirpal ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; h\n..neh ---&gt; a\n.neha ---&gt; r\nnehar ---&gt; i\nehari ---&gt; k\nharik ---&gt; a\narika ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; r\n..ber ---&gt; a\n.bera ---&gt; g\nberag ---&gt; i\neragi ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; a\n..gha ---&gt; n\n.ghan ---&gt; t\nghant ---&gt; o\nhanto ---&gt; l\nantol ---&gt; i\nntoli ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; i\n.niti ---&gt; s\nnitis ---&gt; h\nitish ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; h\n.budh ---&gt; .\n..... ---&gt; a\n....a ---&gt; c\n...ac ---&gt; h\n..ach ---&gt; i\n.achi ---&gt; n\nachin ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; e\n.inde ---&gt; r\ninder ---&gt; j\nnderj ---&gt; i\nderji ---&gt; t\nerjit ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; a\n.lila ---&gt; k\nlilak ---&gt; i\nilaki ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; e\n.jite ---&gt; n\njiten ---&gt; d\nitend ---&gt; e\ntende ---&gt; r\nender ---&gt; a\nndera ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; b\n.mehb ---&gt; o\nmehbo ---&gt; b\nehbob ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; r\n..gor ---&gt; a\n.gora ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; m\n..mom ---&gt; i\n.momi ---&gt; t\nmomit ---&gt; a\nomita ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; p\n..tap ---&gt; s\n.taps ---&gt; y\ntapsy ---&gt; a\napsya ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; l\n.bhul ---&gt; a\nbhula ---&gt; e\nhulae ---&gt; e\nulaee ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; a\ndhara ---&gt; m\nharam ---&gt; v\naramv ---&gt; i\nramvi ---&gt; r\namvir ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; u\n.tanu ---&gt; j\ntanuj ---&gt; a\nanuja ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; j\namarj ---&gt; i\nmarji ---&gt; t\narjit ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; u\naashu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; b\n.sabb ---&gt; a\nsabba ---&gt; r\nabbar ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; n\n.jain ---&gt; u\njainu ---&gt; b\nainub ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; j\n..snj ---&gt; a\n.snja ---&gt; n\nsnjan ---&gt; a\nnjana ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; a\neepaa ---&gt; k\nepaak ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; k\n..rek ---&gt; h\n.rekh ---&gt; w\nrekhw ---&gt; a\nekhwa ---&gt; n\nkhwan ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; t\n.mitt ---&gt; h\nmitth ---&gt; u\nitthu ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; s\n..ars ---&gt; a\n.arsa ---&gt; d\narsad ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; a\nmanga ---&gt; l\nangal ---&gt; a\nngala ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; m\n..lim ---&gt; c\n.limc ---&gt; a\nlimca ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; z\n..kaz ---&gt; a\n.kaza ---&gt; l\nkazal ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; l\n..all ---&gt; a\n.alla ---&gt; r\nallar ---&gt; a\nllara ---&gt; j\nlaraj ---&gt; i\naraji ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; a\nshada ---&gt; n\nhadan ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; o\n..foo ---&gt; l\n.fool ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; h\n..suh ---&gt; a\n.suha ---&gt; l\nsuhal ---&gt; i\nuhali ---&gt; y\nhaliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; e\nranje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; e\n.rube ---&gt; e\nrubee ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; r\n..asr ---&gt; u\n.asru ---&gt; d\nasrud ---&gt; d\nsrudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; n\n..dun ---&gt; d\n.dund ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; h\n..vah ---&gt; a\n.vaha ---&gt; b\nvahab ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; u\n.saku ---&gt; n\nsakun ---&gt; t\nakunt ---&gt; a\nkunta ---&gt; l\nuntal ---&gt; a\nntala ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; i\n.ashi ---&gt; s\nashis ---&gt; h\nshish ---&gt; i\nhishi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; i\nranji ---&gt; t\nanjit ---&gt; a\nnjita ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; r\nanjar ---&gt; a\nnjara ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; s\n.nars ---&gt; a\nnarsa ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; d\n..lad ---&gt; l\n.ladl ---&gt; i\nladli ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; c\n.ramc ---&gt; h\nramch ---&gt; a\namcha ---&gt; n\nmchan ---&gt; d\nchand ---&gt; r\nhandr ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; r\n.bhur ---&gt; a\nbhura ---&gt; l\nhural ---&gt; a\nurala ---&gt; l\nralal ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; j\n.harj ---&gt; i\nharji ---&gt; t\narjit ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; v\n..liv ---&gt; e\n.live ---&gt; r\nliver ---&gt; i\niveri ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; s\n..afs ---&gt; h\n.afsh ---&gt; a\nafsha ---&gt; n\nfshan ---&gt; a\nshana ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; p\n..sip ---&gt; r\n.sipr ---&gt; a\nsipra ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; b\n.munb ---&gt; u\nmunbu ---&gt; r\nunbur ---&gt; a\nnbura ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; i\nnoori ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; r\n.sawr ---&gt; n\nsawrn ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; l\n..aal ---&gt; a\n.aala ---&gt; m\naalam ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; k\n.malk ---&gt; e\nmalke ---&gt; e\nalkee ---&gt; t\nlkeet ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; n\n..kin ---&gt; i\n.kini ---&gt; y\nkiniy ---&gt; a\niniya ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; f\n..raf ---&gt; a\n.rafa ---&gt; l\nrafal ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; n\n.sohn ---&gt; a\nsohna ---&gt; l\nohnal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; w\nshanw ---&gt; a\nhanwa ---&gt; z\nanwaz ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; v\n.prav ---&gt; e\nprave ---&gt; e\nravee ---&gt; n\naveen ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; d\n.kuld ---&gt; e\nkulde ---&gt; e\nuldee ---&gt; l\nldeel ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; y\nsamiy ---&gt; a\namiya ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; h\n.ragh ---&gt; u\nraghu ---&gt; v\naghuv ---&gt; i\nghuvi ---&gt; r\nhuvir ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; k\n..lok ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; j\n..muj ---&gt; e\n.muje ---&gt; e\nmujee ---&gt; b\nujeeb ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; k\n.musk ---&gt; h\nmuskh ---&gt; a\nuskha ---&gt; n\nskhan ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; a\n.dipa ---&gt; n\ndipan ---&gt; s\nipans ---&gt; u\npansu ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; j\nrabhj ---&gt; o\nabhjo ---&gt; t\nbhjot ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; i\n.savi ---&gt; h\nsavih ---&gt; a\naviha ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; e\nprame ---&gt; e\nramee ---&gt; l\nameel ---&gt; a\nmeela ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; t\n..jat ---&gt; a\n.jata ---&gt; n\njatan ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; s\n..das ---&gt; h\n.dash ---&gt; r\ndashr ---&gt; a\nashra ---&gt; t\nshrat ---&gt; h\nhrath ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; z\n..riz ---&gt; w\n.rizw ---&gt; a\nrizwa ---&gt; n\nizwan ---&gt; a\nzwana ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; h\n.mukh ---&gt; t\nmukht ---&gt; i\nukhti ---&gt; y\nkhtiy ---&gt; a\nhtiya ---&gt; r\ntiyar ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; n\n..tin ---&gt; v\n.tinv ---&gt; k\ntinvk ---&gt; a\ninvka ---&gt; l\nnvkal ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; t\n..jet ---&gt; a\n.jeta ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; a\nhanda ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; n\n.satn ---&gt; o\nsatno ---&gt; s\natnos ---&gt; h\ntnosh ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; r\nhandr ---&gt; r\nandrr ---&gt; a\nndrra ---&gt; m\ndrram ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; s\n..ars ---&gt; h\n.arsh ---&gt; a\narsha ---&gt; d\nrshad ---&gt; .\n..... ---&gt; d\n....d ---&gt; r\n...dr ---&gt; u\n..dru ---&gt; g\n.drug ---&gt; a\ndruga ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; o\n.saro ---&gt; z\nsaroz ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; n\n.parn ---&gt; i\nparni ---&gt; t\narnit ---&gt; i\nrniti ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; a\nshana ---&gt; z\nhanaz ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; e\n.nave ---&gt; e\nnavee ---&gt; d\naveed ---&gt; a\nveeda ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; i\n.giri ---&gt; j\ngirij ---&gt; a\nirija ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; p\n..dep ---&gt; a\n.depa ---&gt; n\ndepan ---&gt; d\nepand ---&gt; e\npande ---&gt; r\nander ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; a\n.vira ---&gt; j\nviraj ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; m\n..pam ---&gt; p\n.pamp ---&gt; a\npampa ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; t\n..lat ---&gt; a\n.lata ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; e\n..ame ---&gt; e\n.amee ---&gt; r\nameer ---&gt; a\nmeera ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; t\n.chit ---&gt; u\nchitu ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; k\n..tuk ---&gt; a\n.tuka ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; t\n.neet ---&gt; u\nneetu ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; d\n..vad ---&gt; h\n.vadh ---&gt; i\nvadhi ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; r\n..var ---&gt; s\n.vars ---&gt; a\nvarsa ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; i\n.jami ---&gt; r\njamir ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; e\n.jite ---&gt; n\njiten ---&gt; d\nitend ---&gt; e\ntende ---&gt; r\nender ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; j\n.pooj ---&gt; a\npooja ---&gt; d\noojad ---&gt; e\nojade ---&gt; v\njadev ---&gt; i\nadevi ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; v\n..giv ---&gt; i\n.givi ---&gt; n\ngivin ---&gt; d\nivind ---&gt; a\nvinda ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; i\n..kai ---&gt; l\n.kail ---&gt; a\nkaila ---&gt; s\nailas ---&gt; h\nilash ---&gt; i\nlashi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; a\n.sura ---&gt; j\nsuraj ---&gt; p\nurajp ---&gt; a\nrajpa ---&gt; l\najpal ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; a\n..bra ---&gt; j\n.braj ---&gt; e\nbraje ---&gt; n\nrajen ---&gt; d\najend ---&gt; r\njendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; a\n..gha ---&gt; s\n.ghas ---&gt; e\nghase ---&gt; e\nhasee ---&gt; t\naseet ---&gt; a\nseeta ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; a\nmanja ---&gt; r\nanjar ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; m\nkushm ---&gt; i\nushmi ---&gt; t\nshmit ---&gt; a\nhmita ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; r\n..pur ---&gt; n\n.purn ---&gt; i\npurni ---&gt; m\nurnim ---&gt; a\nrnima ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; n\nsantn ---&gt; a\nantna ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; u\nkeshu ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; r\n.amir ---&gt; i\namiri ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; b\nshamb ---&gt; u\nhambu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; v\nrajiv ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; a\n.rina ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; j\n..rej ---&gt; a\n.reja ---&gt; u\nrejau ---&gt; l\nejaul ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; e\n.mahe ---&gt; n\nmahen ---&gt; d\nahend ---&gt; e\nhende ---&gt; r\nender ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; k\n.mank ---&gt; u\nmanku ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; r\n.beer ---&gt; u\nbeeru ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; n\nhiman ---&gt; s\nimans ---&gt; u\nmansu ---&gt; .\n..... ---&gt; i\n....i ---&gt; k\n...ik ---&gt; l\n..ikl ---&gt; a\n.ikla ---&gt; k\niklak ---&gt; h\nklakh ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; n\n.jagn ---&gt; a\njagna ---&gt; t\nagnat ---&gt; h\ngnath ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; a\n.mona ---&gt; .\n..... ---&gt; a\n....a ---&gt; y\n...ay ---&gt; u\n..ayu ---&gt; s\n.ayus ---&gt; h\nayush ---&gt; i\nyushi ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; b\n..jeb ---&gt; a\n.jeba ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; i\n.gani ---&gt; t\nganit ---&gt; a\nanita ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; v\nsatyv ---&gt; a\natyva ---&gt; t\ntyvat ---&gt; i\nyvati ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; b\n..sob ---&gt; a\n.soba ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; v\nbhanv ---&gt; a\nhanva ---&gt; r\nanvar ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; a\n.kuma ---&gt; r\nkumar ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; d\n..mud ---&gt; r\n.mudr ---&gt; i\nmudri ---&gt; k\nudrik ---&gt; a\ndrika ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; n\n..jon ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; a\n.vika ---&gt; r\nvikar ---&gt; m\nikarm ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; i\nbhagi ---&gt; p\nhagip ---&gt; u\nagipu ---&gt; r\ngipur ---&gt; i\nipuri ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; y\n.pray ---&gt; a\npraya ---&gt; t\nrayat ---&gt; a\nayata ---&gt; n\nyatan ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; n\n.ashn ---&gt; u\nashnu ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; d\n..prd ---&gt; e\n.prde ---&gt; e\nprdee ---&gt; p\nrdeep ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; k\nhandk ---&gt; i\nandki ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; g\n.surg ---&gt; a\nsurga ---&gt; y\nurgay ---&gt; a\nrgaya ---&gt; n\ngayan ---&gt; i\nayani ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; e\n..gee ---&gt; t\n.geet ---&gt; a\ngeeta ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; w\nahnaw ---&gt; a\nhnawa ---&gt; j\nnawaj ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; c\n..vic ---&gt; h\n.vich ---&gt; i\nvichi ---&gt; t\nichit ---&gt; r\nchitr ---&gt; a\nhitra ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; a\n.sura ---&gt; j\nsuraj ---&gt; m\nurajm ---&gt; a\nrajma ---&gt; l\najmal ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; v\n..yuv ---&gt; r\n.yuvr ---&gt; a\nyuvra ---&gt; j\nuvraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; o\n.shro ---&gt; t\nshrot ---&gt; a\nhrota ---&gt; m\nrotam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; g\n.sang ---&gt; r\nsangr ---&gt; a\nangra ---&gt; m\nngram ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; x\n..lax ---&gt; m\n.laxm ---&gt; e\nlaxme ---&gt; e\naxmee ---&gt; n\nxmeen ---&gt; a\nmeena ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; s\n..jis ---&gt; a\n.jisa ---&gt; a\njisaa ---&gt; n\nisaan ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; p\n..pop ---&gt; i\n.popi ---&gt; n\npopin ---&gt; d\nopind ---&gt; e\npinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; k\n..zak ---&gt; i\n.zaki ---&gt; r\nzakir ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; e\n.kare ---&gt; s\nkares ---&gt; h\naresh ---&gt; a\nresha ---&gt; n\neshan ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; z\n.faiz ---&gt; u\nfaizu ---&gt; r\naizur ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; i\n.safi ---&gt; q\nsafiq ---&gt; .\n..... ---&gt; a\n....a ---&gt; o\n...ao ---&gt; o\n..aoo ---&gt; s\n.aoos ---&gt; a\naoosa ---&gt; f\noosaf ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; r\n..sir ---&gt; j\n.sirj ---&gt; n\nsirjn ---&gt; a\nirjna ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; l\n.ashl ---&gt; a\nashla ---&gt; m\nshlam ---&gt; .\n..... ---&gt; a\n....a ---&gt; x\n...ax ---&gt; a\n..axa ---&gt; t\n.axat ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; r\n..sir ---&gt; i\n.siri ---&gt; s\nsiris ---&gt; h\nirish ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; n\n.punn ---&gt; e\npunne ---&gt; t\nunnet ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; f\n.gulf ---&gt; a\ngulfa ---&gt; n\nulfan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; h\n..nah ---&gt; i\n.nahi ---&gt; d\nnahid ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; t\n..aft ---&gt; a\n.afta ---&gt; b\naftab ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; h\n.mith ---&gt; u\nmithu ---&gt; n\nithun ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; s\n.raks ---&gt; h\nraksh ---&gt; a\naksha ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; e\n..sae ---&gt; s\n.saes ---&gt; h\nsaesh ---&gt; t\naesht ---&gt; a\neshta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; i\n.saki ---&gt; b\nsakib ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; a\n.suga ---&gt; n\nsugan ---&gt; t\nugant ---&gt; i\nganti ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; i\n..dhi ---&gt; r\n.dhir ---&gt; a\ndhira ---&gt; j\nhiraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; e\n.sume ---&gt; d\nsumed ---&gt; h\numedh ---&gt; a\nmedha ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; a\n..aja ---&gt; y\n.ajay ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; o\n.kuno ---&gt; d\nkunod ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; o\nparmo ---&gt; o\narmoo ---&gt; d\nrmood ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; d\n.jaid ---&gt; e\njaide ---&gt; e\naidee ---&gt; p\nideep ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; r\n..jar ---&gt; e\n.jare ---&gt; e\njaree ---&gt; n\nareen ---&gt; a\nreena ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; m\n..prm ---&gt; o\n.prmo ---&gt; d\nprmod ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; e\n..gee ---&gt; t\n.geet ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; e\n.deve ---&gt; n\ndeven ---&gt; d\nevend ---&gt; r\nvendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; i\n.niti ---&gt; k\nnitik ---&gt; a\nitika ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; n\nvishn ---&gt; u\nishnu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; i\n.sabi ---&gt; t\nsabit ---&gt; a\nabita ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; j\n..haj ---&gt; i\n.haji ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; k\n.shek ---&gt; h\nshekh ---&gt; a\nhekha ---&gt; r\nekhar ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; i\n..joi ---&gt; t\n.joit ---&gt; y\njoity ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; t\n.bhat ---&gt; e\nbhate ---&gt; r\nhater ---&gt; i\nateri ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; o\nsubho ---&gt; d\nubhod ---&gt; h\nbhodh ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; j\n..gaj ---&gt; r\n.gajr ---&gt; a\ngajra ---&gt; j\najraj ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; l\n..bil ---&gt; k\n.bilk ---&gt; i\nbilki ---&gt; s\nilkis ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; h\n..suh ---&gt; a\n.suha ---&gt; n\nsuhan ---&gt; i\nuhani ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; s\n..mis ---&gt; b\n.misb ---&gt; a\nmisba ---&gt; h\nisbah ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; a\n.susa ---&gt; n\nsusan ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; b\nchhab ---&gt; i\nhhabi ---&gt; l\nhabil ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; u\n..alu ---&gt; d\n.alud ---&gt; d\naludd ---&gt; i\nluddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; d\n.somd ---&gt; e\nsomde ---&gt; v\nomdev ---&gt; i\nmdevi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; d\n.said ---&gt; u\nsaidu ---&gt; l\naidul ---&gt; .\n..... ---&gt; h\n....h ---&gt; o\n...ho ---&gt; o\n..hoo ---&gt; r\n.hoor ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; h\nshadh ---&gt; i\nhadhi ---&gt; k\nadhik ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; a\nmunna ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; w\n.bhaw ---&gt; n\nbhawn ---&gt; a\nhawna ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; d\n..ked ---&gt; a\n.keda ---&gt; r\nkedar ---&gt; m\nedarm ---&gt; a\ndarma ---&gt; l\narmal ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; e\n..ane ---&gt; e\n.anee ---&gt; t\naneet ---&gt; a\nneeta ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; t\n..snt ---&gt; o\n.snto ---&gt; s\nsntos ---&gt; h\nntosh ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; r\n..dar ---&gt; f\n.darf ---&gt; a\ndarfa ---&gt; s\narfas ---&gt; h\nrfash ---&gt; a\nfasha ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; i\n.aari ---&gt; f\naarif ---&gt; a\narifa ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; i\n..avi ---&gt; n\n.avin ---&gt; a\navina ---&gt; s\nvinas ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; v\n.jaiv ---&gt; i\njaivi ---&gt; n\naivin ---&gt; d\nivind ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; m\n.ashm ---&gt; a\nashma ---&gt; n\nshman ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; a\n.rama ---&gt; n\nraman ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; a\n.roha ---&gt; a\nrohaa ---&gt; n\nohaan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; i\n.mami ---&gt; t\nmamit ---&gt; a\namita ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; l\n..mil ---&gt; k\n.milk ---&gt; h\nmilkh ---&gt; i\nilkhi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; t\n..lat ---&gt; o\n.lato ---&gt; o\nlatoo ---&gt; r\natoor ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; m\n..zam ---&gt; e\n.zame ---&gt; e\nzamee ---&gt; r\nameer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; i\n.sari ---&gt; f\nsarif ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; b\nshabb ---&gt; i\nhabbi ---&gt; r\nabbir ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; l\n..tul ---&gt; s\n.tuls ---&gt; a\ntulsa ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; p\n.ramp ---&gt; r\nrampr ---&gt; a\nampra ---&gt; s\nmpras ---&gt; a\nprasa ---&gt; d\nrasad ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; m\namarm ---&gt; u\nmarmu ---&gt; l\narmul ---&gt; a\nrmula ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; f\n.gulf ---&gt; s\ngulfs ---&gt; a\nulfsa ---&gt; l\nlfsal ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; r\n..gar ---&gt; u\n.garu ---&gt; p\ngarup ---&gt; a\narupa ---&gt; l\nrupal ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; r\n..dar ---&gt; i\n.dari ---&gt; y\ndariy ---&gt; a\nariya ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; l\n..tul ---&gt; k\n.tulk ---&gt; i\ntulki ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; h\n.ragh ---&gt; u\nraghu ---&gt; v\naghuv ---&gt; e\nghuve ---&gt; e\nhuvee ---&gt; r\nuveer ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; o\n.ramo ---&gt; t\nramot ---&gt; a\namota ---&gt; r\nmotar ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; j\n..rij ---&gt; a\n.rija ---&gt; k\nrijak ---&gt; p\nijakp ---&gt; a\njakpa ---&gt; l\nakpal ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; e\n.fare ---&gt; e\nfaree ---&gt; m\nareem ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; n\n.rajn ---&gt; e\nrajne ---&gt; s\najnes ---&gt; h\njnesh ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; b\n..jub ---&gt; e\n.jube ---&gt; d\njubed ---&gt; a\nubeda ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; s\n..hus ---&gt; i\n.husi ---&gt; n\nhusin ---&gt; a\nusina ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; e\n.mune ---&gt; n\nmunen ---&gt; d\nunend ---&gt; e\nnende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; t\n.sawt ---&gt; r\nsawtr ---&gt; i\nawtri ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; t\n.reet ---&gt; i\nreeti ---&gt; m\neetim ---&gt; a\netima ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; s\n.sars ---&gt; a\nsarsa ---&gt; w\narsaw ---&gt; a\nrsawa ---&gt; t\nsawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; r\n..afr ---&gt; i\n.afri ---&gt; n\nafrin ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; k\n.dilk ---&gt; h\ndilkh ---&gt; u\nilkhu ---&gt; s\nlkhus ---&gt; h\nkhush ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; r\n..dar ---&gt; s\n.dars ---&gt; h\ndarsh ---&gt; n\narshn ---&gt; a\nrshna ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; r\n..chr ---&gt; a\n.chra ---&gt; n\nchran ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; h\n.kish ---&gt; n\nkishn ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; l\n..bel ---&gt; o\n.belo ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; u\n..aru ---&gt; n\n.arun ---&gt; k\narunk ---&gt; u\nrunku ---&gt; m\nunkum ---&gt; a\nnkuma ---&gt; r\nkumar ---&gt; .\n..... ---&gt; i\n....i ---&gt; q\n...iq ---&gt; u\n..iqu ---&gt; b\n.iqub ---&gt; a\niquba ---&gt; l\nqubal ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; n\n.pann ---&gt; a\npanna ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; j\n.jagj ---&gt; i\njagji ---&gt; w\nagjiw ---&gt; a\ngjiwa ---&gt; n\njiwan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; r\n.najr ---&gt; e\nnajre ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; u\n.paru ---&gt; l\nparul ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; a\n..soa ---&gt; m\n.soam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; l\n.sahl ---&gt; e\nsahle ---&gt; s\nahles ---&gt; h\nhlesh ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; t\n..hit ---&gt; e\n.hite ---&gt; s\nhites ---&gt; h\nitesh ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; k\n..vak ---&gt; e\n.vake ---&gt; s\nvakes ---&gt; h\nakesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; v\n.prav ---&gt; i\npravi ---&gt; n\nravin ---&gt; d\navind ---&gt; r\nvindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; v\n..sev ---&gt; a\n.seva ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; j\n.shaj ---&gt; i\nshaji ---&gt; y\nhajiy ---&gt; a\najiya ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; p\nharmp ---&gt; a\narmpa ---&gt; l\nrmpal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; e\n.sare ---&gt; e\nsaree ---&gt; n\nareen ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; d\n..mud ---&gt; s\n.muds ---&gt; s\nmudss ---&gt; i\nudssi ---&gt; r\ndssir ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; y\n..aay ---&gt; u\n.aayu ---&gt; s\naayus ---&gt; h\nayush ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; d\n..mud ---&gt; s\n.muds ---&gt; a\nmudsa ---&gt; y\nudsay ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; k\n.sahk ---&gt; i\nsahki ---&gt; r\nahkir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; j\nsahaj ---&gt; h\nahajh ---&gt; a\nhajha ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; m\n..sim ---&gt; r\n.simr ---&gt; e\nsimre ---&gt; n\nimren ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; o\n.vino ---&gt; s\nvinos ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; t\n.adit ---&gt; i\naditi ---&gt; y\nditiy ---&gt; a\nitiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; d\n.sahd ---&gt; a\nsahda ---&gt; b\nahdab ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; a\n.kana ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; i\n..khi ---&gt; m\n.khim ---&gt; a\nkhima ---&gt; n\nhiman ---&gt; a\nimana ---&gt; n\nmanan ---&gt; d\nanand ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; h\n.megh ---&gt; a\nmegha ---&gt; n\neghan ---&gt; a\nghana ---&gt; t\nhanat ---&gt; h\nanath ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; m\n.salm ---&gt; a\nsalma ---&gt; m\nalmam ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; n\n.mitn ---&gt; u\nmitnu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; a\n.susa ---&gt; n\nsusan ---&gt; t\nusant ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; m\n..sem ---&gt; a\n.sema ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; j\n.aarj ---&gt; u\naarju ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; w\nvishw ---&gt; j\nishwj ---&gt; e\nshwje ---&gt; e\nhwjee ---&gt; t\nwjeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; b\n..abb ---&gt; a\n.abba ---&gt; l\nabbal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; y\nsamay ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; d\n.bald ---&gt; e\nbalde ---&gt; v\naldev ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; r\nhandr ---&gt; i\nandri ---&gt; k\nndrik ---&gt; a\ndrika ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; a\nrajia ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; m\n..rim ---&gt; j\n.rimj ---&gt; h\nrimjh ---&gt; i\nimjhi ---&gt; m\nmjhim ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; b\n..jab ---&gt; b\n.jabb ---&gt; a\njabba ---&gt; r\nabbar ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; t\n..avt ---&gt; a\n.avta ---&gt; r\navtar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; t\n..mat ---&gt; o\n.mato ---&gt; k\nmatok ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; u\n..dau ---&gt; a\n.daua ---&gt; d\ndauad ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; d\n..mud ---&gt; b\n.mudb ---&gt; i\nmudbi ---&gt; r\nudbir ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; d\nshard ---&gt; a\nharda ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; l\n.shil ---&gt; a\nshila ---&gt; p\nhilap ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; b\nkhusb ---&gt; o\nhusbo ---&gt; o\nusboo ---&gt; .\n..... ---&gt; u\n....u ---&gt; s\n...us ---&gt; m\n..usm ---&gt; a\n.usma ---&gt; n\nusman ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; a\n.abha ---&gt; y\nabhay ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; i\nshami ---&gt; m\nhamim ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; a\n.kuma ---&gt; r\nkumar ---&gt; e\numare ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; i\n.mali ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; r\n.dhur ---&gt; v\ndhurv ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; a\n.suma ---&gt; n\nsuman ---&gt; l\numanl ---&gt; t\nmanlt ---&gt; a\nanlta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; k\nsamik ---&gt; s\namiks ---&gt; h\nmiksh ---&gt; a\niksha ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; r\n.babr ---&gt; a\nbabra ---&gt; m\nabram ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; o\n..foo ---&gt; l\n.fool ---&gt; m\nfoolm ---&gt; a\noolma ---&gt; y\nolmay ---&gt; a\nlmaya ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; n\n..dan ---&gt; i\n.dani ---&gt; s\ndanis ---&gt; t\nanist ---&gt; a\nnista ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; a\n.gyaa ---&gt; n\ngyaan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; h\n.nanh ---&gt; e\nnanhe ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; c\n.aanc ---&gt; h\naanch ---&gt; a\nancha ---&gt; l\nnchal ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; e\nparte ---&gt; e\nartee ---&gt; k\nrteek ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; l\n..dul ---&gt; a\n.dula ---&gt; l\ndulal ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; o\n..deo ---&gt; u\n.deou ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; d\n..yad ---&gt; h\n.yadh ---&gt; o\nyadho ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; s\n.kirs ---&gt; h\nkirsh ---&gt; n\nirshn ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; s\n.rajs ---&gt; h\nrajsh ---&gt; r\najshr ---&gt; e\njshre ---&gt; e\nshree ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; e\nchote ---&gt; l\nhotel ---&gt; a\notela ---&gt; l\ntelal ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; n\n..din ---&gt; e\n.dine ---&gt; e\ndinee ---&gt; s\ninees ---&gt; h\nneesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; a\n.aana ---&gt; n\naanan ---&gt; d\nanand ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; l\n.devl ---&gt; i\ndevli ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; b\n.sabb ---&gt; a\nsabba ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; t\npritt ---&gt; a\nritta ---&gt; m\nittam ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; u\n.banu ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; l\n..gal ---&gt; i\n.gali ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; k\n..tek ---&gt; c\n.tekc ---&gt; h\ntekch ---&gt; a\nekcha ---&gt; n\nkchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; t\n..art ---&gt; h\n.arth ---&gt; i\narthi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; t\nrajit ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; t\nshant ---&gt; a\nhanta ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; i\n.mali ---&gt; k\nmalik ---&gt; a\nalika ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; m\ngulam ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; w\nbhanw ---&gt; a\nhanwa ---&gt; r\nanwar ---&gt; i\nnwari ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; m\n.gurm ---&gt; i\ngurmi ---&gt; t\nurmit ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; b\n..arb ---&gt; a\n.arba ---&gt; j\narbaj ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; a\n.isha ---&gt; t\nishat ---&gt; k\nshatk ---&gt; a\nhatka ---&gt; r\natkar ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; u\n.jitu ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; n\n.meen ---&gt; a\nmeena ---&gt; t\neenat ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; i\n.devi ---&gt; l\ndevil ---&gt; a\nevila ---&gt; l\nvilal ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; r\n..nur ---&gt; b\n.nurb ---&gt; i\nnurbi ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; s\n..bis ---&gt; h\n.bish ---&gt; u\nbishu ---&gt; n\nishun ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; b\n..jeb ---&gt; i\n.jebi ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; k\n.anik ---&gt; e\nanike ---&gt; s\nnikes ---&gt; h\nikesh ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; e\n..lee ---&gt; l\n.leel ---&gt; u\nleelu ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; o\n.phoo ---&gt; l\nphool ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; o\n.kalo ---&gt; o\nkaloo ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; m\n..umm ---&gt; e\n.umme ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; m\n.faim ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; a\n.asha ---&gt; d\nashad ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; n\n.sayn ---&gt; a\nsayna ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; t\n..jat ---&gt; i\n.jati ---&gt; n\njatin ---&gt; .\n..... ---&gt; a\n....a ---&gt; y\n...ay ---&gt; a\n..aya ---&gt; n\n.ayan ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; n\nmohan ---&gt; l\nohanl ---&gt; a\nhanla ---&gt; l\nanlal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; g\n.shag ---&gt; u\nshagu ---&gt; f\nhaguf ---&gt; t\naguft ---&gt; a\ngufta ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; h\n.megh ---&gt; a\nmegha ---&gt; r\neghar ---&gt; a\nghara ---&gt; j\nharaj ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; h\n.vash ---&gt; u\nvashu ---&gt; d\nashud ---&gt; e\nshude ---&gt; v\nhudev ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; u\n..mou ---&gt; s\n.mous ---&gt; i\nmousi ---&gt; n\nousin ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; b\n.anub ---&gt; h\nanubh ---&gt; a\nnubha ---&gt; v\nubhav ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; u\n..sau ---&gt; r\n.saur ---&gt; a\nsaura ---&gt; b\naurab ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; u\n.mahu ---&gt; d\nmahud ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; u\n..kou ---&gt; s\n.kous ---&gt; h\nkoush ---&gt; i\noushi ---&gt; k\nushik ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; k\n.park ---&gt; a\nparka ---&gt; s\narkas ---&gt; h\nrkash ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; n\n..ajn ---&gt; a\n.ajna ---&gt; b\najnab ---&gt; i\njnabi ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; i\n.giri ---&gt; s\ngiris ---&gt; h\nirish ---&gt; i\nrishi ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; p\n.dhap ---&gt; u\ndhapu ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; d\n.mohd ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; b\n..jub ---&gt; e\n.jube ---&gt; r\njuber ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; g\n..nag ---&gt; i\n.nagi ---&gt; a\nnagia ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; h\n.budh ---&gt; r\nbudhr ---&gt; a\nudhra ---&gt; m\ndhram ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; m\n.khum ---&gt; l\nkhuml ---&gt; o\nhumlo ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; m\n..rim ---&gt; z\n.rimz ---&gt; i\nrimzi ---&gt; m\nimzim ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; m\n..rum ---&gt; a\n.ruma ---&gt; l\nrumal ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; j\n.farj ---&gt; a\nfarja ---&gt; n\narjan ---&gt; d\nrjand ---&gt; .\n..... ---&gt; a\n....a ---&gt; e\n...ae ---&gt; j\n..aej ---&gt; a\n.aeja ---&gt; z\naejaz ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; u\n.manu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; a\n.suma ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; u\n..mau ---&gt; s\n.maus ---&gt; h\nmaush ---&gt; i\naushi ---&gt; d\nushid ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; i\n.sari ---&gt; t\nsarit ---&gt; a\narita ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; e\n.amre ---&gt; e\namree ---&gt; t\nmreet ---&gt; a\nreeta ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; r\n..gor ---&gt; v\n.gorv ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; e\n.same ---&gt; e\nsamee ---&gt; n\nameen ---&gt; a\nmeena ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; j\n.nirj ---&gt; a\nnirja ---&gt; l\nirjal ---&gt; a\nrjala ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; n\nrajen ---&gt; d\najend ---&gt; a\njenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; g\n..dig ---&gt; a\n.diga ---&gt; m\ndigam ---&gt; b\nigamb ---&gt; e\ngambe ---&gt; r\namber ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; z\n..aaz ---&gt; a\n.aaza ---&gt; d\naazad ---&gt; i\nazadi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; h\n.ragh ---&gt; u\nraghu ---&gt; b\naghub ---&gt; e\nghube ---&gt; e\nhubee ---&gt; r\nubeer ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; a\n.hara ---&gt; r\nharar ---&gt; a\narara ---&gt; t\nrarat ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; d\n..rid ---&gt; h\n.ridh ---&gt; i\nridhi ---&gt; m\nidhim ---&gt; a\ndhima ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; k\n..dak ---&gt; s\n.daks ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; u\nkushu ---&gt; m\nushum ---&gt; .\n..... ---&gt; p\n....p ---&gt; y\n...py ---&gt; a\n..pya ---&gt; r\n.pyar ---&gt; e\npyare ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; a\n.sawa ---&gt; n\nsawan ---&gt; .\n..... ---&gt; i\n....i ---&gt; r\n...ir ---&gt; f\n..irf ---&gt; a\n.irfa ---&gt; n\nirfan ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; b\n..deb ---&gt; a\n.deba ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; r\n..mer ---&gt; c\n.merc ---&gt; y\nmercy ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; w\n..jiw ---&gt; a\n.jiwa ---&gt; n\njiwan ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; u\n..chu ---&gt; m\n.chum ---&gt; k\nchumk ---&gt; i\nhumki ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; a\n.jaya ---&gt; n\njayan ---&gt; t\nayant ---&gt; i\nyanti ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; r\n..var ---&gt; s\n.vars ---&gt; h\nvarsh ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; m\n..azm ---&gt; i\n.azmi ---&gt; r\nazmir ---&gt; a\nzmira ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; f\n.ramf ---&gt; a\nramfa ---&gt; r\namfar ---&gt; e\nmfare ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; h\n.rukh ---&gt; s\nrukhs ---&gt; n\nukhsn ---&gt; a\nkhsna ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; h\nsandh ---&gt; y\nandhy ---&gt; a\nndhya ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; l\n.mall ---&gt; i\nmalli ---&gt; k\nallik ---&gt; a\nllika ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; k\n.punk ---&gt; a\npunka ---&gt; j\nunkaj ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; z\n..afz ---&gt; a\n.afza ---&gt; j\nafzaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; r\n.sahr ---&gt; u\nsahru ---&gt; l\nahrul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; h\n.shoh ---&gt; a\nshoha ---&gt; n\nhohan ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; i\n..avi ---&gt; d\n.avid ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; i\n..dhi ---&gt; r\n.dhir ---&gt; u\ndhiru ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; s\n.umas ---&gt; a\numasa ---&gt; n\nmasan ---&gt; k\nasank ---&gt; a\nsanka ---&gt; r\nankar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; n\n.shun ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; g\n.sang ---&gt; e\nsange ---&gt; e\nangee ---&gt; n\nngeen ---&gt; a\ngeena ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; t\n..gat ---&gt; t\n.gatt ---&gt; u\ngattu ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; b\nhushb ---&gt; a\nushba ---&gt; r\nshbar ---&gt; i\nhbari ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; b\n..bob ---&gt; i\n.bobi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; k\n..jak ---&gt; h\n.jakh ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; a\n.mura ---&gt; r\nmurar ---&gt; i\nurari ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; l\n..dul ---&gt; i\n.duli ---&gt; c\ndulic ---&gt; h\nulich ---&gt; a\nlicha ---&gt; n\nichan ---&gt; d\nchand ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; m\n.najm ---&gt; a\nnajma ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; i\n.lavi ---&gt; s\nlavis ---&gt; h\navish ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; m\n..vim ---&gt; l\n.viml ---&gt; a\nvimla ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; s\n.vans ---&gt; h\nvansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; u\n..sou ---&gt; r\n.sour ---&gt; a\nsoura ---&gt; b\nourab ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; u\n.lalu ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; y\n..aay ---&gt; s\n.aays ---&gt; h\naaysh ---&gt; a\naysha ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; s\n..des ---&gt; h\n.desh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; i\nmandi ---&gt; r\nandir ---&gt; a\nndira ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; t\n.kunt ---&gt; e\nkunte ---&gt; s\nuntes ---&gt; h\nntesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; r\nshahr ---&gt; i\nhahri ---&gt; s\nahris ---&gt; h\nhrish ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; s\n..hus ---&gt; a\n.husa ---&gt; n\nhusan ---&gt; a\nusana ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; i\n..aji ---&gt; t\n.ajit ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; o\n.bhoo ---&gt; r\nbhoor ---&gt; i\nhoori ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; k\n.shek ---&gt; h\nshekh ---&gt; e\nhekhe ---&gt; r\nekher ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; y\nsamay ---&gt; d\namayd ---&gt; d\nmaydd ---&gt; i\nayddi ---&gt; n\nyddin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; e\nramde ---&gt; v\namdev ---&gt; i\nmdevi ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; n\n..den ---&gt; e\n.dene ---&gt; s\ndenes ---&gt; h\nenesh ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; a\nfarha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; i\n.navi ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; r\n.poor ---&gt; n\npoorn ---&gt; i\noorni ---&gt; m\nornim ---&gt; a\nrnima ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; k\n..vak ---&gt; i\n.vaki ---&gt; l\nvakil ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; m\n.yasm ---&gt; i\nyasmi ---&gt; n\nasmin ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; o\n..ano ---&gt; o\n.anoo ---&gt; p\nanoop ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; o\n.bhoo ---&gt; p\nbhoop ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; z\n.faiz ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; n\nsahan ---&gt; a\nahana ---&gt; j\nhanaj ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; a\n..aka ---&gt; s\n.akas ---&gt; h\nakash ---&gt; y\nkashy ---&gt; a\nashya ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; i\n.kali ---&gt; s\nkalis ---&gt; h\nalish ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; d\n.band ---&gt; u\nbandu ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; i\nparmi ---&gt; n\narmin ---&gt; d\nrmind ---&gt; e\nminde ---&gt; r\ninder ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; k\n.pusk ---&gt; a\npuska ---&gt; r\nuskar ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; z\n..aaz ---&gt; a\n.aaza ---&gt; d\naazad ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; l\n.kaml ---&gt; e\nkamle ---&gt; s\namles ---&gt; h\nmlesh ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; m\n.nazm ---&gt; a\nnazma ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; s\n.shis ---&gt; h\nshish ---&gt; r\nhishr ---&gt; a\nishra ---&gt; m\nshram ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; t\n.meet ---&gt; h\nmeeth ---&gt; u\neethu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; z\n..raz ---&gt; i\n.razi ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; f\n..guf ---&gt; r\n.gufr ---&gt; a\ngufra ---&gt; n\nufran ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; v\n.bhiv ---&gt; a\nbhiva ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; o\n..amo ---&gt; t\n.amot ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; a\n.kira ---&gt; n\nkiran ---&gt; n\nirann ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; r\nhahir ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; a\n.kosa ---&gt; r\nkosar ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; e\n.sohe ---&gt; l\nsohel ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; u\n..nau ---&gt; l\n.naul ---&gt; a\nnaula ---&gt; k\naulak ---&gt; h\nulakh ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; m\n..aam ---&gt; i\n.aami ---&gt; r\naamir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; i\n.saji ---&gt; n\nsajin ---&gt; i\najini ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; i\n..aji ---&gt; m\n.ajim ---&gt; a\najima ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; o\nmando ---&gt; t\nandot ---&gt; h\nndoth ---&gt; i\ndothi ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; n\n..gen ---&gt; d\n.gend ---&gt; u\ngendu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; n\n.nann ---&gt; u\nnannu ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; s\n..mos ---&gt; h\n.mosh ---&gt; i\nmoshi ---&gt; n\noshin ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; t\n..fat ---&gt; m\n.fatm ---&gt; a\nfatma ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; h\n..suh ---&gt; a\n.suha ---&gt; g\nsuhag ---&gt; i\nuhagi ---&gt; n\nhagin ---&gt; i\nagini ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; g\n..dig ---&gt; v\n.digv ---&gt; i\ndigvi ---&gt; j\nigvij ---&gt; a\ngvija ---&gt; y\nvijay ---&gt; .\n..... ---&gt; o\n....o ---&gt; j\n...oj ---&gt; a\n..oja ---&gt; s\n.ojas ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; i\n.basi ---&gt; b\nbasib ---&gt; u\nasibu ---&gt; l\nsibul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; l\n.shul ---&gt; a\nshula ---&gt; l\nhulal ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; o\npramo ---&gt; o\nramoo ---&gt; d\namood ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; k\n.kumk ---&gt; u\nkumku ---&gt; m\numkum ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; n\n.anan ---&gt; d\nanand ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; k\n.shok ---&gt; e\nshoke ---&gt; s\nhokes ---&gt; h\nokesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; c\n..bac ---&gt; h\n.bach ---&gt; a\nbacha ---&gt; n\nachan ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; n\n..tun ---&gt; n\n.tunn ---&gt; i\ntunni ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; a\n.deva ---&gt; s\ndevas ---&gt; i\nevasi ---&gt; s\nvasis ---&gt; h\nasish ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; d\n.gird ---&gt; h\ngirdh ---&gt; a\nirdha ---&gt; r\nrdhar ---&gt; i\ndhari ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; m\n.lalm ---&gt; u\nlalmu ---&gt; n\nalmun ---&gt; i\nlmuni ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; e\nvishe ---&gt; s\nishes ---&gt; h\nshesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; n\n.prin ---&gt; k\nprink ---&gt; a\nrinka ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; o\n.mamo ---&gt; i\nmamoi ---&gt; t\namoit ---&gt; a\nmoita ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; a\n.naja ---&gt; r\nnajar ---&gt; a\najara ---&gt; n\njaran ---&gt; a\narana ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; d\n..abd ---&gt; u\n.abdu ---&gt; l\nabdul ---&gt; l\nbdull ---&gt; a\ndulla ---&gt; .\n..... ---&gt; e\n....e ---&gt; k\n...ek ---&gt; a\n..eka ---&gt; t\n.ekat ---&gt; a\nekata ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; p\n.samp ---&gt; a\nsampa ---&gt; t\nampat ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; w\n.balw ---&gt; a\nbalwa ---&gt; n\nalwan ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; k\n.rink ---&gt; u\nrinku ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; n\n..ren ---&gt; a\n.rena ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; e\n.hane ---&gt; e\nhanee ---&gt; f\naneef ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; b\nhushb ---&gt; u\nushbu ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; a\nrisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; a\n.sana ---&gt; a\nsanaa ---&gt; l\nanaal ---&gt; i\nnaali ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; b\n..hab ---&gt; i\n.habi ---&gt; b\nhabib ---&gt; a\nabiba ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; s\n.bans ---&gt; i\nbansi ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; a\n.kuma ---&gt; r\nkumar ---&gt; a\numara ---&gt; g\nmarag ---&gt; e\narage ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; a\n..sna ---&gt; h\n.snah ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; v\n.prav ---&gt; e\nprave ---&gt; e\nravee ---&gt; n\naveen ---&gt; a\nveena ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; c\n.gurc ---&gt; h\ngurch ---&gt; a\nurcha ---&gt; r\nrchar ---&gt; a\nchara ---&gt; n\nharan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; i\n..rai ---&gt; s\n.rais ---&gt; i\nraisi ---&gt; n\naisin ---&gt; g\nising ---&gt; h\nsingh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; j\n.sarj ---&gt; u\nsarju ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; v\n.jasv ---&gt; i\njasvi ---&gt; n\nasvin ---&gt; d\nsvind ---&gt; i\nvindi ---&gt; r\nindir ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; b\n..lib ---&gt; i\n.libi ---&gt; n\nlibin ---&gt; .\n..... ---&gt; d\n....d ---&gt; y\n...dy ---&gt; a\n..dya ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; m\nanchm ---&gt; a\nnchma ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; s\n.sams ---&gt; u\nsamsu ---&gt; n\namsun ---&gt; g\nmsung ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; e\n.jite ---&gt; n\njiten ---&gt; d\nitend ---&gt; r\ntendr ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; d\nsamad ---&gt; h\namadh ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; i\n.kasi ---&gt; s\nkasis ---&gt; h\nasish ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; g\n..jog ---&gt; e\n.joge ---&gt; n\njogen ---&gt; d\nogend ---&gt; e\ngende ---&gt; r\nender ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; b\nrajib ---&gt; u\najibu ---&gt; l\njibul ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; a\nmunna ---&gt; d\nunnad ---&gt; e\nnnade ---&gt; v\nnadev ---&gt; i\nadevi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; s\n.suks ---&gt; h\nsuksh ---&gt; a\nuksha ---&gt; n\nkshan ---&gt; a\nshana ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; r\n..azr ---&gt; u\n.azru ---&gt; d\nazrud ---&gt; d\nzrudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; a\nsidha ---&gt; r\nidhar ---&gt; t\ndhart ---&gt; h\nharth ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; v\n.shav ---&gt; i\nshavi ---&gt; n\nhavin ---&gt; g\naving ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; s\n..fas ---&gt; r\n.fasr ---&gt; u\nfasru ---&gt; n\nasrun ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; i\n..bii ---&gt; t\n.biit ---&gt; u\nbiitu ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; n\n.aman ---&gt; d\namand ---&gt; e\nmande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; l\n..hal ---&gt; k\n.halk ---&gt; u\nhalku ---&gt; j\nalkuj ---&gt; i\nlkuji ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; v\nharmv ---&gt; i\narmvi ---&gt; r\nrmvir ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; i\n.nasi ---&gt; m\nnasim ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; a\nnanda ---&gt; n\nandan ---&gt; i\nndani ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; a\n.dipa ---&gt; l\ndipal ---&gt; i\nipali ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; e\n.vije ---&gt; n\nvijen ---&gt; d\nijend ---&gt; e\njende ---&gt; r\nender ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; s\n.muks ---&gt; n\nmuksn ---&gt; d\nuksnd ---&gt; i\nksndi ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; u\n.indu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; d\n.rajd ---&gt; e\nrajde ---&gt; v\najdev ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; e\n.jite ---&gt; n\njiten ---&gt; d\nitend ---&gt; r\ntendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; n\n.tarn ---&gt; u\ntarnu ---&gt; m\narnum ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; t\n.must ---&gt; k\nmustk ---&gt; i\nustki ---&gt; .\n..... ---&gt; e\n....e ---&gt; l\n...el ---&gt; d\n..eld ---&gt; r\n.eldr ---&gt; o\neldro ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; d\n..aad ---&gt; i\n.aadi ---&gt; l\naadil ---&gt; .\n..... ---&gt; b\n....b ---&gt; s\n...bs ---&gt; r\n..bsr ---&gt; a\n.bsra ---&gt; m\nbsram ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; g\n..bag ---&gt; d\n.bagd ---&gt; a\nbagda ---&gt; i\nagdai ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; m\n.anam ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; i\nshabi ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; a\nkusha ---&gt; l\nushal ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; i\nmunni ---&gt; a\nunnia ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; k\n.nank ---&gt; a\nnanka ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; r\n.chir ---&gt; a\nchira ---&gt; g\nhirag ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; l\n..gil ---&gt; b\n.gilb ---&gt; a\ngilba ---&gt; h\nilbah ---&gt; a\nlbaha ---&gt; r\nbahar ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; a\n.muka ---&gt; t\nmukat ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; h\n.prah ---&gt; l\nprahl ---&gt; a\nrahla ---&gt; d\nahlad ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; m\nparam ---&gt; j\naramj ---&gt; e\nramje ---&gt; e\namjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; t\n..dat ---&gt; a\n.data ---&gt; t\ndatat ---&gt; e\natate ---&gt; r\ntater ---&gt; y\natery ---&gt; a\nterya ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; n\n.harn ---&gt; e\nharne ---&gt; e\narnee ---&gt; t\nrneet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; r\n.sahr ---&gt; u\nsahru ---&gt; k\nahruk ---&gt; h\nhrukh ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; r\n.nasr ---&gt; u\nnasru ---&gt; l\nasrul ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; z\n..muz ---&gt; a\n.muza ---&gt; m\nmuzam ---&gt; i\nuzami ---&gt; i\nzamii ---&gt; l\namiil ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; d\n.jaid ---&gt; e\njaide ---&gt; v\naidev ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; a\n.tara ---&gt; n\ntaran ---&gt; j\naranj ---&gt; e\nranje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; n\nhahin ---&gt; a\nahina ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; i\n.ruki ---&gt; y\nrukiy ---&gt; a\nukiya ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; a\n.baba ---&gt; r\nbabar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; e\n.rame ---&gt; s\nrames ---&gt; h\namesh ---&gt; w\nmeshw ---&gt; a\neshwa ---&gt; r\nshwar ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; n\n..bun ---&gt; d\n.bund ---&gt; h\nbundh ---&gt; u\nundhu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; e\n.rave ---&gt; e\nravee ---&gt; n\naveen ---&gt; a\nveena ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; b\n.surb ---&gt; h\nsurbh ---&gt; i\nurbhi ---&gt; .\n..... ---&gt; u\n....u ---&gt; j\n...uj ---&gt; a\n..uja ---&gt; m\n.ujam ---&gt; a\nujama ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; a\n.suja ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; u\n.haru ---&gt; n\nharun ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; y\n..pry ---&gt; a\n.prya ---&gt; n\npryan ---&gt; k\nryank ---&gt; a\nyanka ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; j\n..waj ---&gt; u\n.waju ---&gt; d\nwajud ---&gt; d\najudd ---&gt; i\njuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; a\n.raja ---&gt; n\nrajan ---&gt; t\najant ---&gt; i\njanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; n\n.sain ---&gt; k\nsaink ---&gt; y\nainky ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; w\n.janw ---&gt; i\njanwi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; a\nramba ---&gt; b\nambab ---&gt; u\nmbabu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; v\nsukhv ---&gt; i\nukhvi ---&gt; r\nkhvir ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; n\n.gyan ---&gt; d\ngyand ---&gt; e\nyande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; j\n.virj ---&gt; i\nvirji ---&gt; n\nirjin ---&gt; i\nrjini ---&gt; y\njiniy ---&gt; a\niniya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; r\n.nasr ---&gt; u\nnasru ---&gt; d\nasrud ---&gt; d\nsrudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; j\n..ruj ---&gt; i\n.ruji ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; m\n..ham ---&gt; e\n.hame ---&gt; n\nhamen ---&gt; t\nament ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; k\n.sank ---&gt; a\nsanka ---&gt; r\nankar ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; l\n.bhol ---&gt; i\nbholi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; k\nravik ---&gt; a\navika ---&gt; n\nvikan ---&gt; t\nikant ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; t\n.aast ---&gt; o\naasto ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; d\n.gudd ---&gt; i\nguddi ---&gt; y\nuddiy ---&gt; a\nddiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; a\nsusha ---&gt; m\nusham ---&gt; a\nshama ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; r\n..per ---&gt; d\n.perd ---&gt; e\nperde ---&gt; e\nerdee ---&gt; p\nrdeep ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; w\n..raw ---&gt; i\n.rawi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; a\n.jana ---&gt; m\njanam ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; r\n.harr ---&gt; y\nharry ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; p\n.roop ---&gt; a\nroopa ---&gt; m\noopam ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; i\n.kani ---&gt; k\nkanik ---&gt; a\nanika ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; a\n.bira ---&gt; j\nbiraj ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; i\n.kari ---&gt; s\nkaris ---&gt; m\narism ---&gt; a\nrisma ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; s\n..mes ---&gt; a\n.mesa ---&gt; k\nmesak ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; i\n..rai ---&gt; s\n.rais ---&gt; h\nraish ---&gt; a\naisha ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; a\n.soha ---&gt; l\nsohal ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; r\n..for ---&gt; s\n.fors ---&gt; h\nforsh ---&gt; a\norsha ---&gt; n\nrshan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; j\n.mahj ---&gt; a\nmahja ---&gt; b\nahjab ---&gt; i\nhjabi ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; g\n..bag ---&gt; a\n.baga ---&gt; r\nbagar ---&gt; a\nagara ---&gt; m\ngaram ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; k\nshaik ---&gt; h\nhaikh ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; y\n..pay ---&gt; a\n.paya ---&gt; n\npayan ---&gt; a\nayana ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; a\n.tara ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; u\n.lalu ---&gt; t\nlalut ---&gt; h\naluth ---&gt; a\nlutha ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; k\n.deek ---&gt; s\ndeeks ---&gt; h\neeksh ---&gt; a\neksha ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; j\nshanj ---&gt; h\nhanjh ---&gt; a\nanjha ---&gt; n\nnjhan ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; r\n..sor ---&gt; a\n.sora ---&gt; v\nsorav ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; a\ndhana ---&gt; n\nhanan ---&gt; j\nananj ---&gt; a\nnanja ---&gt; y\nanjay ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; y\n.nary ---&gt; a\nnarya ---&gt; n\naryan ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; r\n.dhur ---&gt; a\ndhura ---&gt; v\nhurav ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; m\n..nam ---&gt; i\n.nami ---&gt; t\nnamit ---&gt; a\namita ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; r\n..sir ---&gt; a\n.sira ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; o\n.amro ---&gt; o\namroo ---&gt; t\nmroot ---&gt; i\nrooti ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; b\nchhab ---&gt; e\nhhabe ---&gt; e\nhabee ---&gt; l\nabeel ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; j\n.ramj ---&gt; a\nramja ---&gt; n\namjan ---&gt; a\nmjana ---&gt; m\njanam ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; d\n..nad ---&gt; i\n.nadi ---&gt; m\nnadim ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; t\n.neet ---&gt; a\nneeta ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; e\n.sune ---&gt; t\nsunet ---&gt; a\nuneta ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; j\n.sehj ---&gt; a\nsehja ---&gt; l\nehjal ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; i\n.dili ---&gt; p\ndilip ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; k\n.dilk ---&gt; u\ndilku ---&gt; s\nilkus ---&gt; h\nlkush ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; h\n..boh ---&gt; a\n.boha ---&gt; t\nbohat ---&gt; i\nohati ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; a\n.lala ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; u\n.salu ---&gt; p\nsalup ---&gt; a\nalupa ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; x\n..lax ---&gt; a\n.laxa ---&gt; m\nlaxam ---&gt; i\naxami ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; f\n..aaf ---&gt; r\n.aafr ---&gt; i\naafri ---&gt; n\nafrin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; i\n.rani ---&gt; p\nranip ---&gt; a\nanipa ---&gt; l\nnipal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; n\n.maan ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; u\n..gau ---&gt; r\n.gaur ---&gt; a\ngaura ---&gt; v\naurav ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; e\n..ume ---&gt; s\n.umes ---&gt; h\numesh ---&gt; a\nmesha ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; y\n.shay ---&gt; r\nshayr ---&gt; a\nhayra ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; s\n.shus ---&gt; i\nshusi ---&gt; l\nhusil ---&gt; a\nusila ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; s\n.hars ---&gt; h\nharsh ---&gt; d\narshd ---&gt; e\nrshde ---&gt; e\nshdee ---&gt; p\nhdeep ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; p\n..dep ---&gt; a\n.depa ---&gt; l\ndepal ---&gt; i\nepali ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; s\n..sas ---&gt; h\n.sash ---&gt; i\nsashi ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; u\n..kau ---&gt; s\n.kaus ---&gt; h\nkaush ---&gt; i\naushi ---&gt; k\nushik ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; l\n..dul ---&gt; a\n.dula ---&gt; r\ndular ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; m\n..umm ---&gt; a\n.umma ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; l\n.rajl ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; .\n..... ---&gt; e\n....e ---&gt; n\n...en ---&gt; g\n..eng ---&gt; l\n.engl ---&gt; a\nengla ---&gt; s\nnglas ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; h\n.sudh ---&gt; a\nsudha ---&gt; n\nudhan ---&gt; s\ndhans ---&gt; u\nhansu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; a\n.kala ---&gt; v\nkalav ---&gt; a\nalava ---&gt; t\nlavat ---&gt; i\navati ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; p\n..sup ---&gt; i\n.supi ---&gt; r\nsupir ---&gt; i\nupiri ---&gt; y\npiriy ---&gt; a\niriya ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; v\n.visv ---&gt; a\nvisva ---&gt; j\nisvaj ---&gt; e\nsvaje ---&gt; e\nvajee ---&gt; t\najeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; j\n.sukj ---&gt; v\nsukjv ---&gt; e\nukjve ---&gt; e\nkjvee ---&gt; r\njveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; i\n.savi ---&gt; t\nsavit ---&gt; r\navitr ---&gt; i\nvitri ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; c\n..rac ---&gt; h\n.rach ---&gt; n\nrachn ---&gt; u\nachnu ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; b\n..mub ---&gt; a\n.muba ---&gt; r\nmubar ---&gt; i\nubari ---&gt; k\nbarik ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; m\n.manm ---&gt; a\nmanma ---&gt; t\nanmat ---&gt; h\nnmath ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; h\n.push ---&gt; p\npushp ---&gt; e\nushpe ---&gt; n\nshpen ---&gt; d\nhpend ---&gt; r\npendr ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; o\n.mano ---&gt; w\nmanow ---&gt; a\nanowa ---&gt; r\nnowar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; a\navina ---&gt; t\nvinat ---&gt; h\ninath ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; k\n..shk ---&gt; u\n.shku ---&gt; n\nshkun ---&gt; t\nhkunt ---&gt; a\nkunta ---&gt; l\nuntal ---&gt; a\nntala ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; h\n.nath ---&gt; a\nnatha ---&gt; n\nathan ---&gt; i\nthani ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; n\n..dun ---&gt; i\n.duni ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; y\n..fiy ---&gt; a\n.fiya ---&gt; z\nfiyaz ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; y\n..pay ---&gt; a\n.paya ---&gt; l\npayal ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; j\n.birj ---&gt; e\nbirje ---&gt; s\nirjes ---&gt; h\nrjesh ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; b\nmohab ---&gt; b\nohabb ---&gt; a\nhabba ---&gt; t\nabbat ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; u\n.tanu ---&gt; j\ntanuj ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; a\n.amra ---&gt; t\namrat ---&gt; a\nmrata ---&gt; l\nratal ---&gt; a\natala ---&gt; l\ntalal ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; i\n.khai ---&gt; r\nkhair ---&gt; u\nhairu ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; l\n.tril ---&gt; o\ntrilo ---&gt; k\nrilok ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; s\n..mis ---&gt; s\n.miss ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; l\n..sil ---&gt; p\n.silp ---&gt; a\nsilpa ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; k\n.kank ---&gt; u\nkanku ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; t\n.surt ---&gt; i\nsurti ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; m\n..mom ---&gt; i\n.momi ---&gt; n\nmomin ---&gt; a\nomina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; p\n.sanp ---&gt; a\nsanpa ---&gt; t\nanpat ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; t\n.gant ---&gt; a\nganta ---&gt; n\nantan ---&gt; t\nntant ---&gt; r\ntantr ---&gt; a\nantra ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; e\n.site ---&gt; n\nsiten ---&gt; d\nitend ---&gt; e\ntende ---&gt; r\nender ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; b\n.balb ---&gt; i\nbalbi ---&gt; r\nalbir ---&gt; i\nlbiri ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; a\n.dhra ---&gt; m\ndhram ---&gt; p\nhramp ---&gt; l\nrampl ---&gt; a\nampla ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; t\n..ist ---&gt; k\n.istk ---&gt; h\nistkh ---&gt; a\nstkha ---&gt; r\ntkhar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; u\nmansu ---&gt; r\nansur ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; i\n.radi ---&gt; k\nradik ---&gt; h\nadikh ---&gt; a\ndikha ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; v\n.sarv ---&gt; a\nsarva ---&gt; s\narvas ---&gt; h\nrvash ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; n\n.guln ---&gt; a\ngulna ---&gt; j\nulnaj ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; v\n..nov ---&gt; i\n.novi ---&gt; s\nnovis ---&gt; h\novish ---&gt; a\nvisha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; h\n.ragh ---&gt; u\nraghu ---&gt; b\naghub ---&gt; i\nghubi ---&gt; r\nhubir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; i\n.saji ---&gt; d\nsajid ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; n\n.been ---&gt; u\nbeenu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; a\nmadha ---&gt; r\nadhar ---&gt; a\ndhara ---&gt; m\nharam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; e\n.sane ---&gt; h\nsaneh ---&gt; a\naneha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; i\n.rani ---&gt; t\nranit ---&gt; a\nanita ---&gt; .\n..... ---&gt; s\n....s ---&gt; r\n...sr ---&gt; i\n..sri ---&gt; s\n.sris ---&gt; h\nsrish ---&gt; t\nrisht ---&gt; y\nishty ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; i\nsubhi ---&gt; y\nubhiy ---&gt; a\nbhiya ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; b\n..rob ---&gt; i\n.robi ---&gt; n\nrobin ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; s\n..res ---&gt; m\n.resm ---&gt; i\nresmi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; l\n.shel ---&gt; a\nshela ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; e\n.pree ---&gt; t\npreet ---&gt; a\nreeta ---&gt; m\neetam ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; r\n.balr ---&gt; a\nbalra ---&gt; m\nalram ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; d\nsukhd ---&gt; e\nukhde ---&gt; v\nkhdev ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; a\nranja ---&gt; n\nanjan ---&gt; a\nnjana ---&gt; .\n..... ---&gt; e\n....e ---&gt; k\n...ek ---&gt; t\n..ekt ---&gt; a\n.ekta ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; t\n.chat ---&gt; a\nchata ---&gt; n\nhatan ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; a\n.fara ---&gt; n\nfaran ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; t\n.jitt ---&gt; e\njitte ---&gt; n\nitten ---&gt; d\nttend ---&gt; e\ntende ---&gt; r\nender ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; y\n.kaly ---&gt; a\nkalya ---&gt; n\nalyan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; a\n.saya ---&gt; r\nsayar ---&gt; a\nayara ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; b\n.seeb ---&gt; a\nseeba ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; r\n..gar ---&gt; s\n.gars ---&gt; h\ngarsh ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; r\n.badr ---&gt; u\nbadru ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; s\n..mis ---&gt; h\n.mish ---&gt; b\nmishb ---&gt; h\nishbh ---&gt; a\nshbha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; h\n.rakh ---&gt; o\nrakho ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; r\n.sabr ---&gt; i\nsabri ---&gt; n\nabrin ---&gt; a\nbrina ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; b\n.parb ---&gt; h\nparbh ---&gt; u\narbhu ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; o\n.yaso ---&gt; d\nyasod ---&gt; h\nasodh ---&gt; a\nsodha ---&gt; .\n..... ---&gt; f\n....f ---&gt; e\n...fe ---&gt; l\n..fel ---&gt; i\n.feli ---&gt; c\nfelic ---&gt; i\nelici ---&gt; a\nlicia ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; d\n.pard ---&gt; e\nparde ---&gt; s\nardes ---&gt; h\nrdesh ---&gt; i\ndeshi ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; o\n..alo ---&gt; k\n.alok ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; n\n.sahn ---&gt; a\nsahna ---&gt; w\nahnaw ---&gt; a\nhnawa ---&gt; z\nnawaz ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; t\n.shat ---&gt; h\nshath ---&gt; i\nhathi ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; l\n.seel ---&gt; a\nseela ---&gt; m\neelam ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; h\n.rosh ---&gt; i\nroshi ---&gt; n\noshin ---&gt; i\nshini ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; a\n.dipa ---&gt; n\ndipan ---&gt; s\nipans ---&gt; h\npansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; h\nmansh ---&gt; i\nanshi ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; m\n..sim ---&gt; p\n.simp ---&gt; i\nsimpi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; u\n..sau ---&gt; r\n.saur ---&gt; a\nsaura ---&gt; v\naurav ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; a\n.sura ---&gt; j\nsuraj ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; e\n..lee ---&gt; l\n.leel ---&gt; a\nleela ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; p\n.ramp ---&gt; h\nramph ---&gt; a\nampha ---&gt; l\nmphal ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; k\n..nek ---&gt; k\n.nekk ---&gt; i\nnekki ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; t\n..hit ---&gt; e\n.hite ---&gt; n\nhiten ---&gt; d\nitend ---&gt; e\ntende ---&gt; r\nender ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; a\nhusha ---&gt; l\nushal ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; w\n..omw ---&gt; a\n.omwa ---&gt; t\nomwat ---&gt; i\nmwati ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; d\n.mund ---&gt; e\nmunde ---&gt; r\nunder ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; s\n.bans ---&gt; h\nbansh ---&gt; i\nanshi ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; n\n..pin ---&gt; k\n.pink ---&gt; o\npinko ---&gt; o\ninkoo ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; s\n..afs ---&gt; h\n.afsh ---&gt; a\nafsha ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; a\nshara ---&gt; d\nharad ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; k\n.rink ---&gt; y\nrinky ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; u\n..aru ---&gt; n\n.arun ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; i\n.naji ---&gt; r\nnajir ---&gt; a\najira ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; p\nshamp ---&gt; a\nhampa ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; e\n..aae ---&gt; n\n.aaen ---&gt; a\naaena ---&gt; b\naenab ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; y\n.sany ---&gt; .\n..... ---&gt; l\n....l ---&gt; u\n...lu ---&gt; s\n..lus ---&gt; h\n.lush ---&gt; i\nlushi ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; a\nmunna ---&gt; w\nunnaw ---&gt; a\nnnawa ---&gt; r\nnawar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; a\n..saa ---&gt; n\n.saan ---&gt; i\nsaani ---&gt; y\naaniy ---&gt; a\naniya ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; i\n..kai ---&gt; f\n.kaif ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; i\n.devi ---&gt; k\ndevik ---&gt; a\nevika ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; l\n..aml ---&gt; e\n.amle ---&gt; s\namles ---&gt; h\nmlesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; a\n.asha ---&gt; k\nashak ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; w\n..paw ---&gt; a\n.pawa ---&gt; n\npawan ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; l\n..jal ---&gt; i\n.jali ---&gt; l\njalil ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; i\nnandi ---&gt; t\nandit ---&gt; a\nndita ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; i\n..aji ---&gt; m\n.ajim ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; b\n..amb ---&gt; e\n.ambe ---&gt; r\namber ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; a\n..kaa ---&gt; r\n.kaar ---&gt; t\nkaart ---&gt; i\naarti ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; a\n.puna ---&gt; m\npunam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; s\n..sas ---&gt; h\n.sash ---&gt; i\nsashi ---&gt; k\nashik ---&gt; a\nshika ---&gt; l\nhikal ---&gt; a\nikala ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; u\n.minu ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; j\n..nej ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; o\n..jho ---&gt; r\n.jhor ---&gt; a\njhora ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; l\n.shel ---&gt; e\nshele ---&gt; n\nhelen ---&gt; d\nelend ---&gt; e\nlende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; t\n.seet ---&gt; a\nseeta ---&gt; l\neetal ---&gt; .\n..... ---&gt; i\n....i ---&gt; r\n...ir ---&gt; s\n..irs ---&gt; h\n.irsh ---&gt; a\nirsha ---&gt; d\nrshad ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; k\n.mukk ---&gt; e\nmukke ---&gt; s\nukkes ---&gt; h\nkkesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; w\n..paw ---&gt; n\n.pawn ---&gt; a\npawna ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; t\n.jeet ---&gt; h\njeeth ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; a\n.rita ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; u\nmadhu ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; h\n.mukh ---&gt; t\nmukht ---&gt; a\nukhta ---&gt; r\nkhtar ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; i\n.giri ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; m\n.bhim ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; g\n..mog ---&gt; a\n.moga ---&gt; n\nmogan ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; k\n..rek ---&gt; a\n.reka ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; w\n.rajw ---&gt; a\nrajwa ---&gt; t\najwat ---&gt; i\njwati ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; t\n..aat ---&gt; a\n.aata ---&gt; m\naatam ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; l\n.khal ---&gt; i\nkhali ---&gt; l\nhalil ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; w\n.kisw ---&gt; a\nkiswa ---&gt; r\niswar ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; y\n..lay ---&gt; b\n.layb ---&gt; a\nlayba ---&gt; .\n..... ---&gt; s\n....s ---&gt; m\n...sm ---&gt; r\n..smr ---&gt; i\n.smri ---&gt; t\nsmrit ---&gt; i\nmriti ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; m\n.manm ---&gt; o\nmanmo ---&gt; h\nanmoh ---&gt; a\nnmoha ---&gt; n\nmohan ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; v\n..kav ---&gt; i\n.kavi ---&gt; t\nkavit ---&gt; a\navita ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; b\n..vib ---&gt; h\n.vibh ---&gt; u\nvibhu ---&gt; t\nibhut ---&gt; i\nbhuti ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; m\n.naim ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; p\n.swap ---&gt; a\nswapa ---&gt; n\nwapan ---&gt; e\napane ---&gt; s\npanes ---&gt; h\nanesh ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; l\n..tul ---&gt; s\n.tuls ---&gt; i\ntulsi ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; t\n..mot ---&gt; i\n.moti ---&gt; l\nmotil ---&gt; a\notila ---&gt; a\ntilaa ---&gt; l\nilaal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; w\n.satw ---&gt; a\nsatwa ---&gt; n\natwan ---&gt; t\ntwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; a\nparva ---&gt; s\narvas ---&gt; h\nrvash ---&gt; i\nvashi ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; i\n.nazi ---&gt; r\nnazir ---&gt; u\naziru ---&gt; l\nzirul ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; u\n.monu ---&gt; p\nmonup ---&gt; a\nonupa ---&gt; l\nnupal ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; u\n..asu ---&gt; t\n.asut ---&gt; o\nasuto ---&gt; s\nsutos ---&gt; h\nutosh ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; t\n.nait ---&gt; i\nnaiti ---&gt; k\naitik ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; w\n..isw ---&gt; e\n.iswe ---&gt; r\niswer ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; n\n..kon ---&gt; t\n.kont ---&gt; i\nkonti ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; h\n.nath ---&gt; i\nnathi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; z\n..haz ---&gt; u\n.hazu ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; u\n..aru ---&gt; n\n.arun ---&gt; i\naruni ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; n\n.ramn ---&gt; i\nramni ---&gt; h\namnih ---&gt; o\nmniho ---&gt; r\nnihor ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; h\n.sanh ---&gt; a\nsanha ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; e\nnoore ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; k\n..bak ---&gt; e\n.bake ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; e\n.sube ---&gt; s\nsubes ---&gt; h\nubesh ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; f\n..jaf ---&gt; f\n.jaff ---&gt; a\njaffa ---&gt; r\naffar ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; l\n..tul ---&gt; s\n.tuls ---&gt; i\ntulsi ---&gt; r\nulsir ---&gt; a\nlsira ---&gt; m\nsiram ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; r\nkeshr ---&gt; a\neshra ---&gt; m\nshram ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; e\n.pare ---&gt; r\nparer ---&gt; n\narern ---&gt; a\nrerna ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; i\nmansi ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; u\n..chu ---&gt; n\n.chun ---&gt; a\nchuna ---&gt; r\nhunar ---&gt; a\nunara ---&gt; m\nnaram ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; i\n.sali ---&gt; m\nsalim ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; a\nbhara ---&gt; t\nharat ---&gt; i\narati ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; b\nlakhb ---&gt; i\nakhbi ---&gt; r\nkhbir ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; i\n.rupi ---&gt; n\nrupin ---&gt; d\nupind ---&gt; e\npinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; k\n..puk ---&gt; h\n.pukh ---&gt; a\npukha ---&gt; r\nukhar ---&gt; a\nkhara ---&gt; m\nharam ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; m\n.khum ---&gt; a\nkhuma ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; d\n..umd ---&gt; a\n.umda ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; m\n..mum ---&gt; t\n.mumt ---&gt; a\nmumta ---&gt; j\numtaj ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; b\n.harb ---&gt; i\nharbi ---&gt; n\narbin ---&gt; d\nrbind ---&gt; e\nbinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; d\n..bid ---&gt; h\n.bidh ---&gt; y\nbidhy ---&gt; a\nidhya ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; a\nshaba ---&gt; n\nhaban ---&gt; a\nabana ---&gt; m\nbanam ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; y\n.baby ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; m\nsharm ---&gt; i\nharmi ---&gt; l\narmil ---&gt; a\nrmila ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; d\n.budd ---&gt; h\nbuddh ---&gt; a\nuddha ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; v\n.bhav ---&gt; i\nbhavi ---&gt; s\nhavis ---&gt; h\navish ---&gt; y\nvishy ---&gt; .\n..... ---&gt; u\n....u ---&gt; t\n...ut ---&gt; p\n..utp ---&gt; a\n.utpa ---&gt; n\nutpan ---&gt; a\ntpana ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; i\n.sagi ---&gt; t\nsagit ---&gt; a\nagita ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; r\n..gar ---&gt; v\n.garv ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; d\n.fird ---&gt; o\nfirdo ---&gt; s\nirdos ---&gt; h\nrdosh ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; r\n..war ---&gt; i\n.wari ---&gt; s\nwaris ---&gt; h\narish ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; e\n.gane ---&gt; s\nganes ---&gt; i\nanesi ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; j\n.gurj ---&gt; e\ngurje ---&gt; e\nurjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; a\n.saba ---&gt; n\nsaban ---&gt; a\nabana ---&gt; m\nbanam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; r\nsahir ---&gt; a\nahira ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; r\n.ashr ---&gt; f\nashrf ---&gt; i\nshrfi ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; m\n..nam ---&gt; e\n.name ---&gt; n\nnamen ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; w\n.sarw ---&gt; a\nsarwa ---&gt; n\narwan ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; b\n..akb ---&gt; a\n.akba ---&gt; r\nakbar ---&gt; i\nkbari ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; a\nshiva ---&gt; l\nhival ---&gt; i\nivali ---&gt; k\nvalik ---&gt; a\nalika ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; i\n.khai ---&gt; r\nkhair ---&gt; t\nhairt ---&gt; i\nairti ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; r\nmohar ---&gt; a\nohara ---&gt; n\nharan ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; e\n.yoge ---&gt; s\nyoges ---&gt; h\nogesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; u\n..atu ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; e\n.hase ---&gt; e\nhasee ---&gt; m\naseem ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; d\n..red ---&gt; h\n.redh ---&gt; e\nredhe ---&gt; m\nedhem ---&gt; a\ndhema ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; a\n.dila ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; l\n..jal ---&gt; w\n.jalw ---&gt; a\njalwa ---&gt; d\nalwad ---&gt; .\n..... ---&gt; v\n....v ---&gt; u\n...vu ---&gt; d\n..vud ---&gt; e\n.vude ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; d\n.jagd ---&gt; e\njagde ---&gt; s\nagdes ---&gt; h\ngdesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; j\nsahaj ---&gt; a\nahaja ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; b\n.gurb ---&gt; a\ngurba ---&gt; x\nurbax ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; n\n..tun ---&gt; d\n.tund ---&gt; i\ntundi ---&gt; r\nundir ---&gt; a\nndira ---&gt; m\ndiram ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; s\n.ruks ---&gt; i\nruksi ---&gt; n\nuksin ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; v\n.prav ---&gt; e\nprave ---&gt; n\nraven ---&gt; d\navend ---&gt; r\nvendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; t\n.bhat ---&gt; a\nbhata ---&gt; r\nhatar ---&gt; i\natari ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; t\n..met ---&gt; h\n.meth ---&gt; u\nmethu ---&gt; n\nethun ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; i\n.pari ---&gt; n\nparin ---&gt; k\narink ---&gt; a\nrinka ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; l\nanjal ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; t\n.lavt ---&gt; a\nlavta ---&gt; r\navtar ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; a\n..aja ---&gt; d\n.ajad ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; u\n.lilu ---&gt; .\n..... ---&gt; i\n....i ---&gt; f\n...if ---&gt; t\n..ift ---&gt; e\n.ifte ---&gt; s\niftes ---&gt; h\nftesh ---&gt; y\nteshy ---&gt; a\neshya ---&gt; m\nshyam ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; b\nahnab ---&gt; a\nhnaba ---&gt; z\nnabaz ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; j\n..gaj ---&gt; l\n.gajl ---&gt; a\ngajla ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; t\n..jot ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; b\n..alb ---&gt; i\n.albi ---&gt; n\nalbin ---&gt; a\nlbina ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; o\n.firo ---&gt; z\nfiroz ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; l\n..tal ---&gt; i\n.tali ---&gt; b\ntalib ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; h\n.kanh ---&gt; i\nkanhi ---&gt; y\nanhiy ---&gt; a\nnhiya ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; i\nanchi ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; u\n.manu ---&gt; j\nmanuj ---&gt; a\nanuja ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; p\n..bap ---&gt; u\n.bapu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; m\n.sudm ---&gt; a\nsudma ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; s\n.hans ---&gt; h\nhansh ---&gt; r\nanshr ---&gt; a\nnshra ---&gt; j\nshraj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; z\n..raz ---&gt; i\n.razi ---&gt; m\nrazim ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; a\nsatya ---&gt; w\natyaw ---&gt; a\ntyawa ---&gt; n\nyawan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; j\n.ramj ---&gt; a\nramja ---&gt; n\namjan ---&gt; k\nmjank ---&gt; i\njanki ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; k\n.jank ---&gt; r\njankr ---&gt; a\nankra ---&gt; j\nnkraj ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; p\nchanp ---&gt; a\nhanpa ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; m\n..gam ---&gt; e\n.game ---&gt; r\ngamer ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; m\n.jagm ---&gt; a\njagma ---&gt; l\nagmal ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; m\n..yam ---&gt; u\n.yamu ---&gt; n\nyamun ---&gt; a\namuna ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; a\n..asa ---&gt; n\n.asan ---&gt; a\nasana ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; k\n..sik ---&gt; a\n.sika ---&gt; n\nsikan ---&gt; d\nikand ---&gt; e\nkande ---&gt; r\nander ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; a\n.lava ---&gt; l\nlaval ---&gt; i\navali ---&gt; .\n..... ---&gt; h\n....h ---&gt; r\n...hr ---&gt; i\n..hri ---&gt; t\n.hrit ---&gt; h\nhrith ---&gt; i\nrithi ---&gt; k\nithik ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; e\n.muke ---&gt; s\nmukes ---&gt; h\nukesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; n\n.bhun ---&gt; d\nbhund ---&gt; k\nhundk ---&gt; i\nundki ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; o\n.saro ---&gt; j\nsaroj ---&gt; n\narojn ---&gt; i\nrojni ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; o\n.rubo ---&gt; .\n..... ---&gt; o\n....o ---&gt; v\n...ov ---&gt; a\n..ova ---&gt; i\n.ovai ---&gt; s\novais ---&gt; h\nvaish ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; e\nchote ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; r\n.vikr ---&gt; a\nvikra ---&gt; m\nikram ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; h\n.budh ---&gt; i\nbudhi ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; i\n..ari ---&gt; n\n.arin ---&gt; d\narind ---&gt; r\nrindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; g\n..nag ---&gt; m\n.nagm ---&gt; a\nnagma ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; d\n..hud ---&gt; i\n.hudi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; l\n.sujl ---&gt; i\nsujli ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; m\n.ratm ---&gt; o\nratmo ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; j\n.bhaj ---&gt; a\nbhaja ---&gt; n\nhajan ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; r\n.kamr ---&gt; a\nkamra ---&gt; a\namraa ---&gt; n\nmraan ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; l\n..dol ---&gt; l\n.doll ---&gt; e\ndolle ---&gt; y\nolley ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; u\n.babu ---&gt; l\nbabul ---&gt; a\nabula ---&gt; l\nbulal ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; m\n.yasm ---&gt; e\nyasme ---&gt; e\nasmee ---&gt; n\nsmeen ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; u\n.ramu ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; v\n..div ---&gt; a\n.diva ---&gt; i\ndivai ---&gt; e\nivaie ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; a\nhanda ---&gt; n\nandan ---&gt; i\nndani ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; f\n..suf ---&gt; i\n.sufi ---&gt; a\nsufia ---&gt; n\nufian ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; i\n.lali ---&gt; t\nlalit ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; b\ngulab ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; m\n..asm ---&gt; i\n.asmi ---&gt; n\nasmin ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; v\n.jagv ---&gt; i\njagvi ---&gt; r\nagvir ---&gt; i\ngviri ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; w\nshahw ---&gt; a\nhahwa ---&gt; j\nahwaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; m\n.salm ---&gt; a\nsalma ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; n\n..jon ---&gt; i\n.joni ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; i\n.sohi ---&gt; l\nsohil ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; r\n..jar ---&gt; i\n.jari ---&gt; n\njarin ---&gt; a\narina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; e\n.same ---&gt; e\nsamee ---&gt; m\nameem ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; a\n.laka ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; g\nchhag ---&gt; a\nhhaga ---&gt; n\nhagan ---&gt; a\nagana ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; h\n.push ---&gt; p\npushp ---&gt; a\nushpa ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; p\n.roop ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; a\n..tra ---&gt; n\n.tran ---&gt; n\ntrann ---&gt; u\nrannu ---&gt; m\nannum ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; q\n..faq ---&gt; i\n.faqi ---&gt; r\nfaqir ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; t\n..kit ---&gt; t\n.kitt ---&gt; i\nkitti ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; a\n.isha ---&gt; .\n..... ---&gt; u\n....u ---&gt; j\n...uj ---&gt; j\n..ujj ---&gt; a\n.ujja ---&gt; w\nujjaw ---&gt; a\njjawa ---&gt; l\njawal ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; u\nlakhu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; a\n.suga ---&gt; n\nsugan ---&gt; d\nugand ---&gt; h\ngandh ---&gt; a\nandha ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; g\n..beg ---&gt; u\n.begu ---&gt; m\nbegum ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; y\n.vidy ---&gt; a\nvidya ---&gt; w\nidyaw ---&gt; a\ndyawa ---&gt; t\nyawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; t\n.shit ---&gt; e\nshite ---&gt; e\nhitee ---&gt; l\niteel ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; e\nsushe ---&gt; l\nushel ---&gt; a\nshela ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; k\n..lek ---&gt; o\n.leko ---&gt; s\nlekos ---&gt; e\nekose ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; p\n.kalp ---&gt; a\nkalpa ---&gt; d\nalpad ---&gt; m\nlpadm ---&gt; a\npadma ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; i\n..hai ---&gt; d\n.haid ---&gt; e\nhaide ---&gt; r\naider ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; r\n.meer ---&gt; a\nmeera ---&gt; m\neeram ---&gt; a\nerama ---&gt; i\nramai ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; m\n.salm ---&gt; a\nsalma ---&gt; n\nalman ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; l\n..gol ---&gt; d\n.gold ---&gt; a\ngolda ---&gt; n\noldan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; u\nramku ---&gt; m\namkum ---&gt; a\nmkuma ---&gt; r\nkumar ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; s\n.hars ---&gt; h\nharsh ---&gt; i\narshi ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; w\ndhanw ---&gt; a\nhanwa ---&gt; n\nanwan ---&gt; t\nnwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; e\nmanje ---&gt; s\nanjes ---&gt; h\nnjesh ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; j\n.tanj ---&gt; i\ntanji ---&gt; m\nanjim ---&gt; a\nnjima ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; i\n.mahi ---&gt; n\nmahin ---&gt; u\nahinu ---&gt; d\nhinud ---&gt; i\ninudi ---&gt; n\nnudin ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; n\n.badn ---&gt; a\nbadna ---&gt; t\nadnat ---&gt; h\ndnath ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; t\n.aast ---&gt; h\naasth ---&gt; a\nastha ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; b\n..omb ---&gt; i\n.ombi ---&gt; r\nombir ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; a\nneela ---&gt; m\neelam ---&gt; .\n..... ---&gt; u\n....u ---&gt; n\n...un ---&gt; k\n..unk ---&gt; n\n.unkn ---&gt; o\nunkno ---&gt; w\nnknow ---&gt; n\nknown ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; r\n.amir ---&gt; a\namira ---&gt; k\nmirak ---&gt; a\niraka ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; e\n.vire ---&gt; n\nviren ---&gt; d\nirend ---&gt; e\nrende ---&gt; r\nender ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; i\n.rati ---&gt; k\nratik ---&gt; a\natika ---&gt; n\ntikan ---&gt; t\nikant ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; a\n.sava ---&gt; n\nsavan ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; u\n.kumu ---&gt; d\nkumud ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; a\n.rama ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; c\n.kanc ---&gt; h\nkanch ---&gt; i\nanchi ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; a\nhanda ---&gt; b\nandab ---&gt; a\nndaba ---&gt; i\ndabai ---&gt; .\n..... ---&gt; a\n....a ---&gt; h\n...ah ---&gt; s\n..ahs ---&gt; a\n.ahsa ---&gt; n\nahsan ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; v\n.palv ---&gt; i\npalvi ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; l\n..gol ---&gt; d\n.gold ---&gt; e\ngolde ---&gt; n\nolden ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; z\n..saz ---&gt; i\n.sazi ---&gt; y\nsaziy ---&gt; a\naziya ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; a\n.suga ---&gt; n\nsugan ---&gt; d\nugand ---&gt; b\ngandb ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; n\n.prin ---&gt; s\nprins ---&gt; h\nrinsh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; b\nshamb ---&gt; h\nhambh ---&gt; u\nambhu ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; s\n..nus ---&gt; r\n.nusr ---&gt; a\nnusra ---&gt; n\nusran ---&gt; t\nsrant ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; i\n.nazi ---&gt; r\nnazir ---&gt; a\nazira ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; r\n..nor ---&gt; t\n.nort ---&gt; i\nnorti ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; a\n.kara ---&gt; m\nkaram ---&gt; j\naramj ---&gt; e\nramje ---&gt; e\namjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; j\n..gaj ---&gt; w\n.gajw ---&gt; a\ngajwa ---&gt; n\najwan ---&gt; t\njwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; a\n..gha ---&gt; n\n.ghan ---&gt; s\nghans ---&gt; y\nhansy ---&gt; a\nansya ---&gt; m\nnsyam ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; e\n.anje ---&gt; e\nanjee ---&gt; v\nnjeev ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; m\n..hum ---&gt; e\n.hume ---&gt; r\nhumer ---&gt; a\numera ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; a\n..kaa ---&gt; r\n.kaar ---&gt; i\nkaari ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; m\n..dam ---&gt; o\n.damo ---&gt; d\ndamod ---&gt; a\namoda ---&gt; r\nmodar ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; b\n..beb ---&gt; y\n.beby ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; l\n.maal ---&gt; t\nmaalt ---&gt; i\naalti ---&gt; .\n..... ---&gt; g\n....g ---&gt; l\n...gl ---&gt; a\n..gla ---&gt; d\n.glad ---&gt; w\ngladw ---&gt; i\nladwi ---&gt; n\nadwin ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; d\nsukhd ---&gt; e\nukhde ---&gt; v\nkhdev ---&gt; i\nhdevi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; i\n.hani ---&gt; s\nhanis ---&gt; h\nanish ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; t\namrit ---&gt; h\nmrith ---&gt; a\nritha ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; i\n.indi ---&gt; r\nindir ---&gt; a\nndira ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; d\nsahad ---&gt; e\nahade ---&gt; b\nhadeb ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; a\n.kira ---&gt; s\nkiras ---&gt; h\nirash ---&gt; a\nrasha ---&gt; n\nashan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; m\n.mamm ---&gt; t\nmammt ---&gt; a\nammta ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; m\ndeepm ---&gt; a\neepma ---&gt; l\nepmal ---&gt; a\npmala ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; a\n..saa ---&gt; n\n.saan ---&gt; u\nsaanu ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; o\n..goo ---&gt; p\n.goop ---&gt; i\ngoopi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; e\n.hare ---&gt; n\nharen ---&gt; d\narend ---&gt; e\nrende ---&gt; r\nender ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; p\n.papp ---&gt; u\npappu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; p\n.kalp ---&gt; a\nkalpa ---&gt; n\nalpan ---&gt; a\nlpana ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; l\n..jul ---&gt; i\n.juli ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; w\n.somw ---&gt; a\nsomwa ---&gt; t\nomwat ---&gt; i\nmwati ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; l\n..til ---&gt; a\n.tila ---&gt; k\ntilak ---&gt; r\nilakr ---&gt; a\nlakra ---&gt; j\nakraj ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; b\n.harb ---&gt; i\nharbi ---&gt; r\narbir ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; t\n.reet ---&gt; a\nreeta ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; s\n.ruks ---&gt; h\nruksh ---&gt; a\nuksha ---&gt; r\nkshar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; r\nsamir ---&gt; o\namiro ---&gt; n\nmiron ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; l\n..jal ---&gt; m\n.jalm ---&gt; i\njalmi ---&gt; .\n..... ---&gt; n\n....n ---&gt; s\n...ns ---&gt; h\n..nsh ---&gt; i\n.nshi ---&gt; m\nnshim ---&gt; a\nshima ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; e\n..hee ---&gt; n\n.heen ---&gt; a\nheena ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; a\n.mina ---&gt; k\nminak ---&gt; s\ninaks ---&gt; i\nnaksi ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; a\n..bra ---&gt; h\n.brah ---&gt; a\nbraha ---&gt; m\nraham ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; i\n.rini ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; r\n.savr ---&gt; e\nsavre ---&gt; e\navree ---&gt; n\nvreen ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; r\n.ramr ---&gt; a\nramra ---&gt; t\namrat ---&gt; i\nmrati ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; v\n..jav ---&gt; a\n.java ---&gt; l\njaval ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; e\n..aje ---&gt; e\n.ajee ---&gt; t\najeet ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; m\n..jim ---&gt; i\n.jimi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; b\n.satb ---&gt; i\nsatbi ---&gt; r\natbir ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; e\n.mane ---&gt; n\nmanen ---&gt; d\nanend ---&gt; e\nnende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; r\n.shir ---&gt; a\nshira ---&gt; j\nhiraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; r\n..sor ---&gt; a\n.sora ---&gt; b\nsorab ---&gt; h\norabh ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; r\nchetr ---&gt; a\nhetra ---&gt; m\netram ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; n\n.chin ---&gt; u\nchinu ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; h\ngulsh ---&gt; t\nulsht ---&gt; a\nlshta ---&gt; b\nshtab ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; y\n..day ---&gt; a\n.daya ---&gt; w\ndayaw ---&gt; a\nayawa ---&gt; t\nyawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; i\n..bui ---&gt; t\n.buit ---&gt; y\nbuity ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; n\nhiman ---&gt; i\nimani ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; z\n..saz ---&gt; y\n.sazy ---&gt; a\nsazya ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; u\n.kusu ---&gt; m\nkusum ---&gt; a\nusuma ---&gt; k\nsumak ---&gt; a\numaka ---&gt; r\nmakar ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; s\n.alis ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; u\nnooru ---&gt; d\noorud ---&gt; d\norudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; r\n..gar ---&gt; i\n.gari ---&gt; b\ngarib ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; m\nmanim ---&gt; e\nanime ---&gt; g\nnimeg ---&gt; l\nimegl ---&gt; a\nmegla ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; s\n.nass ---&gt; i\nnassi ---&gt; n\nassin ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; n\n.anan ---&gt; d\nanand ---&gt; w\nnandw ---&gt; a\nandwa ---&gt; t\nndwat ---&gt; i\ndwati ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; j\n..laj ---&gt; a\n.laja ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; h\n..mih ---&gt; i\n.mihi ---&gt; r\nmihir ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; v\nchhav ---&gt; i\nhhavi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; x\n..lax ---&gt; m\n.laxm ---&gt; a\nlaxma ---&gt; n\naxman ---&gt; r\nxmanr ---&gt; a\nmanra ---&gt; m\nanram ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; a\n.rasa ---&gt; b\nrasab ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; r\n.sajr ---&gt; u\nsajru ---&gt; d\najrud ---&gt; d\njrudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; u\nlakhu ---&gt; a\nakhua ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; l\nshall ---&gt; u\nhallu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; e\n.sate ---&gt; e\nsatee ---&gt; s\natees ---&gt; h\nteesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; g\n..ang ---&gt; a\n.anga ---&gt; d\nangad ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; g\n.narg ---&gt; i\nnargi ---&gt; s\nargis ---&gt; h\nrgish ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; d\n.sund ---&gt; e\nsunde ---&gt; e\nundee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; b\n..bob ---&gt; b\n.bobb ---&gt; i\nbobbi ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; f\n..zaf ---&gt; a\n.zafa ---&gt; r\nzafar ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; a\n.rupa ---&gt; n\nrupan ---&gt; j\nupanj ---&gt; a\npanja ---&gt; l\nanjal ---&gt; i\nnjali ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; p\n.ramp ---&gt; a\nrampa ---&gt; t\nampat ---&gt; i\nmpati ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; h\n.nash ---&gt; i\nnashi ---&gt; m\nashim ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; m\n..bim ---&gt; a\n.bima ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; d\n..yad ---&gt; v\n.yadv ---&gt; i\nyadvi ---&gt; r\nadvir ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; t\nshant ---&gt; i\nhanti ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; r\n.anur ---&gt; a\nanura ---&gt; d\nnurad ---&gt; h\nuradh ---&gt; a\nradha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; w\nramsw ---&gt; r\namswr ---&gt; u\nmswru ---&gt; p\nswrup ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; z\n.sehz ---&gt; a\nsehza ---&gt; d\nehzad ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; v\n.bhav ---&gt; a\nbhava ---&gt; n\nhavan ---&gt; a\navana ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; a\n.kala ---&gt; p\nkalap ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; t\n..bat ---&gt; o\n.bato ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; p\n..sup ---&gt; r\n.supr ---&gt; i\nsupri ---&gt; y\nupriy ---&gt; a\npriya ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; t\n.indt ---&gt; a\nindta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; g\n.sahg ---&gt; u\nsahgu ---&gt; f\nahguf ---&gt; t\nhguft ---&gt; a\ngufta ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; o\n.viro ---&gt; n\nviron ---&gt; i\nironi ---&gt; k\nronik ---&gt; a\nonika ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; l\n..jil ---&gt; e\n.jile ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; a\n.lila ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; p\n..gop ---&gt; a\n.gopa ---&gt; l\ngopal ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; i\n.kali ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; k\n..duk ---&gt; h\n.dukh ---&gt; i\ndukhi ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; a\nsidha ---&gt; r\nidhar ---&gt; a\ndhara ---&gt; t\nharat ---&gt; h\narath ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; u\n.shau ---&gt; r\nshaur ---&gt; a\nhaura ---&gt; b\naurab ---&gt; h\nurabh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; b\n.shib ---&gt; u\nshibu ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; i\n.fari ---&gt; n\nfarin ---&gt; a\narina ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; n\n..gen ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; w\n..law ---&gt; l\n.lawl ---&gt; i\nlawli ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; m\n.naim ---&gt; u\nnaimu ---&gt; d\naimud ---&gt; d\nimudd ---&gt; i\nmuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; p\n.jaip ---&gt; r\njaipr ---&gt; k\naiprk ---&gt; e\niprke ---&gt; s\nprkes ---&gt; h\nrkesh ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; u\nramdu ---&gt; l\namdul ---&gt; a\nmdula ---&gt; r\ndular ---&gt; i\nulari ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; i\nparvi ---&gt; n\narvin ---&gt; d\nrvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; l\n.kall ---&gt; a\nkalla ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; i\n..ati ---&gt; f\n.atif ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; n\n.jamn ---&gt; a\njamna ---&gt; a\namnaa ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; r\n..pir ---&gt; y\n.piry ---&gt; a\npirya ---&gt; n\niryan ---&gt; k\nryank ---&gt; a\nyanka ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; a\nramba ---&gt; r\nambar ---&gt; o\nmbaro ---&gt; s\nbaros ---&gt; e\narose ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; d\n.swad ---&gt; h\nswadh ---&gt; i\nwadhi ---&gt; n\nadhin ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; n\nshadn ---&gt; a\nhadna ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; t\n..fit ---&gt; r\n.fitr ---&gt; a\nfitra ---&gt; t\nitrat ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; v\n.manv ---&gt; i\nmanvi ---&gt; r\nanvir ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; a\n..jaa ---&gt; n\n.jaan ---&gt; k\njaank ---&gt; i\naanki ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; l\n.musl ---&gt; i\nmusli ---&gt; m\nuslim ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; d\n.jagd ---&gt; e\njagde ---&gt; e\nagdee ---&gt; s\ngdees ---&gt; h\ndeesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; j\n..baj ---&gt; i\n.baji ---&gt; n\nbajin ---&gt; d\najind ---&gt; e\njinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; w\n.ramw ---&gt; a\nramwa ---&gt; t\namwat ---&gt; a\nmwata ---&gt; r\nwatar ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; d\n..avd ---&gt; h\n.avdh ---&gt; e\navdhe ---&gt; s\nvdhes ---&gt; h\ndhesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; s\n.bhus ---&gt; a\nbhusa ---&gt; n\nhusan ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; i\naashi ---&gt; s\nashis ---&gt; h\nshish ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; d\n.band ---&gt; a\nbanda ---&gt; n\nandan ---&gt; i\nndani ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; s\n.shis ---&gt; p\nshisp ---&gt; a\nhispa ---&gt; l\nispal ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; n\n.veen ---&gt; a\nveena ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; a\n.taba ---&gt; s\ntabas ---&gt; s\nabass ---&gt; u\nbassu ---&gt; m\nassum ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; z\n..vaz ---&gt; i\n.vazi ---&gt; d\nvazid ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; e\n.dhre ---&gt; e\ndhree ---&gt; j\nhreej ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; c\n..buc ---&gt; h\n.buch ---&gt; h\nbuchh ---&gt; i\nuchhi ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; d\n.gudd ---&gt; i\nguddi ---&gt; b\nuddib ---&gt; a\nddiba ---&gt; i\ndibai ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; i\n.devi ---&gt; k\ndevik ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; n\n..pon ---&gt; e\n.pone ---&gt; e\nponee ---&gt; m\noneem ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; k\n..pak ---&gt; a\n.paka ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; e\n.rube ---&gt; n\nruben ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; u\nshanu ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; b\n..jub ---&gt; e\n.jube ---&gt; d\njubed ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; m\n.narm ---&gt; d\nnarmd ---&gt; a\narmda ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; q\n..maq ---&gt; s\n.maqs ---&gt; o\nmaqso ---&gt; o\naqsoo ---&gt; d\nqsood ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; h\n.ansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; e\n.gane ---&gt; s\nganes ---&gt; h\nanesh ---&gt; i\nneshi ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; h\n.budh ---&gt; u\nbudhu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; i\nmanji ---&gt; t\nanjit ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; i\n..jhi ---&gt; n\n.jhin ---&gt; g\njhing ---&gt; i\nhingi ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; o\n..soo ---&gt; r\n.soor ---&gt; a\nsoora ---&gt; j\nooraj ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; a\n.tara ---&gt; m\ntaram ---&gt; a\narama ---&gt; t\nramat ---&gt; i\namati ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; j\n..haj ---&gt; r\n.hajr ---&gt; a\nhajra ---&gt; t\najrat ---&gt; .\n..... ---&gt; d\n....d ---&gt; r\n...dr ---&gt; o\n..dro ---&gt; p\n.drop ---&gt; t\ndropt ---&gt; i\nropti ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; l\n..vel ---&gt; a\n.vela ---&gt; r\nvelar ---&gt; a\nelara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; s\nsukhs ---&gt; o\nukhso ---&gt; h\nkhsoh ---&gt; a\nhsoha ---&gt; n\nsohan ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; u\n.tanu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; e\n.mane ---&gt; e\nmanee ---&gt; s\nanees ---&gt; h\nneesh ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; i\n.jami ---&gt; l\njamil ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; i\n.navi ---&gt; n\nnavin ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; a\nmanda ---&gt; n\nandan ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; u\n.nitu ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; s\n.tass ---&gt; a\ntassa ---&gt; d\nassad ---&gt; u\nssadu ---&gt; l\nsadul ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; k\n.devk ---&gt; a\ndevka ---&gt; l\nevkal ---&gt; i\nvkali ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; e\n.sume ---&gt; n\nsumen ---&gt; t\nument ---&gt; r\nmentr ---&gt; a\nentra ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; y\n.shiy ---&gt; a\nshiya ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; s\n.jais ---&gt; m\njaism ---&gt; e\naisme ---&gt; e\nismee ---&gt; n\nsmeen ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; r\n.satr ---&gt; u\nsatru ---&gt; g\natrug ---&gt; h\ntrugh ---&gt; a\nrugha ---&gt; n\nughan ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; r\n..jar ---&gt; n\n.jarn ---&gt; a\njarna ---&gt; i\narnai ---&gt; l\nrnail ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; a\n.raja ---&gt; m\nrajam ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; t\n..jot ---&gt; y\n.joty ---&gt; i\njotyi ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; a\n.meha ---&gt; r\nmehar ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; a\n..pea ---&gt; r\n.pear ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; h\n.kish ---&gt; a\nkisha ---&gt; n\nishan ---&gt; a\nshana ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; i\n..bri ---&gt; j\n.brij ---&gt; e\nbrije ---&gt; s\nrijes ---&gt; h\nijesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; g\n...ag ---&gt; y\n..agy ---&gt; a\n.agya ---&gt; p\nagyap ---&gt; a\ngyapa ---&gt; d\nyapad ---&gt; .\n..... ---&gt; s\n....s ---&gt; m\n...sm ---&gt; t\n..smt ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; k\n..aak ---&gt; a\n.aaka ---&gt; n\naakan ---&gt; k\nakank ---&gt; s\nkanks ---&gt; h\nanksh ---&gt; a\nnksha ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; t\n.adit ---&gt; y\nadity ---&gt; a\nditya ---&gt; l\nityal ---&gt; o\ntyalo ---&gt; k\nyalok ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; n\n..kin ---&gt; y\n.kiny ---&gt; a\nkinya ---&gt; .\n..... ---&gt; s\n....s ---&gt; r\n...sr ---&gt; i\n..sri ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; a\n.vija ---&gt; y\nvijay ---&gt; p\nijayp ---&gt; a\njaypa ---&gt; l\naypal ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; k\n.nank ---&gt; e\nnanke ---&gt; s\nankes ---&gt; h\nnkesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; h\n.push ---&gt; p\npushp ---&gt; a\nushpa ---&gt; n\nshpan ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; w\n.ashw ---&gt; a\nashwa ---&gt; n\nshwan ---&gt; i\nhwani ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; p\nveerp ---&gt; a\neerpa ---&gt; l\nerpal ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; n\n..nen ---&gt; h\n.nenh ---&gt; e\nnenhe ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; m\nakshm ---&gt; a\nkshma ---&gt; n\nshman ---&gt; .\n..... ---&gt; s\n....s ---&gt; p\n...sp ---&gt; a\n..spa ---&gt; n\n.span ---&gt; a\nspana ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; y\n..hay ---&gt; a\n.haya ---&gt; t\nhayat ---&gt; u\nayatu ---&gt; l\nyatul ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; v\n..aav ---&gt; e\n.aave ---&gt; s\naaves ---&gt; h\navesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; y\n.shay ---&gt; m\nshaym ---&gt; u\nhaymu ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; l\n..isl ---&gt; a\n.isla ---&gt; m\nislam ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; s\nyashs ---&gt; v\nashsv ---&gt; i\nshsvi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; y\n.samy ---&gt; a\nsamya ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; f\n.shif ---&gt; a\nshifa ---&gt; l\nhifal ---&gt; i\nifali ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; a\n.suna ---&gt; i\nsunai ---&gt; n\nunain ---&gt; a\nnaina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; a\n.saka ---&gt; l\nsakal ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; h\n.nish ---&gt; i\nnishi ---&gt; t\nishit ---&gt; h\nshith ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; c\n..suc ---&gt; h\n.such ---&gt; i\nsuchi ---&gt; t\nuchit ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; y\n.ruky ---&gt; a\nrukya ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; a\n.bira ---&gt; n\nbiran ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; r\n..ver ---&gt; s\n.vers ---&gt; a\nversa ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; k\n.dhuk ---&gt; h\ndhukh ---&gt; u\nhukhu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; a\n.kala ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; y\n..niy ---&gt; a\n.niya ---&gt; a\nniyaa ---&gt; z\niyaaz ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; k\n..fak ---&gt; i\n.faki ---&gt; h\nfakih ---&gt; a\nakiha ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; u\n..atu ---&gt; l\n.atul ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; k\n..sek ---&gt; h\n.sekh ---&gt; a\nsekha ---&gt; .\n..... ---&gt; u\n....u ---&gt; j\n...uj ---&gt; j\n..ujj ---&gt; w\n.ujjw ---&gt; a\nujjwa ---&gt; l\njjwal ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; u\n.vipu ---&gt; l\nvipul ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; j\n..puj ---&gt; a\n.puja ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; o\n.mano ---&gt; t\nmanot ---&gt; i\nanoti ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; d\n.bind ---&gt; i\nbindi ---&gt; y\nindiy ---&gt; a\nndiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; o\n.sado ---&gt; d\nsadod ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; e\n..hee ---&gt; e\n.heee ---&gt; r\nheeer ---&gt; a\neeera ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; b\nsahib ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; e\n.amre ---&gt; e\namree ---&gt; n\nmreen ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; i\n.sohi ---&gt; b\nsohib ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; s\n.jees ---&gt; h\njeesh ---&gt; a\neesha ---&gt; n\neshan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; r\n.nasr ---&gt; e\nnasre ---&gt; e\nasree ---&gt; n\nsreen ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; i\nshami ---&gt; m\nhamim ---&gt; a\namima ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; i\n.rati ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; r\n..bhr ---&gt; k\n.bhrk ---&gt; a\nbhrka ---&gt; t\nhrkat ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; a\ndhara ---&gt; m\nharam ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; j\n.farj ---&gt; a\nfarja ---&gt; n\narjan ---&gt; a\nrjana ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; i\n..vai ---&gt; j\n.vaij ---&gt; a\nvaija ---&gt; n\naijan ---&gt; t\nijant ---&gt; i\njanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; m\nsamim ---&gt; .\n..... ---&gt; d\n....d ---&gt; w\n...dw ---&gt; a\n..dwa ---&gt; r\n.dwar ---&gt; k\ndwark ---&gt; a\nwarka ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; a\nbhaga ---&gt; t\nhagat ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; t\n..bit ---&gt; t\n.bitt ---&gt; o\nbitto ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; i\n.bali ---&gt; r\nbalir ---&gt; a\nalira ---&gt; m\nliram ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; i\nyashi ---&gt; k\nashik ---&gt; a\nshika ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; a\n.hasa ---&gt; n\nhasan ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; t\n..fat ---&gt; h\n.fath ---&gt; e\nfathe ---&gt; r\nather ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; b\n.harb ---&gt; a\nharba ---&gt; n\narban ---&gt; s\nrbans ---&gt; h\nbansh ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; a\n..ina ---&gt; y\n.inay ---&gt; a\ninaya ---&gt; t\nnayat ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; s\n..aks ---&gt; h\n.aksh ---&gt; i\nakshi ---&gt; t\nkshit ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; b\nharmb ---&gt; i\narmbi ---&gt; r\nrmbir ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; s\nrajes ---&gt; h\najesh ---&gt; v\njeshv ---&gt; r\neshvr ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; t\n..ist ---&gt; h\n.isth ---&gt; a\nistha ---&gt; r\nsthar ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; e\n.nase ---&gt; e\nnasee ---&gt; m\naseem ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; l\n..bul ---&gt; b\n.bulb ---&gt; u\nbulbu ---&gt; l\nulbul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; k\n.shok ---&gt; r\nshokr ---&gt; a\nhokra ---&gt; j\nokraj ---&gt; i\nkraji ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; i\n..bai ---&gt; j\n.baij ---&gt; n\nbaijn ---&gt; a\naijna ---&gt; t\nijnat ---&gt; h\njnath ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; a\n.vija ---&gt; y\nvijay ---&gt; a\nijaya ---&gt; n\njayan ---&gt; t\nayant ---&gt; a\nyanta ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; p\n.anup ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; a\navina ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; b\n.rajb ---&gt; i\nrajbi ---&gt; r\najbir ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; h\nmansh ---&gt; a\nansha ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; n\n..tun ---&gt; d\n.tund ---&gt; a\ntunda ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; e\n.gule ---&gt; s\ngules ---&gt; t\nulest ---&gt; a\nlesta ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; p\n.bhup ---&gt; e\nbhupe ---&gt; n\nhupen ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; r\n..war ---&gt; m\n.warm ---&gt; a\nwarma ---&gt; t\narmat ---&gt; i\nrmati ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; n\n.nain ---&gt; a\nnaina ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; w\nshanw ---&gt; a\nhanwa ---&gt; j\nanwaj ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; a\n.kara ---&gt; n\nkaran ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; u\n.anku ---&gt; s\nankus ---&gt; h\nnkush ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; i\n.aasi ---&gt; m\naasim ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; w\n..kuw ---&gt; a\n.kuwa ---&gt; r\nkuwar ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; s\n..bus ---&gt; n\n.busn ---&gt; e\nbusne ---&gt; s\nusnes ---&gt; s\nsness ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; v\n..shv ---&gt; e\n.shve ---&gt; t\nshvet ---&gt; a\nhveta ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; m\n..bim ---&gt; l\n.biml ---&gt; a\nbimla ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; l\n..pul ---&gt; i\n.puli ---&gt; t\npulit ---&gt; a\nulita ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; w\n.bhaw ---&gt; a\nbhawa ---&gt; n\nhawan ---&gt; i\nawani ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; m\n..pem ---&gt; a\n.pema ---&gt; r\npemar ---&gt; a\nemara ---&gt; m\nmaram ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; a\n..aza ---&gt; r\n.azar ---&gt; u\nazaru ---&gt; d\nzarud ---&gt; d\narudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; a\n.raja ---&gt; t\nrajat ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; o\n.rajo ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; z\n..haz ---&gt; i\n.hazi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; k\n..shk ---&gt; u\n.shku ---&gt; r\nshkur ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; e\n.vine ---&gt; e\nvinee ---&gt; t\nineet ---&gt; a\nneeta ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; f\n..guf ---&gt; r\n.gufr ---&gt; a\ngufra ---&gt; a\nufraa ---&gt; n\nfraan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; r\n.nasr ---&gt; e\nnasre ---&gt; e\nasree ---&gt; m\nsreem ---&gt; a\nreema ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; b\n.babb ---&gt; u\nbabbu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; h\n.radh ---&gt; a\nradha ---&gt; b\nadhab ---&gt; a\ndhaba ---&gt; i\nhabai ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; t\n.birt ---&gt; h\nbirth ---&gt; a\nirtha ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; n\n.virn ---&gt; i\nvirni ---&gt; t\nirnit ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; e\n.jaye ---&gt; s\njayes ---&gt; h\nayesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; a\n.suda ---&gt; r\nsudar ---&gt; s\nudars ---&gt; h\ndarsh ---&gt; a\narsha ---&gt; n\nrshan ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; u\n..jhu ---&gt; n\n.jhun ---&gt; n\njhunn ---&gt; a\nhunna ---&gt; .\n..... ---&gt; d\n....d ---&gt; r\n...dr ---&gt; o\n..dro ---&gt; p\n.drop ---&gt; a\ndropa ---&gt; t\nropat ---&gt; i\nopati ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; n\n.rajn ---&gt; a\nrajna ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; m\n..aam ---&gt; i\n.aami ---&gt; n\naamin ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; j\n..gaj ---&gt; a\n.gaja ---&gt; l\ngajal ---&gt; a\najala ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; c\n..bac ---&gt; h\n.bach ---&gt; h\nbachh ---&gt; u\nachhu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; i\n.sali ---&gt; n\nsalin ---&gt; d\nalind ---&gt; e\nlinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; i\n..azi ---&gt; m\n.azim ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; n\n.ramn ---&gt; a\nramna ---&gt; t\namnat ---&gt; h\nmnath ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; e\n.inde ---&gt; r\ninder ---&gt; j\nnderj ---&gt; e\nderje ---&gt; e\nerjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; a\n..uda ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; n\n.sahn ---&gt; a\nsahna ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; k\ndeepk ---&gt; a\neepka ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; c\n..rac ---&gt; h\n.rach ---&gt; p\nrachp ---&gt; r\nachpr ---&gt; e\nchpre ---&gt; e\nhpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; v\n.ramv ---&gt; i\nramvi ---&gt; l\namvil ---&gt; a\nmvila ---&gt; s\nvilas ---&gt; h\nilash ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; k\n..fak ---&gt; e\n.fake ---&gt; r\nfaker ---&gt; e\nakere ---&gt; y\nkerey ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; s\n..pos ---&gt; h\n.posh ---&gt; e\nposhe ---&gt; t\noshet ---&gt; t\nshett ---&gt; y\nhetty ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; l\n.khal ---&gt; i\nkhali ---&gt; d\nhalid ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; s\n.jams ---&gt; h\njamsh ---&gt; e\namshe ---&gt; d\nmshed ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; s\n.kars ---&gt; h\nkarsh ---&gt; a\narsha ---&gt; n\nrshan ---&gt; a\nshana ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; k\n.bark ---&gt; a\nbarka ---&gt; t\narkat ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; m\n.ishm ---&gt; i\nishmi ---&gt; t\nshmit ---&gt; a\nhmita ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; b\n.jasb ---&gt; i\njasbi ---&gt; r\nasbir ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; t\n.mamt ---&gt; a\nmamta ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; k\n..tak ---&gt; i\n.taki ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; a\n.shya ---&gt; m\nshyam ---&gt; l\nhyaml ---&gt; a\nyamla ---&gt; l\namlal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; t\n.mast ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; i\nshari ---&gt; b\nharib ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; m\n..ham ---&gt; i\n.hami ---&gt; d\nhamid ---&gt; a\namida ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; n\n..pin ---&gt; k\n.pink ---&gt; e\npinke ---&gt; y\ninkey ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; t\n.amit ---&gt; a\namita ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; u\n..gau ---&gt; r\n.gaur ---&gt; a\ngaura ---&gt; v\naurav ---&gt; g\nuravg ---&gt; i\nravgi ---&gt; l\navgil ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; n\nriyan ---&gt; s\niyans ---&gt; u\nyansu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; e\nsatye ---&gt; n\natyen ---&gt; d\ntyend ---&gt; e\nyende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; t\n..set ---&gt; h\n.seth ---&gt; i\nsethi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; x\n..lax ---&gt; m\n.laxm ---&gt; a\nlaxma ---&gt; n\naxman ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; m\n..amm ---&gt; a\n.amma ---&gt; r\nammar ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; c\n..arc ---&gt; h\n.arch ---&gt; a\narcha ---&gt; n\nrchan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; s\nhahis ---&gt; t\nahist ---&gt; a\nhista ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; n\n.khan ---&gt; g\nkhang ---&gt; a\nhanga ---&gt; r\nangar ---&gt; a\nngara ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; i\n..vai ---&gt; b\n.vaib ---&gt; a\nvaiba ---&gt; v\naibav ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; i\n.shei ---&gt; s\nsheis ---&gt; h\nheish ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; p\n..sap ---&gt; a\n.sapa ---&gt; n\nsapan ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; m\n..dim ---&gt; p\n.dimp ---&gt; a\ndimpa ---&gt; l\nimpal ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; c\nchanc ---&gt; h\nhanch ---&gt; l\nanchl ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; a\n.basa ---&gt; n\nbasan ---&gt; t\nasant ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; h\n.nanh ---&gt; u\nnanhu ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; j\n..gaj ---&gt; e\n.gaje ---&gt; n\ngajen ---&gt; d\najend ---&gt; r\njendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; a\n....a ---&gt; h\n...ah ---&gt; m\n..ahm ---&gt; a\n.ahma ---&gt; d\nahmad ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; y\n..may ---&gt; u\n.mayu ---&gt; r\nmayur ---&gt; i\nayuri ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; s\n..mos ---&gt; h\n.mosh ---&gt; r\nmoshr ---&gt; i\noshri ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; a\n.vija ---&gt; y\nvijay ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; a\n..aza ---&gt; d\n.azad ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; p\nyashp ---&gt; a\nashpa ---&gt; l\nshpal ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; u\n..nau ---&gt; s\n.naus ---&gt; a\nnausa ---&gt; d\nausad ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; n\n.shin ---&gt; e\nshine ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; n\n..run ---&gt; i\n.runi ---&gt; y\nruniy ---&gt; a\nuniya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; i\n.saki ---&gt; n\nsakin ---&gt; a\nakina ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; i\n.soni ---&gt; a\nsonia ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; a\n..aka ---&gt; a\n.akaa ---&gt; s\nakaas ---&gt; h\nkaash ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; t\n.bant ---&gt; i\nbanti ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; u\n..jhu ---&gt; m\n.jhum ---&gt; a\njhuma ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; i\n.chai ---&gt; n\nchain ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; i\n..avi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; h\n.sudh ---&gt; a\nsudha ---&gt; m\nudham ---&gt; a\ndhama ---&gt; .\n..... ---&gt; u\n....u ---&gt; g\n...ug ---&gt; m\n..ugm ---&gt; a\n.ugma ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; r\n.badr ---&gt; u\nbadru ---&gt; l\nadrul ---&gt; l\ndrull ---&gt; a\nrulla ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; k\n.nikk ---&gt; i\nnikki ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; l\n.gurl ---&gt; a\ngurla ---&gt; l\nurlal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; l\n.shil ---&gt; u\nshilu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; m\n.shim ---&gt; a\nshima ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; j\n.durj ---&gt; a\ndurja ---&gt; n\nurjan ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; p\n.kirp ---&gt; a\nkirpa ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; f\n.praf ---&gt; u\nprafu ---&gt; l\nraful ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; m\n..dam ---&gt; a\n.dama ---&gt; n\ndaman ---&gt; j\namanj ---&gt; e\nmanje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; p\n...sp ---&gt; n\n..spn ---&gt; a\n.spna ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; n\n.jain ---&gt; a\njaina ---&gt; b\nainab ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; i\n.shei ---&gt; l\nsheil ---&gt; e\nheile ---&gt; s\neiles ---&gt; h\nilesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; n\nabhin ---&gt; a\nbhina ---&gt; y\nhinay ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; a\n.sita ---&gt; r\nsitar ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; r\n.ishr ---&gt; a\nishra ---&gt; n\nshran ---&gt; a\nhrana ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; o\n.sano ---&gt; j\nsanoj ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; t\n.maat ---&gt; w\nmaatw ---&gt; a\naatwa ---&gt; r\natwar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; l\nmangl ---&gt; a\nangla ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; l\n..mul ---&gt; i\n.muli ---&gt; n\nmulin ---&gt; a\nulina ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; m\n..nam ---&gt; e\n.name ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; m\n..anm ---&gt; o\n.anmo ---&gt; l\nanmol ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; r\n.umar ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; e\n.anke ---&gt; e\nankee ---&gt; t\nnkeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; i\n..avi ---&gt; n\n.avin ---&gt; s\navins ---&gt; h\nvinsh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; a\n.suma ---&gt; n\nsuman ---&gt; t\numant ---&gt; h\nmanth ---&gt; r\nanthr ---&gt; a\nnthra ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; a\nneela ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; r\n..gar ---&gt; g\n.garg ---&gt; i\ngargi ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; o\n.phoo ---&gt; l\nphool ---&gt; l\nhooll ---&gt; a\noolla ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; k\nriyak ---&gt; i\niyaki ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; n\n..den ---&gt; i\n.deni ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; h\n..reh ---&gt; m\n.rehm ---&gt; a\nrehma ---&gt; t\nehmat ---&gt; i\nhmati ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; a\n.shya ---&gt; m\nshyam ---&gt; v\nhyamv ---&gt; a\nyamva ---&gt; t\namvat ---&gt; i\nmvati ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; i\n.safi ---&gt; a\nsafia ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; e\n.rave ---&gt; n\nraven ---&gt; a\navena ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; o\n.paro ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; s\n.ruks ---&gt; h\nruksh ---&gt; a\nuksha ---&gt; n\nkshan ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; m\n.aasm ---&gt; a\naasma ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; i\n.naji ---&gt; y\nnajiy ---&gt; a\najiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; i\nsanji ---&gt; d\nanjid ---&gt; a\nnjida ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; r\n..ajr ---&gt; a\n.ajra ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; l\n..bil ---&gt; k\n.bilk ---&gt; e\nbilke ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; g\n..gag ---&gt; a\n.gaga ---&gt; n\ngagan ---&gt; d\nagand ---&gt; e\ngande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; n\n.pran ---&gt; n\nprann ---&gt; a\nranna ---&gt; b\nannab ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; k\n..mak ---&gt; b\n.makb ---&gt; u\nmakbu ---&gt; l\nakbul ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; j\n.nirj ---&gt; a\nnirja ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; e\n.chhe ---&gt; d\nchhed ---&gt; i\nhhedi ---&gt; l\nhedil ---&gt; a\nedila ---&gt; l\ndilal ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; m\n..ham ---&gt; z\n.hamz ---&gt; a\nhamza ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; i\n.niti ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; p\n..tap ---&gt; i\n.tapi ---&gt; s\ntapis ---&gt; h\napish ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; o\n.firo ---&gt; j\nfiroj ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; m\n.kalm ---&gt; a\nkalma ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; n\n..non ---&gt; i\n.noni ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; u\n.saku ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; h\n.kish ---&gt; m\nkishm ---&gt; a\nishma ---&gt; t\nshmat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; n\n.sahn ---&gt; a\nsahna ---&gt; r\nahnar ---&gt; a\nhnara ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; m\n..gum ---&gt; a\n.guma ---&gt; n\nguman ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; d\n.gudd ---&gt; u\nguddu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; p\n..sap ---&gt; i\n.sapi ---&gt; t\nsapit ---&gt; a\napita ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; h\n.kosh ---&gt; l\nkoshl ---&gt; y\noshly ---&gt; a\nshlya ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; t\n.vipt ---&gt; a\nvipta ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; s\n..tis ---&gt; a\n.tisa ---&gt; r\ntisar ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; w\n..kew ---&gt; a\n.kewa ---&gt; l\nkewal ---&gt; .\n..... ---&gt; u\n....u ---&gt; r\n...ur ---&gt; m\n..urm ---&gt; i\n.urmi ---&gt; l\nurmil ---&gt; a\nrmila ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; j\n..tej ---&gt; a\n.teja ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; d\n..god ---&gt; a\n.goda ---&gt; m\ngodam ---&gt; b\nodamb ---&gt; a\ndamba ---&gt; r\nambar ---&gt; i\nmbari ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; k\n.prak ---&gt; e\nprake ---&gt; s\nrakes ---&gt; h\nakesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; d\n.sidd ---&gt; h\nsiddh ---&gt; a\niddha ---&gt; r\nddhar ---&gt; t\ndhart ---&gt; h\nharth ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; i\nsamai ---&gt; a\namaia ---&gt; l\nmaial ---&gt; i\naiali ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; a\n.naza ---&gt; h\nnazah ---&gt; a\nazaha ---&gt; t\nzahat ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; a\n.vina ---&gt; y\nvinay ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; b\n.sahb ---&gt; a\nsahba ---&gt; j\nahbaj ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; h\n..muh ---&gt; m\n.muhm ---&gt; a\nmuhma ---&gt; d\nuhmad ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; s\nsahis ---&gt; h\nahish ---&gt; t\nhisht ---&gt; a\nishta ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; s\nrahis ---&gt; h\nahish ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; l\n.dhal ---&gt; c\ndhalc ---&gt; h\nhalch ---&gt; a\nalcha ---&gt; n\nlchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; v\n.jasv ---&gt; a\njasva ---&gt; n\nasvan ---&gt; t\nsvant ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; l\n..all ---&gt; a\n.alla ---&gt; u\nallau ---&gt; d\nllaud ---&gt; d\nlaudd ---&gt; i\nauddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; n\n..ben ---&gt; a\n.bena ---&gt; y\nbenay ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; a\nrisha ---&gt; b\nishab ---&gt; h\nshabh ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; k\n.umak ---&gt; a\numaka ---&gt; n\nmakan ---&gt; t\nakant ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; o\n..moo ---&gt; n\n.moon ---&gt; a\nmoona ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; c\n..pac ---&gt; h\n.pach ---&gt; u\npachu ---&gt; r\nachur ---&gt; a\nchura ---&gt; m\nhuram ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; l\n.vipl ---&gt; o\nviplo ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; b\nhushb ---&gt; h\nushbh ---&gt; u\nshbhu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; s\n.shis ---&gt; u\nshisu ---&gt; p\nhisup ---&gt; a\nisupa ---&gt; l\nsupal ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; m\nrahim ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; b\n..rab ---&gt; p\n.rabp ---&gt; r\nrabpr ---&gt; e\nabpre ---&gt; e\nbpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; a\n.dhra ---&gt; m\ndhram ---&gt; v\nhramv ---&gt; e\nramve ---&gt; e\namvee ---&gt; r\nmveer ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; z\n..riz ---&gt; v\n.rizv ---&gt; a\nrizva ---&gt; n\nizvan ---&gt; a\nzvana ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; j\nparmj ---&gt; e\narmje ---&gt; e\nrmjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; t\n..sut ---&gt; i\n.suti ---&gt; y\nsutiy ---&gt; a\nutiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; b\n.samb ---&gt; h\nsambh ---&gt; u\nambhu ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; n\nabhin ---&gt; a\nbhina ---&gt; s\nhinas ---&gt; h\ninash ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; a\n.fara ---&gt; h\nfarah ---&gt; e\narahe ---&gt; e\nrahee ---&gt; n\naheen ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; a\n.roha ---&gt; n\nrohan ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; j\n..rij ---&gt; w\n.rijw ---&gt; a\nrijwa ---&gt; n\nijwan ---&gt; a\njwana ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; j\n.banj ---&gt; u\nbanju ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; e\nshake ---&gt; e\nhakee ---&gt; n\nakeen ---&gt; a\nkeena ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; v\n..yuv ---&gt; e\n.yuve ---&gt; r\nyuver ---&gt; a\nuvera ---&gt; j\nveraj ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; n\n..nen ---&gt; s\n.nens ---&gt; i\nnensi ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; f\n..asf ---&gt; a\n.asfa ---&gt; q\nasfaq ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; v\nbhagv ---&gt; a\nhagva ---&gt; t\nagvat ---&gt; i\ngvati ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; h\n..rih ---&gt; a\n.riha ---&gt; n\nrihan ---&gt; a\nihana ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; r\n..dar ---&gt; s\n.dars ---&gt; h\ndarsh ---&gt; a\narsha ---&gt; n\nrshan ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; i\n.ashi ---&gt; f\nashif ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; i\n.savi ---&gt; a\nsavia ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; z\n..roz ---&gt; i\n.rozi ---&gt; n\nrozin ---&gt; a\nozina ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; j\n..muj ---&gt; e\n.muje ---&gt; f\nmujef ---&gt; i\nujefi ---&gt; r\njefir ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; a\nvisha ---&gt; k\nishak ---&gt; h\nshakh ---&gt; a\nhakha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; l\n.rajl ---&gt; a\nrajla ---&gt; x\najlax ---&gt; m\njlaxm ---&gt; i\nlaxmi ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; p\n.binp ---&gt; a\nbinpa ---&gt; l\ninpal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; e\n.sahe ---&gt; b\nsaheb ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; r\n..dor ---&gt; d\n.dord ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; t\n..hit ---&gt; e\n.hite ---&gt; n\nhiten ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; t\n.murt ---&gt; i\nmurti ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; u\n.shau ---&gt; b\nshaub ---&gt; h\nhaubh ---&gt; a\naubha ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; s\n.bads ---&gt; h\nbadsh ---&gt; y\nadshy ---&gt; a\ndshya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; i\n.saki ---&gt; l\nsakil ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; y\n..nay ---&gt; e\n.naye ---&gt; e\nnayee ---&gt; m\nayeem ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; s\n.mohs ---&gt; i\nmohsi ---&gt; n\nohsin ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; i\n.suni ---&gt; t\nsunit ---&gt; a\nunita ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; d\n.sund ---&gt; g\nsundg ---&gt; a\nundga ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; i\n.mini ---&gt; t\nminit ---&gt; w\ninitw ---&gt; a\nnitwa ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; r\n.dhur ---&gt; e\ndhure ---&gt; n\nhuren ---&gt; d\nurend ---&gt; e\nrende ---&gt; r\nender ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; r\n..kur ---&gt; b\n.kurb ---&gt; a\nkurba ---&gt; n\nurban ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; b\n.shub ---&gt; h\nshubh ---&gt; a\nhubha ---&gt; m\nubham ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; e\n.pree ---&gt; t\npreet ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; b\n..tub ---&gt; a\n.tuba ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; b\n.sarb ---&gt; a\nsarba ---&gt; s\narbas ---&gt; h\nrbash ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; i\n..phi ---&gt; r\n.phir ---&gt; d\nphird ---&gt; o\nhirdo ---&gt; s\nirdos ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; r\n..nor ---&gt; e\n.nore ---&gt; e\nnoree ---&gt; n\noreen ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; m\n.shym ---&gt; o\nshymo ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; y\n..joy ---&gt; d\n.joyd ---&gt; e\njoyde ---&gt; e\noydee ---&gt; p\nydeep ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; l\n.shel ---&gt; l\nshell ---&gt; y\nhelly ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; r\n..pur ---&gt; a\n.pura ---&gt; n\npuran ---&gt; m\nuranm ---&gt; a\nranma ---&gt; l\nanmal ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; i\nprati ---&gt; k\nratik ---&gt; s\natiks ---&gt; h\ntiksh ---&gt; a\niksha ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; k\n..mak ---&gt; s\n.maks ---&gt; u\nmaksu ---&gt; d\naksud ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; c\n.kanc ---&gt; h\nkanch ---&gt; n\nanchn ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; g\n.gang ---&gt; a\nganga ---&gt; d\nangad ---&gt; h\nngadh ---&gt; a\ngadha ---&gt; r\nadhar ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; l\n.babl ---&gt; u\nbablu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; b\n.jaib ---&gt; i\njaibi ---&gt; r\naibir ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; e\n.sure ---&gt; n\nsuren ---&gt; d\nurend ---&gt; r\nrendr ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; g\n.parg ---&gt; a\nparga ---&gt; t\nargat ---&gt; i\nrgati ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; a\namara ---&gt; p\nmarap ---&gt; a\narapa ---&gt; l\nrapal ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; w\n.kalw ---&gt; a\nkalwa ---&gt; n\nalwan ---&gt; t\nlwant ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; m\n..jum ---&gt; r\n.jumr ---&gt; a\njumra ---&gt; t\numrat ---&gt; i\nmrati ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; w\n..sow ---&gt; a\n.sowa ---&gt; r\nsowar ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; l\neepal ---&gt; i\nepali ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; k\n.rakk ---&gt; i\nrakki ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; s\n..jis ---&gt; a\n.jisa ---&gt; n\njisan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; r\nshahr ---&gt; u\nhahru ---&gt; f\nahruf ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; w\n..raw ---&gt; a\n.rawa ---&gt; n\nrawan ---&gt; a\nawana ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; a\n..gha ---&gt; n\n.ghan ---&gt; s\nghans ---&gt; h\nhansh ---&gt; y\nanshy ---&gt; a\nnshya ---&gt; m\nshyam ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; e\n..gee ---&gt; t\n.geet ---&gt; u\ngeetu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; h\n.kash ---&gt; u\nkashu ---&gt; m\nashum ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; i\nshali ---&gt; n\nhalin ---&gt; i\nalini ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; u\n..fau ---&gt; i\n.faui ---&gt; n\nfauin ---&gt; a\nauina ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; l\nabhil ---&gt; a\nbhila ---&gt; s\nhilas ---&gt; h\nilash ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; r\n..var ---&gt; i\n.vari ---&gt; s\nvaris ---&gt; h\narish ---&gt; a\nrisha ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; s\n.pras ---&gt; h\nprash ---&gt; a\nrasha ---&gt; n\nashan ---&gt; a\nshana ---&gt; t\nhanat ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; a\n..ava ---&gt; n\n.avan ---&gt; t\navant ---&gt; i\nvanti ---&gt; k\nantik ---&gt; a\nntika ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; n\n.gyan ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; m\n.dham ---&gt; i\ndhami ---&gt; n\nhamin ---&gt; i\namini ---&gt; .\n..... ---&gt; i\n....i ---&gt; b\n...ib ---&gt; r\n..ibr ---&gt; a\n.ibra ---&gt; h\nibrah ---&gt; i\nbrahi ---&gt; m\nrahim ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; l\n..jal ---&gt; s\n.jals ---&gt; i\njalsi ---&gt; n\nalsin ---&gt; g\nlsing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; n\nhandn ---&gt; i\nandni ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; l\n..jul ---&gt; e\n.jule ---&gt; k\njulek ---&gt; h\nulekh ---&gt; a\nlekha ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; r\n..gar ---&gt; i\n.gari ---&gt; m\ngarim ---&gt; a\narima ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; l\n.sall ---&gt; y\nsally ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; s\n..tus ---&gt; h\n.tush ---&gt; a\ntusha ---&gt; l\nushal ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; t\n..pat ---&gt; a\n.pata ---&gt; n\npatan ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; n\n..gun ---&gt; j\n.gunj ---&gt; a\ngunja ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; v\n..dav ---&gt; e\n.dave ---&gt; n\ndaven ---&gt; d\navend ---&gt; e\nvende ---&gt; r\nender ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; h\n..prh ---&gt; a\n.prha ---&gt; l\nprhal ---&gt; a\nrhala ---&gt; d\nhalad ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; i\nneeli ---&gt; m\neelim ---&gt; a\nelima ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; b\n.bhab ---&gt; h\nbhabh ---&gt; a\nhabha ---&gt; v\nabhav ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; a\n.fara ---&gt; n\nfaran ---&gt; a\narana ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; e\n.rake ---&gt; e\nrakee ---&gt; b\nakeeb ---&gt; a\nkeeba ---&gt; .\n..... ---&gt; y\n....y ---&gt; e\n...ye ---&gt; s\n..yes ---&gt; p\n.yesp ---&gt; a\nyespa ---&gt; l\nespal ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; h\n.ansh ---&gt; u\nanshu ---&gt; m\nnshum ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; a\n.inda ---&gt; r\nindar ---&gt; a\nndara ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; a\nfarha ---&gt; n\narhan ---&gt; a\nrhana ---&gt; a\nhanaa ---&gt; z\nanaaz ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; e\n.nave ---&gt; d\nnaved ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; y\n..any ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; r\n.sitr ---&gt; a\nsitra ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; g\n..bag ---&gt; a\n.baga ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; m\nrishm ---&gt; a\nishma ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; w\n..baw ---&gt; a\n.bawa ---&gt; n\nbawan ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; m\n..zam ---&gt; i\n.zami ---&gt; l\nzamil ---&gt; e\namile ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; n\n..yun ---&gt; u\n.yunu ---&gt; s\nyunus ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; i\n.hasi ---&gt; b\nhasib ---&gt; u\nasibu ---&gt; l\nsibul ---&gt; .\n..... ---&gt; u\n....u ---&gt; s\n...us ---&gt; m\n..usm ---&gt; a\n.usma ---&gt; a\nusmaa ---&gt; n\nsmaan ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; e\nharie ---&gt; s\naries ---&gt; h\nriesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; a\n.kama ---&gt; l\nkamal ---&gt; a\namala ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; c\n..ruc ---&gt; h\n.ruch ---&gt; i\nruchi ---&gt; k\nuchik ---&gt; a\nchika ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; m\n.sarm ---&gt; i\nsarmi ---&gt; l\narmil ---&gt; a\nrmila ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; l\n.kull ---&gt; i\nkulli ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; j\n.kunj ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; b\nharib ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; j\namarj ---&gt; e\nmarje ---&gt; e\narjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; r\n.umar ---&gt; d\numard ---&gt; a\nmarda ---&gt; r\nardar ---&gt; a\nrdara ---&gt; z\ndaraz ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; t\n.sunt ---&gt; i\nsunti ---&gt; a\nuntia ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; r\n.khur ---&gt; s\nkhurs ---&gt; h\nhursh ---&gt; i\nurshi ---&gt; d\nrshid ---&gt; a\nshida ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; y\n.faiy ---&gt; a\nfaiya ---&gt; a\naiyaa ---&gt; z\niyaaz ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; t\n..cht ---&gt; a\n.chta ---&gt; r\nchtar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; h\n.ragh ---&gt; b\nraghb ---&gt; i\naghbi ---&gt; r\nghbir ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; d\n.rand ---&gt; h\nrandh ---&gt; e\nandhe ---&gt; e\nndhee ---&gt; r\ndheer ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; j\n.gulj ---&gt; a\ngulja ---&gt; r\nuljar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; j\n.ramj ---&gt; a\nramja ---&gt; n\namjan ---&gt; i\nmjani ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; a\n.nana ---&gt; k\nnanak ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; j\n.harj ---&gt; e\nharje ---&gt; e\narjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; q\n..toq ---&gt; u\n.toqu ---&gt; i\ntoqui ---&gt; r\noquir ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; a\n..ata ---&gt; n\n.atan ---&gt; u\natanu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; e\n.sume ---&gt; n\nsumen ---&gt; d\numend ---&gt; e\nmende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; e\n.suje ---&gt; e\nsujee ---&gt; t\nujeet ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; a\n.rupa ---&gt; m\nrupam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; e\n.sake ---&gt; t\nsaket ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; i\n.shri ---&gt; k\nshrik ---&gt; a\nhrika ---&gt; n\nrikan ---&gt; t\nikant ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; v\n..div ---&gt; y\n.divy ---&gt; a\ndivya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; i\n.nazi ---&gt; m\nnazim ---&gt; a\nazima ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; h\n.nath ---&gt; u\nnathu ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; v\n.harv ---&gt; a\nharva ---&gt; n\narvan ---&gt; s\nrvans ---&gt; h\nvansh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; a\nshiva ---&gt; j\nhivaj ---&gt; i\nivaji ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; v\n..juv ---&gt; e\n.juve ---&gt; l\njuvel ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; h\n..adh ---&gt; i\n.adhi ---&gt; s\nadhis ---&gt; h\ndhish ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; i\n..aki ---&gt; f\n.akif ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; s\n.kirs ---&gt; h\nkirsh ---&gt; n\nirshn ---&gt; a\nrshna ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; a\n..asa ---&gt; g\n.asag ---&gt; a\nasaga ---&gt; r\nsagar ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; h\n..jah ---&gt; g\n.jahg ---&gt; e\njahge ---&gt; e\nahgee ---&gt; r\nhgeer ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; a\n.jaya ---&gt; d\njayad ---&gt; a\nayada ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; r\nsushr ---&gt; e\nushre ---&gt; e\nshree ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; y\n..ray ---&gt; i\n.rayi ---&gt; s\nrayis ---&gt; h\nayish ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; i\n.sumi ---&gt; r\nsumir ---&gt; t\numirt ---&gt; a\nmirta ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; n\n.monn ---&gt; y\nmonny ---&gt; .\n..... ---&gt; s\n....s ---&gt; p\n...sp ---&gt; a\n..spa ---&gt; r\n.spar ---&gt; s\nspars ---&gt; h\nparsh ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; n\n..fan ---&gt; n\n.fann ---&gt; i\nfanni ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; a\n.baba ---&gt; n\nbaban ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; d\nmohad ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; y\n..siy ---&gt; a\n.siya ---&gt; .\n..... ---&gt; y\n....y ---&gt; e\n...ye ---&gt; s\n..yes ---&gt; h\n.yesh ---&gt; p\nyeshp ---&gt; a\neshpa ---&gt; l\nshpal ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; n\n.binn ---&gt; u\nbinnu ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; a\nkesha ---&gt; r\neshar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; u\nmangu ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; i\n.gudi ---&gt; a\ngudia ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; e\n.bire ---&gt; n\nbiren ---&gt; d\nirend ---&gt; e\nrende ---&gt; r\nender ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; w\n..jaw ---&gt; h\n.jawh ---&gt; a\njawha ---&gt; r\nawhar ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; r\n..sor ---&gt; a\n.sora ---&gt; b\nsorab ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; b\n.babb ---&gt; y\nbabby ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; u\n..pau ---&gt; l\n.paul ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; d\n.gudd ---&gt; i\nguddi ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; v\n..pav ---&gt; n\n.pavn ---&gt; e\npavne ---&gt; s\navnes ---&gt; h\nvnesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; l\n.sabl ---&gt; u\nsablu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; f\n.shef ---&gt; a\nshefa ---&gt; l\nhefal ---&gt; i\nefali ---&gt; .\n..... ---&gt; c\n....c ---&gt; o\n...co ---&gt; s\n..cos ---&gt; m\n.cosm ---&gt; i\ncosmi ---&gt; c\nosmic ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; s\nshams ---&gt; h\nhamsh ---&gt; e\namshe ---&gt; r\nmsher ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; u\n.monu ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; t\n..set ---&gt; i\n.seti ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; b\n.shob ---&gt; h\nshobh ---&gt; i\nhobhi ---&gt; t\nobhit ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; m\n.salm ---&gt; i\nsalmi ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; l\n..rol ---&gt; l\n.roll ---&gt; y\nrolly ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; a\n..uda ---&gt; i\n.udai ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; i\n.saki ---&gt; l\nsakil ---&gt; a\nakila ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; a\n..ara ---&gt; t\n.arat ---&gt; i\narati ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; u\n.karu ---&gt; n\nkarun ---&gt; a\naruna ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; a\n.kesa ---&gt; r\nkesar ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; g\n.jhag ---&gt; d\njhagd ---&gt; o\nhagdo ---&gt; o\nagdoo ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; s\n.shis ---&gt; h\nshish ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; e\n.mane ---&gt; e\nmanee ---&gt; t\naneet ---&gt; a\nneeta ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; n\nrajen ---&gt; d\najend ---&gt; e\njende ---&gt; r\nender ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; u\nrishu ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; n\n.guln ---&gt; a\ngulna ---&gt; n\nulnan ---&gt; j\nlnanj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; a\nramsa ---&gt; t\namsat ---&gt; i\nmsati ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; r\n..tur ---&gt; v\n.turv ---&gt; e\nturve ---&gt; n\nurven ---&gt; y\nrveny ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; w\n.jasw ---&gt; a\njaswa ---&gt; n\naswan ---&gt; t\nswant ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; d\n..nad ---&gt; e\n.nade ---&gt; e\nnadee ---&gt; m\nadeem ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; d\nhahid ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; d\n.said ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; d\n..mod ---&gt; h\n.modh ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; d\n..fid ---&gt; a\n.fida ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; a\n.kira ---&gt; n\nkiran ---&gt; t\nirant ---&gt; i\nranti ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; a\n.mina ---&gt; x\nminax ---&gt; n\ninaxn ---&gt; i\nnaxni ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; n\n..arn ---&gt; a\n.arna ---&gt; b\narnab ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; o\n..doo ---&gt; d\n.dood ---&gt; h\ndoodh ---&gt; n\noodhn ---&gt; a\nodhna ---&gt; t\ndhnat ---&gt; h\nhnath ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; l\n.adil ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; u\nveeru ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; o\n..leo ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; e\nrajee ---&gt; t\najeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; s\n..shs ---&gt; i\n.shsi ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; d\n.ched ---&gt; d\nchedd ---&gt; i\nheddi ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; b\n.khub ---&gt; h\nkhubh ---&gt; u\nhubhu ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; n\n.vinn ---&gt; a\nvinna ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; a\n.deva ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; s\n..khs ---&gt; h\n.khsh ---&gt; b\nkhshb ---&gt; u\nhshbu ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; b\n..jub ---&gt; a\n.juba ---&gt; i\njubai ---&gt; r\nubair ---&gt; a\nbaira ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; r\n..nur ---&gt; s\n.nurs ---&gt; a\nnursa ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; i\n.dali ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; e\ndurge ---&gt; s\nurges ---&gt; h\nrgesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; k\n.shuk ---&gt; v\nshukv ---&gt; e\nhukve ---&gt; e\nukvee ---&gt; r\nkveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; n\n.somn ---&gt; a\nsomna ---&gt; t\nomnat ---&gt; h\nmnath ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; d\n.rand ---&gt; e\nrande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; e\n..tee ---&gt; k\n.teek ---&gt; a\nteeka ---&gt; m\neekam ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; m\n.naim ---&gt; a\nnaima ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; k\n..fak ---&gt; i\n.faki ---&gt; r\nfakir ---&gt; i\nakiri ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; a\n.saja ---&gt; n\nsajan ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; u\n.aasu ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; m\naashm ---&gt; a\nashma ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; t\namrit ---&gt; p\nmritp ---&gt; a\nritpa ---&gt; l\nitpal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; s\n.bhis ---&gt; a\nbhisa ---&gt; n\nhisan ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; a\n.isha ---&gt; r\nishar ---&gt; a\nshara ---&gt; r\nharar ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; i\n..ati ---&gt; q\n.atiq ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; a\ndurga ---&gt; s\nurgas ---&gt; h\nrgash ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; h\n..zah ---&gt; i\n.zahi ---&gt; r\nzahir ---&gt; a\nahira ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; t\n.shet ---&gt; a\nsheta ---&gt; l\nhetal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; e\n.sale ---&gt; s\nsales ---&gt; h\nalesh ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; e\n..lee ---&gt; n\n.leen ---&gt; a\nleena ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; e\n..sne ---&gt; h\n.sneh ---&gt; l\nsnehl ---&gt; a\nnehla ---&gt; t\nehlat ---&gt; a\nhlata ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; y\n.prey ---&gt; a\npreya ---&gt; n\nreyan ---&gt; k\neyank ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; k\n..lok ---&gt; i\n.loki ---&gt; n\nlokin ---&gt; d\nokind ---&gt; r\nkindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; a\n.sita ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; t\n.jeet ---&gt; r\njeetr ---&gt; a\neetra ---&gt; m\netram ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; e\n.hare ---&gt; n\nharen ---&gt; d\narend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; e\n.sude ---&gt; r\nsuder ---&gt; s\nuders ---&gt; h\ndersh ---&gt; a\nersha ---&gt; n\nrshan ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; k\n..alk ---&gt; a\n.alka ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; t\n.mamt ---&gt; h\nmamth ---&gt; a\namtha ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; m\n..rom ---&gt; a\n.roma ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; a\n.deva ---&gt; n\ndevan ---&gt; a\nevana ---&gt; n\nvanan ---&gt; d\nanand ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; v\n..lov ---&gt; l\n.lovl ---&gt; y\nlovly ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; u\nsantu ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; m\n..nim ---&gt; m\n.nimm ---&gt; y\nnimmy ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; n\n.sehn ---&gt; a\nsehna ---&gt; z\nehnaz ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; e\n.vase ---&gt; e\nvasee ---&gt; m\naseem ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; i\n.devi ---&gt; n\ndevin ---&gt; d\nevind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; h\n.narh ---&gt; a\nnarha ---&gt; s\narhas ---&gt; i\nrhasi ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; i\n.firi ---&gt; d\nfirid ---&gt; a\nirida ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; u\n..mau ---&gt; j\n.mauj ---&gt; i\nmauji ---&gt; m\naujim ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; v\nmandv ---&gt; i\nandvi ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; s\n.khas ---&gt; t\nkhast ---&gt; i\nhasti ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; r\nmohar ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; i\n.hani ---&gt; f\nhanif ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; n\n.aman ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; n\n..men ---&gt; a\n.mena ---&gt; k\nmenak ---&gt; s\nenaks ---&gt; h\nnaksh ---&gt; i\nakshi ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; v\n.dilv ---&gt; a\ndilva ---&gt; r\nilvar ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; i\n.musi ---&gt; b\nmusib ---&gt; a\nusiba ---&gt; t\nsibat ---&gt; .\n..... ---&gt; s\n....s ---&gt; k\n...sk ---&gt; u\n..sku ---&gt; n\n.skun ---&gt; a\nskuna ---&gt; t\nkunat ---&gt; l\nunatl ---&gt; a\nnatla ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; t\n..bat ---&gt; u\n.batu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; t\nmangt ---&gt; a\nangta ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; k\n..lek ---&gt; h\n.lekh ---&gt; r\nlekhr ---&gt; a\nekhra ---&gt; j\nkhraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; r\n.samr ---&gt; a\nsamra ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; r\n..dar ---&gt; a\n.dara ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; m\n.harm ---&gt; e\nharme ---&gt; e\narmee ---&gt; t\nrmeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; e\nsanje ---&gt; e\nanjee ---&gt; v\nnjeev ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; i\n.sumi ---&gt; t\nsumit ---&gt; a\numita ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; n\n..sin ---&gt; a\n.sina ---&gt; a\nsinaa ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; e\n..gee ---&gt; t\n.geet ---&gt; i\ngeeti ---&gt; k\neetik ---&gt; a\netika ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; r\nhandr ---&gt; a\nandra ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; o\n.mano ---&gt; j\nmanoj ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; h\n..fah ---&gt; i\n.fahi ---&gt; r\nfahir ---&gt; a\nahira ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; b\n..sob ---&gt; a\n.soba ---&gt; n\nsoban ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; b\n..kab ---&gt; i\n.kabi ---&gt; d\nkabid ---&gt; .\n..... ---&gt; p\n....p ---&gt; y\n...py ---&gt; a\n..pya ---&gt; r\n.pyar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; a\n.saba ---&gt; n\nsaban ---&gt; a\nabana ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; a\n..ara ---&gt; s\n.aras ---&gt; l\narasl ---&gt; a\nrasla ---&gt; n\naslan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; n\n.nain ---&gt; s\nnains ---&gt; h\nainsh ---&gt; i\ninshi ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; a\n..ara ---&gt; b\n.arab ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; m\n.anam ---&gt; o\nanamo ---&gt; l\nnamol ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; h\n.budh ---&gt; o\nbudho ---&gt; .\n..... ---&gt; e\n....e ---&gt; l\n...el ---&gt; w\n..elw ---&gt; i\n.elwi ---&gt; n\nelwin ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; r\n.khur ---&gt; s\nkhurs ---&gt; h\nhursh ---&gt; i\nurshi ---&gt; d\nrshid ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; l\n..jul ---&gt; f\n.julf ---&gt; i\njulfi ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; b\n.tabb ---&gt; s\ntabbs ---&gt; u\nabbsu ---&gt; m\nbbsum ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; i\n..nai ---&gt; s\n.nais ---&gt; i\nnaisi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; m\nsahim ---&gt; a\nahima ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; s\n.hars ---&gt; h\nharsh ---&gt; i\narshi ---&gt; t\nrshit ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; a\nramba ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; z\n.marz ---&gt; i\nmarzi ---&gt; n\narzin ---&gt; a\nrzina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; r\nsahar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; i\n.rani ---&gt; .\n..... ---&gt; a\n....a ---&gt; h\n...ah ---&gt; s\n..ahs ---&gt; a\n.ahsa ---&gt; m\nahsam ---&gt; i\nhsami ---&gt; n\nsamin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; h\nrambh ---&gt; a\nambha ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; j\n.sajj ---&gt; u\nsajju ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; h\n.mush ---&gt; r\nmushr ---&gt; r\nushrr ---&gt; a\nshrra ---&gt; f\nhrraf ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; j\nabhij ---&gt; i\nbhiji ---&gt; t\nhijit ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; l\n..jul ---&gt; l\n.jull ---&gt; y\njully ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; u\n..kou ---&gt; c\n.kouc ---&gt; h\nkouch ---&gt; u\nouchu ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; r\n..hur ---&gt; j\n.hurj ---&gt; i\nhurji ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; t\n..dat ---&gt; a\n.data ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; i\n.naji ---&gt; m\nnajim ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; e\n.mahe ---&gt; s\nmahes ---&gt; h\nahesh ---&gt; .\n..... ---&gt; j\n....j ---&gt; y\n...jy ---&gt; o\n..jyo ---&gt; t\n.jyot ---&gt; y\njyoty ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; o\nmanjo ---&gt; o\nanjoo ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; i\nsanji ---&gt; t\nanjit ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; u\n.shru ---&gt; t\nshrut ---&gt; i\nhruti ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; a\n..kra ---&gt; s\n.kras ---&gt; h\nkrash ---&gt; a\nrasha ---&gt; n\nashan ---&gt; a\nshana ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; s\n.kirs ---&gt; h\nkirsh ---&gt; a\nirsha ---&gt; n\nrshan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; m\n.rajm ---&gt; a\nrajma ---&gt; n\najman ---&gt; i\njmani ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; e\n..pee ---&gt; n\n.peen ---&gt; u\npeenu ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; k\n.musk ---&gt; a\nmuska ---&gt; a\nuskaa ---&gt; n\nskaan ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; a\n..tra ---&gt; u\n.trau ---&gt; n\ntraun ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; l\n..gil ---&gt; a\n.gila ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; e\n.pare ---&gt; m\nparem ---&gt; .\n..... ---&gt; u\n....u ---&gt; n\n...un ---&gt; n\n..unn ---&gt; a\n.unna ---&gt; t\nunnat ---&gt; i\nnnati ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; n\n..ann ---&gt; a\n.anna ---&gt; t\nannat ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; e\nmunne ---&gt; m\nunnem ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; m\n.harm ---&gt; a\nharma ---&gt; n\narman ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; a\ncheta ---&gt; n\nhetan ---&gt; a\netana ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; s\n.hars ---&gt; h\nharsh ---&gt; i\narshi ---&gt; t\nrshit ---&gt; a\nshita ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; y\nparty ---&gt; u\nartyu ---&gt; s\nrtyus ---&gt; h\ntyush ---&gt; .\n..... ---&gt; u\n....u ---&gt; t\n...ut ---&gt; t\n..utt ---&gt; a\n.utta ---&gt; m\nuttam ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; h\n.vash ---&gt; i\nvashi ---&gt; s\nashis ---&gt; a\nshisa ---&gt; s\nhisas ---&gt; t\nisast ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; y\n.saiy ---&gt; a\nsaiya ---&gt; d\naiyad ---&gt; a\niyada ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; g\n.bang ---&gt; a\nbanga ---&gt; l\nangal ---&gt; i\nngali ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; a\n.shoa ---&gt; n\nshoan ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; a\n.jana ---&gt; r\njanar ---&gt; d\nanard ---&gt; a\nnarda ---&gt; m\nardam ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; j\n..laj ---&gt; j\n.lajj ---&gt; a\nlajja ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; u\nshalu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; u\n.saru ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; i\naashi ---&gt; a\nashia ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; a\n.bina ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; s\nrajes ---&gt; h\najesh ---&gt; w\njeshw ---&gt; a\neshwa ---&gt; r\nshwar ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; e\nlakhe ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; n\n..jun ---&gt; a\n.juna ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; z\n.faiz ---&gt; u\nfaizu ---&gt; d\naizud ---&gt; e\nizude ---&gt; e\nzudee ---&gt; n\nudeen ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; u\n.ramu ---&gt; l\nramul ---&gt; u\namulu ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; j\n..arj ---&gt; i\n.arji ---&gt; n\narjin ---&gt; a\nrjina ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; a\nshaba ---&gt; n\nhaban ---&gt; a\nabana ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; i\n.dipi ---&gt; k\ndipik ---&gt; a\nipika ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; a\n..saa ---&gt; n\n.saan ---&gt; a\nsaana ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; h\n.radh ---&gt; a\nradha ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; r\n.bhur ---&gt; i\nbhuri ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; u\npratu ---&gt; l\nratul ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; t\n.kirt ---&gt; i\nkirti ---&gt; m\nirtim ---&gt; a\nrtima ---&gt; n\ntiman ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; a\n.mita ---&gt; l\nmital ---&gt; i\nitali ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; m\n..aam ---&gt; i\n.aami ---&gt; n\naamin ---&gt; a\namina ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; u\n..tau ---&gt; s\n.taus ---&gt; e\ntause ---&gt; e\nausee ---&gt; k\nuseek ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; d\n.sund ---&gt; r\nsundr ---&gt; i\nundri ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; u\n.taru ---&gt; n\ntarun ---&gt; a\naruna ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; h\nramdh ---&gt; a\namdha ---&gt; n\nmdhan ---&gt; i\ndhani ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; t\n..ist ---&gt; i\n.isti ---&gt; k\nistik ---&gt; a\nstika ---&gt; r\ntikar ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; i\n.rohi ---&gt; t\nrohit ---&gt; a\nohita ---&gt; s\nhitas ---&gt; h\nitash ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; s\n.vans ---&gt; u\nvansu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; a\n.mana ---&gt; s\nmanas ---&gt; h\nanash ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; i\n..omi ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; a\ndurga ---&gt; w\nurgaw ---&gt; a\nrgawa ---&gt; t\ngawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; i\n.lavi ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; e\ndeepe ---&gt; n\neepen ---&gt; d\nepend ---&gt; e\npende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; f\nshaif ---&gt; a\nhaifa ---&gt; l\naifal ---&gt; i\nifali ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; a\n.sita ---&gt; r\nsitar ---&gt; a\nitara ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; w\n..tiw ---&gt; n\n.tiwn ---&gt; k\ntiwnk ---&gt; a\niwnka ---&gt; l\nwnkal ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; h\n.tash ---&gt; n\ntashn ---&gt; i\nashni ---&gt; m\nshnim ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; m\n..sim ---&gt; m\n.simm ---&gt; i\nsimmi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; j\n.harj ---&gt; i\nharji ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; e\n.rube ---&gt; y\nrubey ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; d\n.bhad ---&gt; u\nbhadu ---&gt; r\nhadur ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; j\n.faij ---&gt; a\nfaija ---&gt; l\naijal ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; b\n.tabb ---&gt; a\ntabba ---&gt; s\nabbas ---&gt; u\nbbasu ---&gt; m\nbasum ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; m\n..jim ---&gt; m\n.jimm ---&gt; i\njimmi ---&gt; .\n..... ---&gt; j\n....j ---&gt; y\n...jy ---&gt; o\n..jyo ---&gt; t\n.jyot ---&gt; i\njyoti ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; y\nrajiy ---&gt; a\najiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; r\nsantr ---&gt; a\nantra ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; h\n..toh ---&gt; s\n.tohs ---&gt; e\ntohse ---&gt; e\nohsee ---&gt; e\nhseee ---&gt; m\nseeem ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; a\nsidha ---&gt; r\nidhar ---&gt; t\ndhart ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; y\n..gay ---&gt; t\n.gayt ---&gt; r\ngaytr ---&gt; e\naytre ---&gt; e\nytree ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; t\n.bart ---&gt; i\nbarti ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; d\n.rajd ---&gt; u\nrajdu ---&gt; t\najdut ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; k\n..alk ---&gt; e\n.alke ---&gt; s\nalkes ---&gt; h\nlkesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; a\n..aja ---&gt; j\n.ajaj ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; j\n..inj ---&gt; m\n.injm ---&gt; a\ninjma ---&gt; m\nnjmam ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; m\n.ashm ---&gt; a\nashma ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; h\n.nath ---&gt; u\nnathu ---&gt; l\nathul ---&gt; a\nthula ---&gt; l\nhulal ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; b\n.gurb ---&gt; a\ngurba ---&gt; c\nurbac ---&gt; h\nrbach ---&gt; a\nbacha ---&gt; n\nachan ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; y\n..pry ---&gt; a\n.prya ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; m\nsharm ---&gt; i\nharmi ---&gt; l\narmil ---&gt; i\nrmili ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; v\n.samv ---&gt; e\nsamve ---&gt; d\namved ---&gt; n\nmvedn ---&gt; a\nvedna ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; p\nsukhp ---&gt; a\nukhpa ---&gt; l\nkhpal ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; n\n..jin ---&gt; c\n.jinc ---&gt; y\njincy ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; i\n.pari ---&gt; t\nparit ---&gt; i\nariti ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; h\n..fah ---&gt; i\n.fahi ---&gt; m\nfahim ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; o\nsubho ---&gt; d\nubhod ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; d\n.prad ---&gt; e\nprade ---&gt; p\nradep ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; i\n.sumi ---&gt; n\nsumin ---&gt; d\numind ---&gt; e\nminde ---&gt; r\ninder ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; h\n.kish ---&gt; o\nkisho ---&gt; r\nishor ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; a\nprita ---&gt; m\nritam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; v\n.satv ---&gt; i\nsatvi ---&gt; r\natvir ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; v\n.jaiv ---&gt; e\njaive ---&gt; e\naivee ---&gt; r\niveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; a\n.saka ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; y\n.shay ---&gt; a\nshaya ---&gt; n\nhayan ---&gt; a\nayana ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; r\n.kamr ---&gt; u\nkamru ---&gt; d\namrud ---&gt; d\nmrudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; j\n..muj ---&gt; a\n.muja ---&gt; f\nmujaf ---&gt; f\nujaff ---&gt; a\njaffa ---&gt; r\naffar ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; o\n..dho ---&gt; d\n.dhod ---&gt; i\ndhodi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; w\n..haw ---&gt; a\n.hawa ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; a\n.ompa ---&gt; t\nompat ---&gt; i\nmpati ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; j\nnoorj ---&gt; a\noorja ---&gt; h\norjah ---&gt; a\nrjaha ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; m\n.hasm ---&gt; i\nhasmi ---&gt; t\nasmit ---&gt; a\nsmita ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; w\nramsw ---&gt; r\namswr ---&gt; o\nmswro ---&gt; o\nswroo ---&gt; p\nwroop ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; j\n..maj ---&gt; i\n.maji ---&gt; d\nmajid ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; i\n..rai ---&gt; s\n.rais ---&gt; u\nraisu ---&gt; d\naisud ---&gt; d\nisudd ---&gt; i\nsuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; l\n.shil ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; h\n.sudh ---&gt; d\nsudhd ---&gt; e\nudhde ---&gt; v\ndhdev ---&gt; .\n..... ---&gt; s\n....s ---&gt; y\n...sy ---&gt; h\n..syh ---&gt; a\n.syha ---&gt; m\nsyham ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; e\n..ane ---&gt; k\n.anek ---&gt; h\nanekh ---&gt; a\nnekha ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; r\n.swar ---&gt; n\nswarn ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; k\n.bhik ---&gt; h\nbhikh ---&gt; a\nhikha ---&gt; r\nikhar ---&gt; i\nkhari ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; o\nyasho ---&gt; d\nashod ---&gt; a\nshoda ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; m\n..rim ---&gt; a\n.rima ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; i\naashi ---&gt; y\nashiy ---&gt; a\nshiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; d\nubhad ---&gt; r\nbhadr ---&gt; a\nhadra ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; h\n.sadh ---&gt; u\nsadhu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; i\n.nazi ---&gt; a\nnazia ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; o\n.jayo ---&gt; t\njayot ---&gt; i\nayoti ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; r\n.anar ---&gt; o\nanaro ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; s\n.hars ---&gt; h\nharsh ---&gt; m\narshm ---&gt; e\nrshme ---&gt; e\nshmee ---&gt; t\nhmeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; b\n.satb ---&gt; a\nsatba ---&gt; r\natbar ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; a\nrabha ---&gt; t\nabhat ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; e\n.vire ---&gt; s\nvires ---&gt; h\niresh ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; n\n..dan ---&gt; i\n.dani ---&gt; e\ndanie ---&gt; l\naniel ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; m\n.faim ---&gt; u\nfaimu ---&gt; d\naimud ---&gt; d\nimudd ---&gt; i\nmuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; e\nramse ---&gt; w\namsew ---&gt; a\nmsewa ---&gt; k\nsewak ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; a\n..faa ---&gt; i\n.faai ---&gt; j\nfaaij ---&gt; a\naaija ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; i\n.sumi ---&gt; t\nsumit ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; b\n.narb ---&gt; d\nnarbd ---&gt; a\narbda ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; l\n.lavl ---&gt; i\nlavli ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; r\n.amir ---&gt; u\namiru ---&gt; l\nmirul ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; t\n.meht ---&gt; a\nmehta ---&gt; b\nehtab ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; d\n..ved ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; n\n.amin ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; n\nshain ---&gt; a\nhaina ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; i\n.vini ---&gt; t\nvinit ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; s\n..sis ---&gt; t\n.sist ---&gt; y\nsisty ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; p\n.anup ---&gt; a\nanupa ---&gt; m\nnupam ---&gt; a\nupama ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; j\n..kaj ---&gt; a\n.kaja ---&gt; r\nkajar ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; n\n..sen ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; t\nparvt ---&gt; i\narvti ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; m\n.ashm ---&gt; i\nashmi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; j\nahnaj ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; c\n.kanc ---&gt; h\nkanch ---&gt; a\nancha ---&gt; n\nnchan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; h\n.rath ---&gt; a\nratha ---&gt; n\nathan ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; s\n..rus ---&gt; h\n.rush ---&gt; a\nrusha ---&gt; l\nushal ---&gt; i\nshali ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; n\nnoorn ---&gt; a\noorna ---&gt; b\nornab ---&gt; i\nrnabi ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; h\n.chah ---&gt; a\nchaha ---&gt; t\nhahat ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; b\nhahib ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; s\n.hars ---&gt; h\nharsh ---&gt; a\narsha ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; l\n..kil ---&gt; l\n.kill ---&gt; o\nkillo ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; w\n..diw ---&gt; a\n.diwa ---&gt; k\ndiwak ---&gt; a\niwaka ---&gt; r\nwakar ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; m\n..kim ---&gt; m\n.kimm ---&gt; i\nkimmi ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; h\n..nih ---&gt; a\n.niha ---&gt; l\nnihal ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; a\n.suda ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; n\nanjan ---&gt; a\nnjana ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; t\namart ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; r\n..afr ---&gt; i\n.afri ---&gt; n\nafrin ---&gt; a\nfrina ---&gt; .\n..... ---&gt; u\n....u ---&gt; t\n...ut ---&gt; a\n..uta ---&gt; m\n.utam ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; e\n.lave ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; u\nparsu ---&gt; r\narsur ---&gt; a\nrsura ---&gt; m\nsuram ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; t\n..ist ---&gt; y\n.isty ---&gt; a\nistya ---&gt; r\nstyar ---&gt; a\ntyara ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; e\n.sude ---&gt; s\nsudes ---&gt; h\nudesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; g\n.prag ---&gt; a\npraga ---&gt; t\nragat ---&gt; i\nagati ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; s\n..nos ---&gt; i\n.nosi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; g\n..hag ---&gt; a\n.haga ---&gt; m\nhagam ---&gt; i\nagami ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; u\n..rau ---&gt; n\n.raun ---&gt; a\nrauna ---&gt; f\naunaf ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; d\nharid ---&gt; u\naridu ---&gt; t\nridut ---&gt; t\nidutt ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; t\n..gyt ---&gt; r\n.gytr ---&gt; i\ngytri ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; e\nmange ---&gt; l\nangel ---&gt; a\nngela ---&gt; l\ngelal ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; h\n.budh ---&gt; w\nbudhw ---&gt; a\nudhwa ---&gt; n\ndhwan ---&gt; t\nhwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; v\n..jav ---&gt; a\n.java ---&gt; d\njavad ---&gt; e\navade ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; a\n..daa ---&gt; u\n.daau ---&gt; d\ndaaud ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; k\n..nek ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; j\npremj ---&gt; i\nremji ---&gt; t\nemjit ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; a\n.suma ---&gt; i\nsumai ---&gt; l\numail ---&gt; a\nmaila ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; k\n..akk ---&gt; n\n.akkn ---&gt; i\nakkni ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; g\n..jug ---&gt; a\n.juga ---&gt; n\njugan ---&gt; i\nugani ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; r\ndhanr ---&gt; a\nhanra ---&gt; j\nanraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; t\n.must ---&gt; i\nmusti ---&gt; k\nustik ---&gt; e\nstike ---&gt; e\ntikee ---&gt; m\nikeem ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; l\n.kaml ---&gt; a\nkamla ---&gt; j\namlaj ---&gt; e\nmlaje ---&gt; e\nlajee ---&gt; t\najeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; r\n..afr ---&gt; i\n.afri ---&gt; d\nafrid ---&gt; i\nfridi ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; i\n.moni ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; a\n.mina ---&gt; k\nminak ---&gt; s\ninaks ---&gt; h\nnaksh ---&gt; i\nakshi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; i\n.savi ---&gt; t\nsavit ---&gt; i\naviti ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; t\n.jant ---&gt; a\njanta ---&gt; r\nantar ---&gt; p\nntarp ---&gt; a\ntarpa ---&gt; l\narpal ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; a\n.kuma ---&gt; r\nkumar ---&gt; i\numari ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; y\nramdy ---&gt; a\namdya ---&gt; l\nmdyal ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; n\nchetn ---&gt; a\nhetna ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; b\n..nab ---&gt; i\n.nabi ---&gt; l\nnabil ---&gt; a\nabila ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; e\n.shee ---&gt; n\nsheen ---&gt; s\nheens ---&gt; a\neensa ---&gt; r\nensar ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; d\n.bhud ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; t\n.swat ---&gt; i\nswati ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; t\ndeept ---&gt; i\neepti ---&gt; .\n..... ---&gt; h\n....h ---&gt; o\n...ho ---&gt; n\n..hon ---&gt; g\n.hong ---&gt; s\nhongs ---&gt; i\nongsi ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; w\nbhagw ---&gt; a\nhagwa ---&gt; t\nagwat ---&gt; i\ngwati ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; b\n..aab ---&gt; i\n.aabi ---&gt; d\naabid ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; i\n.sani ---&gt; a\nsania ---&gt; .\n..... ---&gt; v\n....v ---&gt; r\n...vr ---&gt; e\n..vre ---&gt; n\n.vren ---&gt; d\nvrend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; e\nrishe ---&gt; n\nishen ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; d\n.vand ---&gt; h\nvandh ---&gt; n\nandhn ---&gt; a\nndhna ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; e\n.rame ---&gt; n\nramen ---&gt; d\namend ---&gt; e\nmende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; h\nsandh ---&gt; a\nandha ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; e\n.hare ---&gt; s\nhares ---&gt; h\naresh ---&gt; w\nreshw ---&gt; a\neshwa ---&gt; r\nshwar ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; e\n..ame ---&gt; e\n.amee ---&gt; r\nameer ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; a\n.bala ---&gt; r\nbalar ---&gt; a\nalara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; n\n.sahn ---&gt; a\nsahna ---&gt; b\nahnab ---&gt; a\nhnaba ---&gt; j\nnabaj ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; b\n..amb ---&gt; a\n.amba ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; k\n.shik ---&gt; h\nshikh ---&gt; a\nhikha ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; e\n..bhe ---&gt; m\n.bhem ---&gt; j\nbhemj ---&gt; i\nhemji ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; y\n.mary ---&gt; a\nmarya ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; a\n.asha ---&gt; r\nashar ---&gt; a\nshara ---&gt; m\nharam ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; u\nranju ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; s\n.nars ---&gt; i\nnarsi ---&gt; n\narsin ---&gt; g\nrsing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; e\n..ame ---&gt; e\n.amee ---&gt; n\nameen ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; e\n.sune ---&gt; n\nsunen ---&gt; a\nunena ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; m\n..nem ---&gt; s\n.nems ---&gt; i\nnemsi ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; v\n..arv ---&gt; i\n.arvi ---&gt; n\narvin ---&gt; d\nrvind ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; y\n..riy ---&gt; a\n.riya ---&gt; s\nriyas ---&gt; a\niyasa ---&gt; t\nyasat ---&gt; .\n..... ---&gt; i\n....i ---&gt; q\n...iq ---&gt; r\n..iqr ---&gt; a\n.iqra ---&gt; r\niqrar ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; i\n.aasi ---&gt; f\naasif ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; n\n..gen ---&gt; s\n.gens ---&gt; i\ngensi ---&gt; n\nensin ---&gt; g\nnsing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; z\n....z ---&gt; e\n...ze ---&gt; n\n..zen ---&gt; a\n.zena ---&gt; b\nzenab ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; e\nparve ---&gt; e\narvee ---&gt; n\nrveen ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; k\n.nikk ---&gt; h\nnikkh ---&gt; a\nikkha ---&gt; r\nkkhar ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; i\n.nasi ---&gt; r\nnasir ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; e\n.kale ---&gt; e\nkalee ---&gt; m\naleem ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; a\nshiva ---&gt; n\nhivan ---&gt; i\nivani ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; a\nmanga ---&gt; t\nangat ---&gt; .\n..... ---&gt; f\n....f ---&gt; r\n...fr ---&gt; j\n..frj ---&gt; a\n.frja ---&gt; n\nfrjan ---&gt; a\nrjana ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; h\n.megh ---&gt; a\nmegha ---&gt; n\neghan ---&gt; a\nghana ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; i\n.rami ---&gt; n\nramin ---&gt; d\namind ---&gt; e\nminde ---&gt; r\ninder ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; i\n.vipi ---&gt; n\nvipin ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; m\n..aam ---&gt; r\n.aamr ---&gt; i\naamri ---&gt; n\namrin ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; l\n..bul ---&gt; e\n.bule ---&gt; t\nbulet ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; u\n.ishu ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; r\n..hir ---&gt; i\n.hiri ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; f\n.mahf ---&gt; o\nmahfo ---&gt; o\nahfoo ---&gt; j\nhfooj ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; n\n..ren ---&gt; c\n.renc ---&gt; h\nrench ---&gt; o\nencho ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; n\n..pin ---&gt; k\n.pink ---&gt; u\npinku ---&gt; .\n..... ---&gt; h\n....h ---&gt; o\n...ho ---&gt; t\n..hot ---&gt; i\n.hoti ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; p\ndhanp ---&gt; a\nhanpa ---&gt; t\nanpat ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; i\n.jasi ---&gt; r\njasir ---&gt; a\nasira ---&gt; m\nsiram ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; j\n..afj ---&gt; a\n.afja ---&gt; l\nafjal ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; d\n..abd ---&gt; u\n.abdu ---&gt; l\nabdul ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; h\n.radh ---&gt; e\nradhe ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; e\n.nare ---&gt; n\nnaren ---&gt; d\narend ---&gt; a\nrenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; r\n..bhr ---&gt; a\n.bhra ---&gt; m\nbhram ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; w\n..naw ---&gt; e\n.nawe ---&gt; d\nnawed ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; n\n..pin ---&gt; k\n.pink ---&gt; y\npinky ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; n\nrishn ---&gt; a\nishna ---&gt; l\nshnal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; k\nshank ---&gt; r\nhankr ---&gt; i\nankri ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; n\nabhin ---&gt; w\nbhinw ---&gt; a\nhinwa ---&gt; v\ninwav ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; u\n..nau ---&gt; s\n.naus ---&gt; h\nnaush ---&gt; a\nausha ---&gt; d\nushad ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; a\n.raja ---&gt; n\nrajan ---&gt; i\najani ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; r\n.neer ---&gt; a\nneera ---&gt; j\neeraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; y\n..may ---&gt; a\n.maya ---&gt; k\nmayak ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; a\nkusha ---&gt; m\nusham ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; u\n.vasu ---&gt; k\nvasuk ---&gt; i\nasuki ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; h\n.tash ---&gt; k\ntashk ---&gt; k\nashkk ---&gt; u\nshkku ---&gt; r\nhkkur ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; e\n.rupe ---&gt; n\nrupen ---&gt; d\nupend ---&gt; e\npende ---&gt; r\nender ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; n\nriyan ---&gt; k\niyank ---&gt; a\nyanka ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; s\n..rus ---&gt; a\n.rusa ---&gt; n\nrusan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; t\n.mart ---&gt; i\nmarti ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; t\n..met ---&gt; i\n.meti ---&gt; l\nmetil ---&gt; y\netily ---&gt; .\n..... ---&gt; a\n....a ---&gt; i\n...ai ---&gt; s\n..ais ---&gt; h\n.aish ---&gt; e\naishe ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; n\n..dan ---&gt; i\n.dani ---&gt; s\ndanis ---&gt; h\nanish ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; a\n.kara ---&gt; n\nkaran ---&gt; d\narand ---&gt; e\nrande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; u\n..rau ---&gt; d\n.raud ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; i\n.ishi ---&gt; k\nishik ---&gt; a\nshika ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; s\n.ruks ---&gt; a\nruksa ---&gt; n\nuksan ---&gt; a\nksana ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; h\n.kanh ---&gt; a\nkanha ---&gt; i\nanhai ---&gt; y\nnhaiy ---&gt; a\nhaiya ---&gt; .\n..... ---&gt; i\n....i ---&gt; e\n...ie ---&gt; m\n..iem ---&gt; a\n.iema ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; i\n.hani ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; t\nchott ---&gt; u\nhottu ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; y\n.mony ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; a\n.shea ---&gt; k\nsheak ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; i\nnandi ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; d\namard ---&gt; e\nmarde ---&gt; e\nardee ---&gt; p\nrdeep ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; k\n..nek ---&gt; i\n.neki ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; a\ndurga ---&gt; n\nurgan ---&gt; a\nrgana ---&gt; n\nganan ---&gt; d\nanand ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; m\n..dim ---&gt; p\n.dimp ---&gt; u\ndimpu ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; u\n.ansu ---&gt; l\nansul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; s\nshaks ---&gt; h\nhaksh ---&gt; i\nakshi ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; m\nmaham ---&gt; m\nahamm ---&gt; d\nhammd ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; b\n.khub ---&gt; i\nkhubi ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; t\n.bhat ---&gt; r\nbhatr ---&gt; i\nhatri ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; n\n.pann ---&gt; u\npannu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; h\n.nanh ---&gt; e\nnanhe ---&gt; y\nanhey ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; t\n..git ---&gt; i\n.giti ---&gt; k\ngitik ---&gt; a\nitika ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; d\n.jagd ---&gt; i\njagdi ---&gt; e\nagdie ---&gt; s\ngdies ---&gt; h\ndiesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; i\npriti ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; m\nveerm ---&gt; a\neerma ---&gt; t\nermat ---&gt; i\nrmati ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; y\n.kaly ---&gt; a\nkalya ---&gt; n\nalyan ---&gt; i\nlyani ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; i\n.bali ---&gt; y\nbaliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; i\n.tari ---&gt; f\ntarif ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; v\n..tav ---&gt; i\n.tavi ---&gt; n\ntavin ---&gt; d\navind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; l\n....l ---&gt; u\n...lu ---&gt; k\n..luk ---&gt; m\n.lukm ---&gt; a\nlukma ---&gt; n\nukman ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; b\n..sib ---&gt; a\n.siba ---&gt; n\nsiban ---&gt; a\nibana ---&gt; z\nbanaz ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; b\n..omb ---&gt; e\n.ombe ---&gt; e\nombee ---&gt; r\nmbeer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; d\nsarad ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; e\n..ome ---&gt; n\n.omen ---&gt; d\nomend ---&gt; r\nmendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; e\n.save ---&gt; e\nsavee ---&gt; n\naveen ---&gt; a\nveena ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; u\nmunnu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; i\n.sabi ---&gt; l\nsabil ---&gt; a\nabila ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; r\n.umar ---&gt; d\numard ---&gt; e\nmarde ---&gt; e\nardee ---&gt; n\nrdeen ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; n\n..nen ---&gt; s\n.nens ---&gt; h\nnensh ---&gt; i\nenshi ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; d\n..rud ---&gt; r\n.rudr ---&gt; a\nrudra ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; i\nmanji ---&gt; v\nanjiv ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; t\nbhart ---&gt; e\nharte ---&gt; n\narten ---&gt; d\nrtend ---&gt; u\ntendu ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; u\n..gou ---&gt; r\n.gour ---&gt; i\ngouri ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; a\n.sona ---&gt; l\nsonal ---&gt; i\nonali ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; o\n..soo ---&gt; n\n.soon ---&gt; a\nsoona ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; a\n.rada ---&gt; t\nradat ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; t\n.beet ---&gt; i\nbeeti ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; e\n..khe ---&gt; e\n.khee ---&gt; m\nkheem ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; i\n.tani ---&gt; y\ntaniy ---&gt; a\naniya ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; i\n..bai ---&gt; t\n.bait ---&gt; a\nbaita ---&gt; .\n..... ---&gt; u\n....u ---&gt; r\n...ur ---&gt; w\n..urw ---&gt; a\n.urwa ---&gt; s\nurwas ---&gt; h\nrwash ---&gt; i\nwashi ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; j\n.prij ---&gt; u\npriju ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; b\ngulab ---&gt; s\nulabs ---&gt; h\nlabsh ---&gt; a\nabsha ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; u\n..bau ---&gt; d\n.baud ---&gt; i\nbaudi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; h\n.sudh ---&gt; i\nsudhi ---&gt; r\nudhir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; a\nsanja ---&gt; n\nanjan ---&gt; a\nnjana ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; j\npramj ---&gt; e\nramje ---&gt; e\namjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; a\ndurga ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; l\n.kaml ---&gt; a\nkamla ---&gt; s\namlas ---&gt; h\nmlash ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; t\n..alt ---&gt; m\n.altm ---&gt; a\naltma ---&gt; s\nltmas ---&gt; h\ntmash ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; u\n.rahu ---&gt; l\nrahul ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; o\n.saro ---&gt; j\nsaroj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; n\n.rahn ---&gt; u\nrahnu ---&gt; m\nahnum ---&gt; a\nhnuma ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; h\n.radh ---&gt; i\nradhi ---&gt; k\nadhik ---&gt; a\ndhika ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; e\n.inde ---&gt; r\ninder ---&gt; v\nnderv ---&gt; e\nderve ---&gt; s\nerves ---&gt; h\nrvesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; s\nmahas ---&gt; i\nahasi ---&gt; n\nhasin ---&gt; g\nasing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; y\n..tay ---&gt; a\n.taya ---&gt; b\ntayab ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; k\n..mak ---&gt; s\n.maks ---&gt; o\nmakso ---&gt; o\naksoo ---&gt; d\nksood ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; n\n..ren ---&gt; u\n.renu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; i\n..kai ---&gt; l\n.kail ---&gt; e\nkaile ---&gt; s\nailes ---&gt; h\nilesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; a\n.sida ---&gt; r\nsidar ---&gt; t\nidart ---&gt; h\ndarth ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; h\n.megh ---&gt; u\nmeghu ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; v\n.harv ---&gt; i\nharvi ---&gt; n\narvin ---&gt; d\nrvind ---&gt; r\nvindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; e\n.nare ---&gt; n\nnaren ---&gt; d\narend ---&gt; e\nrende ---&gt; r\nender ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; a\n.nisa ---&gt; t\nnisat ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; o\n..too ---&gt; l\n.tool ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; c\n..mac ---&gt; h\n.mach ---&gt; h\nmachh ---&gt; a\nachha ---&gt; l\nchhal ---&gt; a\nhhala ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; e\nshale ---&gt; s\nhales ---&gt; h\nalesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; w\n..suw ---&gt; a\n.suwa ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; a\n.fara ---&gt; h\nfarah ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; i\n.yogi ---&gt; t\nyogit ---&gt; a\nogita ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; n\n.chin ---&gt; k\nchink ---&gt; i\nhinki ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; u\nramku ---&gt; m\namkum ---&gt; a\nmkuma ---&gt; r\nkumar ---&gt; i\numari ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; j\n..nij ---&gt; a\n.nija ---&gt; m\nnijam ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; h\n.sith ---&gt; a\nsitha ---&gt; l\nithal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; c\nbhagc ---&gt; h\nhagch ---&gt; a\nagcha ---&gt; n\ngchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; d\n..ved ---&gt; h\n.vedh ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; a\n.raja ---&gt; r\nrajar ---&gt; a\najara ---&gt; m\njaram ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; v\n.jeev ---&gt; a\njeeva ---&gt; n\neevan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; n\n..shn ---&gt; e\n.shne ---&gt; h\nshneh ---&gt; a\nhneha ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; m\n..dam ---&gt; a\n.dama ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; t\n..imt ---&gt; i\n.imti ---&gt; y\nimtiy ---&gt; a\nmtiya ---&gt; z\ntiyaz ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; i\n.sohi ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; d\nyashd ---&gt; h\nashdh ---&gt; r\nshdhr ---&gt; a\nhdhra ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; o\nsanto ---&gt; s\nantos ---&gt; h\nntosh ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; a\n..bra ---&gt; j\n.braj ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; z\n..riz ---&gt; w\n.rizw ---&gt; a\nrizwa ---&gt; n\nizwan ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; r\n..for ---&gt; a\n.fora ---&gt; n\nforan ---&gt; t\norant ---&gt; i\nranti ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; n\nchann ---&gt; u\nhannu ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; v\n.palv ---&gt; i\npalvi ---&gt; n\nalvin ---&gt; d\nlvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; e\n.dipe ---&gt; e\ndipee ---&gt; k\nipeek ---&gt; a\npeeka ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; m\n.cham ---&gt; e\nchame ---&gt; l\nhamel ---&gt; i\nameli ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; y\n.sony ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; t\n.bint ---&gt; u\nbintu ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; e\ndhane ---&gt; s\nhanes ---&gt; w\nanesw ---&gt; a\nneswa ---&gt; r\neswar ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; i\n.rohi ---&gt; n\nrohin ---&gt; i\nohini ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; r\nhandr ---&gt; u\nandru ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; a\ngulsa ---&gt; n\nulsan ---&gt; a\nlsana ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; p\n.pusp ---&gt; e\npuspe ---&gt; n\nuspen ---&gt; d\nspend ---&gt; a\npenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; o\nsanjo ---&gt; h\nanjoh ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; o\n..dho ---&gt; l\n.dhol ---&gt; i\ndholi ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; h\n.nikh ---&gt; a\nnikha ---&gt; d\nikhad ---&gt; .\n..... ---&gt; a\n....a ---&gt; g\n...ag ---&gt; r\n..agr ---&gt; e\n.agre ---&gt; j\nagrej ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; i\nmanji ---&gt; n\nanjin ---&gt; d\nnjind ---&gt; e\njinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; m\n..jom ---&gt; i\n.jomi ---&gt; y\njomiy ---&gt; a\nomiya ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; h\n..keh ---&gt; k\n.kehk ---&gt; a\nkehka ---&gt; s\nehkas ---&gt; h\nhkash ---&gt; a\nkasha ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; t\n..got ---&gt; i\n.goti ---&gt; y\ngotiy ---&gt; a\notiya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; m\n.nazm ---&gt; a\nnazma ---&gt; a\nazmaa ---&gt; .\n..... ---&gt; e\n....e ---&gt; t\n...et ---&gt; a\n..eta ---&gt; h\n.etah ---&gt; s\netahs ---&gt; a\ntahsa ---&gt; a\nahsaa ---&gt; m\nhsaam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; o\n.saro ---&gt; j\nsaroj ---&gt; i\naroji ---&gt; n\nrojin ---&gt; i\nojini ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; u\n..mou ---&gt; s\n.mous ---&gt; a\nmousa ---&gt; m\nousam ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; u\n..dou ---&gt; l\n.doul ---&gt; a\ndoula ---&gt; t\noulat ---&gt; .\n..... ---&gt; e\n....e ---&gt; m\n...em ---&gt; r\n..emr ---&gt; a\n.emra ---&gt; n\nemran ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; m\n.ramm ---&gt; u\nrammu ---&gt; r\nammur ---&gt; t\nmmurt ---&gt; i\nmurti ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; m\n..rem ---&gt; a\n.rema ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; u\n..sou ---&gt; r\n.sour ---&gt; a\nsoura ---&gt; b\nourab ---&gt; h\nurabh ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; m\n.farm ---&gt; e\nfarme ---&gt; e\narmee ---&gt; n\nrmeen ---&gt; a\nmeena ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; l\nshail ---&gt; a\nhaila ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; p\n.trip ---&gt; u\ntripu ---&gt; r\nripur ---&gt; a\nipura ---&gt; r\npurar ---&gt; i\nurari ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; u\nshaku ---&gt; n\nhakun ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; a\n.bala ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; p\n.pusp ---&gt; h\npusph ---&gt; a\nuspha ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; l\n.mall ---&gt; o\nmallo ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; m\n..ham ---&gt; i\n.hami ---&gt; m\nhamim ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; i\n.ashi ---&gt; y\nashiy ---&gt; a\nshiya ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; a\n..jaa ---&gt; n\n.jaan ---&gt; u\njaanu ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; h\n..aah ---&gt; i\n.aahi ---&gt; m\naahim ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; f\ngulaf ---&gt; s\nulafs ---&gt; a\nlafsa ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; b\nyashb ---&gt; i\nashbi ---&gt; r\nshbir ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; i\n.mari ---&gt; y\nmariy ---&gt; a\nariya ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; u\n..tau ---&gt; s\n.taus ---&gt; h\ntaush ---&gt; i\naushi ---&gt; f\nushif ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; s\nrajes ---&gt; h\najesh ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; n\n..bun ---&gt; t\n.bunt ---&gt; i\nbunti ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; n\n..men ---&gt; p\n.menp ---&gt; a\nmenpa ---&gt; l\nenpal ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; t\n..net ---&gt; r\n.netr ---&gt; a\nnetra ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; r\n..var ---&gt; u\n.varu ---&gt; n\nvarun ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; a\n.nita ---&gt; s\nnitas ---&gt; h\nitash ---&gt; a\ntasha ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; e\n.mune ---&gt; s\nmunes ---&gt; h\nunesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; r\n..mer ---&gt; o\n.mero ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; s\n..jus ---&gt; t\n.just ---&gt; i\njusti ---&gt; n\nustin ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; v\n.ramv ---&gt; i\nramvi ---&gt; h\namvih ---&gt; a\nmviha ---&gt; r\nvihar ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; u\n..sou ---&gt; r\n.sour ---&gt; a\nsoura ---&gt; v\nourav ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; k\n..rek ---&gt; h\n.rekh ---&gt; a\nrekha ---&gt; i\nekhai ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; d\n.gurd ---&gt; e\ngurde ---&gt; e\nurdee ---&gt; p\nrdeep ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; i\nbhagi ---&gt; r\nhagir ---&gt; a\nagira ---&gt; t\ngirat ---&gt; h\nirath ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; n\n..gon ---&gt; s\n.gons ---&gt; a\ngonsa ---&gt; n\nonsan ---&gt; .\n..... ---&gt; n\n....n ---&gt; j\n...nj ---&gt; a\n..nja ---&gt; r\n.njar ---&gt; e\nnjare ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; n\nhushn ---&gt; u\nushnu ---&gt; d\nshnud ---&gt; i\nhnudi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; d\nsahid ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; i\n.jasi ---&gt; m\njasim ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; t\n.natt ---&gt; h\nnatth ---&gt; u\natthu ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; j\n.alij ---&gt; a\nalija ---&gt; n\nlijan ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; f\n..suf ---&gt; i\n.sufi ---&gt; a\nsufia ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; f\n.mahf ---&gt; o\nmahfo ---&gt; o\nahfoo ---&gt; z\nhfooz ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; t\n..ant ---&gt; i\n.anti ---&gt; m\nantim ---&gt; a\nntima ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; a\n.suja ---&gt; l\nsujal ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; w\n.ishw ---&gt; e\nishwe ---&gt; r\nshwer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; r\nsamir ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; s\n..res ---&gt; h\n.resh ---&gt; a\nresha ---&gt; m\nesham ---&gt; i\nshami ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; d\n.jaid ---&gt; u\njaidu ---&gt; l\naidul ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; a\nchana ---&gt; b\nhanab ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; e\n.vije ---&gt; t\nvijet ---&gt; a\nijeta ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; k\n..mak ---&gt; h\n.makh ---&gt; a\nmakha ---&gt; n\nakhan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; h\n.mash ---&gt; e\nmashe ---&gt; e\nashee ---&gt; n\nsheen ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; o\n.mano ---&gt; s\nmanos ---&gt; h\nanosh ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; p\n.trip ---&gt; t\ntript ---&gt; i\nripti ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; a\ndurga ---&gt; b\nurgab ---&gt; a\nrgaba ---&gt; i\ngabai ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; i\nyashi ---&gt; n\nashin ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; v\n.manv ---&gt; e\nmanve ---&gt; r\nanver ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; i\n.ashi ---&gt; k\nashik ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; r\n.anar ---&gt; k\nanark ---&gt; a\nnarka ---&gt; l\narkal ---&gt; i\nrkali ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; a\n.anka ---&gt; j\nankaj ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; v\n.bhav ---&gt; e\nbhave ---&gt; s\nhaves ---&gt; h\navesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; n\nshabn ---&gt; a\nhabna ---&gt; m\nabnam ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; i\naashi ---&gt; f\nashif ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; e\n.shie ---&gt; n\nshien ---&gt; a\nhiena ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; h\n..reh ---&gt; a\n.reha ---&gt; a\nrehaa ---&gt; n\nehaan ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; u\n.vipu ---&gt; n\nvipun ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; n\n.najn ---&gt; i\nnajni ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; i\n.sabi ---&gt; n\nsabin ---&gt; a\nabina ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; h\n..jah ---&gt; a\n.jaha ---&gt; n\njahan ---&gt; i\nahani ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; n\n..ton ---&gt; i\n.toni ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; g\nchang ---&gt; o\nhango ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; o\n.sano ---&gt; s\nsanos ---&gt; h\nanosh ---&gt; i\nnoshi ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; n\n..jun ---&gt; l\n.junl ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; s\nshams ---&gt; h\nhamsh ---&gt; i\namshi ---&gt; n\nmshin ---&gt; a\nshina ---&gt; .\n..... ---&gt; e\n....e ---&gt; l\n...el ---&gt; i\n..eli ---&gt; y\n.eliy ---&gt; a\neliya ---&gt; s\nliyas ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; i\n.suri ---&gt; n\nsurin ---&gt; d\nurind ---&gt; e\nrinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; c\n..anc ---&gt; h\n.anch ---&gt; a\nancha ---&gt; l\nnchal ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; k\n..yuk ---&gt; i\n.yuki ---&gt; l\nyukil ---&gt; a\nukila ---&gt; l\nkilal ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; m\n..gom ---&gt; t\n.gomt ---&gt; i\ngomti ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; r\n.mehr ---&gt; u\nmehru ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; a\n.sana ---&gt; t\nsanat ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; a\nshaba ---&gt; n\nhaban ---&gt; m\nabanm ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; b\n.parb ---&gt; h\nparbh ---&gt; a\narbha ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; t\n..pat ---&gt; e\n.pate ---&gt; n\npaten ---&gt; d\natend ---&gt; e\ntende ---&gt; r\nender ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; t\n.jait ---&gt; u\njaitu ---&gt; n\naitun ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; p\nsushp ---&gt; a\nushpa ---&gt; l\nshpal ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; r\n.balr ---&gt; a\nbalra ---&gt; j\nalraj ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; w\n..jew ---&gt; a\n.jewa ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; t\n.akht ---&gt; a\nakhta ---&gt; r\nkhtar ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; s\n.mohs ---&gt; i\nmohsi ---&gt; n\nohsin ---&gt; a\nhsina ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; c\n.malc ---&gt; h\nmalch ---&gt; a\nalcha ---&gt; n\nlchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; d\navind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; a\nndera ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; u\n.babu ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; m\n..tam ---&gt; m\n.tamm ---&gt; a\ntamma ---&gt; n\namman ---&gt; n\nmmann ---&gt; e\nmanne ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; s\n..afs ---&gt; a\n.afsa ---&gt; n\nafsan ---&gt; a\nfsana ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; d\n.bind ---&gt; e\nbinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; h\n.megh ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; i\n..ati ---&gt; s\n.atis ---&gt; h\natish ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; u\n.shau ---&gt; l\nshaul ---&gt; a\nhaula ---&gt; l\naulal ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; p\n.kalp ---&gt; n\nkalpn ---&gt; a\nalpna ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; j\nshahj ---&gt; a\nhahja ---&gt; d\nahjad ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; y\nakshy ---&gt; a\nkshya ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; e\n.pree ---&gt; t\npreet ---&gt; i\nreeti ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; r\n..ver ---&gt; s\n.vers ---&gt; h\nversh ---&gt; a\nersha ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; g\n.gang ---&gt; a\nganga ---&gt; r\nangar ---&gt; a\nngara ---&gt; m\ngaram ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; o\n.kiro ---&gt; d\nkirod ---&gt; i\nirodi ---&gt; m\nrodim ---&gt; a\nodima ---&gt; l\ndimal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; o\n.saro ---&gt; j\nsaroj ---&gt; a\naroja ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; a\nlakha ---&gt; n\nakhan ---&gt; .\n..... ---&gt; a\n....a ---&gt; y\n...ay ---&gt; s\n..ays ---&gt; h\n.aysh ---&gt; a\naysha ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; v\n.tasv ---&gt; e\ntasve ---&gt; e\nasvee ---&gt; r\nsveer ---&gt; a\nveera ---&gt; n\neeran ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; z\n..saz ---&gt; i\n.sazi ---&gt; a\nsazia ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; l\n..gol ---&gt; u\n.golu ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; s\n.anus ---&gt; o\nanuso ---&gt; y\nnusoy ---&gt; a\nusoya ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; h\n.jash ---&gt; i\njashi ---&gt; n\nashin ---&gt; a\nshina ---&gt; l\nhinal ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; e\n.sure ---&gt; n\nsuren ---&gt; d\nurend ---&gt; a\nrenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; a\nsidha ---&gt; n\nidhan ---&gt; t\ndhant ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; n\nhushn ---&gt; u\nushnu ---&gt; m\nshnum ---&gt; a\nhnuma ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; a\n.sita ---&gt; r\nsitar ---&gt; a\nitara ---&gt; m\ntaram ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; t\n.murt ---&gt; i\nmurti ---&gt; b\nurtib ---&gt; a\nrtiba ---&gt; i\ntibai ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; f\n..aaf ---&gt; r\n.aafr ---&gt; e\naafre ---&gt; e\nafree ---&gt; n\nfreen ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; h\n.kash ---&gt; i\nkashi ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; j\n..taj ---&gt; w\n.tajw ---&gt; a\ntajwa ---&gt; r\najwar ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; r\n.najr ---&gt; i\nnajri ---&gt; n\najrin ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; h\n.budh ---&gt; a\nbudha ---&gt; n\nudhan ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; m\n.nirm ---&gt; a\nnirma ---&gt; l\nirmal ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; k\n.rajk ---&gt; a\nrajka ---&gt; l\najkal ---&gt; i\njkali ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; b\n..beb ---&gt; i\n.bebi ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; i\n..ari ---&gt; f\n.arif ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; p\n.bhop ---&gt; a\nbhopa ---&gt; l\nhopal ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; i\n..vai ---&gt; b\n.vaib ---&gt; a\nvaiba ---&gt; h\naibah ---&gt; v\nibahv ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; u\n.dhru ---&gt; v\ndhruv ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; t\n.isht ---&gt; k\nishtk ---&gt; a\nshtka ---&gt; r\nhtkar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; u\npartu ---&gt; l\nartul ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; i\n.madi ---&gt; n\nmadin ---&gt; a\nadina ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; r\n.badr ---&gt; i\nbadri ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; l\n..tal ---&gt; v\n.talv ---&gt; e\ntalve ---&gt; n\nalven ---&gt; d\nlvend ---&gt; e\nvende ---&gt; r\nender ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; l\n..gal ---&gt; u\n.galu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; o\n..kho ---&gt; k\n.khok ---&gt; a\nkhoka ---&gt; n\nhokan ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; n\n..hen ---&gt; a\n.hena ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; r\n.anar ---&gt; a\nanara ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; a\n.vipa ---&gt; n\nvipan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; u\n..nau ---&gt; s\n.naus ---&gt; h\nnaush ---&gt; i\naushi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; l\n.lall ---&gt; u\nlallu ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; o\n..moo ---&gt; l\n.mool ---&gt; c\nmoolc ---&gt; h\noolch ---&gt; u\nolchu ---&gt; l\nlchul ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; h\n.rosh ---&gt; a\nrosha ---&gt; n\noshan ---&gt; l\nshanl ---&gt; a\nhanla ---&gt; l\nanlal ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; t\n.kast ---&gt; u\nkastu ---&gt; r\nastur ---&gt; i\nsturi ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; s\n.mohs ---&gt; e\nmohse ---&gt; e\nohsee ---&gt; n\nhseen ---&gt; a\nseena ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; a\nnoora ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; t\n.adit ---&gt; y\nadity ---&gt; a\nditya ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; w\n..biw ---&gt; a\n.biwa ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; r\nsushr ---&gt; i\nushri ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; m\nrashm ---&gt; i\nashmi ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; k\n..lok ---&gt; e\n.loke ---&gt; s\nlokes ---&gt; h\nokesh ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; i\n.devi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; a\n.shra ---&gt; v\nshrav ---&gt; a\nhrava ---&gt; n\nravan ---&gt; i\navani ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; i\nfarhi ---&gt; n\narhin ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; e\n.sure ---&gt; n\nsuren ---&gt; d\nurend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; s\n....s ---&gt; m\n...sm ---&gt; a\n..sma ---&gt; r\n.smar ---&gt; i\nsmari ---&gt; t\nmarit ---&gt; i\nariti ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; b\n.rajb ---&gt; a\nrajba ---&gt; l\najbal ---&gt; a\njbala ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; w\nyashw ---&gt; a\nashwa ---&gt; n\nshwan ---&gt; t\nhwant ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; d\n.sadd ---&gt; h\nsaddh ---&gt; a\naddha ---&gt; m\nddham ---&gt; .\n..... ---&gt; u\n....u ---&gt; j\n...uj ---&gt; a\n..uja ---&gt; m\n.ujam ---&gt; m\nujamm ---&gt; a\njamma ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; r\n.gyar ---&gt; s\ngyars ---&gt; i\nyarsi ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; j\n.parj ---&gt; i\nparji ---&gt; n\narjin ---&gt; d\nrjind ---&gt; e\njinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; a\nrabha ---&gt; k\nabhak ---&gt; a\nbhaka ---&gt; r\nhakar ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; h\n..juh ---&gt; i\n.juhi ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; u\n.kasu ---&gt; m\nkasum ---&gt; b\nasumb ---&gt; i\nsumbi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; j\n.surj ---&gt; i\nsurji ---&gt; t\nurjit ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; f\n..aaf ---&gt; t\n.aaft ---&gt; a\naafta ---&gt; b\naftab ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; m\n..gam ---&gt; b\n.gamb ---&gt; h\ngambh ---&gt; i\nambhi ---&gt; r\nmbhir ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; i\n.moni ---&gt; k\nmonik ---&gt; a\nonika ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; j\n.subj ---&gt; e\nsubje ---&gt; e\nubjee ---&gt; t\nbjeet ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; m\n..yam ---&gt; e\n.yame ---&gt; e\nyamee ---&gt; n\nameen ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; u\n..abu ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; l\n..bul ---&gt; a\n.bula ---&gt; d\nbulad ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; p\n..rep ---&gt; u\n.repu ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; l\n.pall ---&gt; a\npalla ---&gt; w\nallaw ---&gt; i\nllawi ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; u\n.masu ---&gt; m\nmasum ---&gt; .\n..... ---&gt; z\n....z ---&gt; i\n...zi ---&gt; n\n..zin ---&gt; a\n.zina ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; w\nbhagw ---&gt; a\nhagwa ---&gt; n\nagwan ---&gt; a\ngwana ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; i\nshami ---&gt; n\nhamin ---&gt; a\namina ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; n\nrishn ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; i\n.rohi ---&gt; t\nrohit ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; c\n..ruc ---&gt; h\n.ruch ---&gt; e\nruche ---&gt; n\nuchen ---&gt; d\nchend ---&gt; r\nhendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; y\n..lay ---&gt; m\n.laym ---&gt; a\nlayma ---&gt; n\nayman ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; i\n.ashi ---&gt; q\nashiq ---&gt; u\nshiqu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; i\n.saji ---&gt; a\nsajia ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; m\n.shim ---&gt; a\nshima ---&gt; l\nhimal ---&gt; a\nimala ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; c\n..suc ---&gt; h\n.such ---&gt; i\nsuchi ---&gt; t\nuchit ---&gt; a\nchita ---&gt; .\n..... ---&gt; z\n....z ---&gt; e\n...ze ---&gt; e\n..zee ---&gt; n\n.zeen ---&gt; a\nzeena ---&gt; t\neenat ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; t\n..pat ---&gt; r\n.patr ---&gt; i\npatri ---&gt; c\natric ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; r\n..akr ---&gt; a\n.akra ---&gt; m\nakram ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; m\n.reem ---&gt; a\nreema ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; r\n.seer ---&gt; i\nseeri ---&gt; y\neeriy ---&gt; a\neriya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; i\n.nazi ---&gt; y\nnaziy ---&gt; a\naziya ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; c\n..vic ---&gt; k\n.vick ---&gt; k\nvickk ---&gt; y\nickky ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; m\n.seem ---&gt; a\nseema ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; m\n.saym ---&gt; a\nsayma ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; h\n..jah ---&gt; o\n.jaho ---&gt; o\njahoo ---&gt; r\nahoor ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; n\n.rann ---&gt; i\nranni ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; m\n..aam ---&gt; i\n.aami ---&gt; l\naamil ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; a\n.sita ---&gt; r\nsitar ---&gt; e\nitare ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; h\nrambh ---&gt; u\nambhu ---&gt; l\nmbhul ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; r\n..sor ---&gt; b\n.sorb ---&gt; h\nsorbh ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; e\n.pune ---&gt; e\npunee ---&gt; t\nuneet ---&gt; a\nneeta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; w\n.sarw ---&gt; e\nsarwe ---&gt; s\narwes ---&gt; h\nrwesh ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; a\n.dhra ---&gt; m\ndhram ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; p\n..tap ---&gt; a\n.tapa ---&gt; s\ntapas ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; u\n..chu ---&gt; r\n.chur ---&gt; a\nchura ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; d\nsubhd ---&gt; r\nubhdr ---&gt; a\nbhdra ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; t\nparat ---&gt; i\narati ---&gt; b\nratib ---&gt; h\natibh ---&gt; a\ntibha ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; a\n.taba ---&gt; s\ntabas ---&gt; u\nabasu ---&gt; m\nbasum ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; e\n.inde ---&gt; r\ninder ---&gt; p\nnderp ---&gt; a\nderpa ---&gt; a\nerpaa ---&gt; l\nrpaal ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; h\n..bah ---&gt; n\n.bahn ---&gt; u\nbahnu ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; s\n..bis ---&gt; h\n.bish ---&gt; a\nbisha ---&gt; n\nishan ---&gt; a\nshana ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; a\n.jama ---&gt; d\njamad ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; w\n.parw ---&gt; a\nparwa ---&gt; t\narwat ---&gt; i\nrwati ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; h\n.sabh ---&gt; y\nsabhy ---&gt; a\nabhya ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; e\nfarhe ---&gt; e\narhee ---&gt; n\nrheen ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; m\n..rim ---&gt; m\n.rimm ---&gt; i\nrimmi ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; s\n..mis ---&gt; h\n.mish ---&gt; r\nmishr ---&gt; i\nishri ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; a\n.abha ---&gt; s\nabhas ---&gt; h\nbhash ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; z\n.shaz ---&gt; i\nshazi ---&gt; d\nhazid ---&gt; a\nazida ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; i\n.sadi ---&gt; q\nsadiq ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; m\n..lam ---&gt; a\n.lama ---&gt; n\nlaman ---&gt; i\namani ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; e\n..sne ---&gt; h\n.sneh ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; r\nrambr ---&gt; i\nambri ---&gt; j\nmbrij ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; c\n..jac ---&gt; o\n.jaco ---&gt; b\njacob ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; m\n..rim ---&gt; p\n.rimp ---&gt; y\nrimpy ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; s\nubhas ---&gt; h\nbhash ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; w\n.deew ---&gt; a\ndeewa ---&gt; n\neewan ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; n\n.tarn ---&gt; n\ntarnn ---&gt; u\narnnu ---&gt; m\nrnnum ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; i\nprati ---&gt; m\nratim ---&gt; a\natima ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; g\n.sang ---&gt; i\nsangi ---&gt; t\nangit ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; t\n..kat ---&gt; w\n.katw ---&gt; a\nkatwa ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; k\n..mok ---&gt; i\n.moki ---&gt; d\nmokid ---&gt; a\nokida ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; i\n..adi ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; n\n.kamn ---&gt; a\nkamna ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; n\nsamin ---&gt; a\namina ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; w\nmadhw ---&gt; i\nadhwi ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; m\nabhim ---&gt; a\nbhima ---&gt; n\nhiman ---&gt; y\nimany ---&gt; u\nmanyu ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; a\ndhara ---&gt; m\nharam ---&gt; b\naramb ---&gt; i\nrambi ---&gt; r\nambir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; i\n.sabi ---&gt; d\nsabid ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; j\n.veej ---&gt; i\nveeji ---&gt; t\neejit ---&gt; a\nejita ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; b\n.sahb ---&gt; u\nsahbu ---&gt; d\nahbud ---&gt; d\nhbudd ---&gt; i\nbuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; x\n..lax ---&gt; i\n.laxi ---&gt; t\nlaxit ---&gt; a\naxita ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; i\n.yogi ---&gt; n\nyogin ---&gt; d\nogind ---&gt; e\nginde ---&gt; r\ninder ---&gt; .\n..... ---&gt; m\n....m ---&gt; n\n...mn ---&gt; y\n..mny ---&gt; a\n.mnya ---&gt; k\nmnyak ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; d\n..lad ---&gt; a\n.lada ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; u\nbhanu ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; w\n.kulw ---&gt; i\nkulwi ---&gt; n\nulwin ---&gt; d\nlwind ---&gt; e\nwinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; i\nranji ---&gt; t\nanjit ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; t\n.mont ---&gt; y\nmonty ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; i\n.hasi ---&gt; m\nhasim ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; b\n..rab ---&gt; i\n.rabi ---&gt; n\nrabin ---&gt; a\nabina ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; a\nnoora ---&gt; l\nooral ---&gt; h\noralh ---&gt; a\nralha ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; q\n..jaq ---&gt; u\n.jaqu ---&gt; i\njaqui ---&gt; r\naquir ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; h\n.nath ---&gt; a\nnatha ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; e\n..bhe ---&gt; e\n.bhee ---&gt; m\nbheem ---&gt; a\nheema ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; o\n.vino ---&gt; d\nvinod ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; b\nshanb ---&gt; h\nhanbh ---&gt; u\nanbhu ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; q\n..muq ---&gt; h\n.muqh ---&gt; t\nmuqht ---&gt; a\nuqhta ---&gt; r\nqhtar ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; i\n.aani ---&gt; l\naanil ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; k\n.shuk ---&gt; l\nshukl ---&gt; a\nhukla ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; m\n..hum ---&gt; a\n.huma ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; y\n..liy ---&gt; a\n.liya ---&gt; k\nliyak ---&gt; a\niyaka ---&gt; t\nyakat ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; t\n..ast ---&gt; h\n.asth ---&gt; a\nastha ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; m\n..tam ---&gt; a\n.tama ---&gt; n\ntaman ---&gt; n\namann ---&gt; a\nmanna ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; f\n..tof ---&gt; i\n.tofi ---&gt; k\ntofik ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; i\n.kasi ---&gt; d\nkasid ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; h\n.push ---&gt; a\npusha ---&gt; n\nushan ---&gt; k\nshank ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; o\n.phoo ---&gt; l\nphool ---&gt; w\nhoolw ---&gt; a\noolwa ---&gt; t\nolwat ---&gt; i\nlwati ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; b\n.trib ---&gt; a\ntriba ---&gt; v\nribav ---&gt; a\nibava ---&gt; n\nbavan ---&gt; .\n..... ---&gt; m\n....m ---&gt; n\n...mn ---&gt; i\n..mni ---&gt; s\n.mnis ---&gt; h\nmnish ---&gt; a\nnisha ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; i\nfarhi ---&gt; b\narhib ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; a\n.mana ---&gt; k\nmanak ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; n\n.sehn ---&gt; a\nsehna ---&gt; w\nehnaw ---&gt; a\nhnawa ---&gt; j\nnawaj ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; v\nvishv ---&gt; a\nishva ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; r\n.sahr ---&gt; i\nsahri ---&gt; k\nahrik ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; s\n.pras ---&gt; h\nprash ---&gt; a\nrasha ---&gt; n\nashan ---&gt; t\nshant ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; r\nsidhr ---&gt; a\nidhra ---&gt; t\ndhrat ---&gt; h\nhrath ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; n\n..ann ---&gt; a\n.anna ---&gt; v\nannav ---&gt; i\nnnavi ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; p\n..bip ---&gt; n\n.bipn ---&gt; e\nbipne ---&gt; s\nipnes ---&gt; h\npnesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; a\n.saja ---&gt; n\nsajan ---&gt; a\najana ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; g\n.chog ---&gt; a\nchoga ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; i\n.papi ---&gt; y\npapiy ---&gt; a\napiya ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; n\n.sumn ---&gt; e\nsumne ---&gt; s\numnes ---&gt; h\nmnesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; r\nshivr ---&gt; a\nhivra ---&gt; m\nivram ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; t\n.aart ---&gt; i\naarti ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; k\n..huk ---&gt; u\n.huku ---&gt; m\nhukum ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; h\n.nash ---&gt; r\nnashr ---&gt; i\nashri ---&gt; n\nshrin ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; a\n.sala ---&gt; m\nsalam ---&gt; a\nalama ---&gt; n\nlaman ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; h\n.sadh ---&gt; a\nsadha ---&gt; n\nadhan ---&gt; a\ndhana ---&gt; .\n..... ---&gt; k\n....k ---&gt; m\n...km ---&gt; o\n..kmo ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; m\n.cham ---&gt; p\nchamp ---&gt; a\nhampa ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; k\n..huk ---&gt; m\n.hukm ---&gt; a\nhukma ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; a\nprema ---&gt; t\nremat ---&gt; u\nematu ---&gt; r\nmatur ---&gt; e\nature ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; n\n..jin ---&gt; a\n.jina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; u\n.saru ---&gt; f\nsaruf ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; k\nbhark ---&gt; a\nharka ---&gt; h\narkah ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; h\n.mash ---&gt; l\nmashl ---&gt; i\nashli ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; n\nrajen ---&gt; d\najend ---&gt; r\njendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; v\n..tav ---&gt; r\n.tavr ---&gt; a\ntavra ---&gt; j\navraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; e\nsukhe ---&gt; e\nukhee ---&gt; y\nkheey ---&gt; a\nheeya ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; j\n.balj ---&gt; i\nbalji ---&gt; n\naljin ---&gt; d\nljind ---&gt; e\njinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; u\n.rahu ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; u\n..jhu ---&gt; n\n.jhun ---&gt; n\njhunn ---&gt; u\nhunnu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; b\n.shob ---&gt; h\nshobh ---&gt; a\nhobha ---&gt; r\nobhar ---&gt; a\nbhara ---&gt; m\nharam ---&gt; .\n..... ---&gt; h\n....h ---&gt; o\n...ho ---&gt; m\n..hom ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; h\ngulsh ---&gt; e\nulshe ---&gt; r\nlsher ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; u\n.sanu ---&gt; .\n..... ---&gt; e\n....e ---&gt; k\n...ek ---&gt; a\n..eka ---&gt; m\n.ekam ---&gt; j\nekamj ---&gt; e\nkamje ---&gt; e\namjee ---&gt; t\nmjeet ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; v\n.mehv ---&gt; i\nmehvi ---&gt; s\nehvis ---&gt; h\nhvish ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; s\n.bhis ---&gt; m\nbhism ---&gt; p\nhismp ---&gt; a\nismpa ---&gt; l\nsmpal ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; r\n..per ---&gt; v\n.perv ---&gt; e\nperve ---&gt; e\nervee ---&gt; n\nrveen ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; e\nveere ---&gt; n\neeren ---&gt; d\nerend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; s\n..jis ---&gt; u\n.jisu ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; w\n.ashw ---&gt; i\nashwi ---&gt; n\nshwin ---&gt; i\nhwini ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; l\nsukhl ---&gt; a\nukhla ---&gt; l\nkhlal ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; a\n.suma ---&gt; n\nsuman ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; t\nmanit ---&gt; a\nanita ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; d\navind ---&gt; r\nvindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; s\n.mars ---&gt; h\nmarsh ---&gt; i\narshi ---&gt; l\nrshil ---&gt; a\nshila ---&gt; h\nhilah ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; z\n..faz ---&gt; u\n.fazu ---&gt; l\nfazul ---&gt; l\nazull ---&gt; a\nzulla ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; r\n.ompr ---&gt; a\nompra ---&gt; k\nmprak ---&gt; e\nprake ---&gt; s\nrakes ---&gt; h\nakesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; b\n...kb ---&gt; e\n..kbe ---&gt; t\n.kbet ---&gt; a\nkbeta ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; i\n.rasi ---&gt; d\nrasid ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; v\nmahav ---&gt; e\nahave ---&gt; e\nhavee ---&gt; r\naveer ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; i\nprami ---&gt; l\nramil ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; o\n.samo ---&gt; n\nsamon ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; o\n.mano ---&gt; r\nmanor ---&gt; m\nanorm ---&gt; a\nnorma ---&gt; .\n..... ---&gt; f\n....f ---&gt; h\n...fh ---&gt; a\n..fha ---&gt; i\n.fhai ---&gt; s\nfhais ---&gt; h\nhaish ---&gt; a\naisha ---&gt; l\nishal ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; a\n.nana ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; a\n.meha ---&gt; n\nmehan ---&gt; d\nehand ---&gt; i\nhandi ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; u\n.anku ---&gt; s\nankus ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; i\n.saji ---&gt; y\nsajiy ---&gt; a\najiya ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; n\n.bann ---&gt; u\nbannu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; e\n.jame ---&gt; e\njamee ---&gt; l\nameel ---&gt; a\nmeela ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; l\n..mil ---&gt; k\n.milk ---&gt; a\nmilka ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; t\nsamit ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; m\n..yam ---&gt; i\n.yami ---&gt; n\nyamin ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; d\n.fird ---&gt; o\nfirdo ---&gt; u\nirdou ---&gt; s\nrdous ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; j\n.rajj ---&gt; i\nrajji ---&gt; t\najjit ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; h\n.push ---&gt; a\npusha ---&gt; p\nushap ---&gt; a\nshapa ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; m\n..aam ---&gt; o\n.aamo ---&gt; s\naamos ---&gt; h\namosh ---&gt; .\n..... ---&gt; o\n....o ---&gt; p\n...op ---&gt; e\n..ope ---&gt; n\n.open ---&gt; d\nopend ---&gt; a\npenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; o\n.mano ---&gt; i\nmanoi ---&gt; s\nanois ---&gt; h\nnoish ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; m\nmoham ---&gt; a\nohama ---&gt; d\nhamad ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; h\n.mith ---&gt; i\nmithi ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; i\n..vai ---&gt; s\n.vais ---&gt; h\nvaish ---&gt; a\naisha ---&gt; l\nishal ---&gt; i\nshali ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; h\n..arh ---&gt; a\n.arha ---&gt; m\narham ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; n\n.mohn ---&gt; i\nmohni ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; m\n.janm ---&gt; e\njanme ---&gt; s\nanmes ---&gt; h\nnmesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; t\n.munt ---&gt; i\nmunti ---&gt; y\nuntiy ---&gt; a\nntiya ---&gt; j\ntiyaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; s\n.shus ---&gt; h\nshush ---&gt; i\nhushi ---&gt; l\nushil ---&gt; a\nshila ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; r\nravir ---&gt; a\navira ---&gt; j\nviraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; a\n.soha ---&gt; n\nsohan ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; v\n.bhav ---&gt; n\nbhavn ---&gt; a\nhavna ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; i\n.bali ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; f\n..sof ---&gt; a\n.sofa ---&gt; l\nsofal ---&gt; i\nofali ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; v\n..kav ---&gt; i\n.kavi ---&gt; t\nkavit ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; b\n.narb ---&gt; i\nnarbi ---&gt; r\narbir ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; v\n.harv ---&gt; a\nharva ---&gt; n\narvan ---&gt; s\nrvans ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; h\n.sudh ---&gt; a\nsudha ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; i\n.niti ---&gt; n\nnitin ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; d\n.kund ---&gt; a\nkunda ---&gt; n\nundan ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; x\n..lax ---&gt; m\n.laxm ---&gt; i\nlaxmi ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; k\n..rik ---&gt; k\n.rikk ---&gt; e\nrikke ---&gt; n\nikken ---&gt; c\nkkenc ---&gt; h\nkench ---&gt; i\nenchi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; p\n.harp ---&gt; r\nharpr ---&gt; e\narpre ---&gt; e\nrpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; l\n.heml ---&gt; a\nhemla ---&gt; t\nemlat ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; i\n..mai ---&gt; n\n.main ---&gt; k\nmaink ---&gt; a\nainka ---&gt; .\n..... ---&gt; l\n....l ---&gt; t\n...lt ---&gt; a\n..lta ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; t\n.pret ---&gt; e\nprete ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; m\n..nim ---&gt; e\n.nime ---&gt; s\nnimes ---&gt; h\nimesh ---&gt; .\n..... ---&gt; n\n....n ---&gt; w\n...nw ---&gt; e\n..nwe ---&gt; d\n.nwed ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; n\n.mann ---&gt; u\nmannu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; i\n.mahi ---&gt; n\nmahin ---&gt; d\nahind ---&gt; r\nhindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; e\n.sure ---&gt; n\nsuren ---&gt; d\nurend ---&gt; e\nrende ---&gt; r\nender ---&gt; a\nndera ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; u\n..ghu ---&gt; r\n.ghur ---&gt; u\nghuru ---&gt; l\nhurul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; h\nshadh ---&gt; a\nhadha ---&gt; n\nadhan ---&gt; a\ndhana ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; g\n..bag ---&gt; w\n.bagw ---&gt; a\nbagwa ---&gt; n\nagwan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; m\n.najm ---&gt; i\nnajmi ---&gt; n\najmin ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; a\ndhara ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; k\n..tik ---&gt; k\n.tikk ---&gt; u\ntikku ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; w\n.kalw ---&gt; a\nkalwa ---&gt; t\nalwat ---&gt; i\nlwati ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; a\n.ompa ---&gt; r\nompar ---&gt; k\nmpark ---&gt; a\nparka ---&gt; s\narkas ---&gt; h\nrkash ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; h\ngulsh ---&gt; a\nulsha ---&gt; n\nlshan ---&gt; a\nshana ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; a\nfarha ---&gt; n\narhan ---&gt; a\nrhana ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; i\nshaki ---&gt; l\nhakil ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; e\nmange ---&gt; r\nanger ---&gt; a\nngera ---&gt; m\ngeram ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; h\nparth ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; r\n..for ---&gt; .\n..... ---&gt; n\n....n ---&gt; t\n...nt ---&gt; a\n..nta ---&gt; s\n.ntas ---&gt; h\nntash ---&gt; a\ntasha ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; l\n..mul ---&gt; a\n.mula ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; e\n.jane ---&gt; s\njanes ---&gt; h\nanesh ---&gt; w\nneshw ---&gt; a\neshwa ---&gt; r\nshwar ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; i\n.vasi ---&gt; y\nvasiy ---&gt; a\nasiya ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; t\n.asht ---&gt; h\nashth ---&gt; a\nshtha ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; b\n..bob ---&gt; b\n.bobb ---&gt; y\nbobby ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; s\n..res ---&gt; h\n.resh ---&gt; m\nreshm ---&gt; i\neshmi ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; m\n.cham ---&gt; a\nchama ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; m\n.chom ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; h\n..reh ---&gt; a\n.reha ---&gt; n\nrehan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; e\n.sale ---&gt; e\nsalee ---&gt; m\naleem ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; i\n.sagi ---&gt; t\nsagit ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; d\nrajid ---&gt; e\najide ---&gt; r\njider ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; r\nmanir ---&gt; a\nanira ---&gt; m\nniram ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; s\n..mos ---&gt; m\n.mosm ---&gt; i\nmosmi ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; o\n..jio ---&gt; t\n.jiot ---&gt; y\njioty ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; p\n.harp ---&gt; a\nharpa ---&gt; l\narpal ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; b\n.rubb ---&gt; i\nrubbi ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; t\n.munt ---&gt; r\nmuntr ---&gt; i\nuntri ---&gt; n\nntrin ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; s\nharis ---&gt; o\nariso ---&gt; n\nrison ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; o\n.phoo ---&gt; l\nphool ---&gt; i\nhooli ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; a\n.suga ---&gt; n\nsugan ---&gt; a\nugana ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; u\n.anuu ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; s\n.jees ---&gt; a\njeesa ---&gt; n\neesan ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; e\n.anke ---&gt; s\nankes ---&gt; h\nnkesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; w\nhahnw ---&gt; a\nahnwa ---&gt; j\nhnwaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; m\n.shum ---&gt; i\nshumi ---&gt; t\nhumit ---&gt; a\numita ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; m\n..nam ---&gt; a\n.nama ---&gt; r\nnamar ---&gt; t\namart ---&gt; a\nmarta ---&gt; .\n..... ---&gt; a\n....a ---&gt; y\n...ay ---&gt; u\n..ayu ---&gt; b\n.ayub ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; a\n.vika ---&gt; s\nvikas ---&gt; .\n..... ---&gt; n\n....n ---&gt; r\n...nr ---&gt; e\n..nre ---&gt; n\n.nren ---&gt; d\nnrend ---&gt; r\nrendr ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; c\n.sakc ---&gt; h\nsakch ---&gt; a\nakcha ---&gt; m\nkcham ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; s\n.khas ---&gt; h\nkhash ---&gt; b\nhashb ---&gt; u\nashbu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; p\n.ramp ---&gt; a\nrampa ---&gt; l\nampal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; j\nshivj ---&gt; i\nhivji ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; v\n..arv ---&gt; i\n.arvi ---&gt; n\narvin ---&gt; d\nrvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; z\n..foz ---&gt; i\n.fozi ---&gt; a\nfozia ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; u\n.kalu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; s\n.sars ---&gt; h\nsarsh ---&gt; w\narshw ---&gt; a\nrshwa ---&gt; t\nshwat ---&gt; i\nhwati ---&gt; .\n..... ---&gt; i\n....i ---&gt; q\n...iq ---&gt; a\n..iqa ---&gt; r\n.iqar ---&gt; a\niqara ---&gt; r\nqarar ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; i\n..swi ---&gt; t\n.swit ---&gt; i\nswiti ---&gt; .\n..... ---&gt; j\n....j ---&gt; w\n...jw ---&gt; a\n..jwa ---&gt; l\n.jwal ---&gt; a\njwala ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; a\n.shya ---&gt; m\nshyam ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; h\n.hash ---&gt; i\nhashi ---&gt; m\nashim ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; r\n.najr ---&gt; e\nnajre ---&gt; e\najree ---&gt; n\njreen ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; e\n.shre ---&gt; e\nshree ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; n\n.chin ---&gt; k\nchink ---&gt; u\nhinku ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; h\nmansh ---&gt; a\nansha ---&gt; r\nnshar ---&gt; a\nshara ---&gt; m\nharam ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; e\n.rupe ---&gt; n\nrupen ---&gt; d\nupend ---&gt; r\npendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; b\n..kab ---&gt; i\n.kabi ---&gt; r\nkabir ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; n\n..arn ---&gt; a\n.arna ---&gt; b\narnab ---&gt; j\nrnabj ---&gt; i\nnabji ---&gt; t\nabjit ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; e\n..ume ---&gt; s\n.umes ---&gt; h\numesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; y\n.shoy ---&gt; a\nshoya ---&gt; b\nhoyab ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; l\n.mall ---&gt; i\nmalli ---&gt; k\nallik ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; p\n.manp ---&gt; h\nmanph ---&gt; o\nanpho ---&gt; o\nnphoo ---&gt; l\nphool ---&gt; .\n..... ---&gt; z\n....z ---&gt; o\n...zo ---&gt; y\n..zoy ---&gt; a\n.zoya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; r\n.sayr ---&gt; a\nsayra ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; m\nbharm ---&gt; a\nharma ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; t\n.must ---&gt; a\nmusta ---&gt; f\nustaf ---&gt; a\nstafa ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; n\n.deen ---&gt; d\ndeend ---&gt; y\neendy ---&gt; a\nendya ---&gt; l\nndyal ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; r\n.kumr ---&gt; e\nkumre ---&gt; s\numres ---&gt; h\nmresh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; i\n.mahi ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; d\n.bhud ---&gt; e\nbhude ---&gt; v\nhudev ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; k\n..huk ---&gt; a\n.huka ---&gt; m\nhukam ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; b\n..dib ---&gt; y\n.diby ---&gt; a\ndibya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; g\n..nag ---&gt; a\n.naga ---&gt; j\nnagaj ---&gt; i\nagaji ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; i\n.tari ---&gt; q\ntariq ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; o\n.kiso ---&gt; r\nkisor ---&gt; i\nisori ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; u\nanchu ---&gt; r\nnchur ---&gt; i\nchuri ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; a\n.naja ---&gt; r\nnajar ---&gt; a\najara ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; a\n.papa ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; s\n.nees ---&gt; h\nneesh ---&gt; a\neesha ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; m\n.birm ---&gt; a\nbirma ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; r\n..ner ---&gt; a\n.nera ---&gt; j\nneraj ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; y\n.chay ---&gt; a\nchaya ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; j\n.anuj ---&gt; a\nanuja ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; a\n..tra ---&gt; n\n.tran ---&gt; o\ntrano ---&gt; o\nranoo ---&gt; m\nanoom ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; r\n.bhar ---&gt; t\nbhart ---&gt; i\nharti ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; s\n..tos ---&gt; h\n.tosh ---&gt; a\ntosha ---&gt; r\noshar ---&gt; .\n..... ---&gt; u\n....u ---&gt; p\n...up ---&gt; a\n..upa ---&gt; s\n.upas ---&gt; n\nupasn ---&gt; a\npasna ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; i\n.susi ---&gt; l\nsusil ---&gt; a\nusila ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; r\n.ompr ---&gt; k\nomprk ---&gt; e\nmprke ---&gt; s\nprkes ---&gt; h\nrkesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; i\nshahi ---&gt; b\nhahib ---&gt; a\nahiba ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; v\nmadhv ---&gt; i\nadhvi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; u\n..sau ---&gt; n\n.saun ---&gt; i\nsauni ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; t\n.reet ---&gt; i\nreeti ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; h\n..tah ---&gt; s\n.tahs ---&gt; e\ntahse ---&gt; e\nahsee ---&gt; n\nhseen ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; c\n..ruc ---&gt; h\n.ruch ---&gt; i\nruchi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; n\n.jain ---&gt; a\njaina ---&gt; f\nainaf ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; i\n.lili ---&gt; m\nlilim ---&gt; a\nilima ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; z\n.prez ---&gt; i\nprezi ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; b\n..dab ---&gt; b\n.dabb ---&gt; u\ndabbu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; e\n.sate ---&gt; n\nsaten ---&gt; d\natend ---&gt; e\ntende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; r\n.sher ---&gt; u\nsheru ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; m\n.jagm ---&gt; o\njagmo ---&gt; h\nagmoh ---&gt; a\ngmoha ---&gt; n\nmohan ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; a\n.mina ---&gt; k\nminak ---&gt; c\ninakc ---&gt; h\nnakch ---&gt; i\nakchi ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; e\n.mahe ---&gt; n\nmahen ---&gt; d\nahend ---&gt; r\nhendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; p\n..gop ---&gt; e\n.gope ---&gt; s\ngopes ---&gt; h\nopesh ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; i\n.nasi ---&gt; r\nnasir ---&gt; a\nasira ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; j\n.anaj ---&gt; i\nanaji ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; r\n.hemr ---&gt; a\nhemra ---&gt; j\nemraj ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; u\n.papu ---&gt; u\npapuu ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; l\n..mil ---&gt; a\n.mila ---&gt; n\nmilan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; r\n.shir ---&gt; i\nshiri ---&gt; s\nhiris ---&gt; h\nirish ---&gt; t\nrisht ---&gt; y\nishty ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; n\nsahan ---&gt; a\nahana ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; i\n..bai ---&gt; c\n.baic ---&gt; h\nbaich ---&gt; a\naicha ---&gt; n\nichan ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; d\n.bind ---&gt; u\nbindu ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; u\n..amu ---&gt; c\n.amuc ---&gt; h\namuch ---&gt; l\nmuchl ---&gt; a\nuchla ---&gt; .\n..... ---&gt; u\n....u ---&gt; j\n...uj ---&gt; j\n..ujj ---&gt; a\n.ujja ---&gt; i\nujjai ---&gt; r\njjair ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; a\nnoora ---&gt; l\nooral ---&gt; a\norala ---&gt; m\nralam ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; r\n.anir ---&gt; u\naniru ---&gt; d\nnirud ---&gt; h\nirudh ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; n\n..avn ---&gt; i\n.avni ---&gt; t\navnit ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; s\n.anus ---&gt; k\nanusk ---&gt; a\nnuska ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; i\ndeepi ---&gt; k\neepik ---&gt; a\nepika ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; t\n.roht ---&gt; a\nrohta ---&gt; s\nohtas ---&gt; h\nhtash ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; b\n.tabb ---&gt; u\ntabbu ---&gt; s\nabbus ---&gt; u\nbbusu ---&gt; m\nbusum ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; n\n.veen ---&gt; u\nveenu ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; t\n.neet ---&gt; i\nneeti ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; t\n.mast ---&gt; a\nmasta ---&gt; n\nastan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; i\n.sali ---&gt; m\nsalim ---&gt; a\nalima ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; b\n.manb ---&gt; a\nmanba ---&gt; i\nanbai ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; u\n.naru ---&gt; r\nnarur ---&gt; a\narura ---&gt; m\nruram ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; c\n.manc ---&gt; h\nmanch ---&gt; a\nancha ---&gt; n\nnchan ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; r\n..for ---&gt; n\n.forn ---&gt; t\nfornt ---&gt; a\nornta ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; j\n..tej ---&gt; p\n.tejp ---&gt; a\ntejpa ---&gt; l\nejpal ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; b\n.rubb ---&gt; y\nrubby ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; k\n..bak ---&gt; h\n.bakh ---&gt; t\nbakht ---&gt; v\nakhtv ---&gt; a\nkhtva ---&gt; a\nhtvaa ---&gt; r\ntvaar ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; a\n.kama ---&gt; r\nkamar ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; e\n.fare ---&gt; e\nfaree ---&gt; n\nareen ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; b\n..shb ---&gt; a\n.shba ---&gt; n\nshban ---&gt; a\nhbana ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; i\n.sami ---&gt; k\nsamik ---&gt; s\namiks ---&gt; a\nmiksa ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; v\n.ranv ---&gt; i\nranvi ---&gt; r\nanvir ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; a\n.raja ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; k\n..sik ---&gt; e\n.sike ---&gt; n\nsiken ---&gt; d\nikend ---&gt; e\nkende ---&gt; r\nender ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; d\n.pard ---&gt; h\npardh ---&gt; u\nardhu ---&gt; m\nrdhum ---&gt; a\ndhuma ---&gt; n\nhuman ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; r\n.deer ---&gt; a\ndeera ---&gt; j\neeraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; k\n..mak ---&gt; a\n.maka ---&gt; n\nmakan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; t\n..nat ---&gt; a\n.nata ---&gt; s\nnatas ---&gt; h\natash ---&gt; a\ntasha ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; p\n.roop ---&gt; a\nroopa ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; r\n..ter ---&gt; e\n.tere ---&gt; n\nteren ---&gt; c\nerenc ---&gt; e\nrence ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; t\n.mamt ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; l\npreml ---&gt; a\nremla ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; y\n.kosy ---&gt; l\nkosyl ---&gt; a\nosyla ---&gt; y\nsylay ---&gt; a\nylaya ---&gt; .\n..... ---&gt; z\n....z ---&gt; u\n...zu ---&gt; v\n..zuv ---&gt; e\n.zuve ---&gt; b\nzuveb ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; j\n.sarj ---&gt; a\nsarja ---&gt; h\narjah ---&gt; a\nrjaha ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; c\n.balc ---&gt; h\nbalch ---&gt; a\nalcha ---&gt; n\nlchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; v\n.jasv ---&gt; e\njasve ---&gt; e\nasvee ---&gt; r\nsveer ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; h\n..toh ---&gt; i\n.tohi ---&gt; r\ntohir ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; j\n..maj ---&gt; i\n.maji ---&gt; d\nmajid ---&gt; a\najida ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; a\ncheta ---&gt; n\nhetan ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; i\n.guli ---&gt; s\ngulis ---&gt; t\nulist ---&gt; a\nlista ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; f\n..naf ---&gt; i\n.nafi ---&gt; s\nnafis ---&gt; a\nafisa ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; a\n.mada ---&gt; n\nmadan ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; r\nabhir ---&gt; a\nbhira ---&gt; j\nhiraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; u\nmadhu ---&gt; r\nadhur ---&gt; i\ndhuri ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; a\n.mama ---&gt; n\nmaman ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; r\n..dir ---&gt; g\n.dirg ---&gt; a\ndirga ---&gt; j\nirgaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; d\n.sund ---&gt; a\nsunda ---&gt; r\nundar ---&gt; i\nndari ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; a\n..uda ---&gt; i\n.udai ---&gt; v\nudaiv ---&gt; e\ndaive ---&gt; e\naivee ---&gt; r\niveer ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; r\n.yogr ---&gt; a\nyogra ---&gt; j\nograj ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; s\nharis ---&gt; h\narish ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; o\n..soo ---&gt; n\n.soon ---&gt; a\nsoona ---&gt; m\noonam ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; s\n.muns ---&gt; h\nmunsh ---&gt; i\nunshi ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; a\n..kra ---&gt; n\n.kran ---&gt; t\nkrant ---&gt; i\nranti ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; i\n..mai ---&gt; g\n.maig ---&gt; o\nmaigo ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; n\n.anjn ---&gt; a\nanjna ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; n\n..sin ---&gt; t\n.sint ---&gt; u\nsintu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; s\nmanis ---&gt; a\nanisa ---&gt; .\n..... ---&gt; z\n....z ---&gt; i\n...zi ---&gt; s\n..zis ---&gt; h\n.zish ---&gt; a\nzisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; y\n..day ---&gt; a\n.daya ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; n\n.aman ---&gt; j\namanj ---&gt; e\nmanje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; n\n.meen ---&gt; u\nmeenu ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; a\naasha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; e\n.nare ---&gt; s\nnares ---&gt; h\naresh ---&gt; p\nreshp ---&gt; a\neshpa ---&gt; l\nshpal ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; b\n.sehb ---&gt; a\nsehba ---&gt; j\nehbaj ---&gt; .\n..... ---&gt; h\n....h ---&gt; r\n...hr ---&gt; i\n..hri ---&gt; t\n.hrit ---&gt; i\nhriti ---&gt; k\nritik ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; e\n..hee ---&gt; r\n.heer ---&gt; a\nheera ---&gt; l\neeral ---&gt; a\nerala ---&gt; l\nralal ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; y\n..soy ---&gt; a\n.soya ---&gt; m\nsoyam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; r\n.sagr ---&gt; i\nsagri ---&gt; k\nagrik ---&gt; a\ngrika ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; i\n.riti ---&gt; k\nritik ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; c\n..nic ---&gt; h\n.nich ---&gt; a\nnicha ---&gt; r\nichar ---&gt; o\ncharo ---&gt; n\nharon ---&gt; .\n..... ---&gt; i\n....i ---&gt; r\n...ir ---&gt; f\n..irf ---&gt; a\n.irfa ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; w\n..naw ---&gt; a\n.nawa ---&gt; l\nnawal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; i\nbhani ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; j\n..omj ---&gt; o\n.omjo ---&gt; n\nomjon ---&gt; e\nmjone ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; r\n..jor ---&gt; i\n.jori ---&gt; a\njoria ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; n\n.mann ---&gt; u\nmannu ---&gt; l\nannul ---&gt; a\nnnula ---&gt; l\nnulal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; v\n.sarv ---&gt; a\nsarva ---&gt; n\narvan ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; h\n.hash ---&gt; r\nhashr ---&gt; a\nashra ---&gt; t\nshrat ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; r\n.kesr ---&gt; i\nkesri ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; n\n.guln ---&gt; a\ngulna ---&gt; z\nulnaz ---&gt; a\nlnaza ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; a\n.rana ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; t\nchhat ---&gt; a\nhhata ---&gt; r\nhatar ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; u\nnandu ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; o\n.biro ---&gt; n\nbiron ---&gt; i\nironi ---&gt; c\nronic ---&gt; a\nonica ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; w\n..gaw ---&gt; a\n.gawa ---&gt; l\ngawal ---&gt; i\nawali ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; e\n.bale ---&gt; s\nbales ---&gt; h\nalesh ---&gt; w\nleshw ---&gt; e\neshwe ---&gt; r\nshwer ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; r\n.mahr ---&gt; a\nmahra ---&gt; j\nahraj ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; w\n..dow ---&gt; l\n.dowl ---&gt; a\ndowla ---&gt; t\nowlat ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; k\n..omk ---&gt; a\n.omka ---&gt; r\nomkar ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; p\n..kup ---&gt; a\n.kupa ---&gt; r\nkupar ---&gt; t\nupart ---&gt; h\nparth ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; r\n..tir ---&gt; l\n.tirl ---&gt; o\ntirlo ---&gt; k\nirlok ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; b\nsahab ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; b\n.somb ---&gt; i\nsombi ---&gt; r\nombir ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; i\nshadi ---&gt; y\nhadiy ---&gt; a\nadiya ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; k\n.jank ---&gt; i\njanki ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; f\n.ashf ---&gt; a\nashfa ---&gt; q\nshfaq ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; g\n.sang ---&gt; e\nsange ---&gt; e\nangee ---&gt; t\nngeet ---&gt; a\ngeeta ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; i\n.aasi ---&gt; s\naasis ---&gt; h\nasish ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; n\n..men ---&gt; a\n.mena ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; a\nkesha ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; l\n.murl ---&gt; i\nmurli ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; y\n..day ---&gt; a\n.daya ---&gt; l\ndayal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; i\n.shei ---&gt; k\nsheik ---&gt; h\nheikh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; w\n.satw ---&gt; a\nsatwa ---&gt; n\natwan ---&gt; d\ntwand ---&gt; e\nwande ---&gt; r\nander ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; e\n.shee ---&gt; n\nsheen ---&gt; u\nheenu ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; r\n..pir ---&gt; o\n.piro ---&gt; j\npiroj ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; i\n..vai ---&gt; s\n.vais ---&gt; h\nvaish ---&gt; a\naisha ---&gt; n\nishan ---&gt; v\nshanv ---&gt; i\nhanvi ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; w\n.manw ---&gt; i\nmanwi ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; s\n..bis ---&gt; h\n.bish ---&gt; n\nbishn ---&gt; u\nishnu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; w\n..kaw ---&gt; i\n.kawi ---&gt; t\nkawit ---&gt; a\nawita ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; d\n.dild ---&gt; a\ndilda ---&gt; r\nildar ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; a\n.kuma ---&gt; i\nkumai ---&gt; l\numail ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; j\n..bij ---&gt; a\n.bija ---&gt; l\nbijal ---&gt; i\nijali ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; j\n.dilj ---&gt; a\ndilja ---&gt; n\niljan ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; l\n..uml ---&gt; e\n.umle ---&gt; s\numles ---&gt; h\nmlesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; t\n..mat ---&gt; b\n.matb ---&gt; a\nmatba ---&gt; r\natbar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; l\n..shl ---&gt; e\n.shle ---&gt; n\nshlen ---&gt; d\nhlend ---&gt; e\nlende ---&gt; r\nender ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; i\nkushi ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; i\n..gai ---&gt; t\n.gait ---&gt; r\ngaitr ---&gt; i\naitri ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; a\n..ima ---&gt; m\n.imam ---&gt; u\nimamu ---&gt; d\nmamud ---&gt; d\namudd ---&gt; i\nmuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; m\n.tasm ---&gt; i\ntasmi ---&gt; n\nasmin ---&gt; a\nsmina ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; s\n..bis ---&gt; h\n.bish ---&gt; e\nbishe ---&gt; s\nishes ---&gt; h\nshesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; v\nsukhv ---&gt; e\nukhve ---&gt; e\nkhvee ---&gt; r\nhveer ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; g\n..nig ---&gt; a\n.niga ---&gt; r\nnigar ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; u\n.anku ---&gt; r\nankur ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; r\n..nur ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; d\n.said ---&gt; a\nsaida ---&gt; s\naidas ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; n\n..pin ---&gt; t\n.pint ---&gt; u\npintu ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; l\n..mil ---&gt; a\n.mila ---&gt; p\nmilap ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; w\n..anw ---&gt; a\n.anwa ---&gt; r\nanwar ---&gt; i\nnwari ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; k\n..dak ---&gt; s\n.daks ---&gt; h\ndaksh ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; k\n..dik ---&gt; s\n.diks ---&gt; h\ndiksh ---&gt; a\niksha ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; i\n.suri ---&gt; t\nsurit ---&gt; i\nuriti ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; j\nshahj ---&gt; a\nhahja ---&gt; d\nahjad ---&gt; i\nhjadi ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; w\n..siw ---&gt; a\n.siwa ---&gt; n\nsiwan ---&gt; i\niwani ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; l\n..tul ---&gt; c\n.tulc ---&gt; h\ntulch ---&gt; a\nulcha ---&gt; r\nlchar ---&gt; a\nchara ---&gt; m\nharam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; u\n..sau ---&gt; r\n.saur ---&gt; a\nsaura ---&gt; b\naurab ---&gt; h\nurabh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; a\nmansa ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; n\n.sehn ---&gt; a\nsehna ---&gt; j\nehnaj ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; d\n.sidd ---&gt; a\nsidda ---&gt; r\niddar ---&gt; t\nddart ---&gt; h\ndarth ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; a\n.jama ---&gt; l\njamal ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; e\nharme ---&gt; n\narmen ---&gt; d\nrmend ---&gt; r\nmendr ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; k\n.mank ---&gt; u\nmanku ---&gt; r\nankur ---&gt; i\nnkuri ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; d\n..ked ---&gt; a\n.keda ---&gt; r\nkedar ---&gt; n\nedarn ---&gt; a\ndarna ---&gt; t\narnat ---&gt; h\nrnath ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; e\n..tre ---&gt; p\n.trep ---&gt; t\ntrept ---&gt; i\nrepti ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; r\n..var ---&gt; s\n.vars ---&gt; h\nvarsh ---&gt; a\narsha ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; t\n..aat ---&gt; i\n.aati ---&gt; r\naatir ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; h\n.mukh ---&gt; t\nmukht ---&gt; y\nukhty ---&gt; a\nkhtya ---&gt; r\nhtyar ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; n\nmohan ---&gt; i\nohani ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; y\nakshy ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; u\n.kusu ---&gt; m\nkusum ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; u\n..nau ---&gt; l\n.naul ---&gt; e\nnaule ---&gt; s\naules ---&gt; h\nulesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; g\nbhagg ---&gt; o\nhaggo ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; l\n..pul ---&gt; k\n.pulk ---&gt; i\npulki ---&gt; t\nulkit ---&gt; .\n..... ---&gt; s\n....s ---&gt; r\n...sr ---&gt; i\n..sri ---&gt; n\n.srin ---&gt; i\nsrini ---&gt; w\nriniw ---&gt; a\niniwa ---&gt; s\nniwas ---&gt; h\niwash ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; p\n..kap ---&gt; i\n.kapi ---&gt; l\nkapil ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; b\n..nab ---&gt; i\n.nabi ---&gt; j\nnabij ---&gt; a\nabija ---&gt; n\nbijan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; b\nmahab ---&gt; i\nahabi ---&gt; r\nhabir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; d\n.sahd ---&gt; e\nsahde ---&gt; v\nahdev ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; s\n.shis ---&gt; h\nshish ---&gt; p\nhishp ---&gt; a\nishpa ---&gt; l\nshpal ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; a\n.vira ---&gt; m\nviram ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; h\n.kanh ---&gt; a\nkanha ---&gt; y\nanhay ---&gt; a\nnhaya ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; p\n.krip ---&gt; a\nkripa ---&gt; l\nripal ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; d\n.pand ---&gt; i\npandi ---&gt; t\nandit ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; k\n..lek ---&gt; h\n.lekh ---&gt; r\nlekhr ---&gt; a\nekhra ---&gt; m\nkhram ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; z\n..faz ---&gt; a\n.faza ---&gt; z\nfazaz ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; k\n.lakk ---&gt; i\nlakki ---&gt; .\n..... ---&gt; u\n....u ---&gt; z\n...uz ---&gt; m\n..uzm ---&gt; a\n.uzma ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; n\n.meen ---&gt; a\nmeena ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; i\n.saki ---&gt; r\nsakir ---&gt; a\nakira ---&gt; n\nkiran ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; a\n.sawa ---&gt; t\nsawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; t\n.seet ---&gt; a\nseeta ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; a\nrisha ---&gt; m\nisham ---&gt; a\nshama ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; n\n.hann ---&gt; i\nhanni ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; a\n.rama ---&gt; v\nramav ---&gt; t\namavt ---&gt; a\nmavta ---&gt; r\navtar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; i\n.sali ---&gt; n\nsalin ---&gt; .\n..... ---&gt; r\n....r ---&gt; v\n...rv ---&gt; i\n..rvi ---&gt; n\n.rvin ---&gt; d\nrvind ---&gt; r\nvindr ---&gt; a\nindra ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; r\n.indr ---&gt; a\nindra ---&gt; j\nndraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; r\n..mer ---&gt; r\n.merr ---&gt; y\nmerry ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; b\n.balb ---&gt; i\nbalbi ---&gt; r\nalbir ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; e\n.nare ---&gt; n\nnaren ---&gt; d\narend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; s\n.jays ---&gt; h\njaysh ---&gt; i\nayshi ---&gt; n\nyshin ---&gt; g\nshing ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; s\n..mis ---&gt; r\n.misr ---&gt; i\nmisri ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; a\nprema ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; r\nsarar ---&gt; w\nararw ---&gt; a\nrarwa ---&gt; t\narwat ---&gt; i\nrwati ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; b\n..amb ---&gt; i\n.ambi ---&gt; y\nambiy ---&gt; a\nmbiya ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; k\n.jayk ---&gt; u\njayku ---&gt; m\naykum ---&gt; a\nykuma ---&gt; r\nkumar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; o\nancho ---&gt; o\nnchoo ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; i\n.rubi ---&gt; n\nrubin ---&gt; a\nubina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; a\nsanta ---&gt; r\nantar ---&gt; a\nntara ---&gt; .\n..... ---&gt; s\n....s ---&gt; r\n...sr ---&gt; i\n..sri ---&gt; m\n.srim ---&gt; a\nsrima ---&gt; t\nrimat ---&gt; i\nimati ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; s\n..jis ---&gt; h\n.jish ---&gt; a\njisha ---&gt; n\nishan ---&gt; t\nshant ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; d\n.band ---&gt; a\nbanda ---&gt; n\nandan ---&gt; a\nndana ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; i\n.akhi ---&gt; l\nakhil ---&gt; e\nkhile ---&gt; s\nhiles ---&gt; h\nilesh ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; v\n..lov ---&gt; e\n.love ---&gt; l\nlovel ---&gt; y\novely ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; n\n.maan ---&gt; s\nmaans ---&gt; i\naansi ---&gt; n\nansin ---&gt; g\nnsing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; n\nshabn ---&gt; u\nhabnu ---&gt; r\nabnur ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; a\nsatya ---&gt; m\natyam ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; b\n.khub ---&gt; c\nkhubc ---&gt; h\nhubch ---&gt; a\nubcha ---&gt; n\nbchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; k\n.jank ---&gt; i\njanki ---&gt; d\nankid ---&gt; a\nnkida ---&gt; s\nkidas ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; u\n.taru ---&gt; n\ntarun ---&gt; n\narunn ---&gt; a\nrunna ---&gt; m\nunnam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; d\n.sard ---&gt; h\nsardh ---&gt; u\nardhu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; c\n..suc ---&gt; h\n.such ---&gt; i\nsuchi ---&gt; t\nuchit ---&gt; r\nchitr ---&gt; a\nhitra ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; i\nveeri ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; e\n.rube ---&gt; e\nrubee ---&gt; n\nubeen ---&gt; a\nbeena ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; t\n..pat ---&gt; i\n.pati ---&gt; k\npatik ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; i\n..dhi ---&gt; r\n.dhir ---&gt; e\ndhire ---&gt; n\nhiren ---&gt; d\nirend ---&gt; e\nrende ---&gt; r\nender ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; a\n..asa ---&gt; r\n.asar ---&gt; f\nasarf ---&gt; i\nsarfi ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; e\n.pree ---&gt; t\npreet ---&gt; y\nreety ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; o\nshabo ---&gt; o\nhaboo ---&gt; b\naboob ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; l\n.jasl ---&gt; i\njasli ---&gt; n\naslin ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; j\n.murj ---&gt; i\nmurji ---&gt; n\nurjin ---&gt; a\nrjina ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; i\n.kari ---&gt; s\nkaris ---&gt; h\narish ---&gt; a\nrisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; o\n..dho ---&gt; l\n.dhol ---&gt; a\ndhola ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; a\n.rama ---&gt; k\nramak ---&gt; a\namaka ---&gt; n\nmakan ---&gt; t\nakant ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; l\n..hal ---&gt; i\n.hali ---&gt; y\nhaliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; b\n.bhab ---&gt; h\nbhabh ---&gt; i\nhabhi ---&gt; y\nabhiy ---&gt; a\nbhiya ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; i\n..ari ---&gt; b\n.arib ---&gt; a\nariba ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; s\nabhis ---&gt; h\nbhish ---&gt; e\nhishe ---&gt; k\nishek ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; i\nmangi ---&gt; l\nangil ---&gt; a\nngila ---&gt; l\ngilal ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; d\n.guld ---&gt; a\ngulda ---&gt; s\nuldas ---&gt; t\nldast ---&gt; a\ndasta ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; b\n..mob ---&gt; s\n.mobs ---&gt; h\nmobsh ---&gt; e\nobshe ---&gt; e\nbshee ---&gt; r\nsheer ---&gt; a\nheera ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; d\n..sod ---&gt; a\n.soda ---&gt; n\nsodan ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; p\n..sup ---&gt; y\n.supy ---&gt; a\nsupya ---&gt; r\nupyar ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; y\n..riy ---&gt; a\n.riya ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; s\n..was ---&gt; i\n.wasi ---&gt; m\nwasim ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; e\n.nare ---&gt; s\nnares ---&gt; h\naresh ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; e\n.pare ---&gt; e\nparee ---&gt; n\nareen ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; y\n.shay ---&gt; m\nshaym ---&gt; a\nhayma ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; e\n.vire ---&gt; n\nviren ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; z\n..waz ---&gt; i\n.wazi ---&gt; d\nwazid ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; i\n.suji ---&gt; t\nsujit ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; r\n.samr ---&gt; e\nsamre ---&gt; e\namree ---&gt; n\nmreen ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; s\n..das ---&gt; h\n.dash ---&gt; a\ndasha ---&gt; r\nashar ---&gt; a\nshara ---&gt; t\nharat ---&gt; h\narath ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; a\n.mina ---&gt; k\nminak ---&gt; x\ninakx ---&gt; i\nnakxi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; o\n.salo ---&gt; m\nsalom ---&gt; i\nalomi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; l\nshail ---&gt; a\nhaila ---&gt; s\nailas ---&gt; h\nilash ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; d\n..kad ---&gt; u\n.kadu ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; r\n.indr ---&gt; e\nindre ---&gt; s\nndres ---&gt; h\ndresh ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; n\n.bhon ---&gt; u\nbhonu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; l\n.shel ---&gt; e\nshele ---&gt; n\nhelen ---&gt; d\nelend ---&gt; r\nlendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; i\n.safi ---&gt; k\nsafik ---&gt; a\nafika ---&gt; .\n..... ---&gt; u\n....u ---&gt; r\n...ur ---&gt; e\n..ure ---&gt; n\n.uren ---&gt; d\nurend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; j\n..haj ---&gt; r\n.hajr ---&gt; a\nhajra ---&gt; t\najrat ---&gt; i\njrati ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; k\n.nank ---&gt; i\nnanki ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; y\n.aliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; t\n..mot ---&gt; o\n.moto ---&gt; l\nmotol ---&gt; a\notola ---&gt; l\ntolal ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; b\n.tasb ---&gt; i\ntasbi ---&gt; h\nasbih ---&gt; a\nsbiha ---&gt; .\n..... ---&gt; f\n....f ---&gt; r\n...fr ---&gt; a\n..fra ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; i\nparmi ---&gt; l\narmil ---&gt; a\nrmila ---&gt; .\n..... ---&gt; i\n....i ---&gt; d\n...id ---&gt; r\n..idr ---&gt; i\n.idri ---&gt; s\nidris ---&gt; h\ndrish ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; d\n..bud ---&gt; d\n.budd ---&gt; h\nbuddh ---&gt; i\nuddhi ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; v\n..div ---&gt; a\n.diva ---&gt; n\ndivan ---&gt; s\nivans ---&gt; h\nvansh ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; l\npreml ---&gt; a\nremla ---&gt; t\nemlat ---&gt; a\nmlata ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; k\nshank ---&gt; a\nhanka ---&gt; r\nankar ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; d\n.khad ---&gt; a\nkhada ---&gt; k\nhadak ---&gt; .\n..... ---&gt; d\n....d ---&gt; w\n...dw ---&gt; a\n..dwa ---&gt; r\n.dwar ---&gt; i\ndwari ---&gt; k\nwarik ---&gt; a\narika ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; b\n..mob ---&gt; i\n.mobi ---&gt; n\nmobin ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; a\ngulsa ---&gt; n\nulsan ---&gt; .\n..... ---&gt; i\n....i ---&gt; q\n...iq ---&gt; b\n..iqb ---&gt; a\n.iqba ---&gt; l\niqbal ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; l\n..bil ---&gt; a\n.bila ---&gt; l\nbilal ---&gt; .\n..... ---&gt; s\n....s ---&gt; m\n...sm ---&gt; t\n..smt ---&gt; s\n.smts ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; k\n..pok ---&gt; h\n.pokh ---&gt; a\npokha ---&gt; r\nokhar ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; w\n.virw ---&gt; a\nvirwa ---&gt; n\nirwan ---&gt; t\nrwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; i\nsanti ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; h\n.mohh ---&gt; m\nmohhm ---&gt; a\nohhma ---&gt; d\nhhmad ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; k\n..vak ---&gt; e\n.vake ---&gt; e\nvakee ---&gt; l\nakeel ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; a\n.suga ---&gt; i\nsugai ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; t\n..sit ---&gt; a\n.sita ---&gt; l\nsital ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; s\n..hus ---&gt; n\n.husn ---&gt; a\nhusna ---&gt; r\nusnar ---&gt; a\nsnara ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; t\n..net ---&gt; r\n.netr ---&gt; a\nnetra ---&gt; m\netram ---&gt; .\n..... ---&gt; l\n....l ---&gt; u\n...lu ---&gt; k\n..luk ---&gt; k\n.lukk ---&gt; a\nlukka ---&gt; d\nukkad ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; i\n.ragi ---&gt; n\nragin ---&gt; a\nagina ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; r\n.nazr ---&gt; e\nnazre ---&gt; e\nazree ---&gt; n\nzreen ---&gt; .\n..... ---&gt; h\n....h ---&gt; y\n...hy ---&gt; a\n..hya ---&gt; t\n.hyat ---&gt; u\nhyatu ---&gt; n\nyatun ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; o\n..moo ---&gt; l\n.mool ---&gt; i\nmooli ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; h\n..ruh ---&gt; i\n.ruhi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; c\n..vic ---&gt; k\n.vick ---&gt; y\nvicky ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; j\n.tarj ---&gt; a\ntarja ---&gt; n\narjan ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; s\nabhis ---&gt; h\nbhish ---&gt; e\nhishe ---&gt; k\nishek ---&gt; h\nshekh ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; h\n..koh ---&gt; i\n.kohi ---&gt; n\nkohin ---&gt; o\nohino ---&gt; o\nhinoo ---&gt; r\ninoor ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; t\n.jagt ---&gt; a\njagta ---&gt; r\nagtar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; k\n.shek ---&gt; h\nshekh ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; e\n..bhe ---&gt; r\n.bher ---&gt; a\nbhera ---&gt; v\nherav ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; j\n.bhaj ---&gt; a\nbhaja ---&gt; n\nhajan ---&gt; l\najanl ---&gt; a\njanla ---&gt; l\nanlal ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; v\n.balv ---&gt; e\nbalve ---&gt; e\nalvee ---&gt; r\nlveer ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; h\nparsh ---&gt; u\narshu ---&gt; .\n..... ---&gt; u\n....u ---&gt; p\n...up ---&gt; e\n..upe ---&gt; n\n.upen ---&gt; d\nupend ---&gt; r\npendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; i\n..rai ---&gt; n\n.rain ---&gt; u\nrainu ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; c\n..aac ---&gt; h\n.aach ---&gt; a\naacha ---&gt; l\nachal ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; v\n..jav ---&gt; e\n.jave ---&gt; d\njaved ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; s\n..tas ---&gt; n\n.tasn ---&gt; i\ntasni ---&gt; m\nasnim ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; i\n.sari ---&gt; k\nsarik ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; t\n.must ---&gt; k\nmustk ---&gt; e\nustke ---&gt; e\nstkee ---&gt; m\ntkeem ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; u\n.suku ---&gt; l\nsukul ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; a\nramka ---&gt; n\namkan ---&gt; y\nmkany ---&gt; a\nkanya ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; j\nrabhj ---&gt; e\nabhje ---&gt; e\nbhjee ---&gt; t\nhjeet ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; n\neepan ---&gt; s\nepans ---&gt; u\npansu ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; a\n..yaa ---&gt; d\n.yaad ---&gt; r\nyaadr ---&gt; a\naadra ---&gt; m\nadram ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; u\n.kalu ---&gt; a\nkalua ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; e\n.chee ---&gt; t\ncheet ---&gt; e\nheete ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; z\n..faz ---&gt; a\n.faza ---&gt; l\nfazal ---&gt; e\nazale ---&gt; y\nzaley ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; j\n..laj ---&gt; j\n.lajj ---&gt; a\nlajja ---&gt; v\najjav ---&gt; a\njjava ---&gt; t\njavat ---&gt; i\navati ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; m\n.birm ---&gt; a\nbirma ---&gt; t\nirmat ---&gt; i\nrmati ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; i\nsushi ---&gt; l\nushil ---&gt; a\nshila ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; s\n..tos ---&gt; h\n.tosh ---&gt; e\ntoshe ---&gt; e\noshee ---&gt; b\nsheeb ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; r\nshair ---&gt; a\nhaira ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; a\n..ala ---&gt; m\n.alam ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; w\n..jaw ---&gt; e\n.jawe ---&gt; d\njawed ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; s\n..jos ---&gt; i\n.josi ---&gt; n\njosin ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; s\n..res ---&gt; h\n.resh ---&gt; m\nreshm ---&gt; a\neshma ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; a\ncheta ---&gt; n\nhetan ---&gt; y\netany ---&gt; a\ntanya ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; h\n..reh ---&gt; m\n.rehm ---&gt; a\nrehma ---&gt; n\nehman ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; v\nvishv ---&gt; a\nishva ---&gt; s\nshvas ---&gt; h\nhvash ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; r\n.mahr ---&gt; o\nmahro ---&gt; j\nahroj ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; e\n..pee ---&gt; n\n.peen ---&gt; a\npeena ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; w\n..sew ---&gt; e\n.sewe ---&gt; t\nsewet ---&gt; a\neweta ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; n\nmanin ---&gt; d\nanind ---&gt; e\nninde ---&gt; r\ninder ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; a\n.aana ---&gt; m\naanam ---&gt; i\nanami ---&gt; k\nnamik ---&gt; a\namika ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; i\n..rai ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; k\n.rajk ---&gt; a\nrajka ---&gt; r\najkar ---&gt; n\njkarn ---&gt; t\nkarnt ---&gt; a\narnta ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; n\n.parn ---&gt; a\nparna ---&gt; v\narnav ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; p\npremp ---&gt; a\nrempa ---&gt; l\nempal ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; g\n..ang ---&gt; a\n.anga ---&gt; n\nangan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; o\n.nano ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; y\n..nay ---&gt; a\n.naya ---&gt; m\nnayam ---&gt; a\nayama ---&gt; t\nyamat ---&gt; i\namati ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; p\n..gop ---&gt; i\n.gopi ---&gt; r\ngopir ---&gt; a\nopira ---&gt; m\npiram ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; i\n.kari ---&gt; s\nkaris ---&gt; h\narish ---&gt; a\nrisha ---&gt; m\nisham ---&gt; a\nshama ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; a\n..saa ---&gt; h\n.saah ---&gt; i\nsaahi ---&gt; n\naahin ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; r\n.jagr ---&gt; t\njagrt ---&gt; i\nagrti ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; s\n.muns ---&gt; i\nmunsi ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; b\n..nab ---&gt; a\n.naba ---&gt; l\nnabal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; p\n.bhup ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; w\n.bhaw ---&gt; a\nbhawa ---&gt; n\nhawan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; h\n.ragh ---&gt; a\nragha ---&gt; v\naghav ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; i\n.hasi ---&gt; n\nhasin ---&gt; a\nasina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; j\n.sarj ---&gt; e\nsarje ---&gt; e\narjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; k\n.shik ---&gt; a\nshika ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; b\n..gob ---&gt; i\n.gobi ---&gt; n\ngobin ---&gt; d\nobind ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; e\n.inde ---&gt; r\ninder ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; i\n.navi ---&gt; y\nnaviy ---&gt; a\naviya ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; a\n.hema ---&gt; n\nheman ---&gt; i\nemani ---&gt; .\n..... ---&gt; u\n....u ---&gt; t\n...ut ---&gt; k\n..utk ---&gt; a\n.utka ---&gt; r\nutkar ---&gt; s\ntkars ---&gt; h\nkarsh ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; g\n.gang ---&gt; a\nganga ---&gt; j\nangaj ---&gt; a\nngaja ---&gt; l\ngajal ---&gt; i\najali ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; a\n.sawa ---&gt; l\nsawal ---&gt; i\nawali ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; a\n.sona ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; a\n.kosa ---&gt; l\nkosal ---&gt; y\nosaly ---&gt; a\nsalya ---&gt; .\n..... ---&gt; m\n....m ---&gt; r\n...mr ---&gt; s\n..mrs ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; a\n.asha ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; s\n.fais ---&gt; a\nfaisa ---&gt; l\naisal ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; k\n.pank ---&gt; u\npanku ---&gt; j\nankuj ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; e\n.kale ---&gt; r\nkaler ---&gt; a\nalera ---&gt; m\nleram ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; e\n.rake ---&gt; s\nrakes ---&gt; h\nakesh ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; d\n.najd ---&gt; a\nnajda ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; k\n.mank ---&gt; a\nmanka ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; i\n..dhi ---&gt; s\n.dhis ---&gt; a\ndhisa ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; o\nhando ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; j\n..tej ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; e\n..gee ---&gt; r\n.geer ---&gt; n\ngeern ---&gt; i\neerni ---&gt; s\nernis ---&gt; h\nrnish ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; a\n..aba ---&gt; s\n.abas ---&gt; h\nabash ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; r\n..pur ---&gt; v\n.purv ---&gt; a\npurva ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; n\n..nen ---&gt; a\n.nena ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; u\n..dau ---&gt; j\n.dauj ---&gt; i\ndauji ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; u\n.ashu ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; s\n.ashs ---&gt; h\nashsh ---&gt; w\nshshw ---&gt; e\nhshwe ---&gt; r\nshwer ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; t\n..bet ---&gt; i\n.beti ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; e\n.rade ---&gt; s\nrades ---&gt; y\nadesy ---&gt; a\ndesya ---&gt; m\nesyam ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; h\n..bah ---&gt; r\n.bahr ---&gt; a\nbahra ---&gt; t\nahrat ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; m\n..aam ---&gt; o\n.aamo ---&gt; d\naamod ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; d\n.hard ---&gt; a\nharda ---&gt; s\nardas ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; n\n..jon ---&gt; y\n.jony ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; b\n..jib ---&gt; a\n.jiba ---&gt; n\njiban ---&gt; t\nibant ---&gt; i\nbanti ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; k\n..tik ---&gt; a\n.tika ---&gt; r\ntikar ---&gt; a\nikara ---&gt; m\nkaram ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; u\n.guru ---&gt; p\ngurup ---&gt; r\nurupr ---&gt; e\nrupre ---&gt; e\nupree ---&gt; t\npreet ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; k\n..mik ---&gt; e\n.mike ---&gt; l\nmikel ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; r\n.musr ---&gt; a\nmusra ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; u\n.amru ---&gt; d\namrud ---&gt; i\nmrudi ---&gt; n\nrudin ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; b\n.sabb ---&gt; a\nsabba ---&gt; b\nabbab ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; a\nbhaga ---&gt; y\nhagay ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; m\n.kham ---&gt; c\nkhamc ---&gt; h\nhamch ---&gt; a\namcha ---&gt; n\nmchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; m\n....m ---&gt; j\n...mj ---&gt; u\n..mju ---&gt; d\n.mjud ---&gt; i\nmjudi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; c\n..vic ---&gt; t\n.vict ---&gt; o\nvicto ---&gt; r\nictor ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; y\n..riy ---&gt; a\n.riya ---&gt; z\nriyaz ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; d\n.jagd ---&gt; e\njagde ---&gt; e\nagdee ---&gt; p\ngdeep ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; h\n.kash ---&gt; i\nkashi ---&gt; v\nashiv ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; e\n.rite ---&gt; s\nrites ---&gt; h\nitesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; a\n.sona ---&gt; l\nsonal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; e\n.sabe ---&gt; e\nsabee ---&gt; h\nabeeh ---&gt; a\nbeeha ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; u\n.guru ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; k\n..shk ---&gt; i\n.shki ---&gt; l\nshkil ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; w\n..asw ---&gt; i\n.aswi ---&gt; n\naswin ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; o\n.chho ---&gt; t\nchhot ---&gt; u\nhhotu ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; a\ndurga ---&gt; l\nurgal ---&gt; a\nrgala ---&gt; l\ngalal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; h\n.sabh ---&gt; a\nsabha ---&gt; n\nabhan ---&gt; a\nbhana ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; m\n..shm ---&gt; a\n.shma ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; a\n.bada ---&gt; m\nbadam ---&gt; i\nadami ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; g\n..mag ---&gt; a\n.maga ---&gt; t\nmagat ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; v\n.jasv ---&gt; i\njasvi ---&gt; r\nasvir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; e\n.sabe ---&gt; e\nsabee ---&gt; r\nabeer ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; e\n.nave ---&gt; e\nnavee ---&gt; n\naveen ---&gt; a\nveena ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; g\n..beg ---&gt; r\n.begr ---&gt; a\nbegra ---&gt; j\negraj ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; n\n.harn ---&gt; a\nharna ---&gt; m\narnam ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; l\n.heml ---&gt; e\nhemle ---&gt; t\nemlet ---&gt; a\nmleta ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; i\nprami ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; b\n.sabb ---&gt; u\nsabbu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; e\nrajee ---&gt; v\najeev ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; a\n..baa ---&gt; m\n.baam ---&gt; .\n..... ---&gt; s\n....s ---&gt; n\n...sn ---&gt; g\n..sng ---&gt; e\n.snge ---&gt; e\nsngee ---&gt; t\nngeet ---&gt; a\ngeeta ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; y\nmunny ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; d\n.bind ---&gt; i\nbindi ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; n\n..men ---&gt; k\n.menk ---&gt; a\nmenka ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; v\n.balv ---&gt; i\nbalvi ---&gt; n\nalvin ---&gt; d\nlvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; a\n.hema ---&gt; n\nheman ---&gt; s\nemans ---&gt; h\nmansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; r\n.amir ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; d\n.sidd ---&gt; h\nsiddh ---&gt; i\niddhi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; i\nshabi ---&gt; n\nhabin ---&gt; a\nabina ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; h\n.rosh ---&gt; a\nrosha ---&gt; n\noshan ---&gt; i\nshani ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; a\n..ava ---&gt; s\n.avas ---&gt; t\navast ---&gt; h\nvasth ---&gt; a\nastha ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; a\n..ala ---&gt; h\n.alah ---&gt; b\nalahb ---&gt; a\nlahba ---&gt; s\nahbas ---&gt; r\nhbasr ---&gt; i\nbasri ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; u\n.rasu ---&gt; l\nrasul ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; a\nancha ---&gt; m\nncham ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; c\n..vic ---&gt; k\n.vick ---&gt; e\nvicke ---&gt; y\nickey ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; d\n.amid ---&gt; a\namida ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; m\n.shim ---&gt; l\nshiml ---&gt; a\nhimla ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; n\n..din ---&gt; k\n.dink ---&gt; a\ndinka ---&gt; r\ninkar ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; d\n.vand ---&gt; n\nvandn ---&gt; a\nandna ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; h\n.sudh ---&gt; a\nsudha ---&gt; n\nudhan ---&gt; s\ndhans ---&gt; h\nhansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; u\n.paru ---&gt; .\n..... ---&gt; s\n....s ---&gt; c\n...sc ---&gt; h\n..sch ---&gt; i\n.schi ---&gt; n\nschin ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; p\n.gulp ---&gt; a\ngulpa ---&gt; c\nulpac ---&gt; h\nlpach ---&gt; a\npacha ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; i\n.mehi ---&gt; n\nmehin ---&gt; d\nehind ---&gt; e\nhinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; s\n..ars ---&gt; i\n.arsi ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; m\n..alm ---&gt; a\n.alma ---&gt; .\n..... ---&gt; j\n....j ---&gt; y\n...jy ---&gt; a\n..jya ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; h\n.vidh ---&gt; y\nvidhy ---&gt; a\nidhya ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; g\n.kang ---&gt; a\nkanga ---&gt; n\nangan ---&gt; a\nngana ---&gt; .\n..... ---&gt; h\n....h ---&gt; o\n...ho ---&gt; r\n..hor ---&gt; a\n.hora ---&gt; m\nhoram ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; e\n.yoge ---&gt; n\nyogen ---&gt; d\nogend ---&gt; r\ngendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; f\n..naf ---&gt; e\n.nafe ---&gt; e\nnafee ---&gt; s\nafees ---&gt; a\nfeesa ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; a\nkrisa ---&gt; n\nrisan ---&gt; a\nisana ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; n\nmohan ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; o\n.jaso ---&gt; d\njasod ---&gt; a\nasoda ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; l\nshivl ---&gt; a\nhivla ---&gt; l\nivlal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; i\n.mahi ---&gt; n\nmahin ---&gt; d\nahind ---&gt; e\nhinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; y\n..rey ---&gt; a\n.reya ---&gt; z\nreyaz ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; e\n.inde ---&gt; r\ninder ---&gt; n\nndern ---&gt; a\nderna ---&gt; t\nernat ---&gt; h\nrnath ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; l\nsantl ---&gt; a\nantla ---&gt; l\nntlal ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; i\n.suni ---&gt; t\nsunit ---&gt; h\nunith ---&gt; a\nnitha ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; r\n.char ---&gt; u\ncharu ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; .\n..... ---&gt; r\n....r ---&gt; m\n...rm ---&gt; a\n..rma ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; w\n..law ---&gt; r\n.lawr ---&gt; e\nlawre ---&gt; n\nawren ---&gt; c\nwrenc ---&gt; e\nrence ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; s\n..aks ---&gt; h\n.aksh ---&gt; a\naksha ---&gt; t\nkshat ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; g\n.surg ---&gt; a\nsurga ---&gt; y\nurgay ---&gt; a\nrgaya ---&gt; n\ngayan ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; v\n..kev ---&gt; a\n.keva ---&gt; l\nkeval ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; i\n.ashi ---&gt; s\nashis ---&gt; h\nshish ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; r\n.sabr ---&gt; a\nsabra ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; t\n..ist ---&gt; y\n.isty ---&gt; a\nistya ---&gt; k\nstyak ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; n\n.prin ---&gt; k\nprink ---&gt; a\nrinka ---&gt; y\ninkay ---&gt; a\nnkaya ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; r\n..per ---&gt; k\n.perk ---&gt; a\nperka ---&gt; s\nerkas ---&gt; h\nrkash ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; r\n.sabr ---&gt; i\nsabri ---&gt; n\nabrin ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; i\n.muki ---&gt; m\nmukim ---&gt; a\nukima ---&gt; n\nkiman ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; y\n..jay ---&gt; o\n.jayo ---&gt; t\njayot ---&gt; l\nayotl ---&gt; i\nyotli ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; i\nparmi ---&gt; t\narmit ---&gt; a\nrmita ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; o\n.chho ---&gt; t\nchhot ---&gt; i\nhhoti ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; e\n.vipe ---&gt; n\nvipen ---&gt; d\nipend ---&gt; r\npendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; s\n..nos ---&gt; a\n.nosa ---&gt; d\nnosad ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; e\n.hare ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; u\n.nanu ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; e\n.sure ---&gt; s\nsures ---&gt; h\nuresh ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; r\n.anur ---&gt; a\nanura ---&gt; g\nnurag ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; l\n..nil ---&gt; a\n.nila ---&gt; m\nnilam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; r\nsantr ---&gt; m\nantrm ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; a\n.sawa ---&gt; n\nsawan ---&gt; a\nawana ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; i\n.shri ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; n\nmohan ---&gt; r\nohanr ---&gt; a\nhanra ---&gt; m\nanram ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; i\n..vai ---&gt; d\n.vaid ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; v\n.shav ---&gt; i\nshavi ---&gt; t\nhavit ---&gt; r\navitr ---&gt; i\nvitri ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; s\n.sars ---&gt; w\nsarsw ---&gt; a\narswa ---&gt; t\nrswat ---&gt; i\nswati ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; m\n..arm ---&gt; i\n.armi ---&gt; t\narmit ---&gt; a\nrmita ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; i\ndhani ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; l\n..nil ---&gt; u\n.nilu ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; n\n..nin ---&gt; i\n.nini ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; l\n.tril ---&gt; o\ntrilo ---&gt; c\nriloc ---&gt; k\nilock ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; a\n.suda ---&gt; m\nsudam ---&gt; a\nudama ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; e\nshahe ---&gt; e\nhahee ---&gt; n\naheen ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; a\n..aja ---&gt; b\n.ajab ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; s\n..nos ---&gt; h\n.nosh ---&gt; a\nnosha ---&gt; d\noshad ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; l\n..sul ---&gt; t\n.sult ---&gt; a\nsulta ---&gt; n\nultan ---&gt; a\nltana ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; u\nbhanu ---&gt; m\nhanum ---&gt; a\nanuma ---&gt; t\nnumat ---&gt; i\numati ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; m\n..sim ---&gt; m\n.simm ---&gt; y\nsimmy ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; n\n.mehn ---&gt; a\nmehna ---&gt; z\nehnaz ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; t\n.lalt ---&gt; h\nlalth ---&gt; a\naltha ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; a\n..uda ---&gt; y\n.uday ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; j\n..rij ---&gt; u\n.riju ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; a\n.lila ---&gt; b\nlilab ---&gt; a\nilaba ---&gt; i\nlabai ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; i\n..moi ---&gt; n\n.moin ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; u\n.kusu ---&gt; m\nkusum ---&gt; l\nusuml ---&gt; a\nsumla ---&gt; t\numlat ---&gt; a\nmlata ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; g\n..nag ---&gt; i\n.nagi ---&gt; n\nnagin ---&gt; a\nagina ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; a\n.nara ---&gt; i\nnarai ---&gt; n\narain ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; o\n..ano ---&gt; j\n.anoj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; e\nrajee ---&gt; n\najeen ---&gt; a\njeena ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; i\n..dhi ---&gt; r\n.dhir ---&gt; p\ndhirp ---&gt; a\nhirpa ---&gt; l\nirpal ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; k\n..fak ---&gt; r\n.fakr ---&gt; u\nfakru ---&gt; d\nakrud ---&gt; e\nkrude ---&gt; e\nrudee ---&gt; n\nudeen ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; t\n.mukt ---&gt; i\nmukti ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; n\n.been ---&gt; a\nbeena ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; d\n..kud ---&gt; t\n.kudt ---&gt; u\nkudtu ---&gt; s\nudtus ---&gt; u\ndtusu ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; z\n..taz ---&gt; a\n.taza ---&gt; u\ntazau ---&gt; d\nazaud ---&gt; e\nzaude ---&gt; e\naudee ---&gt; n\nudeen ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; d\n.hard ---&gt; a\nharda ---&gt; s\nardas ---&gt; s\nrdass ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; r\n.ompr ---&gt; a\nompra ---&gt; k\nmprak ---&gt; a\npraka ---&gt; s\nrakas ---&gt; h\nakash ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; s\n..tus ---&gt; h\n.tush ---&gt; a\ntusha ---&gt; r\nushar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; i\nshami ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; h\n..suh ---&gt; a\n.suha ---&gt; n\nsuhan ---&gt; a\nuhana ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; k\n.rink ---&gt; a\nrinka ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; s\n.anus ---&gt; h\nanush ---&gt; k\nnushk ---&gt; a\nushka ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; d\n.nand ---&gt; i\nnandi ---&gt; n\nandin ---&gt; i\nndini ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; j\n.gulj ---&gt; a\ngulja ---&gt; n\nuljan ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; a\n.bara ---&gt; k\nbarak ---&gt; h\narakh ---&gt; a\nrakha ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; l\n.raml ---&gt; a\nramla ---&gt; l\namlal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; a\n.sama ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; m\ndharm ---&gt; r\nharmr ---&gt; a\narmra ---&gt; j\nrmraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; a\n.mana ---&gt; l\nmanal ---&gt; i\nanali ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; r\n..pir ---&gt; m\n.pirm ---&gt; i\npirmi ---&gt; k\nirmik ---&gt; a\nrmika ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; a\nshana ---&gt; j\nhanaj ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; w\n..diw ---&gt; a\n.diwa ---&gt; n\ndiwan ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; u\n.anju ---&gt; m\nanjum ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; r\n..jar ---&gt; i\n.jari ---&gt; m\njarim ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; p\n.trip ---&gt; t\ntript ---&gt; a\nripta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; t\n.satt ---&gt; u\nsattu ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; y\n.prey ---&gt; o\npreyo ---&gt; j\nreyoj ---&gt; i\neyoji ---&gt; t\nyojit ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; j\n..kaj ---&gt; u\n.kaju ---&gt; l\nkajul ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; u\n.babu ---&gt; d\nbabud ---&gt; d\nabudd ---&gt; i\nbuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; e\n..gee ---&gt; s\n.gees ---&gt; a\ngeesa ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; a\n.kesa ---&gt; v\nkesav ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; e\n.sune ---&gt; e\nsunee ---&gt; t\nuneet ---&gt; a\nneeta ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; r\n..pur ---&gt; a\n.pura ---&gt; n\npuran ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; m\n.kism ---&gt; t\nkismt ---&gt; i\nismti ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; i\n.kali ---&gt; p\nkalip ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; u\nanchu ---&gt; r\nnchur ---&gt; a\nchura ---&gt; m\nhuram ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; a\n.jita ---&gt; n\njitan ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; h\n.vidh ---&gt; a\nvidha ---&gt; v\nidhav ---&gt; a\ndhava ---&gt; t\nhavat ---&gt; i\navati ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; i\nparti ---&gt; m\nartim ---&gt; a\nrtima ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; l\n..val ---&gt; e\n.vale ---&gt; n\nvalen ---&gt; t\nalent ---&gt; i\nlenti ---&gt; n\nentin ---&gt; a\nntina ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; l\n..mul ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; k\n.kank ---&gt; i\nkanki ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; l\n.bhal ---&gt; a\nbhala ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; n\n.poon ---&gt; a\npoona ---&gt; m\noonam ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; v\n.mahv ---&gt; e\nmahve ---&gt; e\nahvee ---&gt; r\nhveer ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; n\nranjn ---&gt; i\nanjni ---&gt; k\nnjnik ---&gt; a\njnika ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; n\n..jun ---&gt; a\n.juna ---&gt; i\njunai ---&gt; d\nunaid ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; i\n.sari ---&gt; k\nsarik ---&gt; a\narika ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; d\n.hard ---&gt; a\nharda ---&gt; m\nardam ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; i\n.pari ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; u\n.kalu ---&gt; r\nkalur ---&gt; a\nalura ---&gt; m\nluram ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; a\nvisha ---&gt; n\nishan ---&gt; u\nshanu ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; k\n..bak ---&gt; i\n.baki ---&gt; l\nbakil ---&gt; a\nakila ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; t\n..pit ---&gt; a\n.pita ---&gt; r\npitar ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; n\neepan ---&gt; t\nepant ---&gt; i\npanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; k\n.shok ---&gt; a\nshoka ---&gt; t\nhokat ---&gt; .\n..... ---&gt; g\n....g ---&gt; r\n...gr ---&gt; o\n..gro ---&gt; t\n.grot ---&gt; i\ngroti ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; j\n.rubj ---&gt; a\nrubja ---&gt; a\nubjaa ---&gt; n\nbjaan ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; u\n.sonu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; s\n.shas ---&gt; h\nshash ---&gt; i\nhashi ---&gt; d\nashid ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; m\n.anim ---&gt; e\nanime ---&gt; s\nnimes ---&gt; h\nimesh ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; n\nhiman ---&gt; s\nimans ---&gt; h\nmansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; n\n.jamn ---&gt; a\njamna ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; h\n..bah ---&gt; a\n.baha ---&gt; r\nbahar ---&gt; t\nahart ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; p\n..tap ---&gt; s\n.taps ---&gt; y\ntapsy ---&gt; u\napsyu ---&gt; m\npsyum ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; p\n..tap ---&gt; a\n.tapa ---&gt; n\ntapan ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; k\n..sik ---&gt; a\n.sika ---&gt; n\nsikan ---&gt; d\nikand ---&gt; a\nkanda ---&gt; r\nandar ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; m\nsushm ---&gt; a\nushma ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; v\n..siv ---&gt; a\n.siva ---&gt; n\nsivan ---&gt; i\nivani ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; a\n.sona ---&gt; k\nsonak ---&gt; i\nonaki ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; j\n..taj ---&gt; u\n.taju ---&gt; d\ntajud ---&gt; d\najudd ---&gt; i\njuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; l\n..dol ---&gt; y\n.doly ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; n\nubhan ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; j\n.amij ---&gt; a\namija ---&gt; n\nmijan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; b\n.manb ---&gt; h\nmanbh ---&gt; a\nanbha ---&gt; r\nnbhar ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; k\n.musk ---&gt; a\nmuska ---&gt; r\nuskar ---&gt; n\nskarn ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; i\n.lali ---&gt; t\nlalit ---&gt; a\nalita ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; h\n..arh ---&gt; i\n.arhi ---&gt; y\narhiy ---&gt; a\nrhiya ---&gt; n\nhiyan ---&gt; t\niyant ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; u\n..amu ---&gt; d\n.amud ---&gt; a\namuda ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; l\n..aal ---&gt; i\n.aali ---&gt; n\naalin ---&gt; a\nalina ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; t\n.kunt ---&gt; i\nkunti ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; j\n..amj ---&gt; a\n.amja ---&gt; d\namjad ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; e\n..swe ---&gt; d\n.swed ---&gt; e\nswede ---&gt; s\nwedes ---&gt; h\nedesh ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; k\n..lok ---&gt; e\n.loke ---&gt; n\nloken ---&gt; d\nokend ---&gt; e\nkende ---&gt; r\nender ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; j\n.ramj ---&gt; a\nramja ---&gt; n\namjan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; h\nshadh ---&gt; n\nhadhn ---&gt; a\nadhna ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; u\n..kau ---&gt; s\n.kaus ---&gt; h\nkaush ---&gt; a\nausha ---&gt; l\nushal ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; y\n.nary ---&gt; a\nnarya ---&gt; n\naryan ---&gt; i\nryani ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; q\n..yaq ---&gt; u\n.yaqu ---&gt; b\nyaqub ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; d\n.mind ---&gt; e\nminde ---&gt; r\ninder ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; b\n..alb ---&gt; a\n.alba ---&gt; k\nalbak ---&gt; s\nlbaks ---&gt; h\nbaksh ---&gt; a\naksha ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; y\n..pry ---&gt; a\n.prya ---&gt; n\npryan ---&gt; i\nryani ---&gt; k\nyanik ---&gt; a\nanika ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; m\n..dhm ---&gt; e\n.dhme ---&gt; n\ndhmen ---&gt; d\nhmend ---&gt; a\nmenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; i\n.aari ---&gt; f\naarif ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; i\n.navi ---&gt; t\nnavit ---&gt; a\navita ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; k\n.pank ---&gt; a\npanka ---&gt; j\nankaj ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; m\n.nirm ---&gt; a\nnirma ---&gt; l\nirmal ---&gt; a\nrmala ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; v\n..sev ---&gt; a\n.seva ---&gt; k\nsevak ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; r\n.kamr ---&gt; e\nkamre ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; e\n.jite ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; m\n..rum ---&gt; e\n.rume ---&gt; e\nrumee ---&gt; t\numeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; u\n.salu ---&gt; .\n..... ---&gt; a\n....a ---&gt; h\n...ah ---&gt; u\n..ahu ---&gt; p\n.ahup ---&gt; e\nahupe ---&gt; n\nhupen ---&gt; d\nupend ---&gt; r\npendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; i\n.kami ---&gt; l\nkamil ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; h\n.nikh ---&gt; a\nnikha ---&gt; t\nikhat ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; y\n..jiy ---&gt; a\n.jiya ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; u\n.muku ---&gt; l\nmukul ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; h\n.nish ---&gt; u\nnishu ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; a\nparma ---&gt; n\narman ---&gt; a\nrmana ---&gt; n\nmanan ---&gt; d\nanand ---&gt; .\n..... ---&gt; t\n....t ---&gt; h\n...th ---&gt; a\n..tha ---&gt; k\n.thak ---&gt; u\nthaku ---&gt; r\nhakur ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; t\nchott ---&gt; i\nhotti ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; k\n..muk ---&gt; a\n.muka ---&gt; r\nmukar ---&gt; a\nukara ---&gt; m\nkaram ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; a\n.kosa ---&gt; l\nkosal ---&gt; e\nosale ---&gt; y\nsaley ---&gt; a\naleya ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; n\n.prin ---&gt; c\nprinc ---&gt; e\nrince ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; i\naashi ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; f\n..gaf ---&gt; f\n.gaff ---&gt; u\ngaffu ---&gt; r\naffur ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; i\nrabhi ---&gt; l\nabhil ---&gt; a\nbhila ---&gt; .\n..... ---&gt; z\n....z ---&gt; h\n...zh ---&gt; i\n..zhi ---&gt; n\n.zhin ---&gt; i\nzhini ---&gt; .\n..... ---&gt; i\n....i ---&gt; e\n...ie ---&gt; s\n..ies ---&gt; h\n.iesh ---&gt; a\niesha ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; c\n..bac ---&gt; c\n.bacc ---&gt; h\nbacch ---&gt; a\naccha ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; z\n..haz ---&gt; r\n.hazr ---&gt; a\nhazra ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; m\n.susm ---&gt; a\nsusma ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; a\n.vika ---&gt; r\nvikar ---&gt; a\nikara ---&gt; m\nkaram ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; u\nshaku ---&gt; n\nhakun ---&gt; a\nakuna ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; i\n.jani ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; y\n.vidy ---&gt; a\nvidya ---&gt; d\nidyad ---&gt; a\ndyada ---&gt; t\nyadat ---&gt; t\nadatt ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; l\n.raml ---&gt; a\nramla ---&gt; k\namlak ---&gt; h\nmlakh ---&gt; a\nlakha ---&gt; n\nakhan ---&gt; .\n..... ---&gt; o\n....o ---&gt; l\n...ol ---&gt; i\n..oli ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; i\n.ragi ---&gt; b\nragib ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; n\n.jhan ---&gt; s\njhans ---&gt; i\nhansi ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; r\n..zar ---&gt; e\n.zare ---&gt; e\nzaree ---&gt; n\nareen ---&gt; a\nreena ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; e\n.mane ---&gt; e\nmanee ---&gt; s\nanees ---&gt; h\nneesh ---&gt; a\neesha ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; a\n.yasa ---&gt; r\nyasar ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; r\n..jor ---&gt; j\n.jorj ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; t\n.rant ---&gt; e\nrante ---&gt; s\nantes ---&gt; h\nntesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; m\n..ajm ---&gt; a\n.ajma ---&gt; l\najmal ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; v\namarv ---&gt; e\nmarve ---&gt; s\narves ---&gt; h\nrvesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; n\n..men ---&gt; a\n.mena ---&gt; d\nmenad ---&gt; e\nenade ---&gt; v\nnadev ---&gt; i\nadevi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; a\nvisha ---&gt; m\nisham ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; p\n..sup ---&gt; r\n.supr ---&gt; i\nsupri ---&gt; y\nupriy ---&gt; a\npriya ---&gt; l\nriyal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; g\n..mag ---&gt; e\n.mage ---&gt; r\nmager ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; b\n..rab ---&gt; h\n.rabh ---&gt; i\nrabhi ---&gt; d\nabhid ---&gt; a\nbhida ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; l\nprabl ---&gt; e\nrable ---&gt; e\nablee ---&gt; n\nbleen ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; k\nubhak ---&gt; a\nbhaka ---&gt; r\nhakar ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; w\ndhanw ---&gt; a\nhanwa ---&gt; t\nanwat ---&gt; i\nnwati ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; i\n.vasi ---&gt; d\nvasid ---&gt; a\nasida ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; k\n..kok ---&gt; i\n.koki ---&gt; l\nkokil ---&gt; a\nokila ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; i\n.kami ---&gt; n\nkamin ---&gt; i\namini ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; a\n.deva ---&gt; s\ndevas ---&gt; h\nevash ---&gt; e\nvashe ---&gt; e\nashee ---&gt; h\nsheeh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; u\n.shou ---&gt; k\nshouk ---&gt; a\nhouka ---&gt; t\noukat ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; s\n..las ---&gt; a\n.lasa ---&gt; r\nlasar ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; l\n..nil ---&gt; a\n.nila ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; s\n.hans ---&gt; a\nhansa ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; c\n.nanc ---&gt; y\nnancy ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; j\n..maj ---&gt; u\n.maju ---&gt; n\nmajun ---&gt; e\najune ---&gt; w\njunew ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; k\n.shok ---&gt; a\nshoka ---&gt; r\nhokar ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; d\n.hard ---&gt; i\nhardi ---&gt; k\nardik ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; k\nriyak ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; i\n.fari ---&gt; d\nfarid ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; h\n..bah ---&gt; a\n.baha ---&gt; l\nbahal ---&gt; e\nahale ---&gt; n\nhalen ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; n\nharin ---&gt; d\narind ---&gt; e\nrinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; a\n.tara ---&gt; w\ntaraw ---&gt; a\narawa ---&gt; t\nrawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; e\n.nase ---&gt; e\nnasee ---&gt; f\naseef ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; a\n.mana ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; e\n..khe ---&gt; m\n.khem ---&gt; r\nkhemr ---&gt; a\nhemra ---&gt; j\nemraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; s\n.shas ---&gt; h\nshash ---&gt; w\nhashw ---&gt; a\nashwa ---&gt; t\nshwat ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; g\n..asg ---&gt; a\n.asga ---&gt; r\nasgar ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; m\n..mem ---&gt; a\n.mema ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; e\n.shee ---&gt; a\nsheea ---&gt; k\nheeak ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; p\n.bhup ---&gt; e\nbhupe ---&gt; n\nhupen ---&gt; d\nupend ---&gt; a\npenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; n\n.guln ---&gt; a\ngulna ---&gt; z\nulnaz ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; j\n..inj ---&gt; u\n.inju ---&gt; m\ninjum ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; a\n.naja ---&gt; r\nnajar ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; e\nnoore ---&gt; e\nooree ---&gt; n\noreen ---&gt; .\n..... ---&gt; i\n....i ---&gt; l\n...il ---&gt; y\n..ily ---&gt; a\n.ilya ---&gt; s\nilyas ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; m\n.alim ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; i\n.jasi ---&gt; n\njasin ---&gt; a\nasina ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; l\n..gol ---&gt; d\n.gold ---&gt; y\ngoldy ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; u\n.tanu ---&gt; s\ntanus ---&gt; h\nanush ---&gt; r\nnushr ---&gt; i\nushri ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; a\n.nisa ---&gt; r\nnisar ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; n\nanjan ---&gt; i\nnjani ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; v\n.janv ---&gt; i\njanvi ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; p\n..gap ---&gt; p\n.gapp ---&gt; u\ngappu ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; d\n.hard ---&gt; e\nharde ---&gt; v\nardev ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; e\n.sabe ---&gt; e\nsabee ---&gt; n\nabeen ---&gt; a\nbeena ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; a\n.mina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; o\n.saro ---&gt; j\nsaroj ---&gt; a\naroja ---&gt; n\nrojan ---&gt; i\nojani ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; t\n..imt ---&gt; i\n.imti ---&gt; a\nimtia ---&gt; z\nmtiaz ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; p\n.ganp ---&gt; a\nganpa ---&gt; t\nanpat ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; o\n..foo ---&gt; l\n.fool ---&gt; j\nfoolj ---&gt; h\nooljh ---&gt; n\noljhn ---&gt; a\nljhna ---&gt; h\njhnah ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; t\n.chat ---&gt; t\nchatt ---&gt; e\nhatte ---&gt; r\natter ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; a\n.meha ---&gt; k\nmehak ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; k\n.jask ---&gt; i\njaski ---&gt; r\naskir ---&gt; a\nskira ---&gt; t\nkirat ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; n\n.jann ---&gt; a\njanna ---&gt; t\nannat ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; t\n.mitt ---&gt; o\nmitto ---&gt; p\nittop ---&gt; u\nttopu ---&gt; r\ntopur ---&gt; i\nopuri ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; i\n.anki ---&gt; t\nankit ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; u\nramsu ---&gt; r\namsur ---&gt; a\nmsura ---&gt; t\nsurat ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; l\n..dol ---&gt; a\n.dola ---&gt; t\ndolat ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; j\n.navj ---&gt; a\nnavja ---&gt; t\navjat ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; h\n..suh ---&gt; s\n.suhs ---&gt; m\nsuhsm ---&gt; i\nuhsmi ---&gt; t\nhsmit ---&gt; a\nsmita ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; m\n.himm ---&gt; a\nhimma ---&gt; t\nimmat ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; z\n.mahz ---&gt; b\nmahzb ---&gt; i\nahzbi ---&gt; n\nhzbin ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; d\n..and ---&gt; h\n.andh ---&gt; a\nandha ---&gt; v\nndhav ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; o\n.asho ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; d\nmahad ---&gt; e\nahade ---&gt; v\nhadev ---&gt; i\nadevi ---&gt; .\n..... ---&gt; j\n....j ---&gt; y\n...jy ---&gt; o\n..jyo ---&gt; t\n.jyot ---&gt; s\njyots ---&gt; a\nyotsa ---&gt; n\notsan ---&gt; a\ntsana ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; m\n.balm ---&gt; i\nbalmi ---&gt; k\nalmik ---&gt; i\nlmiki ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; a\n.rama ---&gt; n\nraman ---&gt; a\namana ---&gt; n\nmanan ---&gt; d\nanand ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; k\n.rink ---&gt; e\nrinke ---&gt; .\n..... ---&gt; c\n....c ---&gt; o\n...co ---&gt; n\n..con ---&gt; s\n.cons ---&gt; t\nconst ---&gt; a\nonsta ---&gt; b\nnstab ---&gt; l\nstabl ---&gt; e\ntable ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; t\n..nut ---&gt; a\n.nuta ---&gt; n\nnutan ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; r\n..imr ---&gt; a\n.imra ---&gt; n\nimran ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; b\nsarab ---&gt; j\narabj ---&gt; i\nrabji ---&gt; t\nabjit ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; w\nsahaw ---&gt; a\nahawa ---&gt; j\nhawaj ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; b\n..jab ---&gt; i\n.jabi ---&gt; r\njabir ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; a\n.akha ---&gt; l\nakhal ---&gt; a\nkhala ---&gt; k\nhalak ---&gt; .\n..... ---&gt; m\n....m ---&gt; n\n...mn ---&gt; o\n..mno ---&gt; j\n.mnoj ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; p\n.pusp ---&gt; a\npuspa ---&gt; k\nuspak ---&gt; .\n..... ---&gt; z\n....z ---&gt; i\n...zi ---&gt; l\n..zil ---&gt; e\n.zile ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; l\n..hal ---&gt; k\n.halk ---&gt; e\nhalke ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; i\n..tai ---&gt; t\n.tait ---&gt; r\ntaitr ---&gt; a\naitra ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; l\n.kaml ---&gt; i\nkamli ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; h\n.nish ---&gt; a\nnisha ---&gt; n\nishan ---&gt; t\nshant ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; d\n.bhud ---&gt; e\nbhude ---&gt; v\nhudev ---&gt; i\nudevi ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; a\n..aza ---&gt; z\n.azaz ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; i\n..bai ---&gt; d\n.baid ---&gt; n\nbaidn ---&gt; a\naidna ---&gt; t\nidnat ---&gt; h\ndnath ---&gt; .\n..... ---&gt; i\n....i ---&gt; q\n...iq ---&gt; r\n..iqr ---&gt; a\n.iqra ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; b\n..abb ---&gt; a\n.abba ---&gt; s\nabbas ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; e\n.sune ---&gt; e\nsunee ---&gt; l\nuneel ---&gt; .\n..... ---&gt; a\n....a ---&gt; p\n...ap ---&gt; a\n..apa ---&gt; r\n.apar ---&gt; n\naparn ---&gt; a\nparna ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; k\n.rank ---&gt; i\nranki ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; h\n.rash ---&gt; i\nrashi ---&gt; d\nashid ---&gt; a\nshida ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; m\n..tam ---&gt; n\n.tamn ---&gt; n\ntamnn ---&gt; a\namnna ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; a\nfarha ---&gt; j\narhaj ---&gt; a\nrhaja ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; n\n.gyan ---&gt; w\ngyanw ---&gt; a\nyanwa ---&gt; t\nanwat ---&gt; i\nnwati ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; u\nmanju ---&gt; l\nanjul ---&gt; a\nnjula ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; j\n..tej ---&gt; v\n.tejv ---&gt; i\ntejvi ---&gt; r\nejvir ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; a\n.suna ---&gt; k\nsunak ---&gt; i\nunaki ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; i\n.soni ---&gt; k\nsonik ---&gt; a\nonika ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; n\n..avn ---&gt; e\n.avne ---&gt; e\navnee ---&gt; t\nvneet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; a\n.sada ---&gt; b\nsadab ---&gt; r\nadabr ---&gt; i\ndabri ---&gt; j\nabrij ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; e\n..rae ---&gt; e\n.raee ---&gt; y\nraeey ---&gt; a\naeeya ---&gt; n\neeyan ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; d\n..mid ---&gt; h\n.midh ---&gt; a\nmidha ---&gt; n\nidhan ---&gt; a\ndhana ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; j\n.marj ---&gt; a\nmarja ---&gt; n\narjan ---&gt; a\nrjana ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; w\n.mehw ---&gt; i\nmehwi ---&gt; s\nehwis ---&gt; h\nhwish ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; i\n.kasi ---&gt; m\nkasim ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; v\n.sarv ---&gt; e\nsarve ---&gt; s\narves ---&gt; h\nrvesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; j\n.sayj ---&gt; a\nsayja ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; i\n.dili ---&gt; s\ndilis ---&gt; h\nilish ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; a\n.mama ---&gt; t\nmamat ---&gt; a\namata ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; p\n.harp ---&gt; r\nharpr ---&gt; a\narpra ---&gt; s\nrpras ---&gt; a\nprasa ---&gt; d\nrasad ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; e\n.pare ---&gt; e\nparee ---&gt; t\nareet ---&gt; i\nreeti ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; n\n.mann ---&gt; o\nmanno ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; v\nsatyv ---&gt; r\natyvr ---&gt; a\ntyvra ---&gt; t\nyvrat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; o\n.samo ---&gt; t\nsamot ---&gt; i\namoti ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; m\n.mohm ---&gt; m\nmohmm ---&gt; d\nohmmd ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; h\n.radh ---&gt; e\nradhe ---&gt; y\nadhey ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; h\n..nah ---&gt; i\n.nahi ---&gt; d\nnahid ---&gt; a\nahida ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; a\n.inda ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; u\n..chu ---&gt; n\n.chun ---&gt; n\nchunn ---&gt; u\nhunnu ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; r\n.badr ---&gt; e\nbadre ---&gt; a\nadrea ---&gt; l\ndreal ---&gt; a\nreala ---&gt; m\nealam ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; v\n.durv ---&gt; e\ndurve ---&gt; s\nurves ---&gt; h\nrvesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; j\n.barj ---&gt; r\nbarjr ---&gt; a\narjra ---&gt; j\nrjraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; m\n.munm ---&gt; u\nmunmu ---&gt; n\nunmun ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; d\navind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; t\n..bat ---&gt; a\n.bata ---&gt; s\nbatas ---&gt; i\natasi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; a\n.suka ---&gt; r\nsukar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; i\n.pari ---&gt; k\nparik ---&gt; s\nariks ---&gt; h\nriksh ---&gt; a\niksha ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; b\n.babb ---&gt; i\nbabbi ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; a\n.bana ---&gt; r\nbanar ---&gt; s\nanars ---&gt; i\nnarsi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; k\n..jak ---&gt; r\n.jakr ---&gt; a\njakra ---&gt; .\n..... ---&gt; i\n....i ---&gt; a\n...ia ---&gt; r\n..iar ---&gt; f\n.iarf ---&gt; a\niarfa ---&gt; n\narfan ---&gt; .\n..... ---&gt; u\n....u ---&gt; r\n...ur ---&gt; v\n..urv ---&gt; a\n.urva ---&gt; r\nurvar ---&gt; s\nrvars ---&gt; h\nvarsh ---&gt; i\narshi ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; l\n.mitl ---&gt; e\nmitle ---&gt; s\nitles ---&gt; h\ntlesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; s\n.sams ---&gt; u\nsamsu ---&gt; d\namsud ---&gt; i\nmsudi ---&gt; n\nsudin ---&gt; .\n..... ---&gt; r\n....r ---&gt; g\n...rg ---&gt; h\n..rgh ---&gt; u\n.rghu ---&gt; r\nrghur ---&gt; a\nghura ---&gt; j\nhuraj ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; t\n..yat ---&gt; i\n.yati ---&gt; n\nyatin ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; i\nrishi ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; d\n.vand ---&gt; a\nvanda ---&gt; n\nandan ---&gt; a\nndana ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; d\n..bid ---&gt; a\n.bida ---&gt; m\nbidam ---&gt; i\nidami ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; p\n.ganp ---&gt; a\nganpa ---&gt; t\nanpat ---&gt; r\nnpatr ---&gt; a\npatra ---&gt; m\natram ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; t\n.part ---&gt; i\nparti ---&gt; k\nartik ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; b\n..sob ---&gt; h\n.sobh ---&gt; a\nsobha ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; s\n.shis ---&gt; h\nshish ---&gt; u\nhishu ---&gt; l\nishul ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; i\n.sagi ---&gt; r\nsagir ---&gt; a\nagira ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; f\n..taf ---&gt; s\n.tafs ---&gt; i\ntafsi ---&gt; r\nafsir ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; k\n.bark ---&gt; h\nbarkh ---&gt; a\narkha ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; h\ngulsh ---&gt; a\nulsha ---&gt; d\nlshad ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; y\nlakhy ---&gt; a\nakhya ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; l\n.pall ---&gt; a\npalla ---&gt; v\nallav ---&gt; i\nllavi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; e\nsanje ---&gt; y\nanjey ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; t\n..lat ---&gt; e\n.late ---&gt; s\nlates ---&gt; h\natesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; u\n.karu ---&gt; n\nkarun ---&gt; a\naruna ---&gt; k\nrunak ---&gt; a\nunaka ---&gt; r\nnakar ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; d\n.hard ---&gt; e\nharde ---&gt; e\nardee ---&gt; p\nrdeep ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; k\n.musk ---&gt; u\nmusku ---&gt; r\nuskur ---&gt; a\nskura ---&gt; n\nkuran ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; r\n..war ---&gt; e\n.ware ---&gt; e\nwaree ---&gt; s\narees ---&gt; h\nreesh ---&gt; a\neesha ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; l\n.bhol ---&gt; u\nbholu ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; l\n..sil ---&gt; a\n.sila ---&gt; .\n..... ---&gt; e\n....e ---&gt; k\n...ek ---&gt; r\n..ekr ---&gt; a\n.ekra ---&gt; m\nekram ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; l\n..mil ---&gt; i\n.mili ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; m\n..dim ---&gt; p\n.dimp ---&gt; l\ndimpl ---&gt; e\nimple ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; u\nsharu ---&gt; n\nharun ---&gt; a\naruna ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; e\n.mahe ---&gt; n\nmahen ---&gt; d\nahend ---&gt; a\nhenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; a\n.baba ---&gt; l\nbabal ---&gt; i\nabali ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; j\n.gurj ---&gt; i\ngurji ---&gt; t\nurjit ---&gt; .\n..... ---&gt; u\n....u ---&gt; p\n...up ---&gt; e\n..upe ---&gt; n\n.upen ---&gt; d\nupend ---&gt; e\npende ---&gt; r\nender ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; a\n.shya ---&gt; d\nshyad ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; i\n.rohi ---&gt; l\nrohil ---&gt; a\nohila ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; l\n..til ---&gt; a\n.tila ---&gt; k\ntilak ---&gt; r\nilakr ---&gt; a\nlakra ---&gt; m\nakram ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; p\n.roop ---&gt; s\nroops ---&gt; i\noopsi ---&gt; n\nopsin ---&gt; g\npsing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; h\n..gah ---&gt; n\n.gahn ---&gt; s\ngahns ---&gt; h\nahnsh ---&gt; y\nhnshy ---&gt; a\nnshya ---&gt; m\nshyam ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; t\nparat ---&gt; h\narath ---&gt; v\nrathv ---&gt; i\nathvi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; a\n.suna ---&gt; r\nsunar ---&gt; k\nunark ---&gt; a\nnarka ---&gt; l\narkal ---&gt; i\nrkali ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; l\nshail ---&gt; i\nhaili ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; h\n.bash ---&gt; u\nbashu ---&gt; d\nashud ---&gt; e\nshude ---&gt; v\nhudev ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; m\n..vim ---&gt; a\n.vima ---&gt; l\nvimal ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; v\n.kulv ---&gt; i\nkulvi ---&gt; n\nulvin ---&gt; d\nlvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; v\n..gov ---&gt; e\n.gove ---&gt; r\ngover ---&gt; d\noverd ---&gt; h\nverdh ---&gt; a\nerdha ---&gt; n\nrdhan ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; e\n..swe ---&gt; t\n.swet ---&gt; h\nsweth ---&gt; a\nwetha ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; n\n.jeen ---&gt; a\njeena ---&gt; t\neenat ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; s\nparas ---&gt; h\narash ---&gt; r\nrashr ---&gt; a\nashra ---&gt; m\nshram ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; m\n..asm ---&gt; i\n.asmi ---&gt; t\nasmit ---&gt; a\nsmita ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; r\n.sehr ---&gt; a\nsehra ---&gt; n\nehran ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; t\n.chit ---&gt; r\nchitr ---&gt; o\nhitro ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; l\n..tul ---&gt; s\n.tuls ---&gt; h\ntulsh ---&gt; i\nulshi ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; n\n.sohn ---&gt; i\nsohni ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; i\n.vipi ---&gt; v\nvipiv ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; i\n..bri ---&gt; n\n.brin ---&gt; d\nbrind ---&gt; a\nrinda ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; i\n..mai ---&gt; s\n.mais ---&gt; a\nmaisa ---&gt; n\naisan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; i\n.sari ---&gt; p\nsarip ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; b\nshahb ---&gt; o\nhahbo ---&gt; o\nahboo ---&gt; b\nhboob ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; u\n.vasu ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; a\nrisha ---&gt; w\nishaw ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; l\n.pall ---&gt; u\npallu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; k\n.jask ---&gt; a\njaska ---&gt; r\naskar ---&gt; a\nskara ---&gt; n\nkaran ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; t\n.nitt ---&gt; i\nnitti ---&gt; n\nittin ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; b\n.tarb ---&gt; e\ntarbe ---&gt; z\narbez ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; i\n.savi ---&gt; n\nsavin ---&gt; a\navina ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; a\n.vika ---&gt; s\nvikas ---&gt; h\nikash ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; n\n.nann ---&gt; i\nnanni ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; s\n.sais ---&gt; t\nsaist ---&gt; a\naista ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; r\n..hir ---&gt; a\n.hira ---&gt; l\nhiral ---&gt; a\nirala ---&gt; l\nralal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; a\n.shoa ---&gt; b\nshoab ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; u\n..phu ---&gt; l\n.phul ---&gt; k\nphulk ---&gt; a\nhulka ---&gt; n\nulkan ---&gt; a\nlkana ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; m\n..kom ---&gt; a\n.koma ---&gt; l\nkomal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; e\n.shre ---&gt; e\nshree ---&gt; m\nhreem ---&gt; a\nreema ---&gt; t\neemat ---&gt; i\nemati ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; i\n.hasi ---&gt; r\nhasir ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; y\n.shay ---&gt; a\nshaya ---&gt; m\nhayam ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; u\n..jau ---&gt; l\n.jaul ---&gt; i\njauli ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; d\n..bod ---&gt; h\n.bodh ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; s\n..aks ---&gt; h\n.aksh ---&gt; i\nakshi ---&gt; t\nkshit ---&gt; a\nshita ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; n\n..sin ---&gt; i\n.sini ---&gt; l\nsinil ---&gt; .\n..... ---&gt; f\n....f ---&gt; e\n...fe ---&gt; f\n..fef ---&gt; l\n.fefl ---&gt; i\nfefli ---&gt; b\neflib ---&gt; a\nfliba ---&gt; i\nlibai ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; g\n.jaig ---&gt; u\njaigu ---&gt; n\naigun ---&gt; a\niguna ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; l\nhimal ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; g\n.prag ---&gt; t\npragt ---&gt; i\nragti ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; j\n.bhaj ---&gt; j\nbhajj ---&gt; u\nhajju ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; e\n..swe ---&gt; t\n.swet ---&gt; n\nswetn ---&gt; a\nwetna ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; s\n..res ---&gt; h\n.resh ---&gt; a\nresha ---&gt; m\nesham ---&gt; a\nshama ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; e\n.bale ---&gt; s\nbales ---&gt; h\nalesh ---&gt; w\nleshw ---&gt; a\neshwa ---&gt; r\nshwar ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; i\n.kani ---&gt; s\nkanis ---&gt; k\nanisk ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; g\n..mog ---&gt; l\n.mogl ---&gt; i\nmogli ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; t\n.kant ---&gt; i\nkanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; n\n.sann ---&gt; o\nsanno ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; l\n..sil ---&gt; v\n.silv ---&gt; i\nsilvi ---&gt; a\nilvia ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; b\n..kab ---&gt; u\n.kabu ---&gt; l\nkabul ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; u\n.naju ---&gt; b\nnajub ---&gt; a\najuba ---&gt; i\njubai ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; i\n.sani ---&gt; y\nsaniy ---&gt; a\naniya ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; a\nchota ---&gt; n\nhotan ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; i\n.rubi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; f\nsaraf ---&gt; a\narafa ---&gt; t\nrafat ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; k\n.pank ---&gt; u\npanku ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; i\n..abi ---&gt; s\n.abis ---&gt; h\nabish ---&gt; e\nbishe ---&gt; k\nishek ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; n\nrajin ---&gt; d\najind ---&gt; e\njinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; s\n.mans ---&gt; o\nmanso ---&gt; o\nansoo ---&gt; r\nnsoor ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; a\n.saga ---&gt; r\nsagar ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; e\n.jame ---&gt; e\njamee ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; a\nshara ---&gt; t\nharat ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; u\n..rau ---&gt; f\n.rauf ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; n\n..gun ---&gt; g\n.gung ---&gt; u\ngungu ---&gt; n\nungun ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; z\n..roz ---&gt; i\n.rozi ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; y\n..koy ---&gt; a\n.koya ---&gt; l\nkoyal ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; e\ndhane ---&gt; y\nhaney ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; i\nsandi ---&gt; y\nandiy ---&gt; a\nndiya ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; l\n..jul ---&gt; f\n.julf ---&gt; i\njulfi ---&gt; k\nulfik ---&gt; a\nlfika ---&gt; a\nfikaa ---&gt; r\nikaar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; j\n.ranj ---&gt; a\nranja ---&gt; y\nanjay ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; n\nsahin ---&gt; a\nahina ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; y\n.deey ---&gt; a\ndeeya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; g\n.narg ---&gt; e\nnarge ---&gt; s\narges ---&gt; h\nrgesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; m\n..mum ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; n\n..ton ---&gt; y\n.tony ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; d\n.gand ---&gt; h\ngandh ---&gt; a\nandha ---&gt; r\nndhar ---&gt; v\ndharv ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; v\n.bhav ---&gt; i\nbhavi ---&gt; s\nhavis ---&gt; h\navish ---&gt; y\nvishy ---&gt; a\nishya ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; e\n.jane ---&gt; s\njanes ---&gt; h\nanesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; d\n.sund ---&gt; e\nsunde ---&gt; r\nunder ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; d\nshard ---&gt; h\nhardh ---&gt; a\nardha ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; r\n.sher ---&gt; .\n..... ---&gt; s\n....s ---&gt; r\n...sr ---&gt; i\n..sri ---&gt; k\n.srik ---&gt; a\nsrika ---&gt; n\nrikan ---&gt; t\nikant ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; b\n..nab ---&gt; a\n.naba ---&gt; v\nnabav ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; i\nmangi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; i\n.jami ---&gt; l\njamil ---&gt; a\namila ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; r\n.sukr ---&gt; i\nsukri ---&gt; t\nukrit ---&gt; a\nkrita ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; w\n..niw ---&gt; a\n.niwa ---&gt; s\nniwas ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; k\n.sark ---&gt; a\nsarka ---&gt; r\narkar ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; s\n.alis ---&gt; h\nalish ---&gt; a\nlisha ---&gt; .\n..... ---&gt; a\n....a ---&gt; t\n...at ---&gt; t\n..att ---&gt; a\n.atta ---&gt; r\nattar ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; k\n..nak ---&gt; c\n.nakc ---&gt; h\nnakch ---&gt; e\nakche ---&gt; d\nkched ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; p\n.rasp ---&gt; a\nraspa ---&gt; l\naspal ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; b\n..amb ---&gt; i\n.ambi ---&gt; k\nambik ---&gt; a\nmbika ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; g\n..jig ---&gt; a\n.jiga ---&gt; r\njigar ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; e\n.sube ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; c\n..suc ---&gt; c\n.succ ---&gt; h\nsucch ---&gt; a\nuccha ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; g\n..shg ---&gt; i\n.shgi ---&gt; t\nshgit ---&gt; a\nhgita ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; n\n.pran ---&gt; s\nprans ---&gt; i\nransi ---&gt; s\nansis ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; f\n..naf ---&gt; e\n.nafe ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; b\n.birb ---&gt; a\nbirba ---&gt; l\nirbal ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; m\nubham ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; i\nshaki ---&gt; n\nhakin ---&gt; a\nakina ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; u\n..you ---&gt; r\n.your ---&gt; a\nyoura ---&gt; j\nouraj ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; i\n..tai ---&gt; y\n.taiy ---&gt; a\ntaiya ---&gt; b\naiyab ---&gt; .\n..... ---&gt; u\n....u ---&gt; g\n...ug ---&gt; a\n..uga ---&gt; n\n.ugan ---&gt; t\nugant ---&gt; a\nganta ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; u\n.saku ---&gt; n\nsakun ---&gt; t\nakunt ---&gt; l\nkuntl ---&gt; a\nuntla ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; m\n.noom ---&gt; i\nnoomi ---&gt; .\n..... ---&gt; g\n....g ---&gt; r\n...gr ---&gt; a\n..gra ---&gt; c\n.grac ---&gt; e\ngrace ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; k\n.mahk ---&gt; a\nmahka ---&gt; r\nahkar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; e\nparve ---&gt; e\narvee ---&gt; n\nrveen ---&gt; a\nveena ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; l\nmangl ---&gt; e\nangle ---&gt; s\nngles ---&gt; h\nglesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; n\n.prin ---&gt; k\nprink ---&gt; y\nrinky ---&gt; a\ninkya ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; p\n..nep ---&gt; a\n.nepa ---&gt; l\nnepal ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; v\n.gurv ---&gt; i\ngurvi ---&gt; n\nurvin ---&gt; d\nrvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; l\n..sul ---&gt; o\n.sulo ---&gt; c\nsuloc ---&gt; h\nuloch ---&gt; n\nlochn ---&gt; a\nochna ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; d\n..ved ---&gt; a\n.veda ---&gt; n\nvedan ---&gt; a\nedana ---&gt; n\ndanan ---&gt; d\nanand ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; n\n.uman ---&gt; a\numana ---&gt; t\nmanat ---&gt; h\nanath ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; l\n..wal ---&gt; i\n.wali ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; s\nrahis ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; n\neepan ---&gt; k\nepank ---&gt; e\npanke ---&gt; r\nanker ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; i\n..ari ---&gt; n\n.arin ---&gt; d\narind ---&gt; o\nrindo ---&gt; m\nindom ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; t\n.munt ---&gt; j\nmuntj ---&gt; a\nuntja ---&gt; r\nntjar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; a\n.sana ---&gt; u\nsanau ---&gt; l\nanaul ---&gt; l\nnaull ---&gt; a\naulla ---&gt; h\nullah ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; m\n.jasm ---&gt; e\njasme ---&gt; e\nasmee ---&gt; t\nsmeet ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; p\n..kap ---&gt; i\n.kapi ---&gt; i\nkapii ---&gt; l\napiil ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; k\n..ink ---&gt; o\n.inko ---&gt; o\ninkoo ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; e\n.rine ---&gt; s\nrines ---&gt; h\ninesh ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; k\n..lok ---&gt; e\n.loke ---&gt; n\nloken ---&gt; d\nokend ---&gt; r\nkendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; h\n..jah ---&gt; a\n.jaha ---&gt; g\njahag ---&gt; i\nahagi ---&gt; r\nhagir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; v\n.satv ---&gt; e\nsatve ---&gt; e\natvee ---&gt; r\ntveer ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; r\n.dhur ---&gt; u\ndhuru ---&gt; v\nhuruv ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; e\n.some ---&gt; n\nsomen ---&gt; d\nomend ---&gt; e\nmende ---&gt; r\nender ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; h\n.mith ---&gt; u\nmithu ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; r\n..mor ---&gt; k\n.mork ---&gt; i\nmorki ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; i\n.muni ---&gt; s\nmunis ---&gt; h\nunish ---&gt; .\n..... ---&gt; m\n....m ---&gt; h\n...mh ---&gt; o\n..mho ---&gt; o\n.mhoo ---&gt; m\nmhoom ---&gt; a\nhooma ---&gt; d\noomad ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; e\n..ume ---&gt; d\n.umed ---&gt; i\numedi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; m\nakshm ---&gt; i\nkshmi ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; j\n..gaj ---&gt; e\n.gaje ---&gt; n\ngajen ---&gt; d\najend ---&gt; e\njende ---&gt; r\nender ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; n\n.tann ---&gt; u\ntannu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; j\n..kaj ---&gt; a\n.kaja ---&gt; l\nkajal ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; l\n.kall ---&gt; u\nkallu ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; n\n.mann ---&gt; i\nmanni ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; e\n.nite ---&gt; s\nnites ---&gt; h\nitesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; a\n.kala ---&gt; s\nkalas ---&gt; h\nalash ---&gt; i\nlashi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; u\nvishu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; u\nmadhu ---&gt; n\nadhun ---&gt; i\ndhuni ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; i\n.chhi ---&gt; t\nchhit ---&gt; a\nhhita ---&gt; r\nhitar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; y\n..may ---&gt; u\n.mayu ---&gt; r\nmayur ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; c\n.sanc ---&gt; h\nsanch ---&gt; i\nanchi ---&gt; t\nnchit ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; a\n.suja ---&gt; t\nsujat ---&gt; a\nujata ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; d\n.sehd ---&gt; e\nsehde ---&gt; v\nehdev ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; u\n..dau ---&gt; l\n.daul ---&gt; a\ndaula ---&gt; t\naulat ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; m\n..tum ---&gt; p\n.tump ---&gt; a\ntumpa ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; s\n.pras ---&gt; h\nprash ---&gt; a\nrasha ---&gt; n\nashan ---&gt; s\nshans ---&gt; a\nhansa ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; u\n..dhu ---&gt; r\n.dhur ---&gt; v\ndhurv ---&gt; e\nhurve ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; l\nshail ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; l\n..gal ---&gt; i\n.gali ---&gt; y\ngaliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; n\n.sahn ---&gt; a\nsahna ---&gt; j\nahnaj ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; n\n.anan ---&gt; d\nanand ---&gt; u\nnandu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; e\n.mahe ---&gt; s\nmahes ---&gt; h\nahesh ---&gt; a\nhesha ---&gt; r\neshar ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; a\nneela ---&gt; j\neelaj ---&gt; a\nelaja ---&gt; n\nlajan ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; t\n..ant ---&gt; i\n.anti ---&gt; m\nantim ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; i\n..mai ---&gt; r\n.mair ---&gt; i\nmairi ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; s\n..was ---&gt; e\n.wase ---&gt; e\nwasee ---&gt; m\naseem ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; e\n.hare ---&gt; e\nharee ---&gt; s\narees ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; u\n.samu ---&gt; n\nsamun ---&gt; d\namund ---&gt; r\nmundr ---&gt; i\nundri ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; f\n.manf ---&gt; o\nmanfo ---&gt; o\nanfoo ---&gt; l\nnfool ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; v\n.shav ---&gt; a\nshava ---&gt; n\nhavan ---&gt; i\navani ---&gt; .\n..... ---&gt; f\n....f ---&gt; h\n...fh ---&gt; a\n..fha ---&gt; r\n.fhar ---&gt; m\nfharm ---&gt; a\nharma ---&gt; n\narman ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; m\n..ajm ---&gt; a\n.ajma ---&gt; n\najman ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; m\n..ism ---&gt; a\n.isma ---&gt; l\nismal ---&gt; i\nsmali ---&gt; y\nmaliy ---&gt; e\naliye ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; o\nsanjo ---&gt; g\nanjog ---&gt; t\nnjogt ---&gt; a\njogta ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; c\nchanc ---&gt; h\nhanch ---&gt; a\nancha ---&gt; l\nnchal ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; s\n..ins ---&gt; a\n.insa ---&gt; f\ninsaf ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; o\n..ano ---&gt; o\n.anoo ---&gt; p\nanoop ---&gt; a\nnoopa ---&gt; m\noopam ---&gt; a\nopama ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; k\n.rook ---&gt; m\nrookm ---&gt; a\nookma ---&gt; n\nokman ---&gt; i\nkmani ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; h\n.kosh ---&gt; a\nkosha ---&gt; l\noshal ---&gt; y\nshaly ---&gt; a\nhalya ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; s\n.dils ---&gt; a\ndilsa ---&gt; d\nilsad ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; a\n.inda ---&gt; l\nindal ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; h\n.mith ---&gt; l\nmithl ---&gt; e\nithle ---&gt; s\nthles ---&gt; h\nhlesh ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; m\n.nazm ---&gt; i\nnazmi ---&gt; n\nazmin ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; i\n..aki ---&gt; b\n.akib ---&gt; .\n..... ---&gt; d\n....d ---&gt; v\n...dv ---&gt; n\n..dvn ---&gt; d\n.dvnd ---&gt; a\ndvnda ---&gt; v\nvndav ---&gt; v\nndavv ---&gt; a\ndavva ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; a\n.suna ---&gt; r\nsunar ---&gt; k\nunark ---&gt; i\nnarki ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; j\n.amrj ---&gt; e\namrje ---&gt; e\nmrjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; l\n.shil ---&gt; p\nshilp ---&gt; a\nhilpa ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; n\n..bun ---&gt; d\n.bund ---&gt; e\nbunde ---&gt; l\nundel ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; a\nhanda ---&gt; n\nandan ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; m\n..asm ---&gt; i\n.asmi ---&gt; n\nasmin ---&gt; a\nsmina ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; n\n.alin ---&gt; a\nalina ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; s\n..sos ---&gt; r\n.sosr ---&gt; i\nsosri ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; h\n.vidh ---&gt; a\nvidha ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; z\n..afz ---&gt; a\n.afza ---&gt; l\nafzal ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; r\n.kamr ---&gt; u\nkamru ---&gt; d\namrud ---&gt; e\nmrude ---&gt; e\nrudee ---&gt; n\nudeen ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; u\n.shau ---&gt; k\nshauk ---&gt; a\nhauka ---&gt; t\naukat ---&gt; .\n..... ---&gt; t\n....t ---&gt; h\n...th ---&gt; o\n..tho ---&gt; m\n.thom ---&gt; a\nthoma ---&gt; s\nhomas ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; t\n.must ---&gt; k\nmustk ---&gt; i\nustki ---&gt; m\nstkim ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; a\n.vija ---&gt; y\nvijay ---&gt; n\nijayn ---&gt; t\njaynt ---&gt; i\naynti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; i\n.sadi ---&gt; y\nsadiy ---&gt; a\nadiya ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; o\nmadho ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; l\n..hal ---&gt; i\n.hali ---&gt; m\nhalim ---&gt; a\nalima ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; z\n.shaz ---&gt; a\nshaza ---&gt; d\nhazad ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; y\nshivy ---&gt; a\nhivya ---&gt; l\nivyal ---&gt; a\nvyala ---&gt; m\nyalam ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; m\n.karm ---&gt; i\nkarmi ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; t\n.chat ---&gt; e\nchate ---&gt; r\nhater ---&gt; p\naterp ---&gt; a\nterpa ---&gt; l\nerpal ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; d\n..rud ---&gt; m\n.rudm ---&gt; a\nrudma ---&gt; l\nudmal ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; v\n..dav ---&gt; e\n.dave ---&gt; e\ndavee ---&gt; n\naveen ---&gt; a\nveena ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; m\nkushm ---&gt; a\nushma ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; d\n..nad ---&gt; e\n.nade ---&gt; m\nnadem ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; e\n.jite ---&gt; s\njites ---&gt; h\nitesh ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; a\n.jana ---&gt; k\njanak ---&gt; i\nanaki ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; m\n..sim ---&gt; i\n.simi ---&gt; l\nsimil ---&gt; a\nimila ---&gt; t\nmilat ---&gt; a\nilata ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; y\n.tany ---&gt; a\ntanya ---&gt; .\n..... ---&gt; a\n....a ---&gt; y\n...ay ---&gt; u\n..ayu ---&gt; s\n.ayus ---&gt; h\nayush ---&gt; .\n..... ---&gt; i\n....i ---&gt; l\n...il ---&gt; e\n..ile ---&gt; s\n.iles ---&gt; h\nilesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; r\n.ashr ---&gt; a\nashra ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; r\n.neer ---&gt; u\nneeru ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; a\n.rupa ---&gt; k\nrupak ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; s\nrajis ---&gt; h\najish ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; m\n..arm ---&gt; a\n.arma ---&gt; n\narman ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; l\n.bhul ---&gt; a\nbhula ---&gt; n\nhulan ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; n\n.meen ---&gt; a\nmeena ---&gt; x\neenax ---&gt; i\nenaxi ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; a\n.mona ---&gt; m\nmonam ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; r\n..tir ---&gt; a\n.tira ---&gt; t\ntirat ---&gt; h\nirath ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; e\n.fare ---&gt; e\nfaree ---&gt; n\nareen ---&gt; a\nreena ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; s\n.shus ---&gt; h\nshush ---&gt; r\nhushr ---&gt; e\nushre ---&gt; e\nshree ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; e\n.rupe ---&gt; n\nrupen ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; u\n..sau ---&gt; h\n.sauh ---&gt; a\nsauha ---&gt; l\nauhal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; b\n.sabb ---&gt; i\nsabbi ---&gt; r\nabbir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; s\n.saks ---&gt; h\nsaksh ---&gt; a\naksha ---&gt; m\nksham ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; k\n..prk ---&gt; e\n.prke ---&gt; s\nprkes ---&gt; h\nrkesh ---&gt; .\n..... ---&gt; u\n....u ---&gt; p\n...up ---&gt; a\n..upa ---&gt; s\n.upas ---&gt; a\nupasa ---&gt; n\npasan ---&gt; a\nasana ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; s\n.jais ---&gt; i\njaisi ---&gt; g\naisig ---&gt; h\nisigh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; t\n.mast ---&gt; e\nmaste ---&gt; r\naster ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; i\nshadi ---&gt; k\nhadik ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; m\n.shim ---&gt; r\nshimr ---&gt; a\nhimra ---&gt; n\nimran ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; i\n..aki ---&gt; l\n.akil ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; r\n.poor ---&gt; a\npoora ---&gt; n\nooran ---&gt; m\noranm ---&gt; a\nranma ---&gt; l\nanmal ---&gt; .\n..... ---&gt; m\n....m ---&gt; h\n...mh ---&gt; o\n..mho ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; i\n.soni ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; v\ndharv ---&gt; e\nharve ---&gt; s\narves ---&gt; h\nrvesh ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; n\n..jin ---&gt; a\n.jina ---&gt; t\njinat ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; e\n.deve ---&gt; s\ndeves ---&gt; h\nevesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; i\n..asi ---&gt; f\n.asif ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; l\n.babl ---&gt; i\nbabli ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; v\n.triv ---&gt; e\ntrive ---&gt; n\nriven ---&gt; i\niveni ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; d\n.kund ---&gt; i\nkundi ---&gt; n\nundin ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; r\n..kur ---&gt; e\n.kure ---&gt; j\nkurej ---&gt; a\nureja ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; e\n.saie ---&gt; s\nsaies ---&gt; t\naiest ---&gt; a\niesta ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; t\n.bant ---&gt; u\nbantu ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; p\n.bhup ---&gt; e\nbhupe ---&gt; n\nhupen ---&gt; d\nupend ---&gt; e\npende ---&gt; r\nender ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; i\n.moni ---&gt; n\nmonin ---&gt; i\nonini ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; n\nshann ---&gt; o\nhanno ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; i\n.shri ---&gt; p\nshrip ---&gt; a\nhripa ---&gt; l\nripal ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; l\n.babl ---&gt; o\nbablo ---&gt; o\nabloo ---&gt; .\n..... ---&gt; g\n....g ---&gt; r\n...gr ---&gt; i\n..gri ---&gt; s\n.gris ---&gt; h\ngrish ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; r\n.anur ---&gt; u\nanuru ---&gt; d\nnurud ---&gt; h\nurudh ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; r\n.mehr ---&gt; u\nmehru ---&gt; d\nehrud ---&gt; d\nhrudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; s\n..kos ---&gt; h\n.kosh ---&gt; a\nkosha ---&gt; l\noshal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; b\n.shub ---&gt; h\nshubh ---&gt; m\nhubhm ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; e\n.ompe ---&gt; r\nomper ---&gt; k\nmperk ---&gt; a\nperka ---&gt; s\nerkas ---&gt; h\nrkash ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; a\nsatya ---&gt; p\natyap ---&gt; a\ntyapa ---&gt; l\nyapal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; t\n..mat ---&gt; a\n.mata ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; i\n..chi ---&gt; t\n.chit ---&gt; p\nchitp ---&gt; r\nhitpr ---&gt; i\nitpri ---&gt; t\ntprit ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; h\n..leh ---&gt; r\n.lehr ---&gt; u\nlehru ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; m\n..ajm ---&gt; e\n.ajme ---&gt; r\najmer ---&gt; i\njmeri ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; n\n.gurn ---&gt; a\ngurna ---&gt; m\nurnam ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; h\n..vih ---&gt; s\n.vihs ---&gt; a\nvihsa ---&gt; l\nihsal ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; t\n..not ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; i\n.kari ---&gt; m\nkarim ---&gt; a\narima ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; s\nabhis ---&gt; e\nbhise ---&gt; k\nhisek ---&gt; h\nisekh ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; e\n.rame ---&gt; s\nrames ---&gt; h\namesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; k\n..aak ---&gt; a\n.aaka ---&gt; s\naakas ---&gt; .\n..... ---&gt; a\n....a ---&gt; i\n...ai ---&gt; s\n..ais ---&gt; h\n.aish ---&gt; w\naishw ---&gt; a\nishwa ---&gt; r\nshwar ---&gt; y\nhwary ---&gt; a\nwarya ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; a\n.aana ---&gt; d\naanad ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; k\n.anik ---&gt; t\nanikt ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; w\n..jaw ---&gt; a\n.jawa ---&gt; h\njawah ---&gt; a\nawaha ---&gt; r\nwahar ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; l\n.akhl ---&gt; e\nakhle ---&gt; s\nkhles ---&gt; h\nhlesh ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; h\n.vidh ---&gt; i\nvidhi ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; s\n..bas ---&gt; a\n.basa ---&gt; n\nbasan ---&gt; t\nasant ---&gt; a\nsanta ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; i\n.tani ---&gt; s\ntanis ---&gt; h\nanish ---&gt; a\nnisha ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; n\n.sugn ---&gt; a\nsugna ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; z\n.saiz ---&gt; a\nsaiza ---&gt; d\naizad ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; e\nprite ---&gt; e\nritee ---&gt; .\n..... ---&gt; m\n....m ---&gt; m\n...mm ---&gt; t\n..mmt ---&gt; a\n.mmta ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; b\n..lab ---&gt; b\n.labb ---&gt; h\nlabbh ---&gt; i\nabbhi ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; b\n.nirb ---&gt; h\nnirbh ---&gt; a\nirbha ---&gt; y\nrbhay ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; h\n.rakh ---&gt; a\nrakha ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; u\n..aru ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; h\n.rith ---&gt; i\nrithi ---&gt; k\nithik ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; s\n..tos ---&gt; i\n.tosi ---&gt; f\ntosif ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; h\n.aash ---&gt; i\naashi ---&gt; s\nashis ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; d\n..nad ---&gt; h\n.nadh ---&gt; i\nnadhi ---&gt; m\nadhim ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; a\n.nana ---&gt; g\nnanag ---&gt; r\nanagr ---&gt; a\nnagra ---&gt; m\nagram ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; a\n.sura ---&gt; i\nsurai ---&gt; y\nuraiy ---&gt; a\nraiya ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; h\n.kash ---&gt; i\nkashi ---&gt; s\nashis ---&gt; h\nshish ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; f\n.gulf ---&gt; a\ngulfa ---&gt; m\nulfam ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; k\n.munk ---&gt; a\nmunka ---&gt; d\nunkad ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; n\namarn ---&gt; a\nmarna ---&gt; t\narnat ---&gt; h\nrnath ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; n\nchhan ---&gt; o\nhhano ---&gt; y\nhanoy ---&gt; a\nanoya ---&gt; r\nnoyar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; a\nshana ---&gt; w\nhanaw ---&gt; a\nanawa ---&gt; z\nnawaz ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; e\n..tee ---&gt; n\n.teen ---&gt; a\nteena ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; s\n.sams ---&gt; i\nsamsi ---&gt; d\namsid ---&gt; a\nmsida ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; n\n.satn ---&gt; a\nsatna ---&gt; m\natnam ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; o\n.firo ---&gt; z\nfiroz ---&gt; a\niroza ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; n\n..rin ---&gt; k\n.rink ---&gt; a\nrinka ---&gt; l\ninkal ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; i\nramki ---&gt; s\namkis ---&gt; h\nmkish ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; o\n.kiro ---&gt; r\nkiror ---&gt; i\nirori ---&gt; m\nrorim ---&gt; a\norima ---&gt; l\nrimal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; e\nsanje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; t\n.jeet ---&gt; u\njeetu ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; t\n..fat ---&gt; e\n.fate ---&gt; h\nfateh ---&gt; .\n..... ---&gt; u\n....u ---&gt; s\n...us ---&gt; h\n..ush ---&gt; a\n.usha ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; r\n.meer ---&gt; a\nmeera ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; i\n.mari ---&gt; a\nmaria ---&gt; m\nariam ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; r\n..ser ---&gt; u\n.seru ---&gt; l\nserul ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; d\n.sard ---&gt; e\nsarde ---&gt; e\nardee ---&gt; p\nrdeep ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; a\nhanda ---&gt; r\nandar ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; j\n..muj ---&gt; i\n.muji ---&gt; b\nmujib ---&gt; u\nujibu ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; y\n..soy ---&gt; a\n.soya ---&gt; b\nsoyab ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; p\n..hap ---&gt; p\n.happ ---&gt; y\nhappy ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; s\n.murs ---&gt; i\nmursi ---&gt; d\nursid ---&gt; a\nrsida ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; u\n.ritu ---&gt; r\nritur ---&gt; a\nitura ---&gt; j\nturaj ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; n\n..jen ---&gt; n\n.jenn ---&gt; y\njenny ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; s\ngulas ---&gt; h\nulash ---&gt; a\nlasha ---&gt; n\nashan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; w\n.sahw ---&gt; a\nsahwa ---&gt; j\nahwaj ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; e\nprate ---&gt; e\nratee ---&gt; k\nateek ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; t\n.sart ---&gt; h\nsarth ---&gt; a\nartha ---&gt; k\nrthak ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; a\n..uma ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; o\n.firo ---&gt; j\nfiroj ---&gt; a\niroja ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; w\n.kunw ---&gt; a\nkunwa ---&gt; r\nunwar ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; j\npramj ---&gt; o\nramjo ---&gt; t\namjot ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; r\n.ashr ---&gt; a\nashra ---&gt; b\nshrab ---&gt; i\nhrabi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; a\n.rata ---&gt; n\nratan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; c\n..rac ---&gt; h\n.rach ---&gt; a\nracha ---&gt; n\nachan ---&gt; a\nchana ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; y\n..riy ---&gt; a\n.riya ---&gt; j\nriyaj ---&gt; u\niyaju ---&gt; l\nyajul ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; z\n..anz ---&gt; u\n.anzu ---&gt; m\nanzum ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; o\n.phoo ---&gt; l\nphool ---&gt; o\nhoolo ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; z\n..haz ---&gt; a\n.haza ---&gt; r\nhazar ---&gt; a\nazara ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; y\n..nay ---&gt; a\n.naya ---&gt; n\nnayan ---&gt; a\nayana ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; v\n..jav ---&gt; i\n.javi ---&gt; t\njavit ---&gt; r\navitr ---&gt; i\nvitri ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; b\n.kulb ---&gt; i\nkulbi ---&gt; r\nulbir ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; b\n..bab ---&gt; a\n.baba ---&gt; l\nbabal ---&gt; u\nabalu ---&gt; .\n..... ---&gt; l\n....l ---&gt; e\n...le ---&gt; e\n..lee ---&gt; l\n.leel ---&gt; a\nleela ---&gt; w\neelaw ---&gt; a\nelawa ---&gt; t\nlawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; w\n.ishw ---&gt; a\nishwa ---&gt; r\nshwar ---&gt; i\nhwari ---&gt; .\n..... ---&gt; i\n....i ---&gt; n\n...in ---&gt; d\n..ind ---&gt; r\n.indr ---&gt; v\nindrv ---&gt; a\nndrva ---&gt; t\ndrvat ---&gt; i\nrvati ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; v\n.ranv ---&gt; e\nranve ---&gt; e\nanvee ---&gt; r\nnveer ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; f\n.gulf ---&gt; s\ngulfs ---&gt; a\nulfsa ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; i\n.gudi ---&gt; y\ngudiy ---&gt; a\nudiya ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; h\nparsh ---&gt; o\narsho ---&gt; t\nrshot ---&gt; a\nshota ---&gt; m\nhotam ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; t\n.kamt ---&gt; a\nkamta ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; e\n.sure ---&gt; k\nsurek ---&gt; h\nurekh ---&gt; a\nrekha ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; c\n..sac ---&gt; h\n.sach ---&gt; i\nsachi ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; k\n..akk ---&gt; a\n.akka ---&gt; s\nakkas ---&gt; h\nkkash ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; m\nmoham ---&gt; m\nohamm ---&gt; a\nhamma ---&gt; d\nammad ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; l\n..tol ---&gt; a\n.tola ---&gt; r\ntolar ---&gt; a\nolara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; m\n..lam ---&gt; b\n.lamb ---&gt; h\nlambh ---&gt; u\nambhu ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; n\neepan ---&gt; s\nepans ---&gt; h\npansh ---&gt; u\nanshu ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; i\n.vidi ---&gt; t\nvidit ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; m\nmoham ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; u\n.ritu ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; j\n..fij ---&gt; a\n.fija ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; t\n..vit ---&gt; a\n.vita ---&gt; n\nvitan ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; n\n.aman ---&gt; a\namana ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; a\n.anja ---&gt; n\nanjan ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; b\nchhab ---&gt; i\nhhabi ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; r\n.sehr ---&gt; u\nsehru ---&gt; n\nehrun ---&gt; i\nhruni ---&gt; s\nrunis ---&gt; a\nunisa ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; l\n.pool ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; h\nrambh ---&gt; a\nambha ---&gt; r\nmbhar ---&gt; o\nbharo ---&gt; s\nharos ---&gt; .\n..... ---&gt; h\n....h ---&gt; o\n...ho ---&gt; m\n..hom ---&gt; a\n.homa ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; a\nneela ---&gt; c\neelac ---&gt; h\nelach ---&gt; a\nlacha ---&gt; l\nachal ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; z\n..raz ---&gt; i\n.razi ---&gt; y\nraziy ---&gt; a\naziya ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; l\nrahil ---&gt; a\nahila ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; i\n.mini ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; i\n.bani ---&gt; y\nbaniy ---&gt; a\naniya ---&gt; .\n..... ---&gt; a\n....a ---&gt; z\n...az ---&gt; a\n..aza ---&gt; m\n.azam ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; a\n.bina ---&gt; n\nbinan ---&gt; i\ninani ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; n\n..dan ---&gt; w\n.danw ---&gt; a\ndanwa ---&gt; t\nanwat ---&gt; i\nnwati ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; o\n..jho ---&gt; t\n.jhot ---&gt; i\njhoti ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; g\n..meg ---&gt; a\n.mega ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; o\n.kuso ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; z\n..niz ---&gt; a\n.niza ---&gt; m\nnizam ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; e\n.yoge ---&gt; n\nyogen ---&gt; d\nogend ---&gt; e\ngende ---&gt; r\nender ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; a\n.kuma ---&gt; r\nkumar ---&gt; a\numara ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; r\n.ashr ---&gt; a\nashra ---&gt; f\nshraf ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; s\nabhis ---&gt; e\nbhise ---&gt; k\nhisek ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; s\n.guls ---&gt; i\ngulsi ---&gt; d\nulsid ---&gt; a\nlsida ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; h\n.yash ---&gt; m\nyashm ---&gt; i\nashmi ---&gt; n\nshmin ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; s\n.kuls ---&gt; u\nkulsu ---&gt; m\nulsum ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; m\n..yam ---&gt; a\n.yama ---&gt; n\nyaman ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; t\n..alt ---&gt; a\n.alta ---&gt; f\naltaf ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; b\n..pab ---&gt; u\n.pabu ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; v\n..biv ---&gt; l\n.bivl ---&gt; a\nbivla ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; i\n..phi ---&gt; l\n.phil ---&gt; o\nphilo ---&gt; m\nhilom ---&gt; i\nilomi ---&gt; n\nlomin ---&gt; a\nomina ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; n\n..tin ---&gt; k\n.tink ---&gt; i\ntinki ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; y\n..shy ---&gt; a\n.shya ---&gt; n\nshyan ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; u\nhandu ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; d\n..rad ---&gt; a\n.rada ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; a\n.ruka ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; o\nparmo ---&gt; d\narmod ---&gt; h\nrmodh ---&gt; .\n..... ---&gt; e\n....e ---&gt; v\n...ev ---&gt; a\n..eva ---&gt; n\n.evan ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; i\n.vasi ---&gt; m\nvasim ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; k\n.lavk ---&gt; u\nlavku ---&gt; s\navkus ---&gt; h\nvkush ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; g\nubhag ---&gt; i\nbhagi ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; n\npriyn ---&gt; k\nriynk ---&gt; a\niynka ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; u\n..gou ---&gt; r\n.gour ---&gt; a\ngoura ---&gt; v\nourav ---&gt; .\n..... ---&gt; v\n....v ---&gt; y\n...vy ---&gt; e\n..vye ---&gt; l\n.vyel ---&gt; e\nvyele ---&gt; t\nyelet ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; w\nbhanw ---&gt; e\nhanwe ---&gt; r\nanwer ---&gt; i\nnweri ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; g\nchhag ---&gt; a\nhhaga ---&gt; n\nhagan ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; i\n.ashi ---&gt; m\nashim ---&gt; a\nshima ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; a\n.bala ---&gt; l\nbalal ---&gt; i\nalali ---&gt; a\nlalia ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; h\n.nish ---&gt; t\nnisht ---&gt; h\nishth ---&gt; a\nshtha ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; r\n..sir ---&gt; a\n.sira ---&gt; j\nsiraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; e\n.sate ---&gt; n\nsaten ---&gt; d\natend ---&gt; r\ntendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; e\n.rame ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; g\n.swag ---&gt; t\nswagt ---&gt; i\nwagti ---&gt; k\nagtik ---&gt; a\ngtika ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; m\n.sajm ---&gt; a\nsajma ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; b\n.narb ---&gt; a\nnarba ---&gt; d\narbad ---&gt; a\nrbada ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; c\n..arc ---&gt; h\n.arch ---&gt; a\narcha ---&gt; n\nrchan ---&gt; a\nchana ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; w\n..asw ---&gt; a\n.aswa ---&gt; n\naswan ---&gt; i\nswani ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; y\n..say ---&gt; a\n.saya ---&gt; r\nsayar ---&gt; i\nayari ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; c\n..bec ---&gt; h\n.bech ---&gt; a\nbecha ---&gt; n\nechan ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; s\n.pras ---&gt; a\nprasa ---&gt; n\nrasan ---&gt; t\nasant ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; i\nrishi ---&gt; r\nishir ---&gt; a\nshira ---&gt; j\nhiraj ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; e\n..nae ---&gt; e\n.naee ---&gt; m\nnaeem ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; p\n.bhup ---&gt; e\nbhupe ---&gt; n\nhupen ---&gt; d\nupend ---&gt; r\npendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; c\n..kac ---&gt; h\n.kach ---&gt; a\nkacha ---&gt; n\nachan ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; y\n..joy ---&gt; a\n.joya ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; g\n.mang ---&gt; a\nmanga ---&gt; l\nangal ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; k\n..sok ---&gt; a\n.soka ---&gt; t\nsokat ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; l\n.pall ---&gt; v\npallv ---&gt; i\nallvi ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; c\ndeepc ---&gt; h\neepch ---&gt; a\nepcha ---&gt; n\npchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; f\n.gulf ---&gt; a\ngulfa ---&gt; s\nulfas ---&gt; h\nlfash ---&gt; a\nfasha ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; n\n..chn ---&gt; d\n.chnd ---&gt; a\nchnda ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; t\n..art ---&gt; i\n.arti ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; c\n..vic ---&gt; c\n.vicc ---&gt; k\nvicck ---&gt; y\niccky ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; d\n..kud ---&gt; e\n.kude ---&gt; e\nkudee ---&gt; p\nudeep ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; e\n.sume ---&gt; s\nsumes ---&gt; h\numesh ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; a\n.fara ---&gt; h\nfarah ---&gt; a\naraha ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; z\n..muz ---&gt; m\n.muzm ---&gt; i\nmuzmi ---&gt; l\nuzmil ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; o\nsanto ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; d\n..bod ---&gt; u\n.bodu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; s\nmanis ---&gt; h\nanish ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; a\nvisha ---&gt; l\nishal ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; l\n..til ---&gt; a\n.tila ---&gt; k\ntilak ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; u\nanchu ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; f\n..haf ---&gt; s\n.hafs ---&gt; a\nhafsa ---&gt; .\n..... ---&gt; c\n....c ---&gt; a\n...ca ---&gt; p\n..cap ---&gt; t\n.capt ---&gt; a\ncapta ---&gt; i\naptai ---&gt; n\nptain ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; l\nsahil ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; i\n..ami ---&gt; t\n.amit ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; j\n.surj ---&gt; e\nsurje ---&gt; e\nurjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; w\nbhagw ---&gt; a\nhagwa ---&gt; t\nagwat ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; e\n.sume ---&gt; m\nsumem ---&gt; t\numemt ---&gt; r\nmemtr ---&gt; a\nemtra ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; a\nshiva ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; g\n..ang ---&gt; d\n.angd ---&gt; a\nangda ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; l\n..all ---&gt; a\n.alla ---&gt; b\nallab ---&gt; a\nllaba ---&gt; k\nlabak ---&gt; s\nabaks ---&gt; h\nbaksh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; a\nmadha ---&gt; n\nadhan ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; l\n.bhol ---&gt; a\nbhola ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; r\n..abr ---&gt; a\n.abra ---&gt; r\nabrar ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; i\n.puni ---&gt; a\npunia ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; m\n..nam ---&gt; r\n.namr ---&gt; a\nnamra ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; l\n.bhal ---&gt; a\nbhala ---&gt; r\nhalar ---&gt; a\nalara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; r\n..pur ---&gt; n\n.purn ---&gt; m\npurnm ---&gt; a\nurnma ---&gt; l\nrnmal ---&gt; .\n..... ---&gt; b\n....b ---&gt; o\n...bo ---&gt; d\n..bod ---&gt; u\n.bodu ---&gt; r\nbodur ---&gt; a\nodura ---&gt; m\nduram ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; f\n..aaf ---&gt; t\n.aaft ---&gt; a\naafta ---&gt; a\naftaa ---&gt; b\nftaab ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; i\n.shri ---&gt; n\nshrin ---&gt; a\nhrina ---&gt; t\nrinat ---&gt; h\ninath ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; v\n.rajv ---&gt; i\nrajvi ---&gt; r\najvir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; n\nsaran ---&gt; j\naranj ---&gt; e\nranje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; a\n.suna ---&gt; l\nsunal ---&gt; i\nunali ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; o\n.phoo ---&gt; l\nphool ---&gt; m\nhoolm ---&gt; a\noolma ---&gt; n\nolman ---&gt; i\nlmani ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; e\n.hase ---&gt; e\nhasee ---&gt; n\naseen ---&gt; a\nseena ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; h\n.kash ---&gt; m\nkashm ---&gt; i\nashmi ---&gt; r\nshmir ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; a\n.gula ---&gt; f\ngulaf ---&gt; s\nulafs ---&gt; h\nlafsh ---&gt; a\nafsha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; a\n.nava ---&gt; l\nnaval ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; s\n.rams ---&gt; i\nramsi ---&gt; n\namsin ---&gt; g\nmsing ---&gt; h\nsingh ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; m\n.musm ---&gt; i\nmusmi ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; z\n.faiz ---&gt; a\nfaiza ---&gt; n\naizan ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; i\nnoori ---&gt; n\noorin ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; i\n.shri ---&gt; m\nshrim ---&gt; a\nhrima ---&gt; t\nrimat ---&gt; i\nimati ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; t\n..put ---&gt; u\n.putu ---&gt; l\nputul ---&gt; u\nutulu ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; p\n.krip ---&gt; y\nkripy ---&gt; a\nripya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; j\n..naj ---&gt; i\n.naji ---&gt; s\nnajis ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; s\n.bhus ---&gt; h\nbhush ---&gt; a\nhusha ---&gt; n\nushan ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; n\n..men ---&gt; u\n.menu ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; j\n..taj ---&gt; r\n.tajr ---&gt; a\ntajra ---&gt; n\najran ---&gt; i\njrani ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; v\n.jasv ---&gt; i\njasvi ---&gt; n\nasvin ---&gt; d\nsvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; v\n.manv ---&gt; i\nmanvi ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; u\n.kusu ---&gt; m\nkusum ---&gt; a\nusuma ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; e\n.yoge ---&gt; s\nyoges ---&gt; h\nogesh ---&gt; w\ngeshw ---&gt; a\neshwa ---&gt; r\nshwar ---&gt; .\n..... ---&gt; k\n....k ---&gt; n\n...kn ---&gt; h\n..knh ---&gt; e\n.knhe ---&gt; y\nknhey ---&gt; a\nnheya ---&gt; l\nheyal ---&gt; a\neyala ---&gt; l\nyalal ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; e\n..are ---&gt; e\n.aree ---&gt; n\nareen ---&gt; .\n..... ---&gt; i\n....i ---&gt; b\n...ib ---&gt; r\n..ibr ---&gt; a\n.ibra ---&gt; h\nibrah ---&gt; e\nbrahe ---&gt; e\nrahee ---&gt; m\naheem ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; i\n.mani ---&gt; r\nmanir ---&gt; a\nanira ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; n\n..son ---&gt; e\n.sone ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; d\n.shad ---&gt; a\nshada ---&gt; b\nhadab ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; u\nchotu ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; e\n..khe ---&gt; r\n.kher ---&gt; u\nkheru ---&gt; l\nherul ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; k\n..rak ---&gt; h\n.rakh ---&gt; i\nrakhi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; i\n.vidi ---&gt; s\nvidis ---&gt; h\nidish ---&gt; a\ndisha ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; t\n.krit ---&gt; i\nkriti ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; l\nshail ---&gt; e\nhaile ---&gt; s\nailes ---&gt; h\nilesh ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; j\n..rij ---&gt; w\n.rijw ---&gt; a\nrijwa ---&gt; n\nijwan ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; p\n..jap ---&gt; n\n.japn ---&gt; e\njapne ---&gt; e\napnee ---&gt; t\npneet ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; i\n.sahi ---&gt; s\nsahis ---&gt; t\nahist ---&gt; a\nhista ---&gt; .\n..... ---&gt; s\n....s ---&gt; m\n...sm ---&gt; a\n..sma ---&gt; r\n.smar ---&gt; t\nsmart ---&gt; i\nmarti ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; v\n..kav ---&gt; a\n.kava ---&gt; l\nkaval ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; s\n.anis ---&gt; h\nanish ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; w\n..diw ---&gt; a\n.diwa ---&gt; n\ndiwan ---&gt; s\niwans ---&gt; i\nwansi ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; r\n..gur ---&gt; m\n.gurm ---&gt; e\ngurme ---&gt; l\nurmel ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; r\n.dhar ---&gt; a\ndhara ---&gt; m\nharam ---&gt; p\naramp ---&gt; a\nrampa ---&gt; l\nampal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; f\n..saf ---&gt; i\n.safi ---&gt; y\nsafiy ---&gt; a\nafiya ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; p\n..omp ---&gt; a\n.ompa ---&gt; r\nompar ---&gt; s\nmpars ---&gt; a\nparsa ---&gt; d\narsad ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; l\n..kel ---&gt; a\n.kela ---&gt; m\nkelam ---&gt; a\nelama ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; k\nhandk ---&gt; o\nandko ---&gt; r\nndkor ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; t\n..ant ---&gt; a\n.anta ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; s\n..jis ---&gt; h\n.jish ---&gt; a\njisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; r\n..asr ---&gt; f\n.asrf ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; j\n..roj ---&gt; i\n.roji ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; a\n.vira ---&gt; t\nvirat ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; m\nharim ---&gt; o\narimo ---&gt; h\nrimoh ---&gt; a\nimoha ---&gt; n\nmohan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; n\n.ragn ---&gt; i\nragni ---&gt; .\n..... ---&gt; r\n....r ---&gt; s\n...rs ---&gt; j\n..rsj ---&gt; e\n.rsje ---&gt; s\nrsjes ---&gt; h\nsjesh ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; i\n..fai ---&gt; z\n.faiz ---&gt; a\nfaiza ---&gt; l\naizal ---&gt; .\n..... ---&gt; y\n....y ---&gt; e\n...ye ---&gt; s\n..yes ---&gt; h\n.yesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; n\n..sin ---&gt; d\n.sind ---&gt; e\nsinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; y\n..joy ---&gt; a\n.joya ---&gt; t\njoyat ---&gt; e\noyate ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; b\n..rub ---&gt; b\n.rubb ---&gt; i\nrubbi ---&gt; n\nubbin ---&gt; a\nbbina ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; d\n.vand ---&gt; i\nvandi ---&gt; t\nandit ---&gt; a\nndita ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; d\n..sid ---&gt; h\n.sidh ---&gt; i\nsidhi ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; s\n..prs ---&gt; a\n.prsa ---&gt; n\nprsan ---&gt; t\nrsant ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; m\n.neem ---&gt; i\nneemi ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; e\n.deve ---&gt; n\ndeven ---&gt; d\nevend ---&gt; r\nvendr ---&gt; i\nendri ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; r\nsantr ---&gt; a\nantra ---&gt; m\nntram ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; b\n..mub ---&gt; a\n.muba ---&gt; s\nmubas ---&gt; h\nubash ---&gt; i\nbashi ---&gt; r\nashir ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; e\nmande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; a\nshiva ---&gt; n\nhivan ---&gt; g\nivang ---&gt; i\nvangi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; a\nshaka ---&gt; r\nhakar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; h\n..pah ---&gt; a\n.paha ---&gt; l\npahal ---&gt; w\nahalw ---&gt; a\nhalwa ---&gt; n\nalwan ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; l\n..mul ---&gt; t\n.mult ---&gt; a\nmulta ---&gt; n\nultan ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; n\ndhann ---&gt; u\nhannu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; n\nsandn ---&gt; a\nandna ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; l\n..kul ---&gt; i\n.kuli ---&gt; n\nkulin ---&gt; a\nulina ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; k\n.sank ---&gt; a\nsanka ---&gt; r\nankar ---&gt; l\nnkarl ---&gt; a\nkarla ---&gt; l\narlal ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; f\n..gaf ---&gt; f\n.gaff ---&gt; a\ngaffa ---&gt; r\naffar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; g\n.parg ---&gt; a\nparga ---&gt; t\nargat ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; a\ndhana ---&gt; n\nhanan ---&gt; j\nananj ---&gt; a\nnanja ---&gt; i\nanjai ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; k\n..dak ---&gt; s\n.daks ---&gt; h\ndaksh ---&gt; i\nakshi ---&gt; n\nkshin ---&gt; a\nshina ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; v\n..shv ---&gt; a\n.shva ---&gt; n\nshvan ---&gt; i\nhvani ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; m\n.reem ---&gt; i\nreemi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; l\n.jasl ---&gt; e\njasle ---&gt; e\naslee ---&gt; n\nsleen ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; p\n.papp ---&gt; e\npappe ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; s\n..afs ---&gt; a\n.afsa ---&gt; r\nafsar ---&gt; i\nfsari ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; f\n..gaf ---&gt; u\n.gafu ---&gt; r\ngafur ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; i\n.chhi ---&gt; d\nchhid ---&gt; u\nhhidu ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; s\n.viks ---&gt; h\nviksh ---&gt; i\nikshi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; m\n.susm ---&gt; i\nsusmi ---&gt; t\nusmit ---&gt; a\nsmita ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; m\n..rim ---&gt; a\n.rima ---&gt; s\nrimas ---&gt; h\nimash ---&gt; a\nmasha ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; n\n..sin ---&gt; t\n.sint ---&gt; a\nsinta ---&gt; .\n..... ---&gt; y\n....y ---&gt; o\n...yo ---&gt; g\n..yog ---&gt; n\n.yogn ---&gt; d\nyognd ---&gt; e\nognde ---&gt; r\ngnder ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; u\n..gau ---&gt; r\n.gaur ---&gt; a\ngaura ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; v\n..tav ---&gt; v\n.tavv ---&gt; a\ntavva ---&gt; s\navvas ---&gt; u\nvvasu ---&gt; m\nvasum ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; o\n..ano ---&gt; u\n.anou ---&gt; r\nanour ---&gt; a\nnoura ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; n\n.reen ---&gt; u\nreenu ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; n\n.anan ---&gt; d\nanand ---&gt; i\nnandi ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; w\n.shiw ---&gt; a\nshiwa ---&gt; n\nhiwan ---&gt; i\niwani ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; l\n..gol ---&gt; d\n.gold ---&gt; i\ngoldi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; v\n..viv ---&gt; e\n.vive ---&gt; k\nvivek ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; d\n..nad ---&gt; i\n.nadi ---&gt; r\nnadir ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; z\n..ruz ---&gt; i\n.ruzi ---&gt; n\nruzin ---&gt; a\nuzina ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; l\n..jul ---&gt; f\n.julf ---&gt; i\njulfi ---&gt; c\nulfic ---&gt; a\nlfica ---&gt; r\nficar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; n\n.ramn ---&gt; i\nramni ---&gt; w\namniw ---&gt; a\nmniwa ---&gt; s\nniwas ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; u\n.ratu ---&gt; l\nratul ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; i\nshaki ---&gt; r\nhakir ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; m\n.nirm ---&gt; a\nnirma ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; y\n.mary ---&gt; a\nmarya ---&gt; n\naryan ---&gt; a\nryana ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; e\n.some ---&gt; s\nsomes ---&gt; h\nomesh ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; m\n..dum ---&gt; a\n.duma ---&gt; n\nduman ---&gt; i\numani ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; m\n.mohm ---&gt; a\nmohma ---&gt; d\nohmad ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; n\nabhin ---&gt; a\nbhina ---&gt; v\nhinav ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; g\n..rag ---&gt; h\n.ragh ---&gt; u\nraghu ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; t\n.lalt ---&gt; e\nlalte ---&gt; s\naltes ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; p\nnoorp ---&gt; a\noorpa ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; m\n..ham ---&gt; s\n.hams ---&gt; i\nhamsi ---&gt; r\namsir ---&gt; a\nmsira ---&gt; n\nsiran ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; n\n..ann ---&gt; u\n.annu ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; i\n..avi ---&gt; n\n.avin ---&gt; a\navina ---&gt; s\nvinas ---&gt; h\ninash ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; a\n.nika ---&gt; h\nnikah ---&gt; a\nikaha ---&gt; t\nkahat ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; a\n.soma ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; v\n.tanv ---&gt; e\ntanve ---&gt; e\nanvee ---&gt; r\nnveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; y\n.sury ---&gt; a\nsurya ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; l\n.shul ---&gt; e\nshule ---&gt; k\nhulek ---&gt; h\nulekh ---&gt; a\nlekha ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; m\n.tanm ---&gt; a\ntanma ---&gt; y\nanmay ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; s\n..ros ---&gt; i\n.rosi ---&gt; n\nrosin ---&gt; a\nosina ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; e\n..ghe ---&gt; e\n.ghee ---&gt; s\nghees ---&gt; a\nheesa ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; h\n..rih ---&gt; a\n.riha ---&gt; l\nrihal ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; l\n..gol ---&gt; l\n.goll ---&gt; u\ngollu ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; f\n..jaf ---&gt; r\n.jafr ---&gt; i\njafri ---&gt; n\nafrin ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; h\n..reh ---&gt; a\n.reha ---&gt; n\nrehan ---&gt; a\nehana ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; m\n..pam ---&gt; m\n.pamm ---&gt; y\npammy ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; i\nmunni ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; b\n..kab ---&gt; a\n.kaba ---&gt; l\nkabal ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; a\n.rupa ---&gt; n\nrupan ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; n\n..gen ---&gt; d\n.gend ---&gt; a\ngenda ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; m\n.alim ---&gt; a\nalima ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; i\n..udi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; h\nramdh ---&gt; a\namdha ---&gt; n\nmdhan ---&gt; .\n..... ---&gt; z\n....z ---&gt; i\n...zi ---&gt; a\n..zia ---&gt; r\n.ziar ---&gt; u\nziaru ---&gt; l\niarul ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; s\n.sams ---&gt; a\nsamsa ---&gt; d\namsad ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; y\n..joy ---&gt; c\n.joyc ---&gt; e\njoyce ---&gt; e\noycee ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; i\n.kari ---&gt; m\nkarim ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; j\n.farj ---&gt; a\nfarja ---&gt; n\narjan ---&gt; u\nrjanu ---&gt; l\njanul ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; s\n..ras ---&gt; e\n.rase ---&gt; e\nrasee ---&gt; l\naseel ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; h\n.ansh ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; a\n.suja ---&gt; n\nsujan ---&gt; t\nujant ---&gt; i\njanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; h\n.sadh ---&gt; n\nsadhn ---&gt; a\nadhna ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; g\n..pog ---&gt; a\n.poga ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; u\n.niru ---&gt; t\nnirut ---&gt; m\nirutm ---&gt; a\nrutma ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; t\n..nit ---&gt; a\n.nita ---&gt; m\nnitam ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; c\n..roc ---&gt; k\n.rock ---&gt; y\nrocky ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; v\ndeepv ---&gt; h\neepvh ---&gt; a\nepvha ---&gt; n\npvhan ---&gt; d\nvhand ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; t\n.jeet ---&gt; e\njeete ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; a\n.kama ---&gt; l\nkamal ---&gt; d\namald ---&gt; e\nmalde ---&gt; e\naldee ---&gt; p\nldeep ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; s\n..hus ---&gt; a\n.husa ---&gt; i\nhusai ---&gt; n\nusain ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; s\n..ars ---&gt; h\n.arsh ---&gt; i\narshi ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; d\n..pad ---&gt; a\n.pada ---&gt; m\npadam ---&gt; a\nadama ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; d\n..kad ---&gt; i\n.kadi ---&gt; r\nkadir ---&gt; .\n..... ---&gt; z\n....z ---&gt; a\n...za ---&gt; r\n..zar ---&gt; i\n.zari ---&gt; n\nzarin ---&gt; a\narina ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; n\nriyan ---&gt; s\niyans ---&gt; i\nyansi ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; r\n..kir ---&gt; i\n.kiri ---&gt; t\nkirit ---&gt; i\niriti ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; e\n.pree ---&gt; n\npreen ---&gt; a\nreena ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; i\n..ali ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; n\n..ann ---&gt; o\n.anno ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; y\n.shay ---&gt; n\nshayn ---&gt; a\nhayna ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; a\n.bada ---&gt; l\nbadal ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; n\n.sabn ---&gt; a\nsabna ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; m\n..sim ---&gt; r\n.simr ---&gt; a\nsimra ---&gt; n\nimran ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; t\n..mot ---&gt; i\n.moti ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; m\n..sim ---&gt; a\n.sima ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; h\n..rih ---&gt; a\n.riha ---&gt; a\nrihaa ---&gt; n\nihaan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; d\navind ---&gt; a\nvinda ---&gt; r\nindar ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; a\n.nara ---&gt; y\nnaray ---&gt; a\naraya ---&gt; n\nrayan ---&gt; .\n..... ---&gt; p\n....p ---&gt; i\n...pi ---&gt; s\n..pis ---&gt; t\n.pist ---&gt; a\npista ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; n\n..sen ---&gt; s\n.sens ---&gt; e\nsense ---&gt; r\nenser ---&gt; .\n..... ---&gt; t\n....t ---&gt; w\n...tw ---&gt; i\n..twi ---&gt; n\n.twin ---&gt; k\ntwink ---&gt; a\nwinka ---&gt; l\ninkal ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; b\n..nib ---&gt; o\n.nibo ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; w\n..kuw ---&gt; a\n.kuwa ---&gt; r\nkuwar ---&gt; j\nuwarj ---&gt; e\nwarje ---&gt; e\narjee ---&gt; t\nrjeet ---&gt; .\n..... ---&gt; i\n....i ---&gt; j\n...ij ---&gt; h\n..ijh ---&gt; a\n.ijha ---&gt; r\nijhar ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; u\nmandu ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; a\n.abha ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; e\n..tee ---&gt; r\n.teer ---&gt; a\nteera ---&gt; t\neerat ---&gt; h\nerath ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; u\n.raju ---&gt; d\nrajud ---&gt; i\najudi ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; h\n..bah ---&gt; a\n.baha ---&gt; d\nbahad ---&gt; u\nahadu ---&gt; r\nhadur ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; k\n..bak ---&gt; s\n.baks ---&gt; i\nbaksi ---&gt; .\n..... ---&gt; a\n....a ---&gt; f\n...af ---&gt; s\n..afs ---&gt; a\n.afsa ---&gt; r\nafsar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; a\n.sana ---&gt; n\nsanan ---&gt; d\nanand ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; k\nshank ---&gt; e\nhanke ---&gt; r\nanker ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; j\n.ramj ---&gt; a\nramja ---&gt; s\namjas ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; m\nkushm ---&gt; i\nushmi ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; t\n..bit ---&gt; o\n.bito ---&gt; o\nbitoo ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; z\n..arz ---&gt; o\n.arzo ---&gt; o\narzoo ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; n\n..din ---&gt; e\n.dine ---&gt; s\ndines ---&gt; h\ninesh ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; a\n.hema ---&gt; v\nhemav ---&gt; a\nemava ---&gt; n\nmavan ---&gt; t\navant ---&gt; i\nvanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; d\n.sadd ---&gt; i\nsaddi ---&gt; q\naddiq ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; a\n.hara ---&gt; k\nharak ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; u\n.masu ---&gt; m\nmasum ---&gt; a\nasuma ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; a\n.sura ---&gt; j\nsuraj ---&gt; a\nuraja ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; u\n.raju ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; p\n..tep ---&gt; u\n.tepu ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; i\n.nasi ---&gt; m\nnasim ---&gt; a\nasima ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; n\n.kamn ---&gt; i\nkamni ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; i\n.giri ---&gt; r\ngirir ---&gt; a\nirira ---&gt; j\nriraj ---&gt; .\n..... ---&gt; i\n....i ---&gt; r\n...ir ---&gt; s\n..irs ---&gt; a\n.irsa ---&gt; d\nirsad ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; r\n.kamr ---&gt; e\nkamre ---&gt; e\namree ---&gt; n\nmreen ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; j\n..suj ---&gt; a\n.suja ---&gt; n\nsujan ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; p\n.ramp ---&gt; a\nrampa ---&gt; a\nampaa ---&gt; l\nmpaal ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; j\n..tej ---&gt; e\n.teje ---&gt; n\ntejen ---&gt; d\nejend ---&gt; e\njende ---&gt; r\nender ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; b\n.harb ---&gt; h\nharbh ---&gt; a\narbha ---&gt; j\nrbhaj ---&gt; a\nbhaja ---&gt; n\nhajan ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; h\n..toh ---&gt; m\n.tohm ---&gt; e\ntohme ---&gt; e\nohmee ---&gt; n\nhmeen ---&gt; a\nmeena ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; l\n..kal ---&gt; a\n.kala ---&gt; y\nkalay ---&gt; a\nalaya ---&gt; n\nlayan ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; g\n..aag ---&gt; a\n.aaga ---&gt; n\naagan ---&gt; d\nagand ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; s\n..vas ---&gt; h\n.vash ---&gt; u\nvashu ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; r\n..hir ---&gt; a\n.hira ---&gt; m\nhiram ---&gt; a\nirama ---&gt; n\nraman ---&gt; i\namani ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; m\n..som ---&gt; p\n.somp ---&gt; a\nsompa ---&gt; l\nompal ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; a\n.aara ---&gt; t\naarat ---&gt; i\narati ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; t\n.must ---&gt; a\nmusta ---&gt; k\nustak ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; r\n..per ---&gt; v\n.perv ---&gt; e\nperve ---&gt; z\nervez ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; n\n.pran ---&gt; k\nprank ---&gt; u\nranku ---&gt; r\nankur ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; l\n..sil ---&gt; p\n.silp ---&gt; i\nsilpi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; w\n.ratw ---&gt; a\nratwa ---&gt; r\natwar ---&gt; i\ntwari ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; h\n..rih ---&gt; a\n.riha ---&gt; n\nrihan ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; f\n.manf ---&gt; u\nmanfu ---&gt; l\nanful ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; i\n..asi ---&gt; w\n.asiw ---&gt; a\nasiwa ---&gt; n\nsiwan ---&gt; i\niwani ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; k\n.sukk ---&gt; a\nsukka ---&gt; .\n..... ---&gt; i\n....i ---&gt; t\n...it ---&gt; w\n..itw ---&gt; a\n.itwa ---&gt; r\nitwar ---&gt; i\ntwari ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; u\n.mamu ---&gt; n\nmamun ---&gt; a\namuna ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; e\n..mee ---&gt; t\n.meet ---&gt; a\nmeeta ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; j\n.girj ---&gt; e\ngirje ---&gt; s\nirjes ---&gt; h\nrjesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; e\nparve ---&gt; n\narven ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; s\n..mos ---&gt; a\n.mosa ---&gt; m\nmosam ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; v\n.prav ---&gt; i\npravi ---&gt; n\nravin ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; j\ndhanj ---&gt; a\nhanja ---&gt; y\nanjay ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; w\nbhagw ---&gt; a\nhagwa ---&gt; n\nagwan ---&gt; t\ngwant ---&gt; i\nwanti ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; v\n.sarv ---&gt; j\nsarvj ---&gt; e\narvje ---&gt; e\nrvjee ---&gt; t\nvjeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; a\n..ada ---&gt; r\n.adar ---&gt; s\nadars ---&gt; h\ndarsh ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; k\n..aak ---&gt; a\n.aaka ---&gt; s\naakas ---&gt; h\nakash ---&gt; .\n..... ---&gt; i\n....i ---&gt; k\n...ik ---&gt; b\n..ikb ---&gt; a\n.ikba ---&gt; l\nikbal ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; r\n.shur ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; a\n.pala ---&gt; r\npalar ---&gt; a\nalara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; r\n.khur ---&gt; s\nkhurs ---&gt; i\nhursi ---&gt; d\nursid ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; g\n..jug ---&gt; e\n.juge ---&gt; n\njugen ---&gt; d\nugend ---&gt; e\ngende ---&gt; r\nender ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; n\n.anan ---&gt; t\nanant ---&gt; r\nnantr ---&gt; a\nantra ---&gt; m\nntram ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; s\n..tis ---&gt; h\n.tish ---&gt; a\ntisha ---&gt; .\n..... ---&gt; l\n....l ---&gt; o\n...lo ---&gt; n\n..lon ---&gt; g\n.long ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; z\n..roz ---&gt; y\n.rozy ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; y\n..day ---&gt; a\n.daya ---&gt; r\ndayar ---&gt; a\nayara ---&gt; m\nyaram ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; x\n..dix ---&gt; y\n.dixy ---&gt; a\ndixya ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; w\n.bhuw ---&gt; a\nbhuwa ---&gt; n\nhuwan ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; h\n..suh ---&gt; a\n.suha ---&gt; i\nsuhai ---&gt; l\nuhail ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; e\n..ude ---&gt; s\n.udes ---&gt; h\nudesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; r\nshahr ---&gt; u\nhahru ---&gt; k\nahruk ---&gt; h\nhrukh ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; d\n..aad ---&gt; e\n.aade ---&gt; s\naades ---&gt; h\nadesh ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; a\n.bira ---&gt; j\nbiraj ---&gt; p\nirajp ---&gt; a\nrajpa ---&gt; l\najpal ---&gt; .\n..... ---&gt; t\n....t ---&gt; i\n...ti ---&gt; p\n..tip ---&gt; u\n.tipu ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; i\n..kai ---&gt; l\n.kail ---&gt; a\nkaila ---&gt; k\nailak ---&gt; i\nilaki ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; k\n..ank ---&gt; i\n.anki ---&gt; t\nankit ---&gt; a\nnkita ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; n\n..ben ---&gt; u\n.benu ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; c\n.mamc ---&gt; h\nmamch ---&gt; a\namcha ---&gt; n\nmchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; a\n.maha ---&gt; k\nmahak ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; n\n.shen ---&gt; a\nshena ---&gt; z\nhenaz ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; z\n..saz ---&gt; i\n.sazi ---&gt; d\nsazid ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; e\n.saie ---&gt; m\nsaiem ---&gt; a\naiema ---&gt; n\nieman ---&gt; .\n..... ---&gt; i\n....i ---&gt; l\n...il ---&gt; m\n..ilm ---&gt; a\n.ilma ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; k\n..dik ---&gt; s\n.diks ---&gt; h\ndiksh ---&gt; a\niksha ---&gt; n\nkshan ---&gt; t\nshant ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; d\n..gud ---&gt; d\n.gudd ---&gt; o\nguddo ---&gt; .\n..... ---&gt; g\n....g ---&gt; r\n...gr ---&gt; u\n..gru ---&gt; c\n.gruc ---&gt; h\ngruch ---&gt; r\nruchr ---&gt; a\nuchra ---&gt; n\nchran ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; y\n.jeey ---&gt; a\njeeya ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; s\n..dus ---&gt; h\n.dush ---&gt; y\ndushy ---&gt; a\nushya ---&gt; n\nshyan ---&gt; t\nhyant ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; l\n.akhl ---&gt; a\nakhla ---&gt; q\nkhlaq ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; i\n.muni ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; n\n..hin ---&gt; i\n.hini ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; d\n.mand ---&gt; i\nmandi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; f\n..jaf ---&gt; r\n.jafr ---&gt; u\njafru ---&gt; d\nafrud ---&gt; d\nfrudd ---&gt; i\nruddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; u\n.binu ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; a\n.dipa ---&gt; n\ndipan ---&gt; s\nipans ---&gt; h\npansh ---&gt; i\nanshi ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; k\n..tak ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; n\n.nazn ---&gt; e\nnazne ---&gt; e\naznee ---&gt; n\nzneen ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; s\n..nes ---&gt; h\n.nesh ---&gt; a\nnesha ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; r\n..fir ---&gt; a\n.fira ---&gt; k\nfirak ---&gt; a\niraka ---&gt; t\nrakat ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; y\n.pary ---&gt; a\nparya ---&gt; n\naryan ---&gt; k\nryank ---&gt; a\nyanka ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; a\n.suma ---&gt; l\nsumal ---&gt; i\numali ---&gt; a\nmalia ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; d\n..had ---&gt; i\n.hadi ---&gt; s\nhadis ---&gt; h\nadish ---&gt; a\ndisha ---&gt; .\n..... ---&gt; n\n....n ---&gt; r\n...nr ---&gt; o\n..nro ---&gt; t\n.nrot ---&gt; a\nnrota ---&gt; m\nrotam ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; y\n..aay ---&gt; a\n.aaya ---&gt; n\naayan ---&gt; a\nayana ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; e\n..ree ---&gt; t\n.reet ---&gt; u\nreetu ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; e\n..see ---&gt; k\n.seek ---&gt; h\nseekh ---&gt; a\neekha ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; m\nabhim ---&gt; a\nbhima ---&gt; n\nhiman ---&gt; u\nimanu ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; u\n..aru ---&gt; n\n.arun ---&gt; a\naruna ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; m\n.manm ---&gt; e\nmanme ---&gt; e\nanmee ---&gt; t\nnmeet ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; b\n.parb ---&gt; h\nparbh ---&gt; a\narbha ---&gt; t\nrbhat ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; o\n.faro ---&gt; j\nfaroj ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; n\n..mon ---&gt; i\n.moni ---&gt; s\nmonis ---&gt; h\nonish ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; v\n..div ---&gt; a\n.diva ---&gt; r\ndivar ---&gt; a\nivara ---&gt; j\nvaraj ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; u\n..chu ---&gt; n\n.chun ---&gt; i\nchuni ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; b\nshahb ---&gt; a\nhahba ---&gt; z\nahbaz ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; e\nshake ---&gt; e\nhakee ---&gt; l\nakeel ---&gt; a\nkeela ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; h\n..jah ---&gt; i\n.jahi ---&gt; r\njahir ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; s\n.kris ---&gt; h\nkrish ---&gt; k\nrishk ---&gt; a\nishka ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; a\n.kara ---&gt; m\nkaram ---&gt; v\naramv ---&gt; i\nramvi ---&gt; r\namvir ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; d\n.ramd ---&gt; e\nramde ---&gt; v\namdev ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; a\n..aka ---&gt; n\n.akan ---&gt; s\nakans ---&gt; h\nkansh ---&gt; a\nansha ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; b\n..pab ---&gt; i\n.pabi ---&gt; t\npabit ---&gt; r\nabitr ---&gt; a\nbitra ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; f\n.saif ---&gt; a\nsaifa ---&gt; l\naifal ---&gt; i\nifali ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; r\n..shr ---&gt; a\n.shra ---&gt; b\nshrab ---&gt; a\nhraba ---&gt; n\nraban ---&gt; i\nabani ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; i\n.sabi ---&gt; y\nsabiy ---&gt; a\nabiya ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; t\n..ket ---&gt; a\n.keta ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; t\n..bht ---&gt; e\n.bhte ---&gt; r\nbhter ---&gt; i\nhteri ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; h\nsandh ---&gt; a\nandha ---&gt; y\nndhay ---&gt; a\ndhaya ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; s\n..has ---&gt; b\n.hasb ---&gt; u\nhasbu ---&gt; l\nasbul ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; s\n.ruks ---&gt; a\nruksa ---&gt; r\nuksar ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; r\nkeshr ---&gt; i\neshri ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; u\n.gulu ---&gt; r\ngulur ---&gt; a\nulura ---&gt; m\nluram ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; v\n.rajv ---&gt; e\nrajve ---&gt; e\najvee ---&gt; r\njveer ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; j\n.sahj ---&gt; a\nsahja ---&gt; d\nahjad ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; r\n.susr ---&gt; i\nsusri ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; n\n..vin ---&gt; a\n.vina ---&gt; y\nvinay ---&gt; a\ninaya ---&gt; k\nnayak ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; a\n.isha ---&gt; w\nishaw ---&gt; a\nshawa ---&gt; r\nhawar ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; p\n..pap ---&gt; u\n.papu ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; w\nsharw ---&gt; a\nharwa ---&gt; n\narwan ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; l\n.shel ---&gt; e\nshele ---&gt; s\nheles ---&gt; h\nelesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; y\n..may ---&gt; a\n.maya ---&gt; n\nmayan ---&gt; k\nayank ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; m\n..alm ---&gt; i\n.almi ---&gt; n\nalmin ---&gt; a\nlmina ---&gt; .\n..... ---&gt; k\n....k ---&gt; r\n...kr ---&gt; i\n..kri ---&gt; t\n.krit ---&gt; i\nkriti ---&gt; k\nritik ---&gt; a\nitika ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; l\n..dal ---&gt; e\n.dale ---&gt; r\ndaler ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; a\n..uda ---&gt; i\n.udai ---&gt; y\nudaiy ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; h\n..lah ---&gt; i\n.lahi ---&gt; d\nlahid ---&gt; a\nahida ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; i\nshali ---&gt; g\nhalig ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; d\n..abd ---&gt; u\n.abdu ---&gt; l\nabdul ---&gt; l\nbdull ---&gt; a\ndulla ---&gt; h\nullah ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; j\nnoorj ---&gt; a\noorja ---&gt; m\norjam ---&gt; a\nrjama ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; k\n..mak ---&gt; r\n.makr ---&gt; u\nmakru ---&gt; d\nakrud ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; z\n.farz ---&gt; a\nfarza ---&gt; n\narzan ---&gt; a\nrzana ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; j\n.nooj ---&gt; o\nnoojo ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; i\n..ghi ---&gt; s\n.ghis ---&gt; a\nghisa ---&gt; r\nhisar ---&gt; a\nisara ---&gt; m\nsaram ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; j\n.bhoj ---&gt; a\nbhoja ---&gt; r\nhojar ---&gt; a\nojara ---&gt; m\njaram ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; n\n.khan ---&gt; c\nkhanc ---&gt; h\nhanch ---&gt; a\nancha ---&gt; n\nnchan ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; i\n..bri ---&gt; j\n.brij ---&gt; l\nbrijl ---&gt; a\nrijla ---&gt; l\nijlal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; i\n..bhi ---&gt; k\n.bhik ---&gt; i\nbhiki ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; k\n..bik ---&gt; r\n.bikr ---&gt; a\nbikra ---&gt; m\nikram ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; n\n..gun ---&gt; i\n.guni ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; w\n.girw ---&gt; e\ngirwe ---&gt; r\nirwer ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; t\n..jat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; k\n..sak ---&gt; s\n.saks ---&gt; h\nsaksh ---&gt; i\nakshi ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; v\n.harv ---&gt; i\nharvi ---&gt; n\narvin ---&gt; d\nrvind ---&gt; e\nvinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; n\n..gun ---&gt; a\n.guna ---&gt; n\ngunan ---&gt; i\nunani ---&gt; d\nnanid ---&gt; h\nanidh ---&gt; i\nnidhi ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; y\n..koy ---&gt; a\n.koya ---&gt; l\nkoyal ---&gt; i\noyali ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; i\n.tani ---&gt; s\ntanis ---&gt; h\nanish ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; p\n..kap ---&gt; t\n.kapt ---&gt; a\nkapta ---&gt; n\naptan ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; a\n.ansa ---&gt; r\nansar ---&gt; .\n..... ---&gt; k\n....k ---&gt; m\n...km ---&gt; l\n..kml ---&gt; e\n.kmle ---&gt; s\nkmles ---&gt; .\n..... ---&gt; h\n....h ---&gt; e\n...he ---&gt; m\n..hem ---&gt; a\n.hema ---&gt; n\nheman ---&gt; t\nemant ---&gt; i\nmanti ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; v\n..pav ---&gt; i\n.pavi ---&gt; .\n..... ---&gt; u\n....u ---&gt; j\n...uj ---&gt; i\n..uji ---&gt; r\n.ujir ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; r\n.veer ---&gt; o\nveero ---&gt; .\n..... ---&gt; a\n....a ---&gt; i\n...ai ---&gt; d\n..aid ---&gt; t\n.aidt ---&gt; y\naidty ---&gt; a\nidtya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; d\n.sard ---&gt; a\nsarda ---&gt; r\nardar ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; t\n..aat ---&gt; i\n.aati ---&gt; f\naatif ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; g\n.sang ---&gt; e\nsange ---&gt; t\nanget ---&gt; a\nngeta ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; d\n..jod ---&gt; h\n.jodh ---&gt; i\njodhi ---&gt; .\n..... ---&gt; f\n....f ---&gt; u\n...fu ---&gt; l\n..ful ---&gt; j\n.fulj ---&gt; h\nfuljh ---&gt; a\nuljha ---&gt; d\nljhad ---&gt; i\njhadi ---&gt; .\n..... ---&gt; v\n....v ---&gt; a\n...va ---&gt; n\n..van ---&gt; i\n.vani ---&gt; s\nvanis ---&gt; h\nanish ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; t\n.swat ---&gt; r\nswatr ---&gt; i\nwatri ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; v\n.tanv ---&gt; i\ntanvi ---&gt; r\nanvir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; g\n.sang ---&gt; e\nsange ---&gt; e\nangee ---&gt; t\nngeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; m\n.shim ---&gt; p\nshimp ---&gt; i\nhimpi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; l\n..sal ---&gt; a\n.sala ---&gt; m\nsalam ---&gt; a\nalama ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; d\n..pad ---&gt; m\n.padm ---&gt; a\npadma ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; j\n..kaj ---&gt; o\n.kajo ---&gt; l\nkajol ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; i\n.saji ---&gt; d\nsajid ---&gt; a\najida ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; l\n..nel ---&gt; a\n.nela ---&gt; m\nnelam ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; m\n..nim ---&gt; a\n.nima ---&gt; h\nnimah ---&gt; e\nimahe ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; y\n..siy ---&gt; a\n.siya ---&gt; r\nsiyar ---&gt; a\niyara ---&gt; m\nyaram ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; a\n.akha ---&gt; t\nakhat ---&gt; a\nkhata ---&gt; r\nhatar ---&gt; i\natari ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; e\n..she ---&gt; e\n.shee ---&gt; l\nsheel ---&gt; a\nheela ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; b\n..tab ---&gt; b\n.tabb ---&gt; w\ntabbw ---&gt; u\nabbwu ---&gt; m\nbbwum ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; t\nshakt ---&gt; i\nhakti ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; y\n.priy ---&gt; a\npriya ---&gt; k\nriyak ---&gt; a\niyaka ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; s\n..las ---&gt; h\n.lash ---&gt; m\nlashm ---&gt; a\nashma ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; j\n..bij ---&gt; e\n.bije ---&gt; n\nbijen ---&gt; d\nijend ---&gt; r\njendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; s\n..dis ---&gt; h\n.dish ---&gt; u\ndishu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; m\n.samm ---&gt; a\nsamma ---&gt; n\namman ---&gt; .\n..... ---&gt; a\n....a ---&gt; b\n...ab ---&gt; h\n..abh ---&gt; i\n.abhi ---&gt; l\nabhil ---&gt; a\nbhila ---&gt; s\nhilas ---&gt; h\nilash ---&gt; a\nlasha ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; s\n..kas ---&gt; h\n.kash ---&gt; i\nkashi ---&gt; s\nashis ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; w\n.yasw ---&gt; a\nyaswa ---&gt; n\naswan ---&gt; i\nswani ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; n\n..ajn ---&gt; o\n.ajno ---&gt; o\najnoo ---&gt; r\njnoor ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; b\n.prab ---&gt; h\nprabh ---&gt; j\nrabhj ---&gt; i\nabhji ---&gt; t\nbhjit ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; d\n.kand ---&gt; a\nkanda ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; f\n..asf ---&gt; a\n.asfa ---&gt; k\nasfak ---&gt; .\n..... ---&gt; i\n....i ---&gt; l\n...il ---&gt; a\n..ila ---&gt; y\n.ilay ---&gt; a\nilaya ---&gt; t\nlayat ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; s\n..ans ---&gt; h\n.ansh ---&gt; u\nanshu ---&gt; m\nnshum ---&gt; a\nshuma ---&gt; n\nhuman ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; y\nchhay ---&gt; a\nhhaya ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; k\n.ramk ---&gt; e\nramke ---&gt; s\namkes ---&gt; h\nmkesh ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; k\n..gok ---&gt; u\n.goku ---&gt; l\ngokul ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; u\n..bhu ---&gt; r\n.bhur ---&gt; a\nbhura ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; l\n..dil ---&gt; s\n.dils ---&gt; h\ndilsh ---&gt; a\nilsha ---&gt; d\nlshad ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; t\namrit ---&gt; a\nmrita ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; t\n.noot ---&gt; a\nnoota ---&gt; n\nootan ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; l\n..vil ---&gt; r\n.vilr ---&gt; a\nvilra ---&gt; m\nilram ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; k\n.malk ---&gt; i\nmalki ---&gt; a\nalkia ---&gt; t\nlkiat ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; l\n..sul ---&gt; e\n.sule ---&gt; k\nsulek ---&gt; h\nulekh ---&gt; a\nlekha ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; m\n.panm ---&gt; a\npanma ---&gt; t\nanmat ---&gt; i\nnmati ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; s\n..kus ---&gt; h\n.kush ---&gt; a\nkusha ---&gt; m\nusham ---&gt; v\nshamv ---&gt; i\nhamvi ---&gt; r\namvir ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; r\nshabr ---&gt; a\nhabra ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; r\n.anjr ---&gt; e\nanjre ---&gt; j\nnjrej ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; t\n.sant ---&gt; o\nsanto ---&gt; k\nantok ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; r\n..gor ---&gt; a\n.gora ---&gt; v\ngorav ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; t\n..bat ---&gt; l\n.batl ---&gt; o\nbatlo ---&gt; o\natloo ---&gt; n\ntloon ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; m\n..shm ---&gt; e\n.shme ---&gt; j\nshmej ---&gt; h\nhmejh ---&gt; a\nmejha ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; i\n.mahi ---&gt; m\nmahim ---&gt; a\nahima ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; i\n.aani ---&gt; k\naanik ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; v\n..lav ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; s\n.amas ---&gt; i\namasi ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; z\n..faz ---&gt; i\n.fazi ---&gt; a\nfazia ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; i\n..khi ---&gt; m\n.khim ---&gt; a\nkhima ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; l\nsaral ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; r\n..mar ---&gt; i\n.mari ---&gt; y\nmariy ---&gt; a\nariya ---&gt; m\nriyam ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; h\n..wah ---&gt; e\n.wahe ---&gt; e\nwahee ---&gt; d\naheed ---&gt; a\nheeda ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; s\n..pus ---&gt; p\n.pusp ---&gt; e\npuspe ---&gt; n\nuspen ---&gt; d\nspend ---&gt; e\npende ---&gt; r\nender ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; s\n..ris ---&gt; h\n.rish ---&gt; i\nrishi ---&gt; p\niship ---&gt; a\nshipa ---&gt; l\nhipal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; j\n..maj ---&gt; o\n.majo ---&gt; o\nmajoo ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; c\namarc ---&gt; h\nmarch ---&gt; a\narcha ---&gt; n\nrchan ---&gt; d\nchand ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; s\n.chos ---&gt; h\nchosh ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; e\nparve ---&gt; s\narves ---&gt; h\nrvesh ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; n\nhiman ---&gt; s\nimans ---&gt; h\nmansh ---&gt; e\nanshe ---&gt; e\nnshee ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; o\n..kho ---&gt; o\n.khoo ---&gt; s\nkhoos ---&gt; h\nhoosh ---&gt; b\nooshb ---&gt; o\noshbo ---&gt; o\nshboo ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; r\n..isr ---&gt; a\n.isra ---&gt; i\nisrai ---&gt; l\nsrail ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; v\n.shiv ---&gt; n\nshivn ---&gt; a\nhivna ---&gt; t\nivnat ---&gt; h\nvnath ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; m\n..tam ---&gt; s\n.tams ---&gt; a\ntamsa ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; a\n.nasa ---&gt; r\nnasar ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; o\n..roo ---&gt; p\n.roop ---&gt; a\nroopa ---&gt; l\noopal ---&gt; i\nopali ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; p\n.devp ---&gt; a\ndevpa ---&gt; l\nevpal ---&gt; .\n..... ---&gt; d\n....d ---&gt; i\n...di ---&gt; p\n..dip ---&gt; e\n.dipe ---&gt; n\ndipen ---&gt; d\nipend ---&gt; r\npendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; g\n..jog ---&gt; a\n.joga ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; a\n.jita ---&gt; n\njitan ---&gt; d\nitand ---&gt; e\ntande ---&gt; r\nander ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; y\n..gay ---&gt; a\n.gaya ---&gt; t\ngayat ---&gt; r\nayatr ---&gt; i\nyatri ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; c\n..nac ---&gt; h\n.nach ---&gt; i\nnachi ---&gt; t\nachit ---&gt; a\nchita ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; h\n.rukh ---&gt; a\nrukha ---&gt; r\nukhar ---&gt; .\n..... ---&gt; a\n....a ---&gt; w\n...aw ---&gt; d\n..awd ---&gt; h\n.awdh ---&gt; e\nawdhe ---&gt; s\nwdhes ---&gt; h\ndhesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; m\n..kam ---&gt; l\n.kaml ---&gt; a\nkamla ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; h\n.vish ---&gt; v\nvishv ---&gt; n\nishvn ---&gt; a\nshvna ---&gt; t\nhvnat ---&gt; h\nvnath ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; a\n.mada ---&gt; n\nmadan ---&gt; l\nadanl ---&gt; a\ndanla ---&gt; l\nanlal ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; o\n..bho ---&gt; l\n.bhol ---&gt; e\nbhole ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; v\n.ramv ---&gt; i\nramvi ---&gt; r\namvir ---&gt; .\n..... ---&gt; i\n....i ---&gt; m\n...im ---&gt; t\n..imt ---&gt; y\n.imty ---&gt; a\nimtya ---&gt; z\nmtyaz ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; i\nhandi ---&gt; n\nandin ---&gt; i\nndini ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; o\n..moo ---&gt; l\n.mool ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; t\nravit ---&gt; a\navita ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; d\n.jagd ---&gt; i\njagdi ---&gt; s\nagdis ---&gt; h\ngdish ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; a\n.vipa ---&gt; s\nvipas ---&gt; h\nipash ---&gt; a\npasha ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; h\nprith ---&gt; v\nrithv ---&gt; i\nithvi ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; l\n..aal ---&gt; i\n.aali ---&gt; y\naaliy ---&gt; a\naliya ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; e\nparve ---&gt; z\narvez ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; u\n.janu ---&gt; k\njanuk ---&gt; a\nanuka ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; u\n..jhu ---&gt; h\n.jhuh ---&gt; i\njhuhi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; m\n.jaim ---&gt; a\njaima ---&gt; t\naimat ---&gt; i\nimati ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; i\n..aki ---&gt; l\n.akil ---&gt; e\nakile ---&gt; s\nkiles ---&gt; h\nilesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; p\n.mehp ---&gt; h\nmehph ---&gt; a\nehpha ---&gt; l\nhphal ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; r\n.gulr ---&gt; e\ngulre ---&gt; g\nulreg ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; u\n..gou ---&gt; t\n.gout ---&gt; a\ngouta ---&gt; m\noutam ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; k\n..yak ---&gt; s\n.yaks ---&gt; h\nyaksh ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; r\n..bir ---&gt; a\n.bira ---&gt; m\nbiram ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; k\n.shak ---&gt; i\nshaki ---&gt; b\nhakib ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; t\n.kart ---&gt; a\nkarta ---&gt; r\nartar ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; c\n..ruc ---&gt; h\n.ruch ---&gt; i\nruchi ---&gt; t\nuchit ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; t\n.chet ---&gt; t\nchett ---&gt; a\nhetta ---&gt; n\nettan ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; i\n.hari ---&gt; r\nharir ---&gt; a\narira ---&gt; m\nriram ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; v\nshahv ---&gt; a\nhahva ---&gt; j\nahvaj ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; k\n.mink ---&gt; a\nminka ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; n\n..bin ---&gt; i\n.bini ---&gt; t\nbinit ---&gt; a\ninita ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; i\nsarai ---&gt; n\narain ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; d\n.pard ---&gt; e\nparde ---&gt; p\nardep ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; j\n..baj ---&gt; u\n.baju ---&gt; l\nbajul ---&gt; a\najula ---&gt; l\njulal ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; x\n..lax ---&gt; m\n.laxm ---&gt; i\nlaxmi ---&gt; k\naxmik ---&gt; a\nxmika ---&gt; n\nmikan ---&gt; t\nikant ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; z\n..muz ---&gt; i\n.muzi ---&gt; m\nmuzim ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; t\n..rat ---&gt; i\n.rati ---&gt; m\nratim ---&gt; a\natima ---&gt; n\ntiman ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; l\n.parl ---&gt; a\nparla ---&gt; d\narlad ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; o\nparmo ---&gt; d\narmod ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; v\n..rev ---&gt; a\n.reva ---&gt; k\nrevak ---&gt; s\nevaks ---&gt; h\nvaksh ---&gt; i\nakshi ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; m\nsushm ---&gt; i\nushmi ---&gt; t\nshmit ---&gt; a\nhmita ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; a\n.sara ---&gt; h\nsarah ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; r\n..tar ---&gt; a\n.tara ---&gt; n\ntaran ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; r\n.anur ---&gt; a\nanura ---&gt; j\nnuraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; s\n.sans ---&gt; a\nsansa ---&gt; a\nansaa ---&gt; r\nnsaar ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; k\n.prak ---&gt; a\npraka ---&gt; s\nrakas ---&gt; h\nakash ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; d\n..bad ---&gt; r\n.badr ---&gt; u\nbadru ---&gt; d\nadrud ---&gt; e\ndrude ---&gt; e\nrudee ---&gt; n\nudeen ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; g\n..nag ---&gt; e\n.nage ---&gt; n\nnagen ---&gt; d\nagend ---&gt; e\ngende ---&gt; r\nender ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; n\n..hin ---&gt; a\n.hina ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; w\n..kaw ---&gt; a\n.kawa ---&gt; l\nkawal ---&gt; j\nawalj ---&gt; e\nwalje ---&gt; e\naljee ---&gt; t\nljeet ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; u\n..gau ---&gt; r\n.gaur ---&gt; i\ngauri ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; a\n.nara ---&gt; n\nnaran ---&gt; d\narand ---&gt; e\nrande ---&gt; r\nander ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; a\n.sawa ---&gt; r\nsawar ---&gt; m\nawarm ---&gt; a\nwarma ---&gt; l\narmal ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; v\n.nirv ---&gt; a\nnirva ---&gt; t\nirvat ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; n\n..gan ---&gt; d\n.gand ---&gt; h\ngandh ---&gt; i\nandhi ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; b\n..sob ---&gt; i\n.sobi ---&gt; t\nsobit ---&gt; .\n..... ---&gt; a\n....a ---&gt; i\n...ai ---&gt; s\n..ais ---&gt; h\n.aish ---&gt; a\naisha ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; d\n..vid ---&gt; y\n.vidy ---&gt; a\nvidya ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; v\n..pav ---&gt; i\n.pavi ---&gt; t\npavit ---&gt; r\navitr ---&gt; a\nvitra ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; e\n..pee ---&gt; r\n.peer ---&gt; .\n..... ---&gt; f\n....f ---&gt; i\n...fi ---&gt; z\n..fiz ---&gt; a\n.fiza ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; f\n..raf ---&gt; i\n.rafi ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; j\n..rij ---&gt; u\n.riju ---&gt; l\nrijul ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; a\nprema ---&gt; w\nremaw ---&gt; a\nemawa ---&gt; t\nmawat ---&gt; i\nawati ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; n\n..jun ---&gt; e\n.june ---&gt; b\njuneb ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; m\nbhanm ---&gt; a\nhanma ---&gt; t\nanmat ---&gt; i\nnmati ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; i\nhandi ---&gt; .\n..... ---&gt; a\n....a ---&gt; h\n...ah ---&gt; m\n..ahm ---&gt; e\n.ahme ---&gt; d\nahmed ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; a\nparva ---&gt; t\narvat ---&gt; i\nrvati ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; z\n..soz ---&gt; i\n.sozi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; k\n.vikk ---&gt; y\nvikky ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; k\nneelk ---&gt; a\neelka ---&gt; n\nelkan ---&gt; t\nlkant ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; w\n.satw ---&gt; i\nsatwi ---&gt; n\natwin ---&gt; d\ntwind ---&gt; e\nwinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; r\n..mir ---&gt; a\n.mira ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; u\n..phu ---&gt; l\n.phul ---&gt; b\nphulb ---&gt; i\nhulbi ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; a\n.para ---&gt; s\nparas ---&gt; .\n..... ---&gt; m\n....m ---&gt; h\n...mh ---&gt; e\n..mhe ---&gt; n\n.mhen ---&gt; d\nmhend ---&gt; a\nhenda ---&gt; r\nendar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; w\n.ramw ---&gt; a\nramwa ---&gt; t\namwat ---&gt; i\nmwati ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; b\n..sab ---&gt; i\n.sabi ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; r\n..bar ---&gt; j\n.barj ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; b\n..vib ---&gt; h\n.vibh ---&gt; a\nvibha ---&gt; s\nibhas ---&gt; h\nbhash ---&gt; .\n..... ---&gt; k\n....k ---&gt; l\n...kl ---&gt; a\n..kla ---&gt; s\n.klas ---&gt; h\nklash ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; s\nhimas ---&gt; h\nimash ---&gt; u\nmashu ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; b\n.jhab ---&gt; a\njhaba ---&gt; n\nhaban ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; w\n.gulw ---&gt; a\ngulwa ---&gt; s\nulwas ---&gt; h\nlwash ---&gt; a\nwasha ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; n\n.bhan ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; m\n..gom ---&gt; a\n.goma ---&gt; t\ngomat ---&gt; i\nomati ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; b\n..sob ---&gt; i\n.sobi ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; a\n.nisa ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; j\n.surj ---&gt; a\nsurja ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; d\n..sud ---&gt; a\n.suda ---&gt; n\nsudan ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; r\n.ishr ---&gt; a\nishra ---&gt; t\nshrat ---&gt; .\n..... ---&gt; l\n....l ---&gt; u\n...lu ---&gt; c\n..luc ---&gt; k\n.luck ---&gt; y\nlucky ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; a\n..ara ---&gt; v\n.arav ---&gt; i\naravi ---&gt; n\nravin ---&gt; d\navind ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; s\n..sus ---&gt; h\n.sush ---&gt; a\nsusha ---&gt; i\nushai ---&gt; l\nshail ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; a\n.muna ---&gt; j\nmunaj ---&gt; i\nunaji ---&gt; r\nnajir ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; n\n.sehn ---&gt; a\nsehna ---&gt; a\nehnaa ---&gt; z\nhnaaz ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; u\n..kau ---&gt; s\n.kaus ---&gt; a\nkausa ---&gt; l\nausal ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; z\n..haz ---&gt; a\n.haza ---&gt; r\nhazar ---&gt; i\nazari ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; e\n.raje ---&gt; s\nrajes ---&gt; w\najesw ---&gt; a\njeswa ---&gt; r\neswar ---&gt; y\nswary ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; e\n.nare ---&gt; n\nnaren ---&gt; d\narend ---&gt; e\nrende ---&gt; r\nender ---&gt; a\nndera ---&gt; .\n..... ---&gt; r\n....r ---&gt; o\n...ro ---&gt; h\n..roh ---&gt; t\n.roht ---&gt; h\nrohth ---&gt; a\nohtha ---&gt; s\nhthas ---&gt; h\nthash ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; a\n..aja ---&gt; r\n.ajar ---&gt; a\najara ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; d\n..pad ---&gt; a\n.pada ---&gt; m\npadam ---&gt; .\n..... ---&gt; t\n....t ---&gt; w\n...tw ---&gt; i\n..twi ---&gt; n\n.twin ---&gt; k\ntwink ---&gt; l\nwinkl ---&gt; e\ninkle ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; p\n.rajp ---&gt; u\nrajpu ---&gt; t\najput ---&gt; r\njputr ---&gt; a\nputra ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; n\n.pann ---&gt; a\npanna ---&gt; l\nannal ---&gt; a\nnnala ---&gt; l\nnalal ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; a\n.hara ---&gt; n\nharan ---&gt; a\narana ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; j\n.pooj ---&gt; a\npooja ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; b\n.shib ---&gt; a\nshiba ---&gt; .\n..... ---&gt; j\n....j ---&gt; u\n...ju ---&gt; g\n..jug ---&gt; a\n.juga ---&gt; l\njugal ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; s\n..aks ---&gt; h\n.aksh ---&gt; p\nakshp ---&gt; a\nkshpa ---&gt; a\nshpaa ---&gt; t\nhpaat ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; w\n..paw ---&gt; n\n.pawn ---&gt; i\npawni ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; q\n..yaq ---&gt; o\n.yaqo ---&gt; o\nyaqoo ---&gt; b\naqoob ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; s\n..kes ---&gt; h\n.kesh ---&gt; a\nkesha ---&gt; v\neshav ---&gt; e\nshave ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; r\n.mamr ---&gt; a\nmamra ---&gt; j\namraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; r\n.satr ---&gt; o\nsatro ---&gt; h\natroh ---&gt; a\ntroha ---&gt; n\nrohan ---&gt; .\n..... ---&gt; b\n....b ---&gt; r\n...br ---&gt; i\n..bri ---&gt; j\n.brij ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; k\n.jhak ---&gt; a\njhaka ---&gt; r\nhakar ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; i\n..asi ---&gt; y\n.asiy ---&gt; a\nasiya ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; o\n.asho ---&gt; k\nashok ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; s\n.niks ---&gt; i\nniksi ---&gt; y\niksiy ---&gt; a\nksiya ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; y\n..ary ---&gt; a\n.arya ---&gt; n\naryan ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; e\n..ade ---&gt; s\n.ades ---&gt; h\nadesh ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; a\n.moha ---&gt; t\nmohat ---&gt; t\nohatt ---&gt; e\nhatte ---&gt; r\natter ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; p\n..nip ---&gt; a\n.nipa ---&gt; m\nnipam ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; h\nprath ---&gt; a\nratha ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; p\n.satp ---&gt; a\nsatpa ---&gt; l\natpal ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; j\nnoorj ---&gt; a\noorja ---&gt; h\norjah ---&gt; a\nrjaha ---&gt; n\njahan ---&gt; .\n..... ---&gt; r\n....r ---&gt; i\n...ri ---&gt; t\n..rit ---&gt; i\n.riti ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; s\n..yas ---&gt; o\n.yaso ---&gt; d\nyasod ---&gt; a\nasoda ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; d\n..aad ---&gt; i\n.aadi ---&gt; s\naadis ---&gt; h\nadish ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; m\n..nam ---&gt; a\n.nama ---&gt; n\nnaman ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; z\n..amz ---&gt; a\n.amza ---&gt; d\namzad ---&gt; .\n..... ---&gt; p\n....p ---&gt; o\n...po ---&gt; o\n..poo ---&gt; r\n.poor ---&gt; a\npoora ---&gt; n\nooran ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; e\n..nee ---&gt; l\n.neel ---&gt; i\nneeli ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; l\n..bal ---&gt; j\n.balj ---&gt; e\nbalje ---&gt; e\naljee ---&gt; t\nljeet ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; i\n.shoi ---&gt; b\nshoib ---&gt; a\nhoiba ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; v\n.nanv ---&gt; e\nnanve ---&gt; e\nanvee ---&gt; t\nnveet ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; l\n..pal ---&gt; a\n.pala ---&gt; k\npalak ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; t\n..lat ---&gt; e\n.late ---&gt; e\nlatee ---&gt; f\nateef ---&gt; .\n..... ---&gt; l\n....l ---&gt; u\n...lu ---&gt; x\n..lux ---&gt; m\n.luxm ---&gt; i\nluxmi ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; o\n..foo ---&gt; l\n.fool ---&gt; a\nfoola ---&gt; .\n..... ---&gt; d\n....d ---&gt; o\n...do ---&gt; r\n..dor ---&gt; i\n.dori ---&gt; l\ndoril ---&gt; a\norila ---&gt; l\nrilal ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; c\n..arc ---&gt; h\n.arch ---&gt; i\narchi ---&gt; t\nrchit ---&gt; a\nchita ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; i\n.aari ---&gt; f\naarif ---&gt; u\narifu ---&gt; n\nrifun ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; m\n..asm ---&gt; a\n.asma ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; i\n.devi ---&gt; d\ndevid ---&gt; e\nevide ---&gt; e\nvidee ---&gt; n\nideen ---&gt; .\n..... ---&gt; g\n....g ---&gt; o\n...go ---&gt; r\n..gor ---&gt; i\n.gori ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; n\n..kun ---&gt; a\n.kuna ---&gt; l\nkunal ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; v\n..pav ---&gt; a\n.pava ---&gt; n\npavan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; m\n..sam ---&gt; s\n.sams ---&gt; e\nsamse ---&gt; r\namser ---&gt; .\n..... ---&gt; h\n....h ---&gt; o\n...ho ---&gt; n\n..hon ---&gt; e\n.hone ---&gt; y\nhoney ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; k\n.hark ---&gt; e\nharke ---&gt; s\narkes ---&gt; h\nrkesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; r\n.shar ---&gt; e\nshare ---&gt; e\nharee ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; n\nubhan ---&gt; g\nbhang ---&gt; i\nhangi ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; e\nmanje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; .\n..... ---&gt; a\n....a ---&gt; k\n...ak ---&gt; h\n..akh ---&gt; l\n.akhl ---&gt; i\nakhli ---&gt; s\nkhlis ---&gt; h\nhlish ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; a\n..saa ---&gt; d\n.saad ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; t\n.gyat ---&gt; r\ngyatr ---&gt; i\nyatri ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; o\n..sho ---&gt; b\n.shob ---&gt; i\nshobi ---&gt; n\nhobin ---&gt; a\nobina ---&gt; .\n..... ---&gt; n\n....n ---&gt; u\n...nu ---&gt; m\n..num ---&gt; e\n.nume ---&gt; s\nnumes ---&gt; h\numesh ---&gt; .\n..... ---&gt; k\n....k ---&gt; o\n...ko ---&gt; k\n..kok ---&gt; i\n.koki ---&gt; l\nkokil ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; m\n..him ---&gt; a\n.hima ---&gt; n\nhiman ---&gt; s\nimans ---&gt; h\nmansh ---&gt; i\nanshi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; m\n..vim ---&gt; u\n.vimu ---&gt; k\nvimuk ---&gt; t\nimukt ---&gt; .\n..... ---&gt; g\n....g ---&gt; e\n...ge ---&gt; n\n..gen ---&gt; d\n.gend ---&gt; i\ngendi ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; e\n..vee ---&gt; n\n.veen ---&gt; i\nveeni ---&gt; t\neenit ---&gt; a\nenita ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; h\n.lakh ---&gt; a\nlakha ---&gt; m\nakham ---&gt; i\nkhami ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; a\n.suba ---&gt; s\nsubas ---&gt; h\nubash ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; t\n.prat ---&gt; i\nprati ---&gt; b\nratib ---&gt; h\natibh ---&gt; a\ntibha ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; j\n.chaj ---&gt; j\nchajj ---&gt; u\nhajju ---&gt; .\n..... ---&gt; f\n....f ---&gt; o\n...fo ---&gt; r\n..for ---&gt; a\n.fora ---&gt; n\nforan ---&gt; t\norant ---&gt; a\nranta ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; b\n.chab ---&gt; i\nchabi ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; b\nhushb ---&gt; o\nushbo ---&gt; o\nshboo ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; c\n.panc ---&gt; h\npanch ---&gt; h\nanchh ---&gt; i\nnchhi ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; p\n..tap ---&gt; s\n.taps ---&gt; a\ntapsa ---&gt; m\napsam ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; n\n..mun ---&gt; n\n.munn ---&gt; e\nmunne ---&gt; .\n..... ---&gt; s\n....s ---&gt; y\n...sy ---&gt; e\n..sye ---&gt; d\n.syed ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; m\n..jam ---&gt; u\n.jamu ---&gt; n\njamun ---&gt; a\namuna ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; t\n..gut ---&gt; a\n.guta ---&gt; m\ngutam ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; p\n.ship ---&gt; r\nshipr ---&gt; a\nhipra ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; g\n..big ---&gt; a\n.biga ---&gt; n\nbigan ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; i\n.kari ---&gt; s\nkaris ---&gt; h\narish ---&gt; m\nrishm ---&gt; a\nishma ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; f\n..avf ---&gt; e\n.avfe ---&gt; s\navfes ---&gt; h\nvfesh ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; z\nshahz ---&gt; a\nhahza ---&gt; d\nahzad ---&gt; i\nhzadi ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; l\n..kel ---&gt; a\n.kela ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; l\n..lal ---&gt; a\n.lala ---&gt; r\nlalar ---&gt; a\nalara ---&gt; m\nlaram ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; a\ndeepa ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; y\n.maay ---&gt; a\nmaaya ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; u\n..tau ---&gt; f\n.tauf ---&gt; i\ntaufi ---&gt; k\naufik ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; v\n.prav ---&gt; e\nprave ---&gt; s\nraves ---&gt; h\navesh ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; a\n.pana ---&gt; j\npanaj ---&gt; .\n..... ---&gt; a\n....a ---&gt; p\n...ap ---&gt; h\n..aph ---&gt; a\n.apha ---&gt; s\naphas ---&gt; a\nphasa ---&gt; n\nhasan ---&gt; a\nasana ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; n\n..gun ---&gt; j\n.gunj ---&gt; a\ngunja ---&gt; n\nunjan ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; n\n..aan ---&gt; s\n.aans ---&gt; i\naansi ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; h\n.nish ---&gt; a\nnisha ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; t\n..mit ---&gt; h\n.mith ---&gt; e\nmithe ---&gt; l\nithel ---&gt; a\nthela ---&gt; s\nhelas ---&gt; h\nelash ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; e\n..khe ---&gt; t\n.khet ---&gt; r\nkhetr ---&gt; a\nhetra ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; m\n..sum ---&gt; e\n.sume ---&gt; r\nsumer ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; g\n..gug ---&gt; n\n.gugn ---&gt; a\ngugna ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; m\n.saim ---&gt; u\nsaimu ---&gt; n\naimun ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; m\n.pram ---&gt; i\nprami ---&gt; l\nramil ---&gt; a\namila ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; l\n.anjl ---&gt; i\nanjli ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; h\n.nikh ---&gt; i\nnikhi ---&gt; l\nikhil ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; e\nsanje ---&gt; e\nanjee ---&gt; t\nnjeet ---&gt; a\njeeta ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; e\n..bee ---&gt; r\n.beer ---&gt; a\nbeera ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; u\nsanju ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; b\n.rajb ---&gt; i\nrajbi ---&gt; r\najbir ---&gt; i\njbiri ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; r\n.chor ---&gt; a\nchora ---&gt; g\nhorag ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; s\nshans ---&gt; h\nhansh ---&gt; a\nansha ---&gt; k\nnshak ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; k\namrik ---&gt; a\nmrika ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; d\n.jagd ---&gt; a\njagda ---&gt; m\nagdam ---&gt; b\ngdamb ---&gt; a\ndamba ---&gt; .\n..... ---&gt; b\n....b ---&gt; i\n...bi ---&gt; t\n..bit ---&gt; t\n.bitt ---&gt; a\nbitta ---&gt; n\nittan ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; k\n..ruk ---&gt; s\n.ruks ---&gt; a\nruksa ---&gt; a\nuksaa ---&gt; r\nksaar ---&gt; .\n..... ---&gt; y\n....y ---&gt; u\n...yu ---&gt; s\n..yus ---&gt; a\n.yusa ---&gt; f\nyusaf ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; i\n.sari ---&gt; d\nsarid ---&gt; e\naride ---&gt; v\nridev ---&gt; i\nidevi ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; d\n..lad ---&gt; d\n.ladd ---&gt; h\nladdh ---&gt; a\naddha ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; h\n..jah ---&gt; i\n.jahi ---&gt; d\njahid ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; a\n..gha ---&gt; n\n.ghan ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; e\n.suke ---&gt; n\nsuken ---&gt; t\nukent ---&gt; a\nkenta ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; b\n.ramb ---&gt; a\nramba ---&gt; i\nambai ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; k\n.nikk ---&gt; y\nnikky ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; a\n.mala ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; n\ndhann ---&gt; a\nhanna ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; j\n..taj ---&gt; m\n.tajm ---&gt; a\ntajma ---&gt; h\najmah ---&gt; a\njmaha ---&gt; .\n..... ---&gt; n\n....n ---&gt; e\n...ne ---&gt; n\n..nen ---&gt; i\n.neni ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; k\n.lakk ---&gt; y\nlakky ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; u\n.sahu ---&gt; n\nsahun ---&gt; .\n..... ---&gt; o\n....o ---&gt; m\n...om ---&gt; e\n..ome ---&gt; s\n.omes ---&gt; h\nomesh ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; a\n.aasa ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; h\nparsh ---&gt; a\narsha ---&gt; n\nrshan ---&gt; t\nshant ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; w\npremw ---&gt; a\nremwa ---&gt; t\nemwat ---&gt; i\nmwati ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; b\n..mob ---&gt; a\n.moba ---&gt; r\nmobar ---&gt; k\nobark ---&gt; a\nbarka ---&gt; r\narkar ---&gt; .\n..... ---&gt; h\n....h ---&gt; i\n...hi ---&gt; s\n..his ---&gt; h\n.hish ---&gt; a\nhisha ---&gt; m\nisham ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; s\n..mos ---&gt; i\n.mosi ---&gt; m\nmosim ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; b\nshabb ---&gt; o\nhabbo ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; r\n..vir ---&gt; p\n.virp ---&gt; a\nvirpa ---&gt; l\nirpal ---&gt; .\n..... ---&gt; a\n....a ---&gt; d\n...ad ---&gt; a\n..ada ---&gt; n\n.adan ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; s\n..rus ---&gt; t\n.rust ---&gt; a\nrusta ---&gt; m\nustam ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; m\n..ajm ---&gt; i\n.ajmi ---&gt; t\najmit ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; z\n..maz ---&gt; h\n.mazh ---&gt; a\nmazha ---&gt; r\nazhar ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; i\n.sani ---&gt; t\nsanit ---&gt; a\nanita ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; t\n..cht ---&gt; a\n.chta ---&gt; r\nchtar ---&gt; p\nhtarp ---&gt; a\ntarpa ---&gt; l\narpal ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; g\n..gag ---&gt; a\n.gaga ---&gt; n\ngagan ---&gt; .\n..... ---&gt; m\n....m ---&gt; i\n...mi ---&gt; n\n..min ---&gt; k\n.mink ---&gt; a\nminka ---&gt; s\ninkas ---&gt; h\nnkash ---&gt; i\nkashi ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; t\n.amrt ---&gt; a\namrta ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; j\n..vij ---&gt; a\n.vija ---&gt; y\nvijay ---&gt; t\nijayt ---&gt; a\njayta ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; n\n..ban ---&gt; w\n.banw ---&gt; a\nbanwa ---&gt; r\nanwar ---&gt; i\nnwari ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; r\n..dhr ---&gt; a\n.dhra ---&gt; m\ndhram ---&gt; b\nhramb ---&gt; i\nrambi ---&gt; r\nambir ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; v\n.parv ---&gt; e\nparve ---&gt; j\narvej ---&gt; .\n..... ---&gt; o\n....o ---&gt; n\n...on ---&gt; e\n..one ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; e\n..pee ---&gt; n\n.peen ---&gt; t\npeent ---&gt; u\neentu ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; a\n..uda ---&gt; l\n.udal ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; e\n..aje ---&gt; e\n.ajee ---&gt; m\najeem ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; n\n.chan ---&gt; d\nchand ---&gt; .\n..... ---&gt; d\n....d ---&gt; h\n...dh ---&gt; a\n..dha ---&gt; n\n.dhan ---&gt; a\ndhana ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; z\n..maz ---&gt; i\n.mazi ---&gt; d\nmazid ---&gt; a\nazida ---&gt; .\n..... ---&gt; i\n....i ---&gt; s\n...is ---&gt; h\n..ish ---&gt; a\n.isha ---&gt; n\nishan ---&gt; t\nshant ---&gt; .\n..... ---&gt; s\n....s ---&gt; t\n...st ---&gt; i\n..sti ---&gt; f\n.stif ---&gt; e\nstife ---&gt; n\ntifen ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; s\n..nas ---&gt; e\n.nase ---&gt; e\nnasee ---&gt; m\naseem ---&gt; a\nseema ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; i\n.suni ---&gt; l\nsunil ---&gt; .\n..... ---&gt; e\n....e ---&gt; z\n...ez ---&gt; a\n..eza ---&gt; z\n.ezaz ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; m\n.jasm ---&gt; e\njasme ---&gt; n\nasmen ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; t\n..bat ---&gt; u\n.batu ---&gt; l\nbatul ---&gt; .\n..... ---&gt; k\n....k ---&gt; e\n...ke ---&gt; l\n..kel ---&gt; a\n.kela ---&gt; s\nkelas ---&gt; h\nelash ---&gt; i\nlashi ---&gt; .\n..... ---&gt; a\n....a ---&gt; l\n...al ---&gt; h\n..alh ---&gt; a\n.alha ---&gt; m\nalham ---&gt; d\nlhamd ---&gt; i\nhamdi ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; s\n..bus ---&gt; r\n.busr ---&gt; a\nbusra ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; a\n..rua ---&gt; n\n.ruan ---&gt; b\nruanb ---&gt; z\nuanbz ---&gt; a\nanbza ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; r\n..bhr ---&gt; a\n.bhra ---&gt; t\nbhrat ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; v\n..bav ---&gt; i\n.bavi ---&gt; t\nbavit ---&gt; a\navita ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; s\namris ---&gt; h\nmrish ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; w\n.bhaw ---&gt; a\nbhawa ---&gt; r\nhawar ---&gt; i\nawari ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; a\n..pra ---&gt; k\n.prak ---&gt; u\npraku ---&gt; l\nrakul ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; a\n..ama ---&gt; r\n.amar ---&gt; e\namare ---&gt; n\nmaren ---&gt; d\narend ---&gt; r\nrendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; d\n..yad ---&gt; r\n.yadr ---&gt; a\nyadra ---&gt; m\nadram ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; u\n.satu ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; z\n..haz ---&gt; r\n.hazr ---&gt; a\nhazra ---&gt; t\nazrat ---&gt; .\n..... ---&gt; m\n....m ---&gt; e\n...me ---&gt; h\n..meh ---&gt; b\n.mehb ---&gt; o\nmehbo ---&gt; o\nehboo ---&gt; b\nhboob ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; w\n..anw ---&gt; a\n.anwa ---&gt; r\nanwar ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; m\n.jaim ---&gt; a\njaima ---&gt; t\naimat ---&gt; a\nimata ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; d\n..nid ---&gt; a\n.nida ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; b\n..rab ---&gt; i\n.rabi ---&gt; t\nrabit ---&gt; a\nabita ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; i\n.subi ---&gt; n\nsubin ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; g\n.jang ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; u\n.anju ---&gt; m\nanjum ---&gt; a\nnjuma ---&gt; n\njuman ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; y\n.devy ---&gt; a\ndevya ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; w\n.bhaw ---&gt; a\nbhawa ---&gt; r\nhawar ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; s\n..lis ---&gt; h\n.lish ---&gt; a\nlisha ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; p\n.sunp ---&gt; r\nsunpr ---&gt; e\nunpre ---&gt; e\nnpree ---&gt; t\npreet ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; k\n..lak ---&gt; s\n.laks ---&gt; h\nlaksh ---&gt; a\naksha ---&gt; y\nkshay ---&gt; a\nshaya ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; y\n..nay ---&gt; a\n.naya ---&gt; n\nnayan ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; m\n..udm ---&gt; a\n.udma ---&gt; i\nudmai ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; p\n.narp ---&gt; e\nnarpe ---&gt; n\narpen ---&gt; d\nrpend ---&gt; r\npendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; i\n..sai ---&gt; f\n.saif ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; i\n.sadi ---&gt; p\nsadip ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; w\n..saw ---&gt; e\n.sawe ---&gt; t\nsawet ---&gt; a\naweta ---&gt; .\n..... ---&gt; u\n....u ---&gt; n\n...un ---&gt; k\n..unk ---&gt; a\n.unka ---&gt; r\nunkar ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; s\n..las ---&gt; k\n.lask ---&gt; s\nlasks ---&gt; h\nasksh ---&gt; i\nskshi ---&gt; t\nkshit ---&gt; a\nshita ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; o\n..amo ---&gt; l\n.amol ---&gt; .\n..... ---&gt; u\n....u ---&gt; d\n...ud ---&gt; i\n..udi ---&gt; t\n.udit ---&gt; a\nudita ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; s\n..kis ---&gt; h\n.kish ---&gt; a\nkisha ---&gt; n\nishan ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; a\n..taa ---&gt; r\n.taar ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; e\nshahe ---&gt; r\nhaher ---&gt; a\nahera ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; k\n.rajk ---&gt; i\nrajki ---&gt; r\najkir ---&gt; a\njkira ---&gt; n\nkiran ---&gt; .\n..... ---&gt; s\n....s ---&gt; i\n...si ---&gt; f\n..sif ---&gt; a\n.sifa ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; i\n..ani ---&gt; t\n.anit ---&gt; a\nanita ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; s\n..sos ---&gt; a\n.sosa ---&gt; n\nsosan ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; m\n.swam ---&gt; i\nswami ---&gt; .\n..... ---&gt; l\n....l ---&gt; a\n...la ---&gt; u\n..lau ---&gt; k\n.lauk ---&gt; u\nlauku ---&gt; s\naukus ---&gt; h\nukush ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; r\n..ver ---&gt; o\n.vero ---&gt; n\nveron ---&gt; i\neroni ---&gt; k\nronik ---&gt; a\nonika ---&gt; .\n..... ---&gt; d\n....d ---&gt; a\n...da ---&gt; r\n..dar ---&gt; s\n.dars ---&gt; h\ndarsh ---&gt; a\narsha ---&gt; n\nrshan ---&gt; a\nshana ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; n\n.sajn ---&gt; i\nsajni ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; a\n.saha ---&gt; j\nsahaj ---&gt; a\nahaja ---&gt; h\nhajah ---&gt; a\najaha ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; e\n.sohe ---&gt; b\nsoheb ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; p\n.swap ---&gt; a\nswapa ---&gt; m\nwapam ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; h\n..soh ---&gt; a\n.soha ---&gt; i\nsohai ---&gt; l\nohail ---&gt; .\n..... ---&gt; u\n....u ---&gt; m\n...um ---&gt; r\n..umr ---&gt; a\n.umra ---&gt; w\numraw ---&gt; t\nmrawt ---&gt; i\nrawti ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; f\n..tuf ---&gt; e\n.tufe ---&gt; l\ntufel ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; u\n..khu ---&gt; s\n.khus ---&gt; h\nkhush ---&gt; i\nhushi ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; a\n.sana ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; b\n.gulb ---&gt; a\ngulba ---&gt; s\nulbas ---&gt; h\nlbash ---&gt; a\nbasha ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; h\n..nah ---&gt; a\n.naha ---&gt; n\nnahan ---&gt; i\nahani ---&gt; .\n..... ---&gt; g\n....g ---&gt; y\n...gy ---&gt; a\n..gya ---&gt; n\n.gyan ---&gt; u\ngyanu ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; r\n..per ---&gt; a\n.pera ---&gt; .\n..... ---&gt; g\n....g ---&gt; i\n...gi ---&gt; r\n..gir ---&gt; r\n.girr ---&gt; a\ngirra ---&gt; j\nirraj ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; j\n..maj ---&gt; r\n.majr ---&gt; u\nmajru ---&gt; l\najrul ---&gt; a\njrula ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; a\n.sana ---&gt; m\nsanam ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; f\n..sef ---&gt; a\n.sefa ---&gt; l\nsefal ---&gt; i\nefali ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; r\n..kar ---&gt; s\n.kars ---&gt; h\nkarsh ---&gt; m\narshm ---&gt; a\nrshma ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; b\n.mahb ---&gt; o\nmahbo ---&gt; o\nahboo ---&gt; b\nhboob ---&gt; .\n..... ---&gt; a\n....a ---&gt; m\n...am ---&gt; r\n..amr ---&gt; i\n.amri ---&gt; t\namrit ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; p\n.jasp ---&gt; r\njaspr ---&gt; e\naspre ---&gt; e\nspree ---&gt; t\npreet ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; s\n..mas ---&gt; o\n.maso ---&gt; o\nmasoo ---&gt; m\nasoom ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; b\n..sub ---&gt; h\n.subh ---&gt; a\nsubha ---&gt; g\nubhag ---&gt; y\nbhagy ---&gt; a\nhagya ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; i\nsanji ---&gt; v\nanjiv ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; n\n..han ---&gt; s\n.hans ---&gt; r\nhansr ---&gt; a\nansra ---&gt; j\nnsraj ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; h\n.shah ---&gt; n\nshahn ---&gt; a\nhahna ---&gt; z\nahnaz ---&gt; a\nhnaza ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; e\n.nave ---&gt; n\nnaven ---&gt; d\navend ---&gt; r\nvendr ---&gt; a\nendra ---&gt; .\n..... ---&gt; k\n....k ---&gt; h\n...kh ---&gt; a\n..kha ---&gt; l\n.khal ---&gt; i\nkhali ---&gt; d\nhalid ---&gt; u\nalidu ---&gt; r\nlidur ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; d\n.rand ---&gt; h\nrandh ---&gt; i\nandhi ---&gt; r\nndhir ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; t\n..sat ---&gt; y\n.saty ---&gt; a\nsatya ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; r\n.gulr ---&gt; a\ngulra ---&gt; n\nulran ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; s\n..mus ---&gt; h\n.mush ---&gt; k\nmushk ---&gt; a\nushka ---&gt; n\nshkan ---&gt; .\n..... ---&gt; t\n....t ---&gt; r\n...tr ---&gt; i\n..tri ---&gt; j\n.trij ---&gt; u\ntriju ---&gt; g\nrijug ---&gt; i\nijugi ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; p\n..vip ---&gt; o\n.vipo ---&gt; l\nvipol ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; b\n.shab ---&gt; i\nshabi ---&gt; l\nhabil ---&gt; a\nabila ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; a\n..maa ---&gt; n\n.maan ---&gt; m\nmaanm ---&gt; a\naanma ---&gt; t\nanmat ---&gt; i\nnmati ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; j\n.manj ---&gt; u\nmanju ---&gt; s\nanjus ---&gt; h\nnjush ---&gt; a\njusha ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; h\n..moh ---&gt; i\n.mohi ---&gt; t\nmohit ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; m\n..pam ---&gt; m\n.pamm ---&gt; i\npammi ---&gt; .\n..... ---&gt; w\n....w ---&gt; a\n...wa ---&gt; k\n..wak ---&gt; a\n.waka ---&gt; r\nwakar ---&gt; .\n..... ---&gt; b\n....b ---&gt; h\n...bh ---&gt; a\n..bha ---&gt; g\n.bhag ---&gt; v\nbhagv ---&gt; a\nhagva ---&gt; a\nagvaa ---&gt; n\ngvaan ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; c\n.jaic ---&gt; h\njaich ---&gt; a\naicha ---&gt; n\nichan ---&gt; d\nchand ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; u\n..shu ---&gt; s\n.shus ---&gt; h\nshush ---&gt; m\nhushm ---&gt; a\nushma ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; u\n..anu ---&gt; p\n.anup ---&gt; a\nanupa ---&gt; m\nnupam ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; l\n.sarl ---&gt; a\nsarla ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; r\n..ser ---&gt; a\n.sera ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; d\n.pard ---&gt; e\nparde ---&gt; e\nardee ---&gt; p\nrdeep ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; k\n..sok ---&gt; i\n.soki ---&gt; n\nsokin ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; r\n..nar ---&gt; o\n.naro ---&gt; o\nnaroo ---&gt; .\n..... ---&gt; f\n....f ---&gt; a\n...fa ---&gt; r\n..far ---&gt; h\n.farh ---&gt; a\nfarha ---&gt; n\narhan ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; i\nchoti ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; g\n..sug ---&gt; a\n.suga ---&gt; r\nsugar ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; n\n..ran ---&gt; g\n.rang ---&gt; e\nrange ---&gt; e\nangee ---&gt; t\nngeet ---&gt; a\ngeeta ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; r\n..har ---&gt; s\n.hars ---&gt; h\nharsh ---&gt; .\n..... ---&gt; a\n....a ---&gt; y\n...ay ---&gt; u\n..ayu ---&gt; b\n.ayub ---&gt; e\nayube ---&gt; .\n..... ---&gt; j\n....j ---&gt; i\n...ji ---&gt; t\n..jit ---&gt; i\n.jiti ---&gt; n\njitin ---&gt; .\n..... ---&gt; p\n....p ---&gt; u\n...pu ---&gt; n\n..pun ---&gt; n\n.punn ---&gt; i\npunni ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; a\nparsa ---&gt; n\narsan ---&gt; n\nrsann ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; o\n..moo ---&gt; l\n.mool ---&gt; d\nmoold ---&gt; a\noolda ---&gt; n\noldan ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; w\n..naw ---&gt; a\n.nawa ---&gt; b\nnawab ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; j\n..saj ---&gt; m\n.sajm ---&gt; e\nsajme ---&gt; e\najmee ---&gt; n\njmeen ---&gt; .\n..... ---&gt; p\n....p ---&gt; e\n...pe ---&gt; n\n..pen ---&gt; z\n.penz ---&gt; i\npenzi ---&gt; n\nenzin ---&gt; .\n..... ---&gt; i\n....i ---&gt; b\n...ib ---&gt; r\n..ibr ---&gt; a\n.ibra ---&gt; h\nibrah ---&gt; a\nbraha ---&gt; m\nraham ---&gt; .\n..... ---&gt; r\n....r ---&gt; e\n...re ---&gt; h\n..reh ---&gt; m\n.rehm ---&gt; a\nrehma ---&gt; t\nehmat ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; w\n.sarw ---&gt; i\nsarwi ---&gt; t\narwit ---&gt; a\nrwita ---&gt; v\nwitav ---&gt; m\nitavm ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; m\n.sham ---&gt; r\nshamr ---&gt; i\nhamri ---&gt; n\namrin ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; k\n..nik ---&gt; i\n.niki ---&gt; l\nnikil ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; t\n..mat ---&gt; a\n.mata ---&gt; d\nmatad ---&gt; e\natade ---&gt; e\ntadee ---&gt; n\nadeen ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; r\n..sur ---&gt; j\n.surj ---&gt; i\nsurji ---&gt; y\nurjiy ---&gt; a\nrjiya ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; a\n.jaga ---&gt; n\njagan ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; s\n..vis ---&gt; a\n.visa ---&gt; n\nvisan ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; l\n.deel ---&gt; i\ndeeli ---&gt; p\neelip ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; i\n..shi ---&gt; l\n.shil ---&gt; p\nshilp ---&gt; i\nhilpi ---&gt; .\n..... ---&gt; v\n....v ---&gt; e\n...ve ---&gt; s\n..ves ---&gt; h\n.vesh ---&gt; a\nvesha ---&gt; l\neshal ---&gt; i\nshali ---&gt; .\n..... ---&gt; b\n....b ---&gt; e\n...be ---&gt; n\n..ben ---&gt; i\n.beni ---&gt; .\n..... ---&gt; b\n....b ---&gt; a\n...ba ---&gt; g\n..bag ---&gt; g\n.bagg ---&gt; a\nbagga ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; n\n..pan ---&gt; g\n.pang ---&gt; i\npangi ---&gt; t\nangit ---&gt; a\nngita ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; n\nchhan ---&gt; t\nhhant ---&gt; u\nhantu ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; o\n..pho ---&gt; l\n.phol ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; v\n.ramv ---&gt; a\nramva ---&gt; t\namvat ---&gt; i\nmvati ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; v\n..nav ---&gt; j\n.navj ---&gt; o\nnavjo ---&gt; o\navjoo ---&gt; t\nvjoot ---&gt; .\n..... ---&gt; a\n....a ---&gt; v\n...av ---&gt; a\n..ava ---&gt; d\n.avad ---&gt; h\navadh ---&gt; .\n..... ---&gt; a\n....a ---&gt; s\n...as ---&gt; h\n..ash ---&gt; a\n.asha ---&gt; n\nashan ---&gt; j\nshanj ---&gt; a\nhanja ---&gt; l\nanjal ---&gt; i\nnjali ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; e\n..pre ---&gt; m\n.prem ---&gt; o\npremo ---&gt; .\n..... ---&gt; g\n....g ---&gt; a\n...ga ---&gt; j\n..gaj ---&gt; a\n.gaja ---&gt; n\ngajan ---&gt; a\najana ---&gt; .\n..... ---&gt; r\n....r ---&gt; u\n...ru ---&gt; p\n..rup ---&gt; a\n.rupa ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; a\n.raja ---&gt; n\nrajan ---&gt; .\n..... ---&gt; s\n....s ---&gt; e\n...se ---&gt; h\n..seh ---&gt; j\n.sehj ---&gt; a\nsehja ---&gt; m\nehjam ---&gt; a\nhjama ---&gt; l\njamal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; m\n..mam ---&gt; u\n.mamu ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; p\n..sap ---&gt; a\n.sapa ---&gt; n\nsapan ---&gt; a\napana ---&gt; .\n..... ---&gt; m\n....m ---&gt; u\n...mu ---&gt; r\n..mur ---&gt; a\n.mura ---&gt; d\nmurad ---&gt; .\n..... ---&gt; v\n....v ---&gt; i\n...vi ---&gt; k\n..vik ---&gt; r\n.vikr ---&gt; a\nvikra ---&gt; n\nikran ---&gt; t\nkrant ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; i\n.shai ---&gt; d\nshaid ---&gt; .\n..... ---&gt; f\n....f ---&gt; e\n...fe ---&gt; r\n..fer ---&gt; u\n.feru ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; t\n.kant ---&gt; i\nkanti ---&gt; l\nantil ---&gt; .\n..... ---&gt; y\n....y ---&gt; a\n...ya ---&gt; m\n..yam ---&gt; i\n.yami ---&gt; n\nyamin ---&gt; i\namini ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; u\n.raju ---&gt; d\nrajud ---&gt; d\najudd ---&gt; i\njuddi ---&gt; n\nuddin ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; v\n.sarv ---&gt; e\nsarve ---&gt; s\narves ---&gt; .\n..... ---&gt; n\n....n ---&gt; o\n...no ---&gt; o\n..noo ---&gt; r\n.noor ---&gt; .\n..... ---&gt; t\n....t ---&gt; u\n...tu ---&gt; f\n..tuf ---&gt; a\n.tufa ---&gt; i\ntufai ---&gt; l\nufail ---&gt; .\n..... ---&gt; k\n....k ---&gt; i\n...ki ---&gt; n\n..kin ---&gt; g\n.king ---&gt; k\nkingk ---&gt; a\ningka ---&gt; r\nngkar ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; n\n..jan ---&gt; t\n.jant ---&gt; a\njanta ---&gt; .\n..... ---&gt; b\n....b ---&gt; u\n...bu ---&gt; h\n..buh ---&gt; a\n.buha ---&gt; n\nbuhan ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; s\n.jass ---&gt; i\njassi ---&gt; .\n..... ---&gt; j\n....j ---&gt; e\n...je ---&gt; e\n..jee ---&gt; r\n.jeer ---&gt; e\njeere ---&gt; n\neeren ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; b\n..job ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; b\n..arb ---&gt; a\n.arba ---&gt; z\narbaz ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; g\n..sag ---&gt; i\n.sagi ---&gt; r\nsagir ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; z\n..naz ---&gt; i\n.nazi ---&gt; m\nnazim ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; l\n..mal ---&gt; v\n.malv ---&gt; i\nmalvi ---&gt; k\nalvik ---&gt; a\nlvika ---&gt; .\n..... ---&gt; g\n....g ---&gt; u\n...gu ---&gt; l\n..gul ---&gt; n\n.guln ---&gt; a\ngulna ---&gt; a\nulnaa ---&gt; z\nlnaaz ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; v\n..sav ---&gt; r\n.savr ---&gt; i\nsavri ---&gt; n\navrin ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; r\n..sar ---&gt; m\n.sarm ---&gt; e\nsarme ---&gt; e\narmee ---&gt; l\nrmeel ---&gt; a\nmeela ---&gt; .\n..... ---&gt; t\n....t ---&gt; o\n...to ---&gt; h\n..toh ---&gt; i\n.tohi ---&gt; d\ntohid ---&gt; .\n..... ---&gt; s\n....s ---&gt; o\n...so ---&gt; u\n..sou ---&gt; v\n.souv ---&gt; i\nsouvi ---&gt; k\nouvik ---&gt; .\n..... ---&gt; n\n....n ---&gt; a\n...na ---&gt; n\n..nan ---&gt; k\n.nank ---&gt; u\nnanku ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; l\n.shal ---&gt; i\nshali ---&gt; d\nhalid ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; d\n..mad ---&gt; h\n.madh ---&gt; u\nmadhu ---&gt; b\nadhub ---&gt; a\ndhuba ---&gt; l\nhubal ---&gt; a\nubala ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; e\n..che ---&gt; l\n.chel ---&gt; a\nchela ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; j\n..ajj ---&gt; u\n.ajju ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; d\n.sand ---&gt; e\nsande ---&gt; e\nandee ---&gt; p\nndeep ---&gt; a\ndeepa ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; c\n..suc ---&gt; h\n.such ---&gt; e\nsuche ---&gt; t\nuchet ---&gt; a\ncheta ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; h\n..rah ---&gt; i\n.rahi ---&gt; l\nrahil ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; s\n..aas ---&gt; m\n.aasm ---&gt; i\naasmi ---&gt; n\nasmin ---&gt; .\n..... ---&gt; h\n....h ---&gt; a\n...ha ---&gt; m\n..ham ---&gt; i\n.hami ---&gt; d\nhamid ---&gt; .\n..... ---&gt; r\n....r ---&gt; v\n...rv ---&gt; i\n..rvi ---&gt; n\n.rvin ---&gt; a\nrvina ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; t\n.maht ---&gt; a\nmahta ---&gt; b\nahtab ---&gt; .\n..... ---&gt; p\n....p ---&gt; r\n...pr ---&gt; i\n..pri ---&gt; t\n.prit ---&gt; i\npriti ---&gt; k\nritik ---&gt; a\nitika ---&gt; .\n..... ---&gt; l\n....l ---&gt; i\n...li ---&gt; l\n..lil ---&gt; a\n.lila ---&gt; d\nlilad ---&gt; e\nilade ---&gt; v\nladev ---&gt; i\nadevi ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; j\n..raj ---&gt; i\n.raji ---&gt; n\nrajin ---&gt; a\najina ---&gt; .\n..... ---&gt; d\n....d ---&gt; u\n...du ---&gt; r\n..dur ---&gt; g\n.durg ---&gt; p\ndurgp ---&gt; a\nurgpa ---&gt; l\nrgpal ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; e\n.mane ---&gt; s\nmanes ---&gt; h\nanesh ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; a\n.jaga ---&gt; .\n..... ---&gt; j\n....j ---&gt; h\n...jh ---&gt; a\n..jha ---&gt; w\n.jhaw ---&gt; a\njhawa ---&gt; r\nhawar ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; t\n..kat ---&gt; y\n.katy ---&gt; a\nkatya ---&gt; y\natyay ---&gt; a\ntyaya ---&gt; n\nyayan ---&gt; i\nayani ---&gt; .\n..... ---&gt; k\n....k ---&gt; u\n...ku ---&gt; m\n..kum ---&gt; o\n.kumo ---&gt; d\nkumod ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; m\n..nim ---&gt; l\n.niml ---&gt; a\nnimla ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; g\n.sang ---&gt; i\nsangi ---&gt; t\nangit ---&gt; a\nngita ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; r\n..jar ---&gt; i\n.jari ---&gt; f\njarif ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; m\n..tam ---&gt; a\n.tama ---&gt; n\ntaman ---&gt; a\namana ---&gt; .\n..... ---&gt; a\n....a ---&gt; r\n...ar ---&gt; m\n..arm ---&gt; a\n.arma ---&gt; a\narmaa ---&gt; n\nrmaan ---&gt; .\n..... ---&gt; a\n....a ---&gt; j\n...aj ---&gt; m\n..ajm ---&gt; e\n.ajme ---&gt; r\najmer ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; a\n.aara ---&gt; v\naarav ---&gt; .\n..... ---&gt; t\n....t ---&gt; a\n...ta ---&gt; n\n..tan ---&gt; i\n.tani ---&gt; a\ntania ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; m\n..ram ---&gt; p\n.ramp ---&gt; h\nramph ---&gt; e\namphe ---&gt; r\nmpher ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; d\n..sad ---&gt; a\n.sada ---&gt; b\nsadab ---&gt; .\n..... ---&gt; t\n....t ---&gt; e\n...te ---&gt; h\n..teh ---&gt; m\n.tehm ---&gt; i\ntehmi ---&gt; n\nehmin ---&gt; a\nhmina ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; o\n..cho ---&gt; t\n.chot ---&gt; u\nchotu ---&gt; m\nhotum ---&gt; .\n..... ---&gt; k\n....k ---&gt; a\n...ka ---&gt; n\n..kan ---&gt; a\n.kana ---&gt; k\nkanak ---&gt; .\n..... ---&gt; s\n....s ---&gt; w\n...sw ---&gt; a\n..swa ---&gt; t\n.swat ---&gt; a\nswata ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; i\n..jai ---&gt; j\n.jaij ---&gt; a\njaija ---&gt; i\naijai ---&gt; r\nijair ---&gt; a\njaira ---&gt; m\nairam ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; s\n..nis ---&gt; h\n.nish ---&gt; a\nnisha ---&gt; r\nishar ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; m\n.sukm ---&gt; a\nsukma ---&gt; n\nukman ---&gt; i\nkmani ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; a\n..ana ---&gt; d\n.anad ---&gt; i\nanadi ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; s\n..jas ---&gt; w\n.jasw ---&gt; i\njaswi ---&gt; n\naswin ---&gt; d\nswind ---&gt; e\nwinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; n\n..man ---&gt; t\n.mant ---&gt; i\nmanti ---&gt; .\n..... ---&gt; a\n....a ---&gt; n\n...an ---&gt; j\n..anj ---&gt; u\n.anju ---&gt; .\n..... ---&gt; h\n....h ---&gt; u\n...hu ---&gt; s\n..hus ---&gt; n\n.husn ---&gt; o\nhusno ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; k\n..mok ---&gt; a\n.moka ---&gt; .\n..... ---&gt; m\n....m ---&gt; a\n...ma ---&gt; h\n..mah ---&gt; i\n.mahi ---&gt; p\nmahip ---&gt; a\nahipa ---&gt; l\nhipal ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; t\n..jat ---&gt; i\n.jati ---&gt; n\njatin ---&gt; d\natind ---&gt; e\ntinde ---&gt; r\ninder ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; n\n..sun ---&gt; n\n.sunn ---&gt; y\nsunny ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; a\n.deva ---&gt; n\ndevan ---&gt; t\nevant ---&gt; i\nvanti ---&gt; .\n..... ---&gt; j\n....j ---&gt; o\n...jo ---&gt; y\n..joy ---&gt; a\n.joya ---&gt; a\njoyaa ---&gt; .\n..... ---&gt; g\n....g ---&gt; h\n...gh ---&gt; a\n..gha ---&gt; s\n.ghas ---&gt; i\nghasi ---&gt; r\nhasir ---&gt; a\nasira ---&gt; m\nsiram ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; v\n..dev ---&gt; y\n.devy ---&gt; a\ndevya ---&gt; n\nevyan ---&gt; i\nvyani ---&gt; .\n..... ---&gt; m\n....m ---&gt; o\n...mo ---&gt; m\n..mom ---&gt; e\n.mome ---&gt; n\nmomen ---&gt; a\nomena ---&gt; .\n..... ---&gt; p\n....p ---&gt; h\n...ph ---&gt; u\n..phu ---&gt; l\n.phul ---&gt; .\n..... ---&gt; f\n....f ---&gt; u\n...fu ---&gt; l\n..ful ---&gt; m\n.fulm ---&gt; i\nfulmi ---&gt; y\nulmiy ---&gt; a\nlmiya ---&gt; .\n..... ---&gt; n\n....n ---&gt; i\n...ni ---&gt; r\n..nir ---&gt; m\n.nirm ---&gt; l\nnirml ---&gt; a\nirmla ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; n\n..san ---&gt; j\n.sanj ---&gt; e\nsanje ---&gt; e\nanjee ---&gt; v\nnjeev ---&gt; a\njeeva ---&gt; n\neevan ---&gt; .\n..... ---&gt; s\n....s ---&gt; a\n...sa ---&gt; h\n..sah ---&gt; n\n.sahn ---&gt; a\nsahna ---&gt; z\nahnaz ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; h\n..chh ---&gt; a\n.chha ---&gt; m\nchham ---&gt; o\nhhamo ---&gt; .\n..... ---&gt; a\n....a ---&gt; a\n...aa ---&gt; r\n..aar ---&gt; d\n.aard ---&gt; h\naardh ---&gt; n\nardhn ---&gt; a\nrdhna ---&gt; .\n..... ---&gt; j\n....j ---&gt; a\n...ja ---&gt; g\n..jag ---&gt; j\n.jagj ---&gt; e\njagje ---&gt; e\nagjee ---&gt; t\ngjeet ---&gt; .\n..... ---&gt; c\n....c ---&gt; h\n...ch ---&gt; a\n..cha ---&gt; k\n.chak ---&gt; r\nchakr ---&gt; a\nhakra ---&gt; .\n..... ---&gt; r\n....r ---&gt; a\n...ra ---&gt; v\n..rav ---&gt; i\n.ravi ---&gt; n\nravin ---&gt; d\navind ---&gt; r\nvindr ---&gt; .\n..... ---&gt; s\n....s ---&gt; h\n...sh ---&gt; a\n..sha ---&gt; n\n.shan ---&gt; i\nshani ---&gt; .\n..... ---&gt; s\n....s ---&gt; u\n...su ---&gt; k\n..suk ---&gt; h\n.sukh ---&gt; m\nsukhm ---&gt; i\nukhmi ---&gt; t\nkhmit ---&gt; .\n..... ---&gt; d\n....d ---&gt; e\n...de ---&gt; e\n..dee ---&gt; p\n.deep ---&gt; u\ndeepu ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; s\n.pars ---&gt; i\nparsi ---&gt; .\n..... ---&gt; p\n....p ---&gt; a\n...pa ---&gt; r\n..par ---&gt; m\n.parm ---&gt; o\nparmo ---&gt; .\n\n\n\nX.shape, X.dtype, Y.shape, Y.dtype\n\n(torch.Size([44325, 5]), torch.int64, torch.Size([44325]), torch.int64)\n\n\n\n# Embedding layer for the context\n\nemb_dim = 4\nemb = torch.nn.Embedding(len(stoi), emb_dim)\n\n\nemb.weight\n\nParameter containing:\ntensor([[ 0.7694, -0.0425,  0.4451, -0.5473],\n        [-1.4513,  2.3222,  0.5003, -1.9549],\n        [-1.2120,  2.3934,  0.3531, -1.4271],\n        [-1.1517, -0.7280, -0.9900, -1.0232],\n        [ 0.5035, -0.0431, -1.4582,  1.7057],\n        [ 1.1632,  0.8851, -0.0941,  0.6955],\n        [ 0.7277,  0.8689, -2.3394, -0.1301],\n        [ 0.6960,  0.9702,  0.6731,  1.3605],\n        [ 0.4484,  0.5055,  0.2673,  0.8483],\n        [-1.9781,  0.9819, -1.0400,  0.1364],\n        [-0.3586,  0.1020, -0.2296,  0.0335],\n        [-0.7084,  0.4625, -0.8382,  0.4765],\n        [-0.1664,  0.9483, -0.8406,  1.2484],\n        [ 2.3739,  1.0939,  0.3967, -1.0831],\n        [ 0.1758,  0.6867, -0.2124, -1.3171],\n        [-1.7160,  0.1922, -0.9100,  0.2590],\n        [-0.6906,  0.6579, -1.8157,  0.7363],\n        [ 0.6869,  1.0605,  0.3869,  2.5261],\n        [-1.7279, -0.5944,  0.6080, -2.3156],\n        [-0.0682, -0.8245, -0.0153,  1.1720],\n        [-0.0647, -1.4478,  0.7775, -0.6562],\n        [ 0.5954,  0.0164, -0.2450, -0.1502],\n        [ 0.0467,  0.4786, -0.7976, -0.5191],\n        [ 0.4017, -1.2129,  1.4015, -0.9391],\n        [-0.9076,  1.1577,  0.6700,  1.2061],\n        [-1.3892, -0.1162, -0.1681,  0.3348],\n        [-0.7747, -1.5393,  0.9535,  0.5241]], requires_grad=True)\n\n\n\nemb.weight.shape\n\ntorch.Size([27, 4])\n\n\n\n# Function to visualize the embedding in 2d space\n\ndef plot_emb(emb, itos, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    for i in range(len(itos)):\n        x, y = emb.weight[i].detach().cpu().numpy()\n        ax.scatter(x, y, color='k')\n        ax.text(x + 0.05, y + 0.05, itos[i])\n    return ax\n\nplot_emb(emb, itos)\n\n\n\n\n\n\n\n\n\nclass NextChar(nn.Module):\n  def __init__(self, block_size, vocab_size, emb_dim, hidden_size):\n    super().__init__()\n    self.emb = nn.Embedding(vocab_size, emb_dim)\n    self.lin1 = nn.Linear(block_size * emb_dim, hidden_size)\n    self.lin2 = nn.Linear(hidden_size, vocab_size)\n\n  def forward(self, x):\n    x = self.emb(x)\n    x = x.view(x.shape[0], -1)\n    x = torch.sin(self.lin1(x))\n    x = self.lin2(x)\n    return x\n    \n\n\n# Generate names from untrained model\n\n\nmodel = NextChar(block_size, len(stoi), emb_dim, 10).to(device)\nmodel = torch.compile(model)\n\ng = torch.Generator()\ng.manual_seed(4000002)\ndef generate_name(model, itos, stoi, block_size, max_len=10):\n    context = [0] * block_size\n    name = ''\n    for i in range(max_len):\n        x = torch.tensor(context).view(1, -1).to(device)\n        y_pred = model(x)\n        ix = torch.distributions.categorical.Categorical(logits=y_pred).sample().item()\n        ch = itos[ix]\n        if ch == '.':\n            break\n        name += ch\n        context = context[1:] + [ix]\n    return name\n\nfor i in range(10):\n    print(generate_name(model, itos, stoi, block_size))\n\necxoxdwsrd\nsfeeukszll\nnuprryrzgx\ngkiktjbsj\ngovlmjrzgg\nllqslrttsz\ngw\nuchbnxysaj\nrwgswujgse\nspbohliyft\n\n\n\nfor param_name, param in model.named_parameters():\n    print(param_name, param.shape)\n\n_orig_mod.emb.weight torch.Size([27, 4])\n_orig_mod.lin1.weight torch.Size([10, 20])\n_orig_mod.lin1.bias torch.Size([10])\n_orig_mod.lin2.weight torch.Size([27, 10])\n_orig_mod.lin2.bias torch.Size([27])\n\n\n\n# Train the model\n\nloss_fn = nn.CrossEntropyLoss()\nopt = torch.optim.AdamW(model.parameters(), lr=0.01)\nimport time\n# Mini-batch training\nbatch_size = 4096\nprint_every = 100\nelapsed_time = []\nfor epoch in range(10000):\n    start_time = time.time()\n    for i in range(0, X.shape[0], batch_size):\n        x = X[i:i+batch_size]\n        y = Y[i:i+batch_size]\n        y_pred = model(x)\n        loss = loss_fn(y_pred, y)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n    end_time = time.time()\n    elapsed_time.append(end_time - start_time)\n    if epoch % print_every == 0:\n        print(epoch, loss.item())\n\n0 2.9544246196746826\n100 2.1499083042144775\n200 2.136932849884033\n300 2.132302761077881\n400 2.1297848224639893\n500 2.127420425415039\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[93], line 17\n     15 y_pred = model(x)\n     16 loss = loss_fn(y_pred, y)\n---&gt; 17 loss.backward()\n     18 opt.step()\n     19 opt.zero_grad()\n\nFile ~/mambaforge/lib/python3.10/site-packages/torch/_tensor.py:487, in Tensor.backward(self, gradient, retain_graph, create_graph, inputs)\n    477 if has_torch_function_unary(self):\n    478     return handle_torch_function(\n    479         Tensor.backward,\n    480         (self,),\n   (...)\n    485         inputs=inputs,\n    486     )\n--&gt; 487 torch.autograd.backward(\n    488     self, gradient, retain_graph, create_graph, inputs=inputs\n    489 )\n\nFile ~/mambaforge/lib/python3.10/site-packages/torch/autograd/__init__.py:200, in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\n    195     retain_graph = create_graph\n    197 # The reason we repeat same the comment below is that\n    198 # some Python versions print out the first line of a multi-line function\n    199 # calls in the traceback and some print out the last line\n--&gt; 200 Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    201     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n    202     allow_unreachable=True, accumulate_grad=True)\n\nFile ~/mambaforge/lib/python3.10/site-packages/torch/autograd/function.py:274, in BackwardCFunction.apply(self, *args)\n    270     raise RuntimeError(\"Implementing both 'backward' and 'vjp' for a custom \"\n    271                        \"Function is not allowed. You should only implement one \"\n    272                        \"of them.\")\n    273 user_fn = vjp_fn if vjp_fn is not Function.vjp else backward_fn\n--&gt; 274 return user_fn(self, *args)\n\nFile ~/mambaforge/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2365, in aot_dispatch_autograd.&lt;locals&gt;.CompiledFunction.backward(ctx, *flat_args)\n   2363     out = CompiledFunctionBackward.apply(*all_args)\n   2364 else:\n-&gt; 2365     out = call_compiled_backward()\n   2366 return out\n\nFile ~/mambaforge/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2341, in aot_dispatch_autograd.&lt;locals&gt;.CompiledFunction.backward.&lt;locals&gt;.call_compiled_backward()\n   2336         CompiledFunction.compiled_bw = aot_config.bw_compiler(\n   2337             bw_module, all_args\n   2338         )\n   2340 ctx.maybe_clear_saved_tensors()\n-&gt; 2341 out = call_func_with_args(\n   2342     CompiledFunction.compiled_bw,\n   2343     all_args,\n   2344     steal_args=True,\n   2345     disable_amp=disable_amp,\n   2346 )\n   2348 return tuple(out)\n\nFile ~/mambaforge/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1249, in call_func_with_args(f, args, steal_args, disable_amp)\n   1247 try:\n   1248     if hasattr(f, \"_boxed_call\"):\n-&gt; 1249         out = normalize_as_list(f(args))\n   1250     else:\n   1251         # TODO: Please remove soon\n   1252         # https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\n   1253         warnings.warn(\n   1254             \"Your compiler for AOTAutograd is returning a a function that doesn't take boxed arguments. \"\n   1255             \"Please wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \"\n   1256             \"See https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\"\n   1257         )\n\nFile ~/mambaforge/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209, in _TorchDynamoContext.__call__.&lt;locals&gt;._fn(*args, **kwargs)\n    207 dynamic_ctx.__enter__()\n    208 try:\n--&gt; 209     return fn(*args, **kwargs)\n    210 finally:\n    211     set_eval_frame(prior)\n\nFile /var/folders/z8/gpvqr8mn3w9_f38byxhnsk780000gn/T/torchinductor_nipun/h3/ch3fylbuu2ex65gy5b7bcllk7mkpc2lapogbiq7w5cagi5jq2m2u.py:127, in call(args)\n    125 buf2 = empty_strided((1, 27), (27, 1), device='cpu', dtype=torch.float32)\n    126 buf3 = buf0; del buf0  # reuse\n--&gt; 127 kernel_cpp_0(c_void_p(buf3.data_ptr()), c_void_p(tangents_1.data_ptr()), c_void_p(addmm.data_ptr()), c_void_p(buf2.data_ptr()))\n    128 del addmm\n    129 del tangents_1\n\nKeyboardInterrupt: \n\n\n\n\n# Visualize the embedding\n\nplot_emb(model.emb, itos)\n\n\n\n\n\n\n\n\n\n# Generate names from trained model\n\nfor i in range(10):\n    print(generate_name(model, itos, stoi, block_size))\n\nkoyani\nmantit\nsukaaki\nchawen\nshahi\nchajil\nmahibh\nrazag\nanpara\nsabka\n\n\nTuning knobs\n\nEmbedding size\nMLP\nContext length"
  },
  {
    "objectID": "notebooks/svm-soft-margin.html",
    "href": "notebooks/svm-soft-margin.html",
    "title": "SVM Soft Margin",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n%matplotlib inline\n# retina\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom sklearn.datasets import make_blobs\n\n\nX, y = make_blobs(centers=2, n_samples=100, random_state=0, cluster_std=1.5)\n\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm.autumn)\nplt.gca().set_aspect('equal')\n\n\n\n\n\n\n\n\n\nfrom sklearn import svm\nX_train = X\ny_train = y\nkernel = 'linear'\nfor fig_num, C in enumerate([0.0001, 0.001, 0.1, 1, 100, 10000][:]):\n    clf = svm.SVC(kernel=kernel,C = C )\n    clf.fit(X, y)\n\n    plt.figure(fig_num)\n    plt.clf()\n    plt.scatter(X[:, 0], X[:, 1], c=y, zorder=10, cmap=plt.cm.autumn,\n                edgecolor='k',s=80)\n\n\n    plt.axis('tight')\n    x_min = X[:, 0].min()-1\n    x_max = X[:, 0].max()+1\n    y_min = X[:, 1].min()-1\n    y_max = X[:, 1].max()+1\n\n    XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n   \n    # Put the result into a color plot\n    Z = Z.reshape(XX.shape)\n    #plt.pcolormesh(XX, YY, Z &gt; 0, cmap=plt.cm.autumn, alpha=0.2)\n\n    plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],\n                linestyles=['--', '-', '--'], levels=[-1, 0, 1])\n    plt.scatter(clf.support_vectors_[:, 0],\n                clf.support_vectors_[:, 1],\n                s=5, lw=0.001, facecolors='none',zorder=10,c='k', edgecolors='none');\n\n\n    plt.title(\"Linear Kernel with C= {}\\n Number of support vectors = {}\".format(C, len(clf.support_vectors_)))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### from scratch implementation in Torch\n\n\nimport torch.nn as nn\nimport torch\nimport torch.optim as optim\n\nclass OurSVM(nn.Module):\n    def __init__(self, C = 1.0):\n        super(OurSVM, self).__init__()\n        self.C = C\n                \n    def forward(self, x):\n        return torch.matmul(x, self.w) + self.b\n\n    def hinge_loss(self, y_pred, y_true):\n        return torch.mean(torch.max(torch.zeros_like(y_pred), 1 - y_true * y_pred)) + (torch.sum(self.w**2)) / (2 * self.C)\n\n    def fit(self, X, y, epochs=100, lr=3e-4):\n        self.w = nn.Parameter(torch.randn(X.shape[1], 1, requires_grad=True))\n        self.b = nn.Parameter(torch.randn(1, requires_grad=True))\n        optimizer = optim.Adam(self.parameters(), lr=lr)\n        for epoch in range(epochs):\n            optimizer.zero_grad()\n            y_pred = self.forward(X)\n            loss = self.hinge_loss(y_pred, y)\n            print(f'Epoch {epoch}, Loss: {loss.item()}')\n            loss.backward()\n            optimizer.step()\n\n\nour_svm = OurSVM(C=1.0)\nX_train = torch.tensor(X, dtype=torch.float32)\ny_train = torch.tensor(y.reshape(-1, 1), dtype=torch.float32)\ny_train[y_train == 0] = -1\nour_svm.fit(X_train, y_train, epochs=4500, lr=3e-3)\n\nEpoch 0, Loss: 0.5445287227630615\nEpoch 1, Loss: 0.5369461178779602\nEpoch 2, Loss: 0.5294452905654907\nEpoch 3, Loss: 0.522026538848877\nEpoch 4, Loss: 0.5146908164024353\nEpoch 5, Loss: 0.5074390769004822\nEpoch 6, Loss: 0.500271201133728\nEpoch 7, Loss: 0.4932182729244232\nEpoch 8, Loss: 0.48647040128707886\nEpoch 9, Loss: 0.4799497723579407\nEpoch 10, Loss: 0.4739691913127899\nEpoch 11, Loss: 0.4684266448020935\nEpoch 12, Loss: 0.4629947245121002\nEpoch 13, Loss: 0.45766204595565796\nEpoch 14, Loss: 0.45248934626579285\nEpoch 15, Loss: 0.44752809405326843\nEpoch 16, Loss: 0.44265472888946533\nEpoch 17, Loss: 0.4378686845302582\nEpoch 18, Loss: 0.4333285689353943\nEpoch 19, Loss: 0.4288690388202667\nEpoch 20, Loss: 0.4244857132434845\nEpoch 21, Loss: 0.42017486691474915\nEpoch 22, Loss: 0.4159332513809204\nEpoch 23, Loss: 0.41175782680511475\nEpoch 24, Loss: 0.40772339701652527\nEpoch 25, Loss: 0.40381550788879395\nEpoch 26, Loss: 0.4001219868659973\nEpoch 27, Loss: 0.3968224823474884\nEpoch 28, Loss: 0.39376193284988403\nEpoch 29, Loss: 0.390778124332428\nEpoch 30, Loss: 0.3878651261329651\nEpoch 31, Loss: 0.3850177824497223\nEpoch 32, Loss: 0.38223153352737427\nEpoch 33, Loss: 0.3795021176338196\nEpoch 34, Loss: 0.3768257200717926\nEpoch 35, Loss: 0.3741990625858307\nEpoch 36, Loss: 0.3716188669204712\nEpoch 37, Loss: 0.36908257007598877\nEpoch 38, Loss: 0.3665877878665924\nEpoch 39, Loss: 0.36413198709487915\nEpoch 40, Loss: 0.3617134690284729\nEpoch 41, Loss: 0.35934144258499146\nEpoch 42, Loss: 0.3570731580257416\nEpoch 43, Loss: 0.354861855506897\nEpoch 44, Loss: 0.35279640555381775\nEpoch 45, Loss: 0.350769579410553\nEpoch 46, Loss: 0.3487788140773773\nEpoch 47, Loss: 0.3469214141368866\nEpoch 48, Loss: 0.34530478715896606\nEpoch 49, Loss: 0.34375762939453125\nEpoch 50, Loss: 0.3422442078590393\nEpoch 51, Loss: 0.3407752215862274\nEpoch 52, Loss: 0.339397132396698\nEpoch 53, Loss: 0.33810731768608093\nEpoch 54, Loss: 0.33684104681015015\nEpoch 55, Loss: 0.3355962038040161\nEpoch 56, Loss: 0.3343706429004669\nEpoch 57, Loss: 0.333162784576416\nEpoch 58, Loss: 0.331971138715744\nEpoch 59, Loss: 0.3307945132255554\nEpoch 60, Loss: 0.32963183522224426\nEpoch 61, Loss: 0.3284817636013031\nEpoch 62, Loss: 0.327343612909317\nEpoch 63, Loss: 0.32621654868125916\nEpoch 64, Loss: 0.325099915266037\nEpoch 65, Loss: 0.3239930272102356\nEpoch 66, Loss: 0.3228954076766968\nEpoch 67, Loss: 0.321806401014328\nEpoch 68, Loss: 0.3207257091999054\nEpoch 69, Loss: 0.31970730423927307\nEpoch 70, Loss: 0.3187556564807892\nEpoch 71, Loss: 0.3178614675998688\nEpoch 72, Loss: 0.3170294463634491\nEpoch 73, Loss: 0.3162238895893097\nEpoch 74, Loss: 0.3154330849647522\nEpoch 75, Loss: 0.31466856598854065\nEpoch 76, Loss: 0.3139547109603882\nEpoch 77, Loss: 0.31329113245010376\nEpoch 78, Loss: 0.3126664459705353\nEpoch 79, Loss: 0.31207412481307983\nEpoch 80, Loss: 0.31149277091026306\nEpoch 81, Loss: 0.3109213709831238\nEpoch 82, Loss: 0.3103591501712799\nEpoch 83, Loss: 0.30980539321899414\nEpoch 84, Loss: 0.3092697560787201\nEpoch 85, Loss: 0.3087584376335144\nEpoch 86, Loss: 0.30825164914131165\nEpoch 87, Loss: 0.30774903297424316\nEpoch 88, Loss: 0.30725041031837463\nEpoch 89, Loss: 0.30675557255744934\nEpoch 90, Loss: 0.30626413226127625\nEpoch 91, Loss: 0.3057836592197418\nEpoch 92, Loss: 0.30533647537231445\nEpoch 93, Loss: 0.30489417910575867\nEpoch 94, Loss: 0.3044564127922058\nEpoch 95, Loss: 0.30402252078056335\nEpoch 96, Loss: 0.30359259247779846\nEpoch 97, Loss: 0.303166002035141\nEpoch 98, Loss: 0.3027426600456238\nEpoch 99, Loss: 0.30232229828834534\nEpoch 100, Loss: 0.30190470814704895\nEpoch 101, Loss: 0.3014896512031555\nEpoch 102, Loss: 0.30107709765434265\nEpoch 103, Loss: 0.30066677927970886\nEpoch 104, Loss: 0.3002585172653198\nEpoch 105, Loss: 0.2998523414134979\nEpoch 106, Loss: 0.29944801330566406\nEpoch 107, Loss: 0.29904553294181824\nEpoch 108, Loss: 0.2986447513103485\nEpoch 109, Loss: 0.2982456684112549\nEpoch 110, Loss: 0.297848105430603\nEpoch 111, Loss: 0.29745209217071533\nEpoch 112, Loss: 0.29705744981765747\nEpoch 113, Loss: 0.2966642379760742\nEpoch 114, Loss: 0.2962724268436432\nEpoch 115, Loss: 0.29588189721107483\nEpoch 116, Loss: 0.29549261927604675\nEpoch 117, Loss: 0.29510459303855896\nEpoch 118, Loss: 0.29471778869628906\nEpoch 119, Loss: 0.29433220624923706\nEpoch 120, Loss: 0.2939477264881134\nEpoch 121, Loss: 0.2935643792152405\nEpoch 122, Loss: 0.2932094931602478\nEpoch 123, Loss: 0.2928715646266937\nEpoch 124, Loss: 0.292537659406662\nEpoch 125, Loss: 0.29220718145370483\nEpoch 126, Loss: 0.2918799817562103\nEpoch 127, Loss: 0.2915557324886322\nEpoch 128, Loss: 0.2912343144416809\nEpoch 129, Loss: 0.2909153699874878\nEpoch 130, Loss: 0.29059872031211853\nEpoch 131, Loss: 0.29028424620628357\nEpoch 132, Loss: 0.28997179865837097\nEpoch 133, Loss: 0.2896611988544464\nEpoch 134, Loss: 0.2893523573875427\nEpoch 135, Loss: 0.28904515504837036\nEpoch 136, Loss: 0.2887394428253174\nEpoch 137, Loss: 0.2884352505207062\nEpoch 138, Loss: 0.28813236951828003\nEpoch 139, Loss: 0.2878306806087494\nEpoch 140, Loss: 0.28753024339675903\nEpoch 141, Loss: 0.2872309684753418\nEpoch 142, Loss: 0.28693267703056335\nEpoch 143, Loss: 0.2866355776786804\nEpoch 144, Loss: 0.28633934259414673\nEpoch 145, Loss: 0.2860441207885742\nEpoch 146, Loss: 0.28574976325035095\nEpoch 147, Loss: 0.28545626997947693\nEpoch 148, Loss: 0.285163551568985\nEpoch 149, Loss: 0.28487175703048706\nEpoch 150, Loss: 0.28459009528160095\nEpoch 151, Loss: 0.28431206941604614\nEpoch 152, Loss: 0.28404179215431213\nEpoch 153, Loss: 0.28378623723983765\nEpoch 154, Loss: 0.2835325300693512\nEpoch 155, Loss: 0.28328052163124084\nEpoch 156, Loss: 0.2830301523208618\nEpoch 157, Loss: 0.28278109431266785\nEpoch 158, Loss: 0.28253334760665894\nEpoch 159, Loss: 0.2822974920272827\nEpoch 160, Loss: 0.28209662437438965\nEpoch 161, Loss: 0.2818947434425354\nEpoch 162, Loss: 0.28169211745262146\nEpoch 163, Loss: 0.28148898482322693\nEpoch 164, Loss: 0.28128549456596375\nEpoch 165, Loss: 0.28108176589012146\nEpoch 166, Loss: 0.28087809681892395\nEpoch 167, Loss: 0.2806745171546936\nEpoch 168, Loss: 0.2804712653160095\nEpoch 169, Loss: 0.28026852011680603\nEpoch 170, Loss: 0.2800663411617279\nEpoch 171, Loss: 0.2798648774623871\nEpoch 172, Loss: 0.2796642482280731\nEpoch 173, Loss: 0.27946457266807556\nEpoch 174, Loss: 0.27926722168922424\nEpoch 175, Loss: 0.2790791988372803\nEpoch 176, Loss: 0.2788916528224945\nEpoch 177, Loss: 0.278704434633255\nEpoch 178, Loss: 0.278517484664917\nEpoch 179, Loss: 0.27833402156829834\nEpoch 180, Loss: 0.2781514823436737\nEpoch 181, Loss: 0.2779638171195984\nEpoch 182, Loss: 0.2777714133262634\nEpoch 183, Loss: 0.27758973836898804\nEpoch 184, Loss: 0.2774166762828827\nEpoch 185, Loss: 0.27725502848625183\nEpoch 186, Loss: 0.27709096670150757\nEpoch 187, Loss: 0.2769249379634857\nEpoch 188, Loss: 0.2767632007598877\nEpoch 189, Loss: 0.27660679817199707\nEpoch 190, Loss: 0.2764519155025482\nEpoch 191, Loss: 0.2762983739376068\nEpoch 192, Loss: 0.2761460542678833\nEpoch 193, Loss: 0.27599477767944336\nEpoch 194, Loss: 0.2758445739746094\nEpoch 195, Loss: 0.27569520473480225\nEpoch 196, Loss: 0.27554675936698914\nEpoch 197, Loss: 0.2753990888595581\nEpoch 198, Loss: 0.275252103805542\nEpoch 199, Loss: 0.2751057744026184\nEpoch 200, Loss: 0.2749599814414978\nEpoch 201, Loss: 0.2748226523399353\nEpoch 202, Loss: 0.2746867537498474\nEpoch 203, Loss: 0.2745511829853058\nEpoch 204, Loss: 0.27441567182540894\nEpoch 205, Loss: 0.27428048849105835\nEpoch 206, Loss: 0.2741456925868988\nEpoch 207, Loss: 0.27401140332221985\nEpoch 208, Loss: 0.2738775610923767\nEpoch 209, Loss: 0.27374520897865295\nEpoch 210, Loss: 0.27362340688705444\nEpoch 211, Loss: 0.273501992225647\nEpoch 212, Loss: 0.27338114380836487\nEpoch 213, Loss: 0.2732609212398529\nEpoch 214, Loss: 0.2731415331363678\nEpoch 215, Loss: 0.27302291989326477\nEpoch 216, Loss: 0.27291393280029297\nEpoch 217, Loss: 0.2728149890899658\nEpoch 218, Loss: 0.27271199226379395\nEpoch 219, Loss: 0.2726204991340637\nEpoch 220, Loss: 0.2725154459476471\nEpoch 221, Loss: 0.27240556478500366\nEpoch 222, Loss: 0.2722984850406647\nEpoch 223, Loss: 0.272208034992218\nEpoch 224, Loss: 0.2721158266067505\nEpoch 225, Loss: 0.2720220386981964\nEpoch 226, Loss: 0.2719269394874573\nEpoch 227, Loss: 0.2718387246131897\nEpoch 228, Loss: 0.2717510461807251\nEpoch 229, Loss: 0.27166229486465454\nEpoch 230, Loss: 0.2715725898742676\nEpoch 231, Loss: 0.2714819014072418\nEpoch 232, Loss: 0.2713940739631653\nEpoch 233, Loss: 0.27131757140159607\nEpoch 234, Loss: 0.2712397873401642\nEpoch 235, Loss: 0.27116310596466064\nEpoch 236, Loss: 0.2710861265659332\nEpoch 237, Loss: 0.27100899815559387\nEpoch 238, Loss: 0.27093154191970825\nEpoch 239, Loss: 0.27085381746292114\nEpoch 240, Loss: 0.2707763910293579\nEpoch 241, Loss: 0.27069947123527527\nEpoch 242, Loss: 0.2706226408481598\nEpoch 243, Loss: 0.2705455422401428\nEpoch 244, Loss: 0.2704699635505676\nEpoch 245, Loss: 0.27039241790771484\nEpoch 246, Loss: 0.2703161835670471\nEpoch 247, Loss: 0.27023959159851074\nEpoch 248, Loss: 0.2701627314090729\nEpoch 249, Loss: 0.2700885832309723\nEpoch 250, Loss: 0.2700103223323822\nEpoch 251, Loss: 0.2699359953403473\nEpoch 252, Loss: 0.26986148953437805\nEpoch 253, Loss: 0.26978644728660583\nEpoch 254, Loss: 0.2697109878063202\nEpoch 255, Loss: 0.2696352005004883\nEpoch 256, Loss: 0.2695590853691101\nEpoch 257, Loss: 0.2694825828075409\nEpoch 258, Loss: 0.2694151997566223\nEpoch 259, Loss: 0.2693588137626648\nEpoch 260, Loss: 0.2692992389202118\nEpoch 261, Loss: 0.26924222707748413\nEpoch 262, Loss: 0.26918697357177734\nEpoch 263, Loss: 0.26913267374038696\nEpoch 264, Loss: 0.2690791189670563\nEpoch 265, Loss: 0.2690261900424957\nEpoch 266, Loss: 0.26897382736206055\nEpoch 267, Loss: 0.2689231336116791\nEpoch 268, Loss: 0.26887282729148865\nEpoch 269, Loss: 0.26882070302963257\nEpoch 270, Loss: 0.26876965165138245\nEpoch 271, Loss: 0.2687196135520935\nEpoch 272, Loss: 0.26866987347602844\nEpoch 273, Loss: 0.26862025260925293\nEpoch 274, Loss: 0.26857614517211914\nEpoch 275, Loss: 0.2685270309448242\nEpoch 276, Loss: 0.26847538352012634\nEpoch 277, Loss: 0.26842811703681946\nEpoch 278, Loss: 0.2683810591697693\nEpoch 279, Loss: 0.26833394169807434\nEpoch 280, Loss: 0.2682870328426361\nEpoch 281, Loss: 0.2682413160800934\nEpoch 282, Loss: 0.26819348335266113\nEpoch 283, Loss: 0.26814594864845276\nEpoch 284, Loss: 0.26809898018836975\nEpoch 285, Loss: 0.2680518925189972\nEpoch 286, Loss: 0.26801103353500366\nEpoch 287, Loss: 0.26796412467956543\nEpoch 288, Loss: 0.26791414618492126\nEpoch 289, Loss: 0.26786908507347107\nEpoch 290, Loss: 0.2678239941596985\nEpoch 291, Loss: 0.26777878403663635\nEpoch 292, Loss: 0.26773345470428467\nEpoch 293, Loss: 0.26768797636032104\nEpoch 294, Loss: 0.2676423490047455\nEpoch 295, Loss: 0.2675968408584595\nEpoch 296, Loss: 0.2675510048866272\nEpoch 297, Loss: 0.2675052583217621\nEpoch 298, Loss: 0.26745933294296265\nEpoch 299, Loss: 0.2674180269241333\nEpoch 300, Loss: 0.2673722803592682\nEpoch 301, Loss: 0.26732730865478516\nEpoch 302, Loss: 0.2672819197177887\nEpoch 303, Loss: 0.2672363221645355\nEpoch 304, Loss: 0.26719221472740173\nEpoch 305, Loss: 0.267147958278656\nEpoch 306, Loss: 0.2671040892601013\nEpoch 307, Loss: 0.26705998182296753\nEpoch 308, Loss: 0.2670157551765442\nEpoch 309, Loss: 0.266971230506897\nEpoch 310, Loss: 0.26692861318588257\nEpoch 311, Loss: 0.26688313484191895\nEpoch 312, Loss: 0.26683953404426575\nEpoch 313, Loss: 0.26679563522338867\nEpoch 314, Loss: 0.2667514383792877\nEpoch 315, Loss: 0.26670995354652405\nEpoch 316, Loss: 0.26666420698165894\nEpoch 317, Loss: 0.266620934009552\nEpoch 318, Loss: 0.2665773332118988\nEpoch 319, Loss: 0.26653358340263367\nEpoch 320, Loss: 0.2664945125579834\nEpoch 321, Loss: 0.26644858717918396\nEpoch 322, Loss: 0.26640719175338745\nEpoch 323, Loss: 0.26636621356010437\nEpoch 324, Loss: 0.2663249373435974\nEpoch 325, Loss: 0.26628395915031433\nEpoch 326, Loss: 0.26624563336372375\nEpoch 327, Loss: 0.26620498299598694\nEpoch 328, Loss: 0.2661649286746979\nEpoch 329, Loss: 0.26612526178359985\nEpoch 330, Loss: 0.26608505845069885\nEpoch 331, Loss: 0.26604607701301575\nEpoch 332, Loss: 0.2660079002380371\nEpoch 333, Loss: 0.26596909761428833\nEpoch 334, Loss: 0.2659297585487366\nEpoch 335, Loss: 0.2658909559249878\nEpoch 336, Loss: 0.2658534049987793\nEpoch 337, Loss: 0.26581481099128723\nEpoch 338, Loss: 0.2657768130302429\nEpoch 339, Loss: 0.26573896408081055\nEpoch 340, Loss: 0.2657001316547394\nEpoch 341, Loss: 0.2656632661819458\nEpoch 342, Loss: 0.2656254172325134\nEpoch 343, Loss: 0.2655894160270691\nEpoch 344, Loss: 0.2655583322048187\nEpoch 345, Loss: 0.2655336260795593\nEpoch 346, Loss: 0.2655073404312134\nEpoch 347, Loss: 0.2654823958873749\nEpoch 348, Loss: 0.2654588222503662\nEpoch 349, Loss: 0.2654360830783844\nEpoch 350, Loss: 0.2654118239879608\nEpoch 351, Loss: 0.2653883695602417\nEpoch 352, Loss: 0.2653670608997345\nEpoch 353, Loss: 0.2653454542160034\nEpoch 354, Loss: 0.26532217860221863\nEpoch 355, Loss: 0.26530128717422485\nEpoch 356, Loss: 0.2652801275253296\nEpoch 357, Loss: 0.2652590572834015\nEpoch 358, Loss: 0.26524001359939575\nEpoch 359, Loss: 0.26521867513656616\nEpoch 360, Loss: 0.26519912481307983\nEpoch 361, Loss: 0.26517999172210693\nEpoch 362, Loss: 0.2651609480381012\nEpoch 363, Loss: 0.2651415467262268\nEpoch 364, Loss: 0.265121728181839\nEpoch 365, Loss: 0.2651044726371765\nEpoch 366, Loss: 0.2650856375694275\nEpoch 367, Loss: 0.26506489515304565\nEpoch 368, Loss: 0.2650465965270996\nEpoch 369, Loss: 0.26502782106399536\nEpoch 370, Loss: 0.26501044631004333\nEpoch 371, Loss: 0.26499271392822266\nEpoch 372, Loss: 0.2649737000465393\nEpoch 373, Loss: 0.2649560868740082\nEpoch 374, Loss: 0.2649381160736084\nEpoch 375, Loss: 0.2649214267730713\nEpoch 376, Loss: 0.26490381360054016\nEpoch 377, Loss: 0.26488620042800903\nEpoch 378, Loss: 0.2648692727088928\nEpoch 379, Loss: 0.26485154032707214\nEpoch 380, Loss: 0.2648339867591858\nEpoch 381, Loss: 0.2648165225982666\nEpoch 382, Loss: 0.2648000717163086\nEpoch 383, Loss: 0.26478326320648193\nEpoch 384, Loss: 0.2647656500339508\nEpoch 385, Loss: 0.26474785804748535\nEpoch 386, Loss: 0.2647307515144348\nEpoch 387, Loss: 0.26471444964408875\nEpoch 388, Loss: 0.26469770073890686\nEpoch 389, Loss: 0.2646801471710205\nEpoch 390, Loss: 0.2646642029285431\nEpoch 391, Loss: 0.26464858651161194\nEpoch 392, Loss: 0.26463213562965393\nEpoch 393, Loss: 0.2646145522594452\nEpoch 394, Loss: 0.2645981013774872\nEpoch 395, Loss: 0.2645813524723053\nEpoch 396, Loss: 0.26456713676452637\nEpoch 397, Loss: 0.26455143094062805\nEpoch 398, Loss: 0.2645343840122223\nEpoch 399, Loss: 0.26451924443244934\nEpoch 400, Loss: 0.26450422406196594\nEpoch 401, Loss: 0.2644881010055542\nEpoch 402, Loss: 0.2644708752632141\nEpoch 403, Loss: 0.26445630192756653\nEpoch 404, Loss: 0.264441579580307\nEpoch 405, Loss: 0.26442551612854004\nEpoch 406, Loss: 0.26440829038619995\nEpoch 407, Loss: 0.264393150806427\nEpoch 408, Loss: 0.2643786370754242\nEpoch 409, Loss: 0.26436328887939453\nEpoch 410, Loss: 0.2643454670906067\nEpoch 411, Loss: 0.26433131098747253\nEpoch 412, Loss: 0.2643158733844757\nEpoch 413, Loss: 0.2642998695373535\nEpoch 414, Loss: 0.2642846703529358\nEpoch 415, Loss: 0.2642705738544464\nEpoch 416, Loss: 0.2642557919025421\nEpoch 417, Loss: 0.2642408609390259\nEpoch 418, Loss: 0.2642250955104828\nEpoch 419, Loss: 0.26421138644218445\nEpoch 420, Loss: 0.2641964256763458\nEpoch 421, Loss: 0.264182448387146\nEpoch 422, Loss: 0.2641683518886566\nEpoch 423, Loss: 0.26415324211120605\nEpoch 424, Loss: 0.26413896679878235\nEpoch 425, Loss: 0.2641248106956482\nEpoch 426, Loss: 0.26410961151123047\nEpoch 427, Loss: 0.26409539580345154\nEpoch 428, Loss: 0.2640811800956726\nEpoch 429, Loss: 0.264066606760025\nEpoch 430, Loss: 0.2640525698661804\nEpoch 431, Loss: 0.26403874158859253\nEpoch 432, Loss: 0.2640245258808136\nEpoch 433, Loss: 0.26401183009147644\nEpoch 434, Loss: 0.26399701833724976\nEpoch 435, Loss: 0.26398366689682007\nEpoch 436, Loss: 0.26396939158439636\nEpoch 437, Loss: 0.26395660638809204\nEpoch 438, Loss: 0.26394203305244446\nEpoch 439, Loss: 0.26392874121665955\nEpoch 440, Loss: 0.26391464471817017\nEpoch 441, Loss: 0.26390308141708374\nEpoch 442, Loss: 0.2638896107673645\nEpoch 443, Loss: 0.26387476921081543\nEpoch 444, Loss: 0.26386186480522156\nEpoch 445, Loss: 0.26384833455085754\nEpoch 446, Loss: 0.2638355791568756\nEpoch 447, Loss: 0.26382169127464294\nEpoch 448, Loss: 0.26380980014801025\nEpoch 449, Loss: 0.26379743218421936\nEpoch 450, Loss: 0.26378440856933594\nEpoch 451, Loss: 0.26377198100090027\nEpoch 452, Loss: 0.26375868916511536\nEpoch 453, Loss: 0.2637450397014618\nEpoch 454, Loss: 0.26373180747032166\nEpoch 455, Loss: 0.2637205421924591\nEpoch 456, Loss: 0.26370787620544434\nEpoch 457, Loss: 0.2636945843696594\nEpoch 458, Loss: 0.2636827230453491\nEpoch 459, Loss: 0.2636702358722687\nEpoch 460, Loss: 0.26365748047828674\nEpoch 461, Loss: 0.2636449933052063\nEpoch 462, Loss: 0.263632208108902\nEpoch 463, Loss: 0.2636202275753021\nEpoch 464, Loss: 0.2636083960533142\nEpoch 465, Loss: 0.26359665393829346\nEpoch 466, Loss: 0.2635844051837921\nEpoch 467, Loss: 0.2635717988014221\nEpoch 468, Loss: 0.26356005668640137\nEpoch 469, Loss: 0.26354798674583435\nEpoch 470, Loss: 0.26353660225868225\nEpoch 471, Loss: 0.2635248601436615\nEpoch 472, Loss: 0.2635127007961273\nEpoch 473, Loss: 0.26350080966949463\nEpoch 474, Loss: 0.26348912715911865\nEpoch 475, Loss: 0.2634775638580322\nEpoch 476, Loss: 0.2634657621383667\nEpoch 477, Loss: 0.263454794883728\nEpoch 478, Loss: 0.26344361901283264\nEpoch 479, Loss: 0.2634322941303253\nEpoch 480, Loss: 0.26342108845710754\nEpoch 481, Loss: 0.263409823179245\nEpoch 482, Loss: 0.26339825987815857\nEpoch 483, Loss: 0.2633868157863617\nEpoch 484, Loss: 0.2633758783340454\nEpoch 485, Loss: 0.2633647918701172\nEpoch 486, Loss: 0.2633536756038666\nEpoch 487, Loss: 0.26334357261657715\nEpoch 488, Loss: 0.2633356750011444\nEpoch 489, Loss: 0.26332730054855347\nEpoch 490, Loss: 0.26331618428230286\nEpoch 491, Loss: 0.26330748200416565\nEpoch 492, Loss: 0.2633008062839508\nEpoch 493, Loss: 0.26329293847084045\nEpoch 494, Loss: 0.2632836401462555\nEpoch 495, Loss: 0.2632758617401123\nEpoch 496, Loss: 0.26326820254325867\nEpoch 497, Loss: 0.2632605731487274\nEpoch 498, Loss: 0.26325303316116333\nEpoch 499, Loss: 0.26324552297592163\nEpoch 500, Loss: 0.2632380723953247\nEpoch 501, Loss: 0.26323071122169495\nEpoch 502, Loss: 0.2632233798503876\nEpoch 503, Loss: 0.263216108083725\nEpoch 504, Loss: 0.26320889592170715\nEpoch 505, Loss: 0.26320168375968933\nEpoch 506, Loss: 0.2631945013999939\nEpoch 507, Loss: 0.26318734884262085\nEpoch 508, Loss: 0.2631801962852478\nEpoch 509, Loss: 0.2631731629371643\nEpoch 510, Loss: 0.2631661295890808\nEpoch 511, Loss: 0.2631600797176361\nEpoch 512, Loss: 0.2631523311138153\nEpoch 513, Loss: 0.26314547657966614\nEpoch 514, Loss: 0.26313862204551697\nEpoch 515, Loss: 0.26313188672065735\nEpoch 516, Loss: 0.26312509179115295\nEpoch 517, Loss: 0.2631183862686157\nEpoch 518, Loss: 0.26311159133911133\nEpoch 519, Loss: 0.2631049156188965\nEpoch 520, Loss: 0.26309826970100403\nEpoch 521, Loss: 0.2630915939807892\nEpoch 522, Loss: 0.26308485865592957\nEpoch 523, Loss: 0.2630782723426819\nEpoch 524, Loss: 0.26307159662246704\nEpoch 525, Loss: 0.26306501030921936\nEpoch 526, Loss: 0.2630583941936493\nEpoch 527, Loss: 0.2630518674850464\nEpoch 528, Loss: 0.2630453109741211\nEpoch 529, Loss: 0.263038694858551\nEpoch 530, Loss: 0.26303309202194214\nEpoch 531, Loss: 0.2630258798599243\nEpoch 532, Loss: 0.2630195617675781\nEpoch 533, Loss: 0.26301321387290955\nEpoch 534, Loss: 0.26300692558288574\nEpoch 535, Loss: 0.26300060749053955\nEpoch 536, Loss: 0.26299425959587097\nEpoch 537, Loss: 0.2629879117012024\nEpoch 538, Loss: 0.262981653213501\nEpoch 539, Loss: 0.2629753053188324\nEpoch 540, Loss: 0.2629690170288086\nEpoch 541, Loss: 0.2629627287387848\nEpoch 542, Loss: 0.2629563510417938\nEpoch 543, Loss: 0.26295122504234314\nEpoch 544, Loss: 0.2629440724849701\nEpoch 545, Loss: 0.2629380226135254\nEpoch 546, Loss: 0.2629319131374359\nEpoch 547, Loss: 0.26292580366134644\nEpoch 548, Loss: 0.26291975378990173\nEpoch 549, Loss: 0.26291364431381226\nEpoch 550, Loss: 0.2629075050354004\nEpoch 551, Loss: 0.26290133595466614\nEpoch 552, Loss: 0.26289522647857666\nEpoch 553, Loss: 0.2628893554210663\nEpoch 554, Loss: 0.2628832459449768\nEpoch 555, Loss: 0.26287737488746643\nEpoch 556, Loss: 0.26287150382995605\nEpoch 557, Loss: 0.26286566257476807\nEpoch 558, Loss: 0.26285967230796814\nEpoch 559, Loss: 0.262853741645813\nEpoch 560, Loss: 0.2628476917743683\nEpoch 561, Loss: 0.26284176111221313\nEpoch 562, Loss: 0.26283571124076843\nEpoch 563, Loss: 0.2628302574157715\nEpoch 564, Loss: 0.2628239691257477\nEpoch 565, Loss: 0.26281821727752686\nEpoch 566, Loss: 0.26281246542930603\nEpoch 567, Loss: 0.2628067433834076\nEpoch 568, Loss: 0.262800931930542\nEpoch 569, Loss: 0.2627950608730316\nEpoch 570, Loss: 0.26278913021087646\nEpoch 571, Loss: 0.2627841830253601\nEpoch 572, Loss: 0.262777715921402\nEpoch 573, Loss: 0.2627721428871155\nEpoch 574, Loss: 0.2627665400505066\nEpoch 575, Loss: 0.26276084780693054\nEpoch 576, Loss: 0.2627550959587097\nEpoch 577, Loss: 0.2627493441104889\nEpoch 578, Loss: 0.2627438008785248\nEpoch 579, Loss: 0.2627381682395935\nEpoch 580, Loss: 0.26273274421691895\nEpoch 581, Loss: 0.2627272307872772\nEpoch 582, Loss: 0.26272162795066833\nEpoch 583, Loss: 0.26271599531173706\nEpoch 584, Loss: 0.262710303068161\nEpoch 585, Loss: 0.2627045512199402\nEpoch 586, Loss: 0.26270028948783875\nEpoch 587, Loss: 0.26269370317459106\nEpoch 588, Loss: 0.26268866658210754\nEpoch 589, Loss: 0.2626836597919464\nEpoch 590, Loss: 0.2626785933971405\nEpoch 591, Loss: 0.26267340779304504\nEpoch 592, Loss: 0.2626681625843048\nEpoch 593, Loss: 0.26266276836395264\nEpoch 594, Loss: 0.2626573443412781\nEpoch 595, Loss: 0.2626517713069916\nEpoch 596, Loss: 0.2626461684703827\nEpoch 597, Loss: 0.2626405954360962\nEpoch 598, Loss: 0.26263484358787537\nEpoch 599, Loss: 0.26262912154197693\nEpoch 600, Loss: 0.2626242935657501\nEpoch 601, Loss: 0.26261889934539795\nEpoch 602, Loss: 0.26261329650878906\nEpoch 603, Loss: 0.2626084089279175\nEpoch 604, Loss: 0.26260340213775635\nEpoch 605, Loss: 0.26259827613830566\nEpoch 606, Loss: 0.26259300112724304\nEpoch 607, Loss: 0.26258769631385803\nEpoch 608, Loss: 0.2625822424888611\nEpoch 609, Loss: 0.26257675886154175\nEpoch 610, Loss: 0.26257193088531494\nEpoch 611, Loss: 0.2625662386417389\nEpoch 612, Loss: 0.2625616788864136\nEpoch 613, Loss: 0.2625569701194763\nEpoch 614, Loss: 0.2625522017478943\nEpoch 615, Loss: 0.26254725456237793\nEpoch 616, Loss: 0.262542188167572\nEpoch 617, Loss: 0.26253700256347656\nEpoch 618, Loss: 0.26253166794776917\nEpoch 619, Loss: 0.2625262439250946\nEpoch 620, Loss: 0.26252076029777527\nEpoch 621, Loss: 0.2625172734260559\nEpoch 622, Loss: 0.26251220703125\nEpoch 623, Loss: 0.26250576972961426\nEpoch 624, Loss: 0.2625012695789337\nEpoch 625, Loss: 0.2624965012073517\nEpoch 626, Loss: 0.2624915540218353\nEpoch 627, Loss: 0.2624865472316742\nEpoch 628, Loss: 0.26248136162757874\nEpoch 629, Loss: 0.2624766528606415\nEpoch 630, Loss: 0.26247134804725647\nEpoch 631, Loss: 0.26246652007102966\nEpoch 632, Loss: 0.26246193051338196\nEpoch 633, Loss: 0.26245710253715515\nEpoch 634, Loss: 0.2624526619911194\nEpoch 635, Loss: 0.26244789361953735\nEpoch 636, Loss: 0.262446790933609\nEpoch 637, Loss: 0.2624450623989105\nEpoch 638, Loss: 0.2624428868293762\nEpoch 639, Loss: 0.2624402642250061\nEpoch 640, Loss: 0.2624373733997345\nEpoch 641, Loss: 0.2624342143535614\nEpoch 642, Loss: 0.2624308466911316\nEpoch 643, Loss: 0.26242730021476746\nEpoch 644, Loss: 0.2624265253543854\nEpoch 645, Loss: 0.26242515444755554\nEpoch 646, Loss: 0.2624232769012451\nEpoch 647, Loss: 0.2624208927154541\nEpoch 648, Loss: 0.26241815090179443\nEpoch 649, Loss: 0.2624148428440094\nEpoch 650, Loss: 0.2624121606349945\nEpoch 651, Loss: 0.26241087913513184\nEpoch 652, Loss: 0.2624092996120453\nEpoch 653, Loss: 0.262407511472702\nEpoch 654, Loss: 0.26240551471710205\nEpoch 655, Loss: 0.26240330934524536\nEpoch 656, Loss: 0.26240110397338867\nEpoch 657, Loss: 0.26240190863609314\nEpoch 658, Loss: 0.26240256428718567\nEpoch 659, Loss: 0.26240018010139465\nEpoch 660, Loss: 0.2623966634273529\nEpoch 661, Loss: 0.2623964548110962\nEpoch 662, Loss: 0.262395977973938\nEpoch 663, Loss: 0.26239511370658875\nEpoch 664, Loss: 0.2623942196369171\nEpoch 665, Loss: 0.26239335536956787\nEpoch 666, Loss: 0.2623929977416992\nEpoch 667, Loss: 0.2623923122882843\nEpoch 668, Loss: 0.2623913884162903\nEpoch 669, Loss: 0.2623903453350067\nEpoch 670, Loss: 0.26238906383514404\nEpoch 671, Loss: 0.2623891234397888\nEpoch 672, Loss: 0.26238828897476196\nEpoch 673, Loss: 0.2623867690563202\nEpoch 674, Loss: 0.2623860836029053\nEpoch 675, Loss: 0.26238584518432617\nEpoch 676, Loss: 0.26238542795181274\nEpoch 677, Loss: 0.2623847424983978\nEpoch 678, Loss: 0.2623843252658844\nEpoch 679, Loss: 0.26238369941711426\nEpoch 680, Loss: 0.2623829245567322\nEpoch 681, Loss: 0.26238304376602173\nEpoch 682, Loss: 0.26238277554512024\nEpoch 683, Loss: 0.26238155364990234\nEpoch 684, Loss: 0.2623809576034546\nEpoch 685, Loss: 0.2623807191848755\nEpoch 686, Loss: 0.2623802125453949\nEpoch 687, Loss: 0.26237961649894714\nEpoch 688, Loss: 0.26238077878952026\nEpoch 689, Loss: 0.26237910985946655\nEpoch 690, Loss: 0.26237815618515015\nEpoch 691, Loss: 0.26237770915031433\nEpoch 692, Loss: 0.26237770915031433\nEpoch 693, Loss: 0.26237764954566956\nEpoch 694, Loss: 0.26237753033638\nEpoch 695, Loss: 0.26237723231315613\nEpoch 696, Loss: 0.2623768150806427\nEpoch 697, Loss: 0.26237618923187256\nEpoch 698, Loss: 0.26237618923187256\nEpoch 699, Loss: 0.26237553358078003\nEpoch 700, Loss: 0.2623749077320099\nEpoch 701, Loss: 0.262374609708786\nEpoch 702, Loss: 0.2623741030693054\nEpoch 703, Loss: 0.26237356662750244\nEpoch 704, Loss: 0.26237282156944275\nEpoch 705, Loss: 0.2623741030693054\nEpoch 706, Loss: 0.262373149394989\nEpoch 707, Loss: 0.26237180829048157\nEpoch 708, Loss: 0.26237204670906067\nEpoch 709, Loss: 0.26237204670906067\nEpoch 710, Loss: 0.26237210631370544\nEpoch 711, Loss: 0.26237204670906067\nEpoch 712, Loss: 0.2623717486858368\nEpoch 713, Loss: 0.26237136125564575\nEpoch 714, Loss: 0.2623710036277771\nEpoch 715, Loss: 0.2623707056045532\nEpoch 716, Loss: 0.2623703181743622\nEpoch 717, Loss: 0.26236990094184875\nEpoch 718, Loss: 0.26236990094184875\nEpoch 719, Loss: 0.2623690962791443\nEpoch 720, Loss: 0.26236867904663086\nEpoch 721, Loss: 0.262369841337204\nEpoch 722, Loss: 0.26236867904663086\nEpoch 723, Loss: 0.26236850023269653\nEpoch 724, Loss: 0.2623687982559204\nEpoch 725, Loss: 0.2623688578605652\nEpoch 726, Loss: 0.2623687982559204\nEpoch 727, Loss: 0.26236867904663086\nEpoch 728, Loss: 0.26236891746520996\nEpoch 729, Loss: 0.262368381023407\nEpoch 730, Loss: 0.26236820220947266\nEpoch 731, Loss: 0.26236802339553833\nEpoch 732, Loss: 0.26236796379089355\nEpoch 733, Loss: 0.26236775517463684\nEpoch 734, Loss: 0.26236751675605774\nEpoch 735, Loss: 0.2623671591281891\nEpoch 736, Loss: 0.26236680150032043\nEpoch 737, Loss: 0.26236623525619507\nEpoch 738, Loss: 0.2623661756515503\nEpoch 739, Loss: 0.26236647367477417\nEpoch 740, Loss: 0.2623661160469055\nEpoch 741, Loss: 0.2623656392097473\nEpoch 742, Loss: 0.2623656988143921\nEpoch 743, Loss: 0.26236581802368164\nEpoch 744, Loss: 0.2623659372329712\nEpoch 745, Loss: 0.26236599683761597\nEpoch 746, Loss: 0.26236599683761597\nEpoch 747, Loss: 0.26236581802368164\nEpoch 748, Loss: 0.2623656392097473\nEpoch 749, Loss: 0.26236531138420105\nEpoch 750, Loss: 0.26236552000045776\nEpoch 751, Loss: 0.26236507296562195\nEpoch 752, Loss: 0.2623648941516876\nEpoch 753, Loss: 0.2623648941516876\nEpoch 754, Loss: 0.2623647153377533\nEpoch 755, Loss: 0.26236581802368164\nEpoch 756, Loss: 0.26236459612846375\nEpoch 757, Loss: 0.26236459612846375\nEpoch 758, Loss: 0.2623644769191742\nEpoch 759, Loss: 0.26236435770988464\nEpoch 760, Loss: 0.2623639702796936\nEpoch 761, Loss: 0.2623637318611145\nEpoch 762, Loss: 0.2623647153377533\nEpoch 763, Loss: 0.26236408948898315\nEpoch 764, Loss: 0.26236391067504883\nEpoch 765, Loss: 0.26236414909362793\nEpoch 766, Loss: 0.26236429810523987\nEpoch 767, Loss: 0.26236435770988464\nEpoch 768, Loss: 0.26236429810523987\nEpoch 769, Loss: 0.26236408948898315\nEpoch 770, Loss: 0.26236391067504883\nEpoch 771, Loss: 0.26236361265182495\nEpoch 772, Loss: 0.2623632550239563\nEpoch 773, Loss: 0.2623639702796936\nEpoch 774, Loss: 0.2623632550239563\nEpoch 775, Loss: 0.2623630166053772\nEpoch 776, Loss: 0.262363076210022\nEpoch 777, Loss: 0.2623630166053772\nEpoch 778, Loss: 0.26236313581466675\nEpoch 779, Loss: 0.26236313581466675\nEpoch 780, Loss: 0.26236313581466675\nEpoch 781, Loss: 0.26236337423324585\nEpoch 782, Loss: 0.2623631954193115\nEpoch 783, Loss: 0.2623633146286011\nEpoch 784, Loss: 0.2623631954193115\nEpoch 785, Loss: 0.2623631954193115\nEpoch 786, Loss: 0.2623629570007324\nEpoch 787, Loss: 0.2623631954193115\nEpoch 788, Loss: 0.2623627483844757\nEpoch 789, Loss: 0.26236262917518616\nEpoch 790, Loss: 0.2623625099658966\nEpoch 791, Loss: 0.2623623311519623\nEpoch 792, Loss: 0.2623634934425354\nEpoch 793, Loss: 0.2623623311519623\nEpoch 794, Loss: 0.2623625099658966\nEpoch 795, Loss: 0.26236262917518616\nEpoch 796, Loss: 0.26236262917518616\nEpoch 797, Loss: 0.2623625695705414\nEpoch 798, Loss: 0.26236245036125183\nEpoch 799, Loss: 0.2623622715473175\nEpoch 800, Loss: 0.2623620927333832\nEpoch 801, Loss: 0.2623627483844757\nEpoch 802, Loss: 0.26236239075660706\nEpoch 803, Loss: 0.2623625099658966\nEpoch 804, Loss: 0.2623625695705414\nEpoch 805, Loss: 0.2623625695705414\nEpoch 806, Loss: 0.26236245036125183\nEpoch 807, Loss: 0.26236268877983093\nEpoch 808, Loss: 0.2623623311519623\nEpoch 809, Loss: 0.2623623311519623\nEpoch 810, Loss: 0.26236221194267273\nEpoch 811, Loss: 0.26236215233802795\nEpoch 812, Loss: 0.26236221194267273\nEpoch 813, Loss: 0.2623622715473175\nEpoch 814, Loss: 0.2623622715473175\nEpoch 815, Loss: 0.26236215233802795\nEpoch 816, Loss: 0.2623625695705414\nEpoch 817, Loss: 0.26236215233802795\nEpoch 818, Loss: 0.2623620331287384\nEpoch 819, Loss: 0.26236197352409363\nEpoch 820, Loss: 0.2623617947101593\nEpoch 821, Loss: 0.2623630166053772\nEpoch 822, Loss: 0.2623617947101593\nEpoch 823, Loss: 0.2623620927333832\nEpoch 824, Loss: 0.26236197352409363\nEpoch 825, Loss: 0.2623620927333832\nEpoch 826, Loss: 0.26236215233802795\nEpoch 827, Loss: 0.26236215233802795\nEpoch 828, Loss: 0.2623620331287384\nEpoch 829, Loss: 0.2623618543148041\nEpoch 830, Loss: 0.26236215233802795\nEpoch 831, Loss: 0.2623617947101593\nEpoch 832, Loss: 0.2623622715473175\nEpoch 833, Loss: 0.26236197352409363\nEpoch 834, Loss: 0.2623620927333832\nEpoch 835, Loss: 0.26236215233802795\nEpoch 836, Loss: 0.26236215233802795\nEpoch 837, Loss: 0.2623620331287384\nEpoch 838, Loss: 0.26236191391944885\nEpoch 839, Loss: 0.2623617351055145\nEpoch 840, Loss: 0.26236191391944885\nEpoch 841, Loss: 0.26236262917518616\nEpoch 842, Loss: 0.2623617351055145\nEpoch 843, Loss: 0.2623618543148041\nEpoch 844, Loss: 0.26236191391944885\nEpoch 845, Loss: 0.2623620331287384\nEpoch 846, Loss: 0.26236191391944885\nEpoch 847, Loss: 0.2623617947101593\nEpoch 848, Loss: 0.26236164569854736\nEpoch 849, Loss: 0.2623620331287384\nEpoch 850, Loss: 0.26236197352409363\nEpoch 851, Loss: 0.2623617351055145\nEpoch 852, Loss: 0.26236191391944885\nEpoch 853, Loss: 0.2623620331287384\nEpoch 854, Loss: 0.2623620927333832\nEpoch 855, Loss: 0.2623620331287384\nEpoch 856, Loss: 0.26236191391944885\nEpoch 857, Loss: 0.2623620927333832\nEpoch 858, Loss: 0.2623618543148041\nEpoch 859, Loss: 0.2623617947101593\nEpoch 860, Loss: 0.2623617351055145\nEpoch 861, Loss: 0.2623625099658966\nEpoch 862, Loss: 0.26236164569854736\nEpoch 863, Loss: 0.2623617947101593\nEpoch 864, Loss: 0.2623618543148041\nEpoch 865, Loss: 0.2623617947101593\nEpoch 866, Loss: 0.26236164569854736\nEpoch 867, Loss: 0.2623622715473175\nEpoch 868, Loss: 0.2623615860939026\nEpoch 869, Loss: 0.2623615860939026\nEpoch 870, Loss: 0.2623615860939026\nEpoch 871, Loss: 0.2623617351055145\nEpoch 872, Loss: 0.2623618543148041\nEpoch 873, Loss: 0.26236191391944885\nEpoch 874, Loss: 0.26236191391944885\nEpoch 875, Loss: 0.2623622715473175\nEpoch 876, Loss: 0.26236197352409363\nEpoch 877, Loss: 0.2623620331287384\nEpoch 878, Loss: 0.26236191391944885\nEpoch 879, Loss: 0.2623618543148041\nEpoch 880, Loss: 0.2623617351055145\nEpoch 881, Loss: 0.26236197352409363\nEpoch 882, Loss: 0.26236164569854736\nEpoch 883, Loss: 0.2623617947101593\nEpoch 884, Loss: 0.2623617947101593\nEpoch 885, Loss: 0.26236215233802795\nEpoch 886, Loss: 0.2623618543148041\nEpoch 887, Loss: 0.2623617947101593\nEpoch 888, Loss: 0.2623617351055145\nEpoch 889, Loss: 0.26236164569854736\nEpoch 890, Loss: 0.2623615264892578\nEpoch 891, Loss: 0.2623623311519623\nEpoch 892, Loss: 0.2623615264892578\nEpoch 893, Loss: 0.2623615264892578\nEpoch 894, Loss: 0.2623620927333832\nEpoch 895, Loss: 0.2623617351055145\nEpoch 896, Loss: 0.2623618543148041\nEpoch 897, Loss: 0.2623618543148041\nEpoch 898, Loss: 0.2623617947101593\nEpoch 899, Loss: 0.2623617351055145\nEpoch 900, Loss: 0.2623615860939026\nEpoch 901, Loss: 0.2623620331287384\nEpoch 902, Loss: 0.2623615264892578\nEpoch 903, Loss: 0.2623615860939026\nEpoch 904, Loss: 0.26236239075660706\nEpoch 905, Loss: 0.2623617947101593\nEpoch 906, Loss: 0.2623618543148041\nEpoch 907, Loss: 0.2623618543148041\nEpoch 908, Loss: 0.2623618543148041\nEpoch 909, Loss: 0.2623617947101593\nEpoch 910, Loss: 0.2623615264892578\nEpoch 911, Loss: 0.26236146688461304\nEpoch 912, Loss: 0.2623615264892578\nEpoch 913, Loss: 0.2623620927333832\nEpoch 914, Loss: 0.2623617947101593\nEpoch 915, Loss: 0.26236191391944885\nEpoch 916, Loss: 0.2623620331287384\nEpoch 917, Loss: 0.2623620331287384\nEpoch 918, Loss: 0.26236191391944885\nEpoch 919, Loss: 0.2623617947101593\nEpoch 920, Loss: 0.26236164569854736\nEpoch 921, Loss: 0.26236146688461304\nEpoch 922, Loss: 0.2623623311519623\nEpoch 923, Loss: 0.2623617351055145\nEpoch 924, Loss: 0.2623615860939026\nEpoch 925, Loss: 0.2623617947101593\nEpoch 926, Loss: 0.2623617947101593\nEpoch 927, Loss: 0.2623618543148041\nEpoch 928, Loss: 0.2623617947101593\nEpoch 929, Loss: 0.2623617351055145\nEpoch 930, Loss: 0.2623615264892578\nEpoch 931, Loss: 0.2623613476753235\nEpoch 932, Loss: 0.2623615264892578\nEpoch 933, Loss: 0.26236239075660706\nEpoch 934, Loss: 0.2623618543148041\nEpoch 935, Loss: 0.2623620331287384\nEpoch 936, Loss: 0.2623620331287384\nEpoch 937, Loss: 0.2623620331287384\nEpoch 938, Loss: 0.26236197352409363\nEpoch 939, Loss: 0.26236191391944885\nEpoch 940, Loss: 0.26236164569854736\nEpoch 941, Loss: 0.2623615264892578\nEpoch 942, Loss: 0.26236140727996826\nEpoch 943, Loss: 0.262363076210022\nEpoch 944, Loss: 0.26236146688461304\nEpoch 945, Loss: 0.26236164569854736\nEpoch 946, Loss: 0.2623617351055145\nEpoch 947, Loss: 0.2623617947101593\nEpoch 948, Loss: 0.2623617947101593\nEpoch 949, Loss: 0.26236164569854736\nEpoch 950, Loss: 0.2623615264892578\nEpoch 951, Loss: 0.26236146688461304\nEpoch 952, Loss: 0.2623615860939026\nEpoch 953, Loss: 0.2623622715473175\nEpoch 954, Loss: 0.26236191391944885\nEpoch 955, Loss: 0.2623620331287384\nEpoch 956, Loss: 0.26236215233802795\nEpoch 957, Loss: 0.26236215233802795\nEpoch 958, Loss: 0.2623620331287384\nEpoch 959, Loss: 0.26236197352409363\nEpoch 960, Loss: 0.2623617947101593\nEpoch 961, Loss: 0.26236164569854736\nEpoch 962, Loss: 0.2623617947101593\nEpoch 963, Loss: 0.2623617947101593\nEpoch 964, Loss: 0.2623615860939026\nEpoch 965, Loss: 0.2623617351055145\nEpoch 966, Loss: 0.2623618543148041\nEpoch 967, Loss: 0.26236197352409363\nEpoch 968, Loss: 0.26236191391944885\nEpoch 969, Loss: 0.2623617947101593\nEpoch 970, Loss: 0.2623617351055145\nEpoch 971, Loss: 0.26236146688461304\nEpoch 972, Loss: 0.2623612880706787\nEpoch 973, Loss: 0.26236337423324585\nEpoch 974, Loss: 0.2623613476753235\nEpoch 975, Loss: 0.2623615264892578\nEpoch 976, Loss: 0.2623617351055145\nEpoch 977, Loss: 0.2623617947101593\nEpoch 978, Loss: 0.2623617947101593\nEpoch 979, Loss: 0.2623617947101593\nEpoch 980, Loss: 0.26236164569854736\nEpoch 981, Loss: 0.26236146688461304\nEpoch 982, Loss: 0.26236164569854736\nEpoch 983, Loss: 0.26236239075660706\nEpoch 984, Loss: 0.2623615264892578\nEpoch 985, Loss: 0.2623617947101593\nEpoch 986, Loss: 0.26236191391944885\nEpoch 987, Loss: 0.26236197352409363\nEpoch 988, Loss: 0.2623620331287384\nEpoch 989, Loss: 0.26236197352409363\nEpoch 990, Loss: 0.2623617947101593\nEpoch 991, Loss: 0.26236164569854736\nEpoch 992, Loss: 0.26236146688461304\nEpoch 993, Loss: 0.2623617947101593\nEpoch 994, Loss: 0.2623615264892578\nEpoch 995, Loss: 0.2623617947101593\nEpoch 996, Loss: 0.26236197352409363\nEpoch 997, Loss: 0.2623620331287384\nEpoch 998, Loss: 0.2623620331287384\nEpoch 999, Loss: 0.26236197352409363\nEpoch 1000, Loss: 0.2623618543148041\nEpoch 1001, Loss: 0.2623617947101593\nEpoch 1002, Loss: 0.2623615264892578\nEpoch 1003, Loss: 0.26236215233802795\nEpoch 1004, Loss: 0.26236215233802795\nEpoch 1005, Loss: 0.2623615264892578\nEpoch 1006, Loss: 0.26236164569854736\nEpoch 1007, Loss: 0.2623617947101593\nEpoch 1008, Loss: 0.26236191391944885\nEpoch 1009, Loss: 0.2623618543148041\nEpoch 1010, Loss: 0.2623617947101593\nEpoch 1011, Loss: 0.26236164569854736\nEpoch 1012, Loss: 0.26236146688461304\nEpoch 1013, Loss: 0.26236191391944885\nEpoch 1014, Loss: 0.2623625099658966\nEpoch 1015, Loss: 0.2623615264892578\nEpoch 1016, Loss: 0.2623617351055145\nEpoch 1017, Loss: 0.2623618543148041\nEpoch 1018, Loss: 0.26236191391944885\nEpoch 1019, Loss: 0.26236191391944885\nEpoch 1020, Loss: 0.26236191391944885\nEpoch 1021, Loss: 0.2623617351055145\nEpoch 1022, Loss: 0.2623615264892578\nEpoch 1023, Loss: 0.26236164569854736\nEpoch 1024, Loss: 0.2623617947101593\nEpoch 1025, Loss: 0.26236164569854736\nEpoch 1026, Loss: 0.26236191391944885\nEpoch 1027, Loss: 0.2623620331287384\nEpoch 1028, Loss: 0.2623620331287384\nEpoch 1029, Loss: 0.2623620927333832\nEpoch 1030, Loss: 0.26236197352409363\nEpoch 1031, Loss: 0.2623618543148041\nEpoch 1032, Loss: 0.2623617351055145\nEpoch 1033, Loss: 0.2623615264892578\nEpoch 1034, Loss: 0.26236215233802795\nEpoch 1035, Loss: 0.2623625099658966\nEpoch 1036, Loss: 0.26236140727996826\nEpoch 1037, Loss: 0.26236164569854736\nEpoch 1038, Loss: 0.2623617947101593\nEpoch 1039, Loss: 0.2623618543148041\nEpoch 1040, Loss: 0.2623618543148041\nEpoch 1041, Loss: 0.2623617351055145\nEpoch 1042, Loss: 0.2623615860939026\nEpoch 1043, Loss: 0.2623617351055145\nEpoch 1044, Loss: 0.26236140727996826\nEpoch 1045, Loss: 0.26236286759376526\nEpoch 1046, Loss: 0.26236164569854736\nEpoch 1047, Loss: 0.2623617947101593\nEpoch 1048, Loss: 0.26236191391944885\nEpoch 1049, Loss: 0.26236197352409363\nEpoch 1050, Loss: 0.26236197352409363\nEpoch 1051, Loss: 0.26236191391944885\nEpoch 1052, Loss: 0.2623617947101593\nEpoch 1053, Loss: 0.2623615860939026\nEpoch 1054, Loss: 0.2623615264892578\nEpoch 1055, Loss: 0.2623620331287384\nEpoch 1056, Loss: 0.2623615860939026\nEpoch 1057, Loss: 0.2623618543148041\nEpoch 1058, Loss: 0.26236197352409363\nEpoch 1059, Loss: 0.2623620331287384\nEpoch 1060, Loss: 0.2623620331287384\nEpoch 1061, Loss: 0.26236197352409363\nEpoch 1062, Loss: 0.2623617947101593\nEpoch 1063, Loss: 0.26236164569854736\nEpoch 1064, Loss: 0.2623617947101593\nEpoch 1065, Loss: 0.26236140727996826\nEpoch 1066, Loss: 0.2623630166053772\nEpoch 1067, Loss: 0.2623615860939026\nEpoch 1068, Loss: 0.2623617947101593\nEpoch 1069, Loss: 0.2623618543148041\nEpoch 1070, Loss: 0.2623618543148041\nEpoch 1071, Loss: 0.2623618543148041\nEpoch 1072, Loss: 0.2623617351055145\nEpoch 1073, Loss: 0.2623615264892578\nEpoch 1074, Loss: 0.26236146688461304\nEpoch 1075, Loss: 0.2623615264892578\nEpoch 1076, Loss: 0.2623617351055145\nEpoch 1077, Loss: 0.2623620331287384\nEpoch 1078, Loss: 0.26236215233802795\nEpoch 1079, Loss: 0.26236221194267273\nEpoch 1080, Loss: 0.2623622715473175\nEpoch 1081, Loss: 0.26236221194267273\nEpoch 1082, Loss: 0.2623620331287384\nEpoch 1083, Loss: 0.26236191391944885\nEpoch 1084, Loss: 0.26236164569854736\nEpoch 1085, Loss: 0.26236215233802795\nEpoch 1086, Loss: 0.2623612880706787\nEpoch 1087, Loss: 0.2623631954193115\nEpoch 1088, Loss: 0.26236146688461304\nEpoch 1089, Loss: 0.26236164569854736\nEpoch 1090, Loss: 0.2623617351055145\nEpoch 1091, Loss: 0.2623617351055145\nEpoch 1092, Loss: 0.26236164569854736\nEpoch 1093, Loss: 0.2623615860939026\nEpoch 1094, Loss: 0.26236146688461304\nEpoch 1095, Loss: 0.26236140727996826\nEpoch 1096, Loss: 0.2623628079891205\nEpoch 1097, Loss: 0.26236164569854736\nEpoch 1098, Loss: 0.26236197352409363\nEpoch 1099, Loss: 0.2623620331287384\nEpoch 1100, Loss: 0.2623620927333832\nEpoch 1101, Loss: 0.26236215233802795\nEpoch 1102, Loss: 0.2623620331287384\nEpoch 1103, Loss: 0.26236191391944885\nEpoch 1104, Loss: 0.26236164569854736\nEpoch 1105, Loss: 0.26236146688461304\nEpoch 1106, Loss: 0.2623622715473175\nEpoch 1107, Loss: 0.26236239075660706\nEpoch 1108, Loss: 0.26236146688461304\nEpoch 1109, Loss: 0.26236164569854736\nEpoch 1110, Loss: 0.2623618543148041\nEpoch 1111, Loss: 0.2623617947101593\nEpoch 1112, Loss: 0.2623617947101593\nEpoch 1113, Loss: 0.2623617351055145\nEpoch 1114, Loss: 0.2623615860939026\nEpoch 1115, Loss: 0.26236215233802795\nEpoch 1116, Loss: 0.26236140727996826\nEpoch 1117, Loss: 0.26236262917518616\nEpoch 1118, Loss: 0.26236164569854736\nEpoch 1119, Loss: 0.2623617947101593\nEpoch 1120, Loss: 0.26236197352409363\nEpoch 1121, Loss: 0.2623620331287384\nEpoch 1122, Loss: 0.2623620331287384\nEpoch 1123, Loss: 0.26236191391944885\nEpoch 1124, Loss: 0.2623617947101593\nEpoch 1125, Loss: 0.2623615860939026\nEpoch 1126, Loss: 0.26236197352409363\nEpoch 1127, Loss: 0.2623615860939026\nEpoch 1128, Loss: 0.26236164569854736\nEpoch 1129, Loss: 0.26236191391944885\nEpoch 1130, Loss: 0.2623620331287384\nEpoch 1131, Loss: 0.2623620927333832\nEpoch 1132, Loss: 0.2623620927333832\nEpoch 1133, Loss: 0.2623620331287384\nEpoch 1134, Loss: 0.26236191391944885\nEpoch 1135, Loss: 0.2623617351055145\nEpoch 1136, Loss: 0.26236221194267273\nEpoch 1137, Loss: 0.26236140727996826\nEpoch 1138, Loss: 0.2623625099658966\nEpoch 1139, Loss: 0.2623615264892578\nEpoch 1140, Loss: 0.2623618543148041\nEpoch 1141, Loss: 0.26236191391944885\nEpoch 1142, Loss: 0.26236197352409363\nEpoch 1143, Loss: 0.26236191391944885\nEpoch 1144, Loss: 0.2623618543148041\nEpoch 1145, Loss: 0.26236164569854736\nEpoch 1146, Loss: 0.2623617351055145\nEpoch 1147, Loss: 0.26236140727996826\nEpoch 1148, Loss: 0.26236286759376526\nEpoch 1149, Loss: 0.26236164569854736\nEpoch 1150, Loss: 0.2623618543148041\nEpoch 1151, Loss: 0.26236197352409363\nEpoch 1152, Loss: 0.2623620331287384\nEpoch 1153, Loss: 0.2623620331287384\nEpoch 1154, Loss: 0.26236191391944885\nEpoch 1155, Loss: 0.2623617947101593\nEpoch 1156, Loss: 0.2623615264892578\nEpoch 1157, Loss: 0.2623620331287384\nEpoch 1158, Loss: 0.26236197352409363\nEpoch 1159, Loss: 0.2623615860939026\nEpoch 1160, Loss: 0.2623617947101593\nEpoch 1161, Loss: 0.26236197352409363\nEpoch 1162, Loss: 0.2623620331287384\nEpoch 1163, Loss: 0.2623620331287384\nEpoch 1164, Loss: 0.26236191391944885\nEpoch 1165, Loss: 0.2623618543148041\nEpoch 1166, Loss: 0.26236164569854736\nEpoch 1167, Loss: 0.2623615860939026\nEpoch 1168, Loss: 0.2623615264892578\nEpoch 1169, Loss: 0.26236313581466675\nEpoch 1170, Loss: 0.2623617351055145\nEpoch 1171, Loss: 0.2623618543148041\nEpoch 1172, Loss: 0.2623620331287384\nEpoch 1173, Loss: 0.2623620331287384\nEpoch 1174, Loss: 0.26236191391944885\nEpoch 1175, Loss: 0.2623617947101593\nEpoch 1176, Loss: 0.2623615860939026\nEpoch 1177, Loss: 0.26236146688461304\nEpoch 1178, Loss: 0.2623622715473175\nEpoch 1179, Loss: 0.2623615860939026\nEpoch 1180, Loss: 0.2623618543148041\nEpoch 1181, Loss: 0.2623620331287384\nEpoch 1182, Loss: 0.26236215233802795\nEpoch 1183, Loss: 0.26236215233802795\nEpoch 1184, Loss: 0.2623620927333832\nEpoch 1185, Loss: 0.2623620331287384\nEpoch 1186, Loss: 0.2623617947101593\nEpoch 1187, Loss: 0.2623618543148041\nEpoch 1188, Loss: 0.2623617947101593\nEpoch 1189, Loss: 0.2623615860939026\nEpoch 1190, Loss: 0.26236361265182495\nEpoch 1191, Loss: 0.2623617351055145\nEpoch 1192, Loss: 0.2623618543148041\nEpoch 1193, Loss: 0.26236191391944885\nEpoch 1194, Loss: 0.26236191391944885\nEpoch 1195, Loss: 0.2623618543148041\nEpoch 1196, Loss: 0.2623617351055145\nEpoch 1197, Loss: 0.2623615264892578\nEpoch 1198, Loss: 0.26236146688461304\nEpoch 1199, Loss: 0.2623629570007324\nEpoch 1200, Loss: 0.2623615264892578\nEpoch 1201, Loss: 0.2623618543148041\nEpoch 1202, Loss: 0.26236197352409363\nEpoch 1203, Loss: 0.2623620331287384\nEpoch 1204, Loss: 0.2623620331287384\nEpoch 1205, Loss: 0.26236191391944885\nEpoch 1206, Loss: 0.2623618543148041\nEpoch 1207, Loss: 0.26236164569854736\nEpoch 1208, Loss: 0.2623622715473175\nEpoch 1209, Loss: 0.2623613476753235\nEpoch 1210, Loss: 0.26236286759376526\nEpoch 1211, Loss: 0.2623615264892578\nEpoch 1212, Loss: 0.2623617351055145\nEpoch 1213, Loss: 0.26236191391944885\nEpoch 1214, Loss: 0.26236191391944885\nEpoch 1215, Loss: 0.26236191391944885\nEpoch 1216, Loss: 0.2623617947101593\nEpoch 1217, Loss: 0.26236164569854736\nEpoch 1218, Loss: 0.26236197352409363\nEpoch 1219, Loss: 0.26236140727996826\nEpoch 1220, Loss: 0.2623628079891205\nEpoch 1221, Loss: 0.2623615860939026\nEpoch 1222, Loss: 0.26236191391944885\nEpoch 1223, Loss: 0.2623620331287384\nEpoch 1224, Loss: 0.2623620331287384\nEpoch 1225, Loss: 0.2623620927333832\nEpoch 1226, Loss: 0.26236191391944885\nEpoch 1227, Loss: 0.2623617351055145\nEpoch 1228, Loss: 0.2623615860939026\nEpoch 1229, Loss: 0.2623622715473175\nEpoch 1230, Loss: 0.2623618543148041\nEpoch 1231, Loss: 0.26236164569854736\nEpoch 1232, Loss: 0.2623617947101593\nEpoch 1233, Loss: 0.2623620331287384\nEpoch 1234, Loss: 0.2623620927333832\nEpoch 1235, Loss: 0.2623620927333832\nEpoch 1236, Loss: 0.2623620331287384\nEpoch 1237, Loss: 0.2623618543148041\nEpoch 1238, Loss: 0.2623618543148041\nEpoch 1239, Loss: 0.26236164569854736\nEpoch 1240, Loss: 0.2623615860939026\nEpoch 1241, Loss: 0.262363076210022\nEpoch 1242, Loss: 0.2623617947101593\nEpoch 1243, Loss: 0.26236191391944885\nEpoch 1244, Loss: 0.2623620331287384\nEpoch 1245, Loss: 0.2623620331287384\nEpoch 1246, Loss: 0.26236197352409363\nEpoch 1247, Loss: 0.26236191391944885\nEpoch 1248, Loss: 0.26236164569854736\nEpoch 1249, Loss: 0.26236140727996826\nEpoch 1250, Loss: 0.2623623311519623\nEpoch 1251, Loss: 0.262363076210022\nEpoch 1252, Loss: 0.26236146688461304\nEpoch 1253, Loss: 0.26236164569854736\nEpoch 1254, Loss: 0.2623618543148041\nEpoch 1255, Loss: 0.26236191391944885\nEpoch 1256, Loss: 0.26236191391944885\nEpoch 1257, Loss: 0.2623617947101593\nEpoch 1258, Loss: 0.26236164569854736\nEpoch 1259, Loss: 0.2623622715473175\nEpoch 1260, Loss: 0.26236140727996826\nEpoch 1261, Loss: 0.26236239075660706\nEpoch 1262, Loss: 0.2623617351055145\nEpoch 1263, Loss: 0.26236197352409363\nEpoch 1264, Loss: 0.2623620927333832\nEpoch 1265, Loss: 0.26236215233802795\nEpoch 1266, Loss: 0.26236221194267273\nEpoch 1267, Loss: 0.2623620331287384\nEpoch 1268, Loss: 0.26236191391944885\nEpoch 1269, Loss: 0.2623617351055145\nEpoch 1270, Loss: 0.26236215233802795\nEpoch 1271, Loss: 0.2623613476753235\nEpoch 1272, Loss: 0.2623632550239563\nEpoch 1273, Loss: 0.2623615860939026\nEpoch 1274, Loss: 0.2623617947101593\nEpoch 1275, Loss: 0.2623618543148041\nEpoch 1276, Loss: 0.26236191391944885\nEpoch 1277, Loss: 0.2623618543148041\nEpoch 1278, Loss: 0.2623617947101593\nEpoch 1279, Loss: 0.2623615264892578\nEpoch 1280, Loss: 0.2623620331287384\nEpoch 1281, Loss: 0.2623615264892578\nEpoch 1282, Loss: 0.2623617351055145\nEpoch 1283, Loss: 0.26236197352409363\nEpoch 1284, Loss: 0.26236221194267273\nEpoch 1285, Loss: 0.2623622715473175\nEpoch 1286, Loss: 0.2623622715473175\nEpoch 1287, Loss: 0.26236221194267273\nEpoch 1288, Loss: 0.2623620927333832\nEpoch 1289, Loss: 0.26236191391944885\nEpoch 1290, Loss: 0.2623620331287384\nEpoch 1291, Loss: 0.2623615860939026\nEpoch 1292, Loss: 0.2623615264892578\nEpoch 1293, Loss: 0.2623637914657593\nEpoch 1294, Loss: 0.2623615860939026\nEpoch 1295, Loss: 0.2623617351055145\nEpoch 1296, Loss: 0.26236191391944885\nEpoch 1297, Loss: 0.2623617947101593\nEpoch 1298, Loss: 0.2623617947101593\nEpoch 1299, Loss: 0.26236164569854736\nEpoch 1300, Loss: 0.26236140727996826\nEpoch 1301, Loss: 0.2623620927333832\nEpoch 1302, Loss: 0.2623630166053772\nEpoch 1303, Loss: 0.2623615264892578\nEpoch 1304, Loss: 0.2623617947101593\nEpoch 1305, Loss: 0.26236197352409363\nEpoch 1306, Loss: 0.2623620331287384\nEpoch 1307, Loss: 0.2623620927333832\nEpoch 1308, Loss: 0.2623620331287384\nEpoch 1309, Loss: 0.2623618543148041\nEpoch 1310, Loss: 0.26236164569854736\nEpoch 1311, Loss: 0.26236164569854736\nEpoch 1312, Loss: 0.2623615860939026\nEpoch 1313, Loss: 0.2623632550239563\nEpoch 1314, Loss: 0.2623617947101593\nEpoch 1315, Loss: 0.2623620331287384\nEpoch 1316, Loss: 0.2623620927333832\nEpoch 1317, Loss: 0.2623620927333832\nEpoch 1318, Loss: 0.2623620331287384\nEpoch 1319, Loss: 0.26236191391944885\nEpoch 1320, Loss: 0.2623617351055145\nEpoch 1321, Loss: 0.26236146688461304\nEpoch 1322, Loss: 0.2623623311519623\nEpoch 1323, Loss: 0.2623630166053772\nEpoch 1324, Loss: 0.26236140727996826\nEpoch 1325, Loss: 0.2623617351055145\nEpoch 1326, Loss: 0.26236191391944885\nEpoch 1327, Loss: 0.26236191391944885\nEpoch 1328, Loss: 0.26236197352409363\nEpoch 1329, Loss: 0.2623618543148041\nEpoch 1330, Loss: 0.26236164569854736\nEpoch 1331, Loss: 0.2623622715473175\nEpoch 1332, Loss: 0.26236146688461304\nEpoch 1333, Loss: 0.2623622715473175\nEpoch 1334, Loss: 0.2623617351055145\nEpoch 1335, Loss: 0.2623620331287384\nEpoch 1336, Loss: 0.26236215233802795\nEpoch 1337, Loss: 0.2623622715473175\nEpoch 1338, Loss: 0.26236221194267273\nEpoch 1339, Loss: 0.2623620927333832\nEpoch 1340, Loss: 0.26236197352409363\nEpoch 1341, Loss: 0.26236164569854736\nEpoch 1342, Loss: 0.26236215233802795\nEpoch 1343, Loss: 0.2623612880706787\nEpoch 1344, Loss: 0.2623632550239563\nEpoch 1345, Loss: 0.2623615860939026\nEpoch 1346, Loss: 0.2623617947101593\nEpoch 1347, Loss: 0.26236191391944885\nEpoch 1348, Loss: 0.26236191391944885\nEpoch 1349, Loss: 0.26236191391944885\nEpoch 1350, Loss: 0.2623617947101593\nEpoch 1351, Loss: 0.2623615860939026\nEpoch 1352, Loss: 0.26236215233802795\nEpoch 1353, Loss: 0.26236146688461304\nEpoch 1354, Loss: 0.2623617351055145\nEpoch 1355, Loss: 0.2623620331287384\nEpoch 1356, Loss: 0.26236221194267273\nEpoch 1357, Loss: 0.2623623311519623\nEpoch 1358, Loss: 0.2623623311519623\nEpoch 1359, Loss: 0.2623623311519623\nEpoch 1360, Loss: 0.26236215233802795\nEpoch 1361, Loss: 0.26236191391944885\nEpoch 1362, Loss: 0.26236215233802795\nEpoch 1363, Loss: 0.26236164569854736\nEpoch 1364, Loss: 0.26236146688461304\nEpoch 1365, Loss: 0.26236391067504883\nEpoch 1366, Loss: 0.2623615264892578\nEpoch 1367, Loss: 0.2623617947101593\nEpoch 1368, Loss: 0.26236191391944885\nEpoch 1369, Loss: 0.26236191391944885\nEpoch 1370, Loss: 0.2623617947101593\nEpoch 1371, Loss: 0.26236164569854736\nEpoch 1372, Loss: 0.26236140727996826\nEpoch 1373, Loss: 0.2623622715473175\nEpoch 1374, Loss: 0.26236164569854736\nEpoch 1375, Loss: 0.26236197352409363\nEpoch 1376, Loss: 0.2623622715473175\nEpoch 1377, Loss: 0.26236239075660706\nEpoch 1378, Loss: 0.26236245036125183\nEpoch 1379, Loss: 0.26236239075660706\nEpoch 1380, Loss: 0.2623622715473175\nEpoch 1381, Loss: 0.26236215233802795\nEpoch 1382, Loss: 0.26236221194267273\nEpoch 1383, Loss: 0.2623617947101593\nEpoch 1384, Loss: 0.26236164569854736\nEpoch 1385, Loss: 0.26236221194267273\nEpoch 1386, Loss: 0.2623617947101593\nEpoch 1387, Loss: 0.2623620331287384\nEpoch 1388, Loss: 0.2623620927333832\nEpoch 1389, Loss: 0.26236215233802795\nEpoch 1390, Loss: 0.2623620927333832\nEpoch 1391, Loss: 0.26236191391944885\nEpoch 1392, Loss: 0.26236164569854736\nEpoch 1393, Loss: 0.2623618543148041\nEpoch 1394, Loss: 0.26236140727996826\nEpoch 1395, Loss: 0.2623617947101593\nEpoch 1396, Loss: 0.2623620927333832\nEpoch 1397, Loss: 0.2623623311519623\nEpoch 1398, Loss: 0.2623623311519623\nEpoch 1399, Loss: 0.26236239075660706\nEpoch 1400, Loss: 0.2623622715473175\nEpoch 1401, Loss: 0.2623620927333832\nEpoch 1402, Loss: 0.26236191391944885\nEpoch 1403, Loss: 0.26236239075660706\nEpoch 1404, Loss: 0.2623615264892578\nEpoch 1405, Loss: 0.2623617947101593\nEpoch 1406, Loss: 0.2623617947101593\nEpoch 1407, Loss: 0.2623620331287384\nEpoch 1408, Loss: 0.26236215233802795\nEpoch 1409, Loss: 0.26236221194267273\nEpoch 1410, Loss: 0.2623620927333832\nEpoch 1411, Loss: 0.2623620331287384\nEpoch 1412, Loss: 0.2623617947101593\nEpoch 1413, Loss: 0.2623620331287384\nEpoch 1414, Loss: 0.2623615264892578\nEpoch 1415, Loss: 0.2623622715473175\nEpoch 1416, Loss: 0.2623617947101593\nEpoch 1417, Loss: 0.2623620331287384\nEpoch 1418, Loss: 0.26236215233802795\nEpoch 1419, Loss: 0.2623622715473175\nEpoch 1420, Loss: 0.26236221194267273\nEpoch 1421, Loss: 0.2623620331287384\nEpoch 1422, Loss: 0.2623618543148041\nEpoch 1423, Loss: 0.26236164569854736\nEpoch 1424, Loss: 0.2623615860939026\nEpoch 1425, Loss: 0.2623617947101593\nEpoch 1426, Loss: 0.26236191391944885\nEpoch 1427, Loss: 0.26236215233802795\nEpoch 1428, Loss: 0.2623622715473175\nEpoch 1429, Loss: 0.26236239075660706\nEpoch 1430, Loss: 0.2623623311519623\nEpoch 1431, Loss: 0.26236215233802795\nEpoch 1432, Loss: 0.26236197352409363\nEpoch 1433, Loss: 0.2623617351055145\nEpoch 1434, Loss: 0.26236239075660706\nEpoch 1435, Loss: 0.26236140727996826\nEpoch 1436, Loss: 0.2623631954193115\nEpoch 1437, Loss: 0.2623615860939026\nEpoch 1438, Loss: 0.2623617947101593\nEpoch 1439, Loss: 0.26236191391944885\nEpoch 1440, Loss: 0.26236197352409363\nEpoch 1441, Loss: 0.26236191391944885\nEpoch 1442, Loss: 0.2623617947101593\nEpoch 1443, Loss: 0.26236164569854736\nEpoch 1444, Loss: 0.2623615860939026\nEpoch 1445, Loss: 0.2623618543148041\nEpoch 1446, Loss: 0.26236191391944885\nEpoch 1447, Loss: 0.2623622715473175\nEpoch 1448, Loss: 0.26236245036125183\nEpoch 1449, Loss: 0.2623625099658966\nEpoch 1450, Loss: 0.26236245036125183\nEpoch 1451, Loss: 0.26236239075660706\nEpoch 1452, Loss: 0.26236221194267273\nEpoch 1453, Loss: 0.2623620331287384\nEpoch 1454, Loss: 0.2623618543148041\nEpoch 1455, Loss: 0.2623615860939026\nEpoch 1456, Loss: 0.2623617947101593\nEpoch 1457, Loss: 0.2623617947101593\nEpoch 1458, Loss: 0.2623620927333832\nEpoch 1459, Loss: 0.26236221194267273\nEpoch 1460, Loss: 0.2623622715473175\nEpoch 1461, Loss: 0.2623620927333832\nEpoch 1462, Loss: 0.2623620331287384\nEpoch 1463, Loss: 0.2623618543148041\nEpoch 1464, Loss: 0.26236215233802795\nEpoch 1465, Loss: 0.2623615264892578\nEpoch 1466, Loss: 0.2623622715473175\nEpoch 1467, Loss: 0.2623617947101593\nEpoch 1468, Loss: 0.2623620331287384\nEpoch 1469, Loss: 0.26236221194267273\nEpoch 1470, Loss: 0.2623622715473175\nEpoch 1471, Loss: 0.26236215233802795\nEpoch 1472, Loss: 0.2623620927333832\nEpoch 1473, Loss: 0.26236191391944885\nEpoch 1474, Loss: 0.26236191391944885\nEpoch 1475, Loss: 0.2623615860939026\nEpoch 1476, Loss: 0.2623617947101593\nEpoch 1477, Loss: 0.26236191391944885\nEpoch 1478, Loss: 0.26236215233802795\nEpoch 1479, Loss: 0.2623623311519623\nEpoch 1480, Loss: 0.2623623311519623\nEpoch 1481, Loss: 0.2623623311519623\nEpoch 1482, Loss: 0.26236215233802795\nEpoch 1483, Loss: 0.2623620927333832\nEpoch 1484, Loss: 0.2623617947101593\nEpoch 1485, Loss: 0.26236262917518616\nEpoch 1486, Loss: 0.2623613476753235\nEpoch 1487, Loss: 0.2623632550239563\nEpoch 1488, Loss: 0.2623615860939026\nEpoch 1489, Loss: 0.2623617947101593\nEpoch 1490, Loss: 0.26236191391944885\nEpoch 1491, Loss: 0.26236191391944885\nEpoch 1492, Loss: 0.26236191391944885\nEpoch 1493, Loss: 0.2623617947101593\nEpoch 1494, Loss: 0.2623618543148041\nEpoch 1495, Loss: 0.2623615860939026\nEpoch 1496, Loss: 0.2623617947101593\nEpoch 1497, Loss: 0.26236197352409363\nEpoch 1498, Loss: 0.2623622715473175\nEpoch 1499, Loss: 0.26236245036125183\nEpoch 1500, Loss: 0.2623625695705414\nEpoch 1501, Loss: 0.2623625099658966\nEpoch 1502, Loss: 0.2623625099658966\nEpoch 1503, Loss: 0.26236221194267273\nEpoch 1504, Loss: 0.2623620331287384\nEpoch 1505, Loss: 0.26236191391944885\nEpoch 1506, Loss: 0.26236164569854736\nEpoch 1507, Loss: 0.2623617947101593\nEpoch 1508, Loss: 0.26236191391944885\nEpoch 1509, Loss: 0.2623620927333832\nEpoch 1510, Loss: 0.2623622715473175\nEpoch 1511, Loss: 0.2623622715473175\nEpoch 1512, Loss: 0.26236221194267273\nEpoch 1513, Loss: 0.2623620331287384\nEpoch 1514, Loss: 0.2623618543148041\nEpoch 1515, Loss: 0.26236221194267273\nEpoch 1516, Loss: 0.2623615264892578\nEpoch 1517, Loss: 0.2623622715473175\nEpoch 1518, Loss: 0.2623617947101593\nEpoch 1519, Loss: 0.2623620927333832\nEpoch 1520, Loss: 0.26236221194267273\nEpoch 1521, Loss: 0.2623622715473175\nEpoch 1522, Loss: 0.2623622715473175\nEpoch 1523, Loss: 0.2623620927333832\nEpoch 1524, Loss: 0.26236191391944885\nEpoch 1525, Loss: 0.26236197352409363\nEpoch 1526, Loss: 0.26236164569854736\nEpoch 1527, Loss: 0.2623617947101593\nEpoch 1528, Loss: 0.26236191391944885\nEpoch 1529, Loss: 0.26236215233802795\nEpoch 1530, Loss: 0.26236239075660706\nEpoch 1531, Loss: 0.26236239075660706\nEpoch 1532, Loss: 0.2623623311519623\nEpoch 1533, Loss: 0.2623622715473175\nEpoch 1534, Loss: 0.2623620331287384\nEpoch 1535, Loss: 0.2623617947101593\nEpoch 1536, Loss: 0.2623627483844757\nEpoch 1537, Loss: 0.26236140727996826\nEpoch 1538, Loss: 0.26236337423324585\nEpoch 1539, Loss: 0.2623615860939026\nEpoch 1540, Loss: 0.2623617947101593\nEpoch 1541, Loss: 0.26236197352409363\nEpoch 1542, Loss: 0.2623620331287384\nEpoch 1543, Loss: 0.26236191391944885\nEpoch 1544, Loss: 0.2623617947101593\nEpoch 1545, Loss: 0.26236191391944885\nEpoch 1546, Loss: 0.2623615264892578\nEpoch 1547, Loss: 0.2623618543148041\nEpoch 1548, Loss: 0.26236197352409363\nEpoch 1549, Loss: 0.2623622715473175\nEpoch 1550, Loss: 0.2623625099658966\nEpoch 1551, Loss: 0.26236262917518616\nEpoch 1552, Loss: 0.2623625695705414\nEpoch 1553, Loss: 0.26236245036125183\nEpoch 1554, Loss: 0.2623622715473175\nEpoch 1555, Loss: 0.2623620331287384\nEpoch 1556, Loss: 0.2623620331287384\nEpoch 1557, Loss: 0.2623615860939026\nEpoch 1558, Loss: 0.2623618543148041\nEpoch 1559, Loss: 0.2623618543148041\nEpoch 1560, Loss: 0.2623620927333832\nEpoch 1561, Loss: 0.2623622715473175\nEpoch 1562, Loss: 0.2623622715473175\nEpoch 1563, Loss: 0.26236221194267273\nEpoch 1564, Loss: 0.2623620927333832\nEpoch 1565, Loss: 0.2623618543148041\nEpoch 1566, Loss: 0.2623622715473175\nEpoch 1567, Loss: 0.2623615264892578\nEpoch 1568, Loss: 0.26236239075660706\nEpoch 1569, Loss: 0.2623617947101593\nEpoch 1570, Loss: 0.2623620331287384\nEpoch 1571, Loss: 0.26236221194267273\nEpoch 1572, Loss: 0.2623622715473175\nEpoch 1573, Loss: 0.26236221194267273\nEpoch 1574, Loss: 0.2623620927333832\nEpoch 1575, Loss: 0.26236191391944885\nEpoch 1576, Loss: 0.2623620331287384\nEpoch 1577, Loss: 0.2623615860939026\nEpoch 1578, Loss: 0.26236191391944885\nEpoch 1579, Loss: 0.26236191391944885\nEpoch 1580, Loss: 0.26236221194267273\nEpoch 1581, Loss: 0.2623623311519623\nEpoch 1582, Loss: 0.26236239075660706\nEpoch 1583, Loss: 0.26236239075660706\nEpoch 1584, Loss: 0.26236221194267273\nEpoch 1585, Loss: 0.2623620331287384\nEpoch 1586, Loss: 0.2623618543148041\nEpoch 1587, Loss: 0.2623617351055145\nEpoch 1588, Loss: 0.2623615264892578\nEpoch 1589, Loss: 0.26236385107040405\nEpoch 1590, Loss: 0.2623617947101593\nEpoch 1591, Loss: 0.26236191391944885\nEpoch 1592, Loss: 0.2623620927333832\nEpoch 1593, Loss: 0.2623620927333832\nEpoch 1594, Loss: 0.26236197352409363\nEpoch 1595, Loss: 0.2623618543148041\nEpoch 1596, Loss: 0.26236164569854736\nEpoch 1597, Loss: 0.2623622715473175\nEpoch 1598, Loss: 0.2623620927333832\nEpoch 1599, Loss: 0.2623617947101593\nEpoch 1600, Loss: 0.26236215233802795\nEpoch 1601, Loss: 0.2623623311519623\nEpoch 1602, Loss: 0.26236245036125183\nEpoch 1603, Loss: 0.26236239075660706\nEpoch 1604, Loss: 0.2623623311519623\nEpoch 1605, Loss: 0.2623620927333832\nEpoch 1606, Loss: 0.26236197352409363\nEpoch 1607, Loss: 0.26236191391944885\nEpoch 1608, Loss: 0.2623617947101593\nEpoch 1609, Loss: 0.2623623311519623\nEpoch 1610, Loss: 0.26236197352409363\nEpoch 1611, Loss: 0.26236221194267273\nEpoch 1612, Loss: 0.26236239075660706\nEpoch 1613, Loss: 0.2623623311519623\nEpoch 1614, Loss: 0.2623622715473175\nEpoch 1615, Loss: 0.26236215233802795\nEpoch 1616, Loss: 0.26236191391944885\nEpoch 1617, Loss: 0.2623618543148041\nEpoch 1618, Loss: 0.2623615264892578\nEpoch 1619, Loss: 0.2623628079891205\nEpoch 1620, Loss: 0.2623617947101593\nEpoch 1621, Loss: 0.2623620331287384\nEpoch 1622, Loss: 0.26236221194267273\nEpoch 1623, Loss: 0.2623622715473175\nEpoch 1624, Loss: 0.26236221194267273\nEpoch 1625, Loss: 0.2623620927333832\nEpoch 1626, Loss: 0.2623618543148041\nEpoch 1627, Loss: 0.26236221194267273\nEpoch 1628, Loss: 0.2623615264892578\nEpoch 1629, Loss: 0.2623623311519623\nEpoch 1630, Loss: 0.26236191391944885\nEpoch 1631, Loss: 0.26236215233802795\nEpoch 1632, Loss: 0.26236239075660706\nEpoch 1633, Loss: 0.26236239075660706\nEpoch 1634, Loss: 0.26236239075660706\nEpoch 1635, Loss: 0.26236221194267273\nEpoch 1636, Loss: 0.26236197352409363\nEpoch 1637, Loss: 0.2623620331287384\nEpoch 1638, Loss: 0.26236164569854736\nEpoch 1639, Loss: 0.2623615264892578\nEpoch 1640, Loss: 0.2623642385005951\nEpoch 1641, Loss: 0.2623617351055145\nEpoch 1642, Loss: 0.26236191391944885\nEpoch 1643, Loss: 0.2623620331287384\nEpoch 1644, Loss: 0.2623620927333832\nEpoch 1645, Loss: 0.26236197352409363\nEpoch 1646, Loss: 0.2623618543148041\nEpoch 1647, Loss: 0.2623615860939026\nEpoch 1648, Loss: 0.26236245036125183\nEpoch 1649, Loss: 0.2623622715473175\nEpoch 1650, Loss: 0.2623617351055145\nEpoch 1651, Loss: 0.2623620927333832\nEpoch 1652, Loss: 0.2623623311519623\nEpoch 1653, Loss: 0.26236245036125183\nEpoch 1654, Loss: 0.26236245036125183\nEpoch 1655, Loss: 0.2623623311519623\nEpoch 1656, Loss: 0.26236215233802795\nEpoch 1657, Loss: 0.2623620927333832\nEpoch 1658, Loss: 0.26236191391944885\nEpoch 1659, Loss: 0.2623617947101593\nEpoch 1660, Loss: 0.26236245036125183\nEpoch 1661, Loss: 0.26236197352409363\nEpoch 1662, Loss: 0.26236221194267273\nEpoch 1663, Loss: 0.26236239075660706\nEpoch 1664, Loss: 0.26236239075660706\nEpoch 1665, Loss: 0.2623622715473175\nEpoch 1666, Loss: 0.26236215233802795\nEpoch 1667, Loss: 0.26236191391944885\nEpoch 1668, Loss: 0.26236191391944885\nEpoch 1669, Loss: 0.26236146688461304\nEpoch 1670, Loss: 0.2623630166053772\nEpoch 1671, Loss: 0.2623617947101593\nEpoch 1672, Loss: 0.26236215233802795\nEpoch 1673, Loss: 0.26236221194267273\nEpoch 1674, Loss: 0.2623622715473175\nEpoch 1675, Loss: 0.2623622715473175\nEpoch 1676, Loss: 0.2623620927333832\nEpoch 1677, Loss: 0.26236191391944885\nEpoch 1678, Loss: 0.2623622715473175\nEpoch 1679, Loss: 0.2623615264892578\nEpoch 1680, Loss: 0.26236239075660706\nEpoch 1681, Loss: 0.2623618543148041\nEpoch 1682, Loss: 0.26236215233802795\nEpoch 1683, Loss: 0.2623623311519623\nEpoch 1684, Loss: 0.26236239075660706\nEpoch 1685, Loss: 0.26236239075660706\nEpoch 1686, Loss: 0.26236221194267273\nEpoch 1687, Loss: 0.26236197352409363\nEpoch 1688, Loss: 0.26236215233802795\nEpoch 1689, Loss: 0.26236164569854736\nEpoch 1690, Loss: 0.2623615860939026\nEpoch 1691, Loss: 0.26236197352409363\nEpoch 1692, Loss: 0.2623622715473175\nEpoch 1693, Loss: 0.2623625099658966\nEpoch 1694, Loss: 0.2623625099658966\nEpoch 1695, Loss: 0.2623625099658966\nEpoch 1696, Loss: 0.2623623311519623\nEpoch 1697, Loss: 0.2623620927333832\nEpoch 1698, Loss: 0.26236197352409363\nEpoch 1699, Loss: 0.2623617947101593\nEpoch 1700, Loss: 0.2623617351055145\nEpoch 1701, Loss: 0.2623637318611145\nEpoch 1702, Loss: 0.2623617947101593\nEpoch 1703, Loss: 0.2623620331287384\nEpoch 1704, Loss: 0.26236215233802795\nEpoch 1705, Loss: 0.26236215233802795\nEpoch 1706, Loss: 0.2623620927333832\nEpoch 1707, Loss: 0.2623618543148041\nEpoch 1708, Loss: 0.26236164569854736\nEpoch 1709, Loss: 0.2623625099658966\nEpoch 1710, Loss: 0.26236191391944885\nEpoch 1711, Loss: 0.2623617947101593\nEpoch 1712, Loss: 0.2623620927333832\nEpoch 1713, Loss: 0.26236239075660706\nEpoch 1714, Loss: 0.26236239075660706\nEpoch 1715, Loss: 0.2623625099658966\nEpoch 1716, Loss: 0.26236239075660706\nEpoch 1717, Loss: 0.26236221194267273\nEpoch 1718, Loss: 0.2623622715473175\nEpoch 1719, Loss: 0.26236191391944885\nEpoch 1720, Loss: 0.2623617947101593\nEpoch 1721, Loss: 0.26236239075660706\nEpoch 1722, Loss: 0.2623620331287384\nEpoch 1723, Loss: 0.2623622715473175\nEpoch 1724, Loss: 0.26236239075660706\nEpoch 1725, Loss: 0.26236239075660706\nEpoch 1726, Loss: 0.2623623311519623\nEpoch 1727, Loss: 0.26236215233802795\nEpoch 1728, Loss: 0.26236191391944885\nEpoch 1729, Loss: 0.2623620927333832\nEpoch 1730, Loss: 0.2623615264892578\nEpoch 1731, Loss: 0.2623630166053772\nEpoch 1732, Loss: 0.2623617947101593\nEpoch 1733, Loss: 0.2623620927333832\nEpoch 1734, Loss: 0.2623622715473175\nEpoch 1735, Loss: 0.2623622715473175\nEpoch 1736, Loss: 0.2623622715473175\nEpoch 1737, Loss: 0.26236215233802795\nEpoch 1738, Loss: 0.2623618543148041\nEpoch 1739, Loss: 0.2623625099658966\nEpoch 1740, Loss: 0.2623615264892578\nEpoch 1741, Loss: 0.26236245036125183\nEpoch 1742, Loss: 0.2623618543148041\nEpoch 1743, Loss: 0.26236215233802795\nEpoch 1744, Loss: 0.2623623311519623\nEpoch 1745, Loss: 0.26236245036125183\nEpoch 1746, Loss: 0.26236239075660706\nEpoch 1747, Loss: 0.2623622715473175\nEpoch 1748, Loss: 0.2623620331287384\nEpoch 1749, Loss: 0.2623622715473175\nEpoch 1750, Loss: 0.2623617351055145\nEpoch 1751, Loss: 0.2623615860939026\nEpoch 1752, Loss: 0.26236197352409363\nEpoch 1753, Loss: 0.2623623311519623\nEpoch 1754, Loss: 0.2623625099658966\nEpoch 1755, Loss: 0.2623625695705414\nEpoch 1756, Loss: 0.2623625099658966\nEpoch 1757, Loss: 0.26236239075660706\nEpoch 1758, Loss: 0.26236215233802795\nEpoch 1759, Loss: 0.2623620331287384\nEpoch 1760, Loss: 0.2623617351055145\nEpoch 1761, Loss: 0.26236164569854736\nEpoch 1762, Loss: 0.26236391067504883\nEpoch 1763, Loss: 0.2623617947101593\nEpoch 1764, Loss: 0.2623620331287384\nEpoch 1765, Loss: 0.26236215233802795\nEpoch 1766, Loss: 0.26236221194267273\nEpoch 1767, Loss: 0.26236215233802795\nEpoch 1768, Loss: 0.26236191391944885\nEpoch 1769, Loss: 0.26236164569854736\nEpoch 1770, Loss: 0.26236262917518616\nEpoch 1771, Loss: 0.2623620331287384\nEpoch 1772, Loss: 0.2623617947101593\nEpoch 1773, Loss: 0.26236215233802795\nEpoch 1774, Loss: 0.26236239075660706\nEpoch 1775, Loss: 0.2623625099658966\nEpoch 1776, Loss: 0.2623625099658966\nEpoch 1777, Loss: 0.26236239075660706\nEpoch 1778, Loss: 0.26236221194267273\nEpoch 1779, Loss: 0.2623623311519623\nEpoch 1780, Loss: 0.26236191391944885\nEpoch 1781, Loss: 0.2623617947101593\nEpoch 1782, Loss: 0.2623625695705414\nEpoch 1783, Loss: 0.2623620331287384\nEpoch 1784, Loss: 0.2623622715473175\nEpoch 1785, Loss: 0.26236239075660706\nEpoch 1786, Loss: 0.26236239075660706\nEpoch 1787, Loss: 0.2623623311519623\nEpoch 1788, Loss: 0.26236215233802795\nEpoch 1789, Loss: 0.26236197352409363\nEpoch 1790, Loss: 0.26236221194267273\nEpoch 1791, Loss: 0.2623615264892578\nEpoch 1792, Loss: 0.2623632550239563\nEpoch 1793, Loss: 0.2623617947101593\nEpoch 1794, Loss: 0.2623620331287384\nEpoch 1795, Loss: 0.2623622715473175\nEpoch 1796, Loss: 0.2623622715473175\nEpoch 1797, Loss: 0.2623622715473175\nEpoch 1798, Loss: 0.26236215233802795\nEpoch 1799, Loss: 0.2623618543148041\nEpoch 1800, Loss: 0.2623625695705414\nEpoch 1801, Loss: 0.2623615264892578\nEpoch 1802, Loss: 0.26236268877983093\nEpoch 1803, Loss: 0.2623618543148041\nEpoch 1804, Loss: 0.26236215233802795\nEpoch 1805, Loss: 0.26236239075660706\nEpoch 1806, Loss: 0.26236245036125183\nEpoch 1807, Loss: 0.26236239075660706\nEpoch 1808, Loss: 0.2623622715473175\nEpoch 1809, Loss: 0.26236197352409363\nEpoch 1810, Loss: 0.2623623311519623\nEpoch 1811, Loss: 0.26236164569854736\nEpoch 1812, Loss: 0.2623617947101593\nEpoch 1813, Loss: 0.26236197352409363\nEpoch 1814, Loss: 0.2623623311519623\nEpoch 1815, Loss: 0.26236245036125183\nEpoch 1816, Loss: 0.2623625695705414\nEpoch 1817, Loss: 0.2623625099658966\nEpoch 1818, Loss: 0.2623623311519623\nEpoch 1819, Loss: 0.26236215233802795\nEpoch 1820, Loss: 0.26236215233802795\nEpoch 1821, Loss: 0.2623617947101593\nEpoch 1822, Loss: 0.26236164569854736\nEpoch 1823, Loss: 0.26236414909362793\nEpoch 1824, Loss: 0.2623617947101593\nEpoch 1825, Loss: 0.2623620927333832\nEpoch 1826, Loss: 0.26236221194267273\nEpoch 1827, Loss: 0.26236215233802795\nEpoch 1828, Loss: 0.26236215233802795\nEpoch 1829, Loss: 0.26236191391944885\nEpoch 1830, Loss: 0.26236164569854736\nEpoch 1831, Loss: 0.2623627483844757\nEpoch 1832, Loss: 0.26236221194267273\nEpoch 1833, Loss: 0.2623617947101593\nEpoch 1834, Loss: 0.26236215233802795\nEpoch 1835, Loss: 0.26236245036125183\nEpoch 1836, Loss: 0.2623625099658966\nEpoch 1837, Loss: 0.2623625695705414\nEpoch 1838, Loss: 0.26236245036125183\nEpoch 1839, Loss: 0.26236221194267273\nEpoch 1840, Loss: 0.26236245036125183\nEpoch 1841, Loss: 0.26236191391944885\nEpoch 1842, Loss: 0.2623617947101593\nEpoch 1843, Loss: 0.26236268877983093\nEpoch 1844, Loss: 0.2623620331287384\nEpoch 1845, Loss: 0.2623622715473175\nEpoch 1846, Loss: 0.26236239075660706\nEpoch 1847, Loss: 0.26236245036125183\nEpoch 1848, Loss: 0.26236239075660706\nEpoch 1849, Loss: 0.26236215233802795\nEpoch 1850, Loss: 0.26236191391944885\nEpoch 1851, Loss: 0.2623623311519623\nEpoch 1852, Loss: 0.26236146688461304\nEpoch 1853, Loss: 0.2623634934425354\nEpoch 1854, Loss: 0.2623617947101593\nEpoch 1855, Loss: 0.2623620927333832\nEpoch 1856, Loss: 0.2623622715473175\nEpoch 1857, Loss: 0.2623623311519623\nEpoch 1858, Loss: 0.2623622715473175\nEpoch 1859, Loss: 0.2623620927333832\nEpoch 1860, Loss: 0.26236191391944885\nEpoch 1861, Loss: 0.26236268877983093\nEpoch 1862, Loss: 0.26236146688461304\nEpoch 1863, Loss: 0.26236286759376526\nEpoch 1864, Loss: 0.2623618543148041\nEpoch 1865, Loss: 0.26236221194267273\nEpoch 1866, Loss: 0.26236239075660706\nEpoch 1867, Loss: 0.26236245036125183\nEpoch 1868, Loss: 0.26236239075660706\nEpoch 1869, Loss: 0.2623622715473175\nEpoch 1870, Loss: 0.2623620331287384\nEpoch 1871, Loss: 0.26236245036125183\nEpoch 1872, Loss: 0.26236164569854736\nEpoch 1873, Loss: 0.26236197352409363\nEpoch 1874, Loss: 0.26236197352409363\nEpoch 1875, Loss: 0.2623622715473175\nEpoch 1876, Loss: 0.2623625695705414\nEpoch 1877, Loss: 0.26236262917518616\nEpoch 1878, Loss: 0.2623625695705414\nEpoch 1879, Loss: 0.26236239075660706\nEpoch 1880, Loss: 0.26236215233802795\nEpoch 1881, Loss: 0.2623622715473175\nEpoch 1882, Loss: 0.2623617947101593\nEpoch 1883, Loss: 0.26236164569854736\nEpoch 1884, Loss: 0.2623644173145294\nEpoch 1885, Loss: 0.2623618543148041\nEpoch 1886, Loss: 0.2623620927333832\nEpoch 1887, Loss: 0.26236215233802795\nEpoch 1888, Loss: 0.26236221194267273\nEpoch 1889, Loss: 0.2623620927333832\nEpoch 1890, Loss: 0.26236191391944885\nEpoch 1891, Loss: 0.26236164569854736\nEpoch 1892, Loss: 0.26236286759376526\nEpoch 1893, Loss: 0.26236239075660706\nEpoch 1894, Loss: 0.2623617351055145\nEpoch 1895, Loss: 0.26236215233802795\nEpoch 1896, Loss: 0.26236245036125183\nEpoch 1897, Loss: 0.2623625695705414\nEpoch 1898, Loss: 0.2623625695705414\nEpoch 1899, Loss: 0.26236245036125183\nEpoch 1900, Loss: 0.2623622715473175\nEpoch 1901, Loss: 0.2623625695705414\nEpoch 1902, Loss: 0.26236191391944885\nEpoch 1903, Loss: 0.2623617947101593\nEpoch 1904, Loss: 0.2623629570007324\nEpoch 1905, Loss: 0.2623620927333832\nEpoch 1906, Loss: 0.2623622715473175\nEpoch 1907, Loss: 0.26236245036125183\nEpoch 1908, Loss: 0.26236245036125183\nEpoch 1909, Loss: 0.26236239075660706\nEpoch 1910, Loss: 0.26236215233802795\nEpoch 1911, Loss: 0.26236191391944885\nEpoch 1912, Loss: 0.26236239075660706\nEpoch 1913, Loss: 0.26236140727996826\nEpoch 1914, Loss: 0.2623636722564697\nEpoch 1915, Loss: 0.2623617351055145\nEpoch 1916, Loss: 0.2623620331287384\nEpoch 1917, Loss: 0.2623622715473175\nEpoch 1918, Loss: 0.2623623311519623\nEpoch 1919, Loss: 0.2623622715473175\nEpoch 1920, Loss: 0.2623620927333832\nEpoch 1921, Loss: 0.26236191391944885\nEpoch 1922, Loss: 0.2623627483844757\nEpoch 1923, Loss: 0.2623615264892578\nEpoch 1924, Loss: 0.26236313581466675\nEpoch 1925, Loss: 0.2623618543148041\nEpoch 1926, Loss: 0.26236221194267273\nEpoch 1927, Loss: 0.26236245036125183\nEpoch 1928, Loss: 0.2623625099658966\nEpoch 1929, Loss: 0.26236239075660706\nEpoch 1930, Loss: 0.2623622715473175\nEpoch 1931, Loss: 0.2623620331287384\nEpoch 1932, Loss: 0.2623625099658966\nEpoch 1933, Loss: 0.26236164569854736\nEpoch 1934, Loss: 0.2623620927333832\nEpoch 1935, Loss: 0.2623620331287384\nEpoch 1936, Loss: 0.26236239075660706\nEpoch 1937, Loss: 0.2623625695705414\nEpoch 1938, Loss: 0.26236268877983093\nEpoch 1939, Loss: 0.2623625099658966\nEpoch 1940, Loss: 0.26236239075660706\nEpoch 1941, Loss: 0.2623620927333832\nEpoch 1942, Loss: 0.2623623311519623\nEpoch 1943, Loss: 0.2623617351055145\nEpoch 1944, Loss: 0.2623615860939026\nEpoch 1945, Loss: 0.2623646557331085\nEpoch 1946, Loss: 0.2623617947101593\nEpoch 1947, Loss: 0.2623620927333832\nEpoch 1948, Loss: 0.26236215233802795\nEpoch 1949, Loss: 0.2623622715473175\nEpoch 1950, Loss: 0.2623620927333832\nEpoch 1951, Loss: 0.26236191391944885\nEpoch 1952, Loss: 0.2623615860939026\nEpoch 1953, Loss: 0.2623615264892578\nEpoch 1954, Loss: 0.262363076210022\nEpoch 1955, Loss: 0.2623620331287384\nEpoch 1956, Loss: 0.26236239075660706\nEpoch 1957, Loss: 0.26236268877983093\nEpoch 1958, Loss: 0.2623627483844757\nEpoch 1959, Loss: 0.2623627483844757\nEpoch 1960, Loss: 0.26236262917518616\nEpoch 1961, Loss: 0.2623623311519623\nEpoch 1962, Loss: 0.26236197352409363\nEpoch 1963, Loss: 0.26236313581466675\nEpoch 1964, Loss: 0.26236146688461304\nEpoch 1965, Loss: 0.26236337423324585\nEpoch 1966, Loss: 0.2623617351055145\nEpoch 1967, Loss: 0.2623620331287384\nEpoch 1968, Loss: 0.26236215233802795\nEpoch 1969, Loss: 0.2623622715473175\nEpoch 1970, Loss: 0.26236221194267273\nEpoch 1971, Loss: 0.2623620331287384\nEpoch 1972, Loss: 0.26236239075660706\nEpoch 1973, Loss: 0.2623617947101593\nEpoch 1974, Loss: 0.26236164569854736\nEpoch 1975, Loss: 0.26236435770988464\nEpoch 1976, Loss: 0.26236191391944885\nEpoch 1977, Loss: 0.2623622715473175\nEpoch 1978, Loss: 0.26236245036125183\nEpoch 1979, Loss: 0.26236245036125183\nEpoch 1980, Loss: 0.26236239075660706\nEpoch 1981, Loss: 0.26236221194267273\nEpoch 1982, Loss: 0.26236191391944885\nEpoch 1983, Loss: 0.26236221194267273\nEpoch 1984, Loss: 0.26236146688461304\nEpoch 1985, Loss: 0.26236361265182495\nEpoch 1986, Loss: 0.2623618543148041\nEpoch 1987, Loss: 0.26236221194267273\nEpoch 1988, Loss: 0.26236239075660706\nEpoch 1989, Loss: 0.26236245036125183\nEpoch 1990, Loss: 0.26236239075660706\nEpoch 1991, Loss: 0.2623622715473175\nEpoch 1992, Loss: 0.26236197352409363\nEpoch 1993, Loss: 0.26236268877983093\nEpoch 1994, Loss: 0.2623615264892578\nEpoch 1995, Loss: 0.26236262917518616\nEpoch 1996, Loss: 0.26236197352409363\nEpoch 1997, Loss: 0.2623622715473175\nEpoch 1998, Loss: 0.2623625099658966\nEpoch 1999, Loss: 0.2623625695705414\nEpoch 2000, Loss: 0.2623625695705414\nEpoch 2001, Loss: 0.26236239075660706\nEpoch 2002, Loss: 0.26236215233802795\nEpoch 2003, Loss: 0.26236262917518616\nEpoch 2004, Loss: 0.26236164569854736\nEpoch 2005, Loss: 0.2623617351055145\nEpoch 2006, Loss: 0.2623620927333832\nEpoch 2007, Loss: 0.26236245036125183\nEpoch 2008, Loss: 0.26236262917518616\nEpoch 2009, Loss: 0.26236268877983093\nEpoch 2010, Loss: 0.26236268877983093\nEpoch 2011, Loss: 0.26236245036125183\nEpoch 2012, Loss: 0.2623622715473175\nEpoch 2013, Loss: 0.26236245036125183\nEpoch 2014, Loss: 0.2623617947101593\nEpoch 2015, Loss: 0.26236164569854736\nEpoch 2016, Loss: 0.26236459612846375\nEpoch 2017, Loss: 0.2623617947101593\nEpoch 2018, Loss: 0.2623620927333832\nEpoch 2019, Loss: 0.26236221194267273\nEpoch 2020, Loss: 0.26236221194267273\nEpoch 2021, Loss: 0.26236215233802795\nEpoch 2022, Loss: 0.26236191391944885\nEpoch 2023, Loss: 0.2623618543148041\nEpoch 2024, Loss: 0.2623615860939026\nEpoch 2025, Loss: 0.2623629570007324\nEpoch 2026, Loss: 0.2623620331287384\nEpoch 2027, Loss: 0.2623625099658966\nEpoch 2028, Loss: 0.26236268877983093\nEpoch 2029, Loss: 0.2623627483844757\nEpoch 2030, Loss: 0.2623627483844757\nEpoch 2031, Loss: 0.26236262917518616\nEpoch 2032, Loss: 0.26236239075660706\nEpoch 2033, Loss: 0.26236197352409363\nEpoch 2034, Loss: 0.26236337423324585\nEpoch 2035, Loss: 0.26236140727996826\nEpoch 2036, Loss: 0.2623634338378906\nEpoch 2037, Loss: 0.26236164569854736\nEpoch 2038, Loss: 0.2623620331287384\nEpoch 2039, Loss: 0.26236221194267273\nEpoch 2040, Loss: 0.2623623311519623\nEpoch 2041, Loss: 0.26236215233802795\nEpoch 2042, Loss: 0.2623620331287384\nEpoch 2043, Loss: 0.2623625099658966\nEpoch 2044, Loss: 0.2623617351055145\nEpoch 2045, Loss: 0.26236164569854736\nEpoch 2046, Loss: 0.26236459612846375\nEpoch 2047, Loss: 0.26236197352409363\nEpoch 2048, Loss: 0.2623622715473175\nEpoch 2049, Loss: 0.26236245036125183\nEpoch 2050, Loss: 0.26236245036125183\nEpoch 2051, Loss: 0.26236239075660706\nEpoch 2052, Loss: 0.26236221194267273\nEpoch 2053, Loss: 0.26236191391944885\nEpoch 2054, Loss: 0.26236239075660706\nEpoch 2055, Loss: 0.26236140727996826\nEpoch 2056, Loss: 0.2623637914657593\nEpoch 2057, Loss: 0.26236191391944885\nEpoch 2058, Loss: 0.26236221194267273\nEpoch 2059, Loss: 0.26236239075660706\nEpoch 2060, Loss: 0.26236245036125183\nEpoch 2061, Loss: 0.26236239075660706\nEpoch 2062, Loss: 0.2623622715473175\nEpoch 2063, Loss: 0.26236197352409363\nEpoch 2064, Loss: 0.26236286759376526\nEpoch 2065, Loss: 0.2623615264892578\nEpoch 2066, Loss: 0.2623628079891205\nEpoch 2067, Loss: 0.26236197352409363\nEpoch 2068, Loss: 0.2623623311519623\nEpoch 2069, Loss: 0.2623625695705414\nEpoch 2070, Loss: 0.2623625695705414\nEpoch 2071, Loss: 0.2623625099658966\nEpoch 2072, Loss: 0.26236245036125183\nEpoch 2073, Loss: 0.2623620927333832\nEpoch 2074, Loss: 0.2623627483844757\nEpoch 2075, Loss: 0.26236164569854736\nEpoch 2076, Loss: 0.2623618543148041\nEpoch 2077, Loss: 0.2623620331287384\nEpoch 2078, Loss: 0.26236245036125183\nEpoch 2079, Loss: 0.26236262917518616\nEpoch 2080, Loss: 0.2623627483844757\nEpoch 2081, Loss: 0.2623627483844757\nEpoch 2082, Loss: 0.2623625099658966\nEpoch 2083, Loss: 0.26236221194267273\nEpoch 2084, Loss: 0.26236262917518616\nEpoch 2085, Loss: 0.2623617947101593\nEpoch 2086, Loss: 0.2623615860939026\nEpoch 2087, Loss: 0.2623648941516876\nEpoch 2088, Loss: 0.2623617947101593\nEpoch 2089, Loss: 0.2623620927333832\nEpoch 2090, Loss: 0.2623622715473175\nEpoch 2091, Loss: 0.26236221194267273\nEpoch 2092, Loss: 0.26236215233802795\nEpoch 2093, Loss: 0.26236191391944885\nEpoch 2094, Loss: 0.26236191391944885\nEpoch 2095, Loss: 0.2623615860939026\nEpoch 2096, Loss: 0.2623632550239563\nEpoch 2097, Loss: 0.2623620331287384\nEpoch 2098, Loss: 0.2623625099658966\nEpoch 2099, Loss: 0.2623627483844757\nEpoch 2100, Loss: 0.2623627483844757\nEpoch 2101, Loss: 0.2623628079891205\nEpoch 2102, Loss: 0.26236262917518616\nEpoch 2103, Loss: 0.2623623311519623\nEpoch 2104, Loss: 0.2623620331287384\nEpoch 2105, Loss: 0.26236197352409363\nEpoch 2106, Loss: 0.2623617947101593\nEpoch 2107, Loss: 0.26236414909362793\nEpoch 2108, Loss: 0.26236197352409363\nEpoch 2109, Loss: 0.2623622715473175\nEpoch 2110, Loss: 0.26236239075660706\nEpoch 2111, Loss: 0.26236245036125183\nEpoch 2112, Loss: 0.2623622715473175\nEpoch 2113, Loss: 0.2623620927333832\nEpoch 2114, Loss: 0.2623617947101593\nEpoch 2115, Loss: 0.2623631954193115\nEpoch 2116, Loss: 0.2623617351055145\nEpoch 2117, Loss: 0.26236191391944885\nEpoch 2118, Loss: 0.26236239075660706\nEpoch 2119, Loss: 0.26236262917518616\nEpoch 2120, Loss: 0.2623628079891205\nEpoch 2121, Loss: 0.2623627483844757\nEpoch 2122, Loss: 0.26236262917518616\nEpoch 2123, Loss: 0.26236239075660706\nEpoch 2124, Loss: 0.262363076210022\nEpoch 2125, Loss: 0.2623620331287384\nEpoch 2126, Loss: 0.26236191391944885\nEpoch 2127, Loss: 0.26236268877983093\nEpoch 2128, Loss: 0.2623620927333832\nEpoch 2129, Loss: 0.26236245036125183\nEpoch 2130, Loss: 0.26236262917518616\nEpoch 2131, Loss: 0.2623625695705414\nEpoch 2132, Loss: 0.2623625099658966\nEpoch 2133, Loss: 0.2623622715473175\nEpoch 2134, Loss: 0.26236197352409363\nEpoch 2135, Loss: 0.262363076210022\nEpoch 2136, Loss: 0.26236140727996826\nEpoch 2137, Loss: 0.26236391067504883\nEpoch 2138, Loss: 0.2623617947101593\nEpoch 2139, Loss: 0.26236215233802795\nEpoch 2140, Loss: 0.2623623311519623\nEpoch 2141, Loss: 0.26236245036125183\nEpoch 2142, Loss: 0.26236239075660706\nEpoch 2143, Loss: 0.26236215233802795\nEpoch 2144, Loss: 0.26236215233802795\nEpoch 2145, Loss: 0.26236191391944885\nEpoch 2146, Loss: 0.2623617947101593\nEpoch 2147, Loss: 0.26236385107040405\nEpoch 2148, Loss: 0.26236215233802795\nEpoch 2149, Loss: 0.2623625099658966\nEpoch 2150, Loss: 0.2623627483844757\nEpoch 2151, Loss: 0.2623627483844757\nEpoch 2152, Loss: 0.26236262917518616\nEpoch 2153, Loss: 0.26236239075660706\nEpoch 2154, Loss: 0.2623620927333832\nEpoch 2155, Loss: 0.2623623311519623\nEpoch 2156, Loss: 0.26236164569854736\nEpoch 2157, Loss: 0.26236313581466675\nEpoch 2158, Loss: 0.26236197352409363\nEpoch 2159, Loss: 0.26236239075660706\nEpoch 2160, Loss: 0.2623625695705414\nEpoch 2161, Loss: 0.26236262917518616\nEpoch 2162, Loss: 0.2623625695705414\nEpoch 2163, Loss: 0.2623623311519623\nEpoch 2164, Loss: 0.2623620927333832\nEpoch 2165, Loss: 0.26236313581466675\nEpoch 2166, Loss: 0.2623615860939026\nEpoch 2167, Loss: 0.26236245036125183\nEpoch 2168, Loss: 0.2623620331287384\nEpoch 2169, Loss: 0.26236239075660706\nEpoch 2170, Loss: 0.26236262917518616\nEpoch 2171, Loss: 0.26236268877983093\nEpoch 2172, Loss: 0.26236262917518616\nEpoch 2173, Loss: 0.26236245036125183\nEpoch 2174, Loss: 0.26236215233802795\nEpoch 2175, Loss: 0.26236313581466675\nEpoch 2176, Loss: 0.2623617351055145\nEpoch 2177, Loss: 0.2623617351055145\nEpoch 2178, Loss: 0.26236215233802795\nEpoch 2179, Loss: 0.2623625099658966\nEpoch 2180, Loss: 0.26236268877983093\nEpoch 2181, Loss: 0.2623628079891205\nEpoch 2182, Loss: 0.2623627483844757\nEpoch 2183, Loss: 0.2623625695705414\nEpoch 2184, Loss: 0.26236221194267273\nEpoch 2185, Loss: 0.262363076210022\nEpoch 2186, Loss: 0.2623617947101593\nEpoch 2187, Loss: 0.2623615860939026\nEpoch 2188, Loss: 0.2623651325702667\nEpoch 2189, Loss: 0.2623617947101593\nEpoch 2190, Loss: 0.26236215233802795\nEpoch 2191, Loss: 0.2623622715473175\nEpoch 2192, Loss: 0.2623622715473175\nEpoch 2193, Loss: 0.26236215233802795\nEpoch 2194, Loss: 0.2623618543148041\nEpoch 2195, Loss: 0.26236221194267273\nEpoch 2196, Loss: 0.2623615860939026\nEpoch 2197, Loss: 0.2623634338378906\nEpoch 2198, Loss: 0.2623620927333832\nEpoch 2199, Loss: 0.2623625099658966\nEpoch 2200, Loss: 0.2623628079891205\nEpoch 2201, Loss: 0.26236286759376526\nEpoch 2202, Loss: 0.26236286759376526\nEpoch 2203, Loss: 0.26236268877983093\nEpoch 2204, Loss: 0.26236239075660706\nEpoch 2205, Loss: 0.26236221194267273\nEpoch 2206, Loss: 0.26236191391944885\nEpoch 2207, Loss: 0.2623617947101593\nEpoch 2208, Loss: 0.2623644769191742\nEpoch 2209, Loss: 0.26236191391944885\nEpoch 2210, Loss: 0.2623623311519623\nEpoch 2211, Loss: 0.26236245036125183\nEpoch 2212, Loss: 0.26236245036125183\nEpoch 2213, Loss: 0.2623623311519623\nEpoch 2214, Loss: 0.26236215233802795\nEpoch 2215, Loss: 0.2623618543148041\nEpoch 2216, Loss: 0.26236164569854736\nEpoch 2217, Loss: 0.26236245036125183\nEpoch 2218, Loss: 0.26236221194267273\nEpoch 2219, Loss: 0.26236268877983093\nEpoch 2220, Loss: 0.2623629570007324\nEpoch 2221, Loss: 0.262363076210022\nEpoch 2222, Loss: 0.262363076210022\nEpoch 2223, Loss: 0.26236286759376526\nEpoch 2224, Loss: 0.26236245036125183\nEpoch 2225, Loss: 0.2623622715473175\nEpoch 2226, Loss: 0.2623620331287384\nEpoch 2227, Loss: 0.26236191391944885\nEpoch 2228, Loss: 0.2623637318611145\nEpoch 2229, Loss: 0.2623620331287384\nEpoch 2230, Loss: 0.26236239075660706\nEpoch 2231, Loss: 0.2623625099658966\nEpoch 2232, Loss: 0.2623625099658966\nEpoch 2233, Loss: 0.26236239075660706\nEpoch 2234, Loss: 0.26236215233802795\nEpoch 2235, Loss: 0.2623620927333832\nEpoch 2236, Loss: 0.2623617947101593\nEpoch 2237, Loss: 0.2623620331287384\nEpoch 2238, Loss: 0.2623622715473175\nEpoch 2239, Loss: 0.26236268877983093\nEpoch 2240, Loss: 0.2623629570007324\nEpoch 2241, Loss: 0.262363076210022\nEpoch 2242, Loss: 0.2623630166053772\nEpoch 2243, Loss: 0.2623628079891205\nEpoch 2244, Loss: 0.2623625099658966\nEpoch 2245, Loss: 0.2623625099658966\nEpoch 2246, Loss: 0.2623620331287384\nEpoch 2247, Loss: 0.26236191391944885\nEpoch 2248, Loss: 0.2623634934425354\nEpoch 2249, Loss: 0.2623620927333832\nEpoch 2250, Loss: 0.26236239075660706\nEpoch 2251, Loss: 0.2623625695705414\nEpoch 2252, Loss: 0.2623625099658966\nEpoch 2253, Loss: 0.2623625099658966\nEpoch 2254, Loss: 0.26236215233802795\nEpoch 2255, Loss: 0.26236221194267273\nEpoch 2256, Loss: 0.2623617947101593\nEpoch 2257, Loss: 0.2623617947101593\nEpoch 2258, Loss: 0.2623623311519623\nEpoch 2259, Loss: 0.2623627483844757\nEpoch 2260, Loss: 0.2623630166053772\nEpoch 2261, Loss: 0.26236313581466675\nEpoch 2262, Loss: 0.262363076210022\nEpoch 2263, Loss: 0.2623628079891205\nEpoch 2264, Loss: 0.2623625695705414\nEpoch 2265, Loss: 0.26236262917518616\nEpoch 2266, Loss: 0.2623620927333832\nEpoch 2267, Loss: 0.26236191391944885\nEpoch 2268, Loss: 0.2623634338378906\nEpoch 2269, Loss: 0.2623620927333832\nEpoch 2270, Loss: 0.26236239075660706\nEpoch 2271, Loss: 0.2623625695705414\nEpoch 2272, Loss: 0.2623625695705414\nEpoch 2273, Loss: 0.26236239075660706\nEpoch 2274, Loss: 0.26236221194267273\nEpoch 2275, Loss: 0.2623623311519623\nEpoch 2276, Loss: 0.2623618543148041\nEpoch 2277, Loss: 0.2623617351055145\nEpoch 2278, Loss: 0.2623622715473175\nEpoch 2279, Loss: 0.2623627483844757\nEpoch 2280, Loss: 0.262363076210022\nEpoch 2281, Loss: 0.26236313581466675\nEpoch 2282, Loss: 0.26236313581466675\nEpoch 2283, Loss: 0.26236286759376526\nEpoch 2284, Loss: 0.2623625695705414\nEpoch 2285, Loss: 0.26236268877983093\nEpoch 2286, Loss: 0.2623620927333832\nEpoch 2287, Loss: 0.26236191391944885\nEpoch 2288, Loss: 0.26236337423324585\nEpoch 2289, Loss: 0.2623620927333832\nEpoch 2290, Loss: 0.26236239075660706\nEpoch 2291, Loss: 0.2623625099658966\nEpoch 2292, Loss: 0.2623625695705414\nEpoch 2293, Loss: 0.26236245036125183\nEpoch 2294, Loss: 0.26236215233802795\nEpoch 2295, Loss: 0.2623623311519623\nEpoch 2296, Loss: 0.2623617947101593\nEpoch 2297, Loss: 0.2623617947101593\nEpoch 2298, Loss: 0.2623622715473175\nEpoch 2299, Loss: 0.2623627483844757\nEpoch 2300, Loss: 0.2623630166053772\nEpoch 2301, Loss: 0.26236313581466675\nEpoch 2302, Loss: 0.262363076210022\nEpoch 2303, Loss: 0.26236286759376526\nEpoch 2304, Loss: 0.2623625099658966\nEpoch 2305, Loss: 0.2623627483844757\nEpoch 2306, Loss: 0.2623620927333832\nEpoch 2307, Loss: 0.26236191391944885\nEpoch 2308, Loss: 0.2623634934425354\nEpoch 2309, Loss: 0.2623620927333832\nEpoch 2310, Loss: 0.26236245036125183\nEpoch 2311, Loss: 0.26236262917518616\nEpoch 2312, Loss: 0.2623625695705414\nEpoch 2313, Loss: 0.26236245036125183\nEpoch 2314, Loss: 0.26236221194267273\nEpoch 2315, Loss: 0.2623623311519623\nEpoch 2316, Loss: 0.2623617947101593\nEpoch 2317, Loss: 0.2623618543148041\nEpoch 2318, Loss: 0.2623623311519623\nEpoch 2319, Loss: 0.2623627483844757\nEpoch 2320, Loss: 0.262363076210022\nEpoch 2321, Loss: 0.26236313581466675\nEpoch 2322, Loss: 0.26236313581466675\nEpoch 2323, Loss: 0.26236286759376526\nEpoch 2324, Loss: 0.2623625695705414\nEpoch 2325, Loss: 0.2623627483844757\nEpoch 2326, Loss: 0.2623620927333832\nEpoch 2327, Loss: 0.26236191391944885\nEpoch 2328, Loss: 0.2623635530471802\nEpoch 2329, Loss: 0.2623620927333832\nEpoch 2330, Loss: 0.26236239075660706\nEpoch 2331, Loss: 0.2623625695705414\nEpoch 2332, Loss: 0.2623625695705414\nEpoch 2333, Loss: 0.26236245036125183\nEpoch 2334, Loss: 0.26236215233802795\nEpoch 2335, Loss: 0.26236239075660706\nEpoch 2336, Loss: 0.2623617947101593\nEpoch 2337, Loss: 0.26236191391944885\nEpoch 2338, Loss: 0.2623623311519623\nEpoch 2339, Loss: 0.2623627483844757\nEpoch 2340, Loss: 0.262363076210022\nEpoch 2341, Loss: 0.26236313581466675\nEpoch 2342, Loss: 0.26236313581466675\nEpoch 2343, Loss: 0.26236286759376526\nEpoch 2344, Loss: 0.2623625695705414\nEpoch 2345, Loss: 0.2623627483844757\nEpoch 2346, Loss: 0.2623620331287384\nEpoch 2347, Loss: 0.2623618543148041\nEpoch 2348, Loss: 0.2623636722564697\nEpoch 2349, Loss: 0.2623620927333832\nEpoch 2350, Loss: 0.26236239075660706\nEpoch 2351, Loss: 0.2623625695705414\nEpoch 2352, Loss: 0.26236262917518616\nEpoch 2353, Loss: 0.26236245036125183\nEpoch 2354, Loss: 0.26236215233802795\nEpoch 2355, Loss: 0.26236245036125183\nEpoch 2356, Loss: 0.2623617947101593\nEpoch 2357, Loss: 0.26236197352409363\nEpoch 2358, Loss: 0.2623622715473175\nEpoch 2359, Loss: 0.2623627483844757\nEpoch 2360, Loss: 0.262363076210022\nEpoch 2361, Loss: 0.2623631954193115\nEpoch 2362, Loss: 0.26236313581466675\nEpoch 2363, Loss: 0.26236286759376526\nEpoch 2364, Loss: 0.2623625099658966\nEpoch 2365, Loss: 0.2623628079891205\nEpoch 2366, Loss: 0.2623620331287384\nEpoch 2367, Loss: 0.26236191391944885\nEpoch 2368, Loss: 0.2623637914657593\nEpoch 2369, Loss: 0.2623620331287384\nEpoch 2370, Loss: 0.26236245036125183\nEpoch 2371, Loss: 0.26236262917518616\nEpoch 2372, Loss: 0.2623625695705414\nEpoch 2373, Loss: 0.26236245036125183\nEpoch 2374, Loss: 0.26236215233802795\nEpoch 2375, Loss: 0.2623625099658966\nEpoch 2376, Loss: 0.2623617947101593\nEpoch 2377, Loss: 0.2623620331287384\nEpoch 2378, Loss: 0.2623622715473175\nEpoch 2379, Loss: 0.2623627483844757\nEpoch 2380, Loss: 0.26236313581466675\nEpoch 2381, Loss: 0.2623632550239563\nEpoch 2382, Loss: 0.26236313581466675\nEpoch 2383, Loss: 0.26236286759376526\nEpoch 2384, Loss: 0.26236262917518616\nEpoch 2385, Loss: 0.26236286759376526\nEpoch 2386, Loss: 0.2623620331287384\nEpoch 2387, Loss: 0.2623618543148041\nEpoch 2388, Loss: 0.26236385107040405\nEpoch 2389, Loss: 0.2623620927333832\nEpoch 2390, Loss: 0.26236239075660706\nEpoch 2391, Loss: 0.26236262917518616\nEpoch 2392, Loss: 0.26236262917518616\nEpoch 2393, Loss: 0.26236245036125183\nEpoch 2394, Loss: 0.26236215233802795\nEpoch 2395, Loss: 0.2623625099658966\nEpoch 2396, Loss: 0.2623617947101593\nEpoch 2397, Loss: 0.26236215233802795\nEpoch 2398, Loss: 0.2623623311519623\nEpoch 2399, Loss: 0.2623627483844757\nEpoch 2400, Loss: 0.26236313581466675\nEpoch 2401, Loss: 0.2623631954193115\nEpoch 2402, Loss: 0.2623631954193115\nEpoch 2403, Loss: 0.2623629570007324\nEpoch 2404, Loss: 0.2623625695705414\nEpoch 2405, Loss: 0.2623629570007324\nEpoch 2406, Loss: 0.2623620331287384\nEpoch 2407, Loss: 0.2623618543148041\nEpoch 2408, Loss: 0.2623639702796936\nEpoch 2409, Loss: 0.2623620927333832\nEpoch 2410, Loss: 0.26236239075660706\nEpoch 2411, Loss: 0.2623625695705414\nEpoch 2412, Loss: 0.2623625695705414\nEpoch 2413, Loss: 0.26236239075660706\nEpoch 2414, Loss: 0.26236215233802795\nEpoch 2415, Loss: 0.2623625695705414\nEpoch 2416, Loss: 0.2623617947101593\nEpoch 2417, Loss: 0.26236221194267273\nEpoch 2418, Loss: 0.2623622715473175\nEpoch 2419, Loss: 0.2623628079891205\nEpoch 2420, Loss: 0.26236313581466675\nEpoch 2421, Loss: 0.2623632550239563\nEpoch 2422, Loss: 0.26236313581466675\nEpoch 2423, Loss: 0.2623629570007324\nEpoch 2424, Loss: 0.2623625695705414\nEpoch 2425, Loss: 0.2623630166053772\nEpoch 2426, Loss: 0.2623620331287384\nEpoch 2427, Loss: 0.2623618543148041\nEpoch 2428, Loss: 0.2623640298843384\nEpoch 2429, Loss: 0.2623620331287384\nEpoch 2430, Loss: 0.26236245036125183\nEpoch 2431, Loss: 0.26236262917518616\nEpoch 2432, Loss: 0.2623625695705414\nEpoch 2433, Loss: 0.26236239075660706\nEpoch 2434, Loss: 0.26236221194267273\nEpoch 2435, Loss: 0.26236262917518616\nEpoch 2436, Loss: 0.2623617351055145\nEpoch 2437, Loss: 0.2623622715473175\nEpoch 2438, Loss: 0.2623622715473175\nEpoch 2439, Loss: 0.2623627483844757\nEpoch 2440, Loss: 0.262363076210022\nEpoch 2441, Loss: 0.2623632550239563\nEpoch 2442, Loss: 0.26236313581466675\nEpoch 2443, Loss: 0.2623629570007324\nEpoch 2444, Loss: 0.26236262917518616\nEpoch 2445, Loss: 0.2623630166053772\nEpoch 2446, Loss: 0.2623620927333832\nEpoch 2447, Loss: 0.2623618543148041\nEpoch 2448, Loss: 0.26236408948898315\nEpoch 2449, Loss: 0.2623620331287384\nEpoch 2450, Loss: 0.26236245036125183\nEpoch 2451, Loss: 0.2623625695705414\nEpoch 2452, Loss: 0.26236262917518616\nEpoch 2453, Loss: 0.26236239075660706\nEpoch 2454, Loss: 0.26236215233802795\nEpoch 2455, Loss: 0.26236262917518616\nEpoch 2456, Loss: 0.2623617351055145\nEpoch 2457, Loss: 0.26236239075660706\nEpoch 2458, Loss: 0.2623623311519623\nEpoch 2459, Loss: 0.2623628079891205\nEpoch 2460, Loss: 0.26236313581466675\nEpoch 2461, Loss: 0.2623632550239563\nEpoch 2462, Loss: 0.2623631954193115\nEpoch 2463, Loss: 0.26236286759376526\nEpoch 2464, Loss: 0.2623625695705414\nEpoch 2465, Loss: 0.262363076210022\nEpoch 2466, Loss: 0.2623620331287384\nEpoch 2467, Loss: 0.2623618543148041\nEpoch 2468, Loss: 0.2623642385005951\nEpoch 2469, Loss: 0.2623620927333832\nEpoch 2470, Loss: 0.26236245036125183\nEpoch 2471, Loss: 0.2623625695705414\nEpoch 2472, Loss: 0.2623625695705414\nEpoch 2473, Loss: 0.26236245036125183\nEpoch 2474, Loss: 0.26236215233802795\nEpoch 2475, Loss: 0.26236262917518616\nEpoch 2476, Loss: 0.2623617947101593\nEpoch 2477, Loss: 0.26236245036125183\nEpoch 2478, Loss: 0.2623623311519623\nEpoch 2479, Loss: 0.26236286759376526\nEpoch 2480, Loss: 0.26236313581466675\nEpoch 2481, Loss: 0.2623632550239563\nEpoch 2482, Loss: 0.26236313581466675\nEpoch 2483, Loss: 0.2623629570007324\nEpoch 2484, Loss: 0.2623625695705414\nEpoch 2485, Loss: 0.26236313581466675\nEpoch 2486, Loss: 0.2623620331287384\nEpoch 2487, Loss: 0.2623618543148041\nEpoch 2488, Loss: 0.26236435770988464\nEpoch 2489, Loss: 0.2623620927333832\nEpoch 2490, Loss: 0.26236239075660706\nEpoch 2491, Loss: 0.26236262917518616\nEpoch 2492, Loss: 0.26236262917518616\nEpoch 2493, Loss: 0.26236245036125183\nEpoch 2494, Loss: 0.26236215233802795\nEpoch 2495, Loss: 0.2623627483844757\nEpoch 2496, Loss: 0.2623617351055145\nEpoch 2497, Loss: 0.26236245036125183\nEpoch 2498, Loss: 0.2623623311519623\nEpoch 2499, Loss: 0.2623628079891205\nEpoch 2500, Loss: 0.26236313581466675\nEpoch 2501, Loss: 0.2623632550239563\nEpoch 2502, Loss: 0.26236313581466675\nEpoch 2503, Loss: 0.2623629570007324\nEpoch 2504, Loss: 0.26236262917518616\nEpoch 2505, Loss: 0.26236313581466675\nEpoch 2506, Loss: 0.2623620331287384\nEpoch 2507, Loss: 0.2623617947101593\nEpoch 2508, Loss: 0.2623644769191742\nEpoch 2509, Loss: 0.2623620331287384\nEpoch 2510, Loss: 0.26236245036125183\nEpoch 2511, Loss: 0.26236262917518616\nEpoch 2512, Loss: 0.26236262917518616\nEpoch 2513, Loss: 0.26236245036125183\nEpoch 2514, Loss: 0.26236215233802795\nEpoch 2515, Loss: 0.2623627483844757\nEpoch 2516, Loss: 0.2623617351055145\nEpoch 2517, Loss: 0.2623625695705414\nEpoch 2518, Loss: 0.2623622715473175\nEpoch 2519, Loss: 0.26236286759376526\nEpoch 2520, Loss: 0.2623631954193115\nEpoch 2521, Loss: 0.2623632550239563\nEpoch 2522, Loss: 0.2623631954193115\nEpoch 2523, Loss: 0.2623629570007324\nEpoch 2524, Loss: 0.2623625695705414\nEpoch 2525, Loss: 0.2623631954193115\nEpoch 2526, Loss: 0.2623620331287384\nEpoch 2527, Loss: 0.2623617947101593\nEpoch 2528, Loss: 0.26236453652381897\nEpoch 2529, Loss: 0.2623620331287384\nEpoch 2530, Loss: 0.26236239075660706\nEpoch 2531, Loss: 0.2623625695705414\nEpoch 2532, Loss: 0.26236262917518616\nEpoch 2533, Loss: 0.26236245036125183\nEpoch 2534, Loss: 0.26236215233802795\nEpoch 2535, Loss: 0.2623628079891205\nEpoch 2536, Loss: 0.26236164569854736\nEpoch 2537, Loss: 0.26236268877983093\nEpoch 2538, Loss: 0.2623622715473175\nEpoch 2539, Loss: 0.2623628079891205\nEpoch 2540, Loss: 0.26236313581466675\nEpoch 2541, Loss: 0.2623632550239563\nEpoch 2542, Loss: 0.2623631954193115\nEpoch 2543, Loss: 0.2623629570007324\nEpoch 2544, Loss: 0.2623625695705414\nEpoch 2545, Loss: 0.2623632550239563\nEpoch 2546, Loss: 0.2623620331287384\nEpoch 2547, Loss: 0.2623617947101593\nEpoch 2548, Loss: 0.2623647153377533\nEpoch 2549, Loss: 0.2623620331287384\nEpoch 2550, Loss: 0.26236245036125183\nEpoch 2551, Loss: 0.26236262917518616\nEpoch 2552, Loss: 0.26236262917518616\nEpoch 2553, Loss: 0.26236245036125183\nEpoch 2554, Loss: 0.26236215233802795\nEpoch 2555, Loss: 0.26236286759376526\nEpoch 2556, Loss: 0.2623617351055145\nEpoch 2557, Loss: 0.2623627483844757\nEpoch 2558, Loss: 0.2623622715473175\nEpoch 2559, Loss: 0.2623628079891205\nEpoch 2560, Loss: 0.2623631954193115\nEpoch 2561, Loss: 0.2623632550239563\nEpoch 2562, Loss: 0.2623631954193115\nEpoch 2563, Loss: 0.2623630166053772\nEpoch 2564, Loss: 0.26236262917518616\nEpoch 2565, Loss: 0.2623632550239563\nEpoch 2566, Loss: 0.2623620331287384\nEpoch 2567, Loss: 0.2623617947101593\nEpoch 2568, Loss: 0.26236477494239807\nEpoch 2569, Loss: 0.2623620927333832\nEpoch 2570, Loss: 0.26236239075660706\nEpoch 2571, Loss: 0.2623625695705414\nEpoch 2572, Loss: 0.26236262917518616\nEpoch 2573, Loss: 0.2623625099658966\nEpoch 2574, Loss: 0.26236215233802795\nEpoch 2575, Loss: 0.26236286759376526\nEpoch 2576, Loss: 0.26236164569854736\nEpoch 2577, Loss: 0.2623628079891205\nEpoch 2578, Loss: 0.2623623311519623\nEpoch 2579, Loss: 0.26236286759376526\nEpoch 2580, Loss: 0.26236313581466675\nEpoch 2581, Loss: 0.2623633146286011\nEpoch 2582, Loss: 0.2623632550239563\nEpoch 2583, Loss: 0.2623630166053772\nEpoch 2584, Loss: 0.2623625695705414\nEpoch 2585, Loss: 0.2623633146286011\nEpoch 2586, Loss: 0.2623620331287384\nEpoch 2587, Loss: 0.2623618543148041\nEpoch 2588, Loss: 0.2623648941516876\nEpoch 2589, Loss: 0.2623620331287384\nEpoch 2590, Loss: 0.26236245036125183\nEpoch 2591, Loss: 0.2623625695705414\nEpoch 2592, Loss: 0.26236262917518616\nEpoch 2593, Loss: 0.26236245036125183\nEpoch 2594, Loss: 0.26236215233802795\nEpoch 2595, Loss: 0.2623629570007324\nEpoch 2596, Loss: 0.2623617351055145\nEpoch 2597, Loss: 0.2623629570007324\nEpoch 2598, Loss: 0.2623622715473175\nEpoch 2599, Loss: 0.2623628079891205\nEpoch 2600, Loss: 0.2623632550239563\nEpoch 2601, Loss: 0.2623633146286011\nEpoch 2602, Loss: 0.2623632550239563\nEpoch 2603, Loss: 0.2623630166053772\nEpoch 2604, Loss: 0.26236262917518616\nEpoch 2605, Loss: 0.26236337423324585\nEpoch 2606, Loss: 0.2623620331287384\nEpoch 2607, Loss: 0.2623617947101593\nEpoch 2608, Loss: 0.2623649537563324\nEpoch 2609, Loss: 0.2623620927333832\nEpoch 2610, Loss: 0.26236245036125183\nEpoch 2611, Loss: 0.26236262917518616\nEpoch 2612, Loss: 0.26236262917518616\nEpoch 2613, Loss: 0.26236245036125183\nEpoch 2614, Loss: 0.26236215233802795\nEpoch 2615, Loss: 0.2623629570007324\nEpoch 2616, Loss: 0.2623617351055145\nEpoch 2617, Loss: 0.262363076210022\nEpoch 2618, Loss: 0.2623623311519623\nEpoch 2619, Loss: 0.26236286759376526\nEpoch 2620, Loss: 0.2623631954193115\nEpoch 2621, Loss: 0.26236337423324585\nEpoch 2622, Loss: 0.2623632550239563\nEpoch 2623, Loss: 0.2623630166053772\nEpoch 2624, Loss: 0.26236262917518616\nEpoch 2625, Loss: 0.26236337423324585\nEpoch 2626, Loss: 0.26236197352409363\nEpoch 2627, Loss: 0.2623617947101593\nEpoch 2628, Loss: 0.2623650133609772\nEpoch 2629, Loss: 0.2623620331287384\nEpoch 2630, Loss: 0.26236239075660706\nEpoch 2631, Loss: 0.26236262917518616\nEpoch 2632, Loss: 0.2623625695705414\nEpoch 2633, Loss: 0.26236245036125183\nEpoch 2634, Loss: 0.26236215233802795\nEpoch 2635, Loss: 0.262363076210022\nEpoch 2636, Loss: 0.26236164569854736\nEpoch 2637, Loss: 0.26236313581466675\nEpoch 2638, Loss: 0.2623623311519623\nEpoch 2639, Loss: 0.2623628079891205\nEpoch 2640, Loss: 0.2623632550239563\nEpoch 2641, Loss: 0.2623633146286011\nEpoch 2642, Loss: 0.2623632550239563\nEpoch 2643, Loss: 0.2623630166053772\nEpoch 2644, Loss: 0.2623625695705414\nEpoch 2645, Loss: 0.2623634338378906\nEpoch 2646, Loss: 0.26236197352409363\nEpoch 2647, Loss: 0.2623617947101593\nEpoch 2648, Loss: 0.2623651325702667\nEpoch 2649, Loss: 0.2623620927333832\nEpoch 2650, Loss: 0.26236245036125183\nEpoch 2651, Loss: 0.26236262917518616\nEpoch 2652, Loss: 0.26236262917518616\nEpoch 2653, Loss: 0.26236245036125183\nEpoch 2654, Loss: 0.26236215233802795\nEpoch 2655, Loss: 0.26236313581466675\nEpoch 2656, Loss: 0.26236164569854736\nEpoch 2657, Loss: 0.26236313581466675\nEpoch 2658, Loss: 0.2623622715473175\nEpoch 2659, Loss: 0.26236286759376526\nEpoch 2660, Loss: 0.2623631954193115\nEpoch 2661, Loss: 0.2623633146286011\nEpoch 2662, Loss: 0.2623632550239563\nEpoch 2663, Loss: 0.2623630166053772\nEpoch 2664, Loss: 0.26236262917518616\nEpoch 2665, Loss: 0.2623634934425354\nEpoch 2666, Loss: 0.26236197352409363\nEpoch 2667, Loss: 0.2623617947101593\nEpoch 2668, Loss: 0.26236531138420105\nEpoch 2669, Loss: 0.26236197352409363\nEpoch 2670, Loss: 0.26236245036125183\nEpoch 2671, Loss: 0.26236262917518616\nEpoch 2672, Loss: 0.26236262917518616\nEpoch 2673, Loss: 0.26236245036125183\nEpoch 2674, Loss: 0.2623620927333832\nEpoch 2675, Loss: 0.26236313581466675\nEpoch 2676, Loss: 0.26236164569854736\nEpoch 2677, Loss: 0.2623632550239563\nEpoch 2678, Loss: 0.2623622715473175\nEpoch 2679, Loss: 0.26236286759376526\nEpoch 2680, Loss: 0.2623631954193115\nEpoch 2681, Loss: 0.2623633146286011\nEpoch 2682, Loss: 0.2623633146286011\nEpoch 2683, Loss: 0.2623630166053772\nEpoch 2684, Loss: 0.26236262917518616\nEpoch 2685, Loss: 0.2623635530471802\nEpoch 2686, Loss: 0.2623620331287384\nEpoch 2687, Loss: 0.2623617947101593\nEpoch 2688, Loss: 0.2623653709888458\nEpoch 2689, Loss: 0.2623620331287384\nEpoch 2690, Loss: 0.26236239075660706\nEpoch 2691, Loss: 0.26236262917518616\nEpoch 2692, Loss: 0.26236262917518616\nEpoch 2693, Loss: 0.26236245036125183\nEpoch 2694, Loss: 0.26236215233802795\nEpoch 2695, Loss: 0.26236313581466675\nEpoch 2696, Loss: 0.26236164569854736\nEpoch 2697, Loss: 0.2623633146286011\nEpoch 2698, Loss: 0.2623622715473175\nEpoch 2699, Loss: 0.2623628079891205\nEpoch 2700, Loss: 0.2623631954193115\nEpoch 2701, Loss: 0.26236337423324585\nEpoch 2702, Loss: 0.2623632550239563\nEpoch 2703, Loss: 0.2623630166053772\nEpoch 2704, Loss: 0.26236262917518616\nEpoch 2705, Loss: 0.2623635530471802\nEpoch 2706, Loss: 0.2623620331287384\nEpoch 2707, Loss: 0.2623617947101593\nEpoch 2708, Loss: 0.26236552000045776\nEpoch 2709, Loss: 0.2623620331287384\nEpoch 2710, Loss: 0.26236245036125183\nEpoch 2711, Loss: 0.26236262917518616\nEpoch 2712, Loss: 0.26236262917518616\nEpoch 2713, Loss: 0.26236239075660706\nEpoch 2714, Loss: 0.26236215233802795\nEpoch 2715, Loss: 0.2623631954193115\nEpoch 2716, Loss: 0.26236164569854736\nEpoch 2717, Loss: 0.2623634338378906\nEpoch 2718, Loss: 0.2623622715473175\nEpoch 2719, Loss: 0.2623629570007324\nEpoch 2720, Loss: 0.2623632550239563\nEpoch 2721, Loss: 0.26236337423324585\nEpoch 2722, Loss: 0.2623633146286011\nEpoch 2723, Loss: 0.2623630166053772\nEpoch 2724, Loss: 0.26236262917518616\nEpoch 2725, Loss: 0.2623636722564697\nEpoch 2726, Loss: 0.2623620331287384\nEpoch 2727, Loss: 0.2623617947101593\nEpoch 2728, Loss: 0.26236557960510254\nEpoch 2729, Loss: 0.2623620331287384\nEpoch 2730, Loss: 0.26236239075660706\nEpoch 2731, Loss: 0.26236262917518616\nEpoch 2732, Loss: 0.2623625695705414\nEpoch 2733, Loss: 0.26236245036125183\nEpoch 2734, Loss: 0.26236215233802795\nEpoch 2735, Loss: 0.2623632550239563\nEpoch 2736, Loss: 0.26236164569854736\nEpoch 2737, Loss: 0.2623634934425354\nEpoch 2738, Loss: 0.2623622715473175\nEpoch 2739, Loss: 0.2623629570007324\nEpoch 2740, Loss: 0.2623632550239563\nEpoch 2741, Loss: 0.2623634338378906\nEpoch 2742, Loss: 0.2623633146286011\nEpoch 2743, Loss: 0.2623630166053772\nEpoch 2744, Loss: 0.26236262917518616\nEpoch 2745, Loss: 0.2623636722564697\nEpoch 2746, Loss: 0.2623620331287384\nEpoch 2747, Loss: 0.2623617351055145\nEpoch 2748, Loss: 0.2623656988143921\nEpoch 2749, Loss: 0.2623620331287384\nEpoch 2750, Loss: 0.26236245036125183\nEpoch 2751, Loss: 0.26236268877983093\nEpoch 2752, Loss: 0.26236262917518616\nEpoch 2753, Loss: 0.26236245036125183\nEpoch 2754, Loss: 0.2623620927333832\nEpoch 2755, Loss: 0.2623632550239563\nEpoch 2756, Loss: 0.26236164569854736\nEpoch 2757, Loss: 0.26236361265182495\nEpoch 2758, Loss: 0.2623623311519623\nEpoch 2759, Loss: 0.26236286759376526\nEpoch 2760, Loss: 0.2623632550239563\nEpoch 2761, Loss: 0.2623634338378906\nEpoch 2762, Loss: 0.2623633146286011\nEpoch 2763, Loss: 0.262363076210022\nEpoch 2764, Loss: 0.26236262917518616\nEpoch 2765, Loss: 0.2623637318611145\nEpoch 2766, Loss: 0.26236197352409363\nEpoch 2767, Loss: 0.2623617351055145\nEpoch 2768, Loss: 0.26236581802368164\nEpoch 2769, Loss: 0.26236197352409363\nEpoch 2770, Loss: 0.26236239075660706\nEpoch 2771, Loss: 0.26236262917518616\nEpoch 2772, Loss: 0.26236268877983093\nEpoch 2773, Loss: 0.26236245036125183\nEpoch 2774, Loss: 0.26236215233802795\nEpoch 2775, Loss: 0.26236337423324585\nEpoch 2776, Loss: 0.2623615860939026\nEpoch 2777, Loss: 0.2623637318611145\nEpoch 2778, Loss: 0.2623622715473175\nEpoch 2779, Loss: 0.26236286759376526\nEpoch 2780, Loss: 0.2623632550239563\nEpoch 2781, Loss: 0.2623634338378906\nEpoch 2782, Loss: 0.2623633146286011\nEpoch 2783, Loss: 0.262363076210022\nEpoch 2784, Loss: 0.26236262917518616\nEpoch 2785, Loss: 0.2623637914657593\nEpoch 2786, Loss: 0.26236197352409363\nEpoch 2787, Loss: 0.2623617947101593\nEpoch 2788, Loss: 0.2623659372329712\nEpoch 2789, Loss: 0.26236197352409363\nEpoch 2790, Loss: 0.26236239075660706\nEpoch 2791, Loss: 0.26236262917518616\nEpoch 2792, Loss: 0.26236268877983093\nEpoch 2793, Loss: 0.26236245036125183\nEpoch 2794, Loss: 0.2623620927333832\nEpoch 2795, Loss: 0.26236337423324585\nEpoch 2796, Loss: 0.26236164569854736\nEpoch 2797, Loss: 0.2623637318611145\nEpoch 2798, Loss: 0.2623622715473175\nEpoch 2799, Loss: 0.2623629570007324\nEpoch 2800, Loss: 0.2623632550239563\nEpoch 2801, Loss: 0.26236337423324585\nEpoch 2802, Loss: 0.26236337423324585\nEpoch 2803, Loss: 0.262363076210022\nEpoch 2804, Loss: 0.26236262917518616\nEpoch 2805, Loss: 0.26236385107040405\nEpoch 2806, Loss: 0.2623620331287384\nEpoch 2807, Loss: 0.2623617351055145\nEpoch 2808, Loss: 0.26236605644226074\nEpoch 2809, Loss: 0.2623620331287384\nEpoch 2810, Loss: 0.26236245036125183\nEpoch 2811, Loss: 0.26236262917518616\nEpoch 2812, Loss: 0.26236268877983093\nEpoch 2813, Loss: 0.26236245036125183\nEpoch 2814, Loss: 0.26236215233802795\nEpoch 2815, Loss: 0.26236337423324585\nEpoch 2816, Loss: 0.2623615860939026\nEpoch 2817, Loss: 0.26236385107040405\nEpoch 2818, Loss: 0.2623622715473175\nEpoch 2819, Loss: 0.2623629570007324\nEpoch 2820, Loss: 0.2623632550239563\nEpoch 2821, Loss: 0.2623634338378906\nEpoch 2822, Loss: 0.26236337423324585\nEpoch 2823, Loss: 0.262363076210022\nEpoch 2824, Loss: 0.26236262917518616\nEpoch 2825, Loss: 0.26236385107040405\nEpoch 2826, Loss: 0.2623620331287384\nEpoch 2827, Loss: 0.2623617351055145\nEpoch 2828, Loss: 0.2623661756515503\nEpoch 2829, Loss: 0.2623620331287384\nEpoch 2830, Loss: 0.26236239075660706\nEpoch 2831, Loss: 0.26236262917518616\nEpoch 2832, Loss: 0.26236262917518616\nEpoch 2833, Loss: 0.2623625099658966\nEpoch 2834, Loss: 0.26236215233802795\nEpoch 2835, Loss: 0.2623634338378906\nEpoch 2836, Loss: 0.26236164569854736\nEpoch 2837, Loss: 0.26236391067504883\nEpoch 2838, Loss: 0.2623623311519623\nEpoch 2839, Loss: 0.26236286759376526\nEpoch 2840, Loss: 0.26236337423324585\nEpoch 2841, Loss: 0.2623634934425354\nEpoch 2842, Loss: 0.2623633146286011\nEpoch 2843, Loss: 0.262363076210022\nEpoch 2844, Loss: 0.26236262917518616\nEpoch 2845, Loss: 0.26236391067504883\nEpoch 2846, Loss: 0.2623620331287384\nEpoch 2847, Loss: 0.2623617351055145\nEpoch 2848, Loss: 0.2623663544654846\nEpoch 2849, Loss: 0.26236197352409363\nEpoch 2850, Loss: 0.26236245036125183\nEpoch 2851, Loss: 0.26236262917518616\nEpoch 2852, Loss: 0.26236262917518616\nEpoch 2853, Loss: 0.26236245036125183\nEpoch 2854, Loss: 0.26236215233802795\nEpoch 2855, Loss: 0.2623634934425354\nEpoch 2856, Loss: 0.2623615860939026\nEpoch 2857, Loss: 0.2623640298843384\nEpoch 2858, Loss: 0.2623622715473175\nEpoch 2859, Loss: 0.26236286759376526\nEpoch 2860, Loss: 0.2623633146286011\nEpoch 2861, Loss: 0.2623634338378906\nEpoch 2862, Loss: 0.26236337423324585\nEpoch 2863, Loss: 0.26236313581466675\nEpoch 2864, Loss: 0.26236262917518616\nEpoch 2865, Loss: 0.2623639702796936\nEpoch 2866, Loss: 0.26236197352409363\nEpoch 2867, Loss: 0.2623617351055145\nEpoch 2868, Loss: 0.2623664140701294\nEpoch 2869, Loss: 0.2623620331287384\nEpoch 2870, Loss: 0.26236245036125183\nEpoch 2871, Loss: 0.26236262917518616\nEpoch 2872, Loss: 0.26236262917518616\nEpoch 2873, Loss: 0.26236245036125183\nEpoch 2874, Loss: 0.2623620927333832\nEpoch 2875, Loss: 0.2623635530471802\nEpoch 2876, Loss: 0.2623615860939026\nEpoch 2877, Loss: 0.26236408948898315\nEpoch 2878, Loss: 0.2623623311519623\nEpoch 2879, Loss: 0.2623629570007324\nEpoch 2880, Loss: 0.26236337423324585\nEpoch 2881, Loss: 0.2623634338378906\nEpoch 2882, Loss: 0.26236337423324585\nEpoch 2883, Loss: 0.26236313581466675\nEpoch 2884, Loss: 0.2623625695705414\nEpoch 2885, Loss: 0.2623640298843384\nEpoch 2886, Loss: 0.26236197352409363\nEpoch 2887, Loss: 0.2623617351055145\nEpoch 2888, Loss: 0.26236653327941895\nEpoch 2889, Loss: 0.26236197352409363\nEpoch 2890, Loss: 0.26236239075660706\nEpoch 2891, Loss: 0.26236262917518616\nEpoch 2892, Loss: 0.26236268877983093\nEpoch 2893, Loss: 0.26236245036125183\nEpoch 2894, Loss: 0.2623620927333832\nEpoch 2895, Loss: 0.26236361265182495\nEpoch 2896, Loss: 0.2623615264892578\nEpoch 2897, Loss: 0.2623642385005951\nEpoch 2898, Loss: 0.2623622715473175\nEpoch 2899, Loss: 0.2623629570007324\nEpoch 2900, Loss: 0.26236337423324585\nEpoch 2901, Loss: 0.2623634934425354\nEpoch 2902, Loss: 0.26236337423324585\nEpoch 2903, Loss: 0.262363076210022\nEpoch 2904, Loss: 0.26236262917518616\nEpoch 2905, Loss: 0.26236408948898315\nEpoch 2906, Loss: 0.26236197352409363\nEpoch 2907, Loss: 0.2623617351055145\nEpoch 2908, Loss: 0.2623666524887085\nEpoch 2909, Loss: 0.26236197352409363\nEpoch 2910, Loss: 0.26236245036125183\nEpoch 2911, Loss: 0.26236262917518616\nEpoch 2912, Loss: 0.26236262917518616\nEpoch 2913, Loss: 0.2623625099658966\nEpoch 2914, Loss: 0.26236215233802795\nEpoch 2915, Loss: 0.26236361265182495\nEpoch 2916, Loss: 0.2623615860939026\nEpoch 2917, Loss: 0.26236429810523987\nEpoch 2918, Loss: 0.2623623311519623\nEpoch 2919, Loss: 0.2623629570007324\nEpoch 2920, Loss: 0.2623633146286011\nEpoch 2921, Loss: 0.2623635530471802\nEpoch 2922, Loss: 0.2623634338378906\nEpoch 2923, Loss: 0.26236313581466675\nEpoch 2924, Loss: 0.26236262917518616\nEpoch 2925, Loss: 0.26236414909362793\nEpoch 2926, Loss: 0.26236197352409363\nEpoch 2927, Loss: 0.2623617351055145\nEpoch 2928, Loss: 0.2623666524887085\nEpoch 2929, Loss: 0.26236197352409363\nEpoch 2930, Loss: 0.2623625099658966\nEpoch 2931, Loss: 0.26236262917518616\nEpoch 2932, Loss: 0.26236268877983093\nEpoch 2933, Loss: 0.26236245036125183\nEpoch 2934, Loss: 0.26236215233802795\nEpoch 2935, Loss: 0.2623636722564697\nEpoch 2936, Loss: 0.2623615264892578\nEpoch 2937, Loss: 0.2623644173145294\nEpoch 2938, Loss: 0.2623622715473175\nEpoch 2939, Loss: 0.2623630166053772\nEpoch 2940, Loss: 0.2623634338378906\nEpoch 2941, Loss: 0.2623634934425354\nEpoch 2942, Loss: 0.26236337423324585\nEpoch 2943, Loss: 0.26236313581466675\nEpoch 2944, Loss: 0.26236268877983093\nEpoch 2945, Loss: 0.26236414909362793\nEpoch 2946, Loss: 0.26236197352409363\nEpoch 2947, Loss: 0.26236164569854736\nEpoch 2948, Loss: 0.26236692070961\nEpoch 2949, Loss: 0.26236197352409363\nEpoch 2950, Loss: 0.26236245036125183\nEpoch 2951, Loss: 0.26236268877983093\nEpoch 2952, Loss: 0.26236268877983093\nEpoch 2953, Loss: 0.2623625099658966\nEpoch 2954, Loss: 0.26236215233802795\nEpoch 2955, Loss: 0.2623636722564697\nEpoch 2956, Loss: 0.2623615860939026\nEpoch 2957, Loss: 0.2623644769191742\nEpoch 2958, Loss: 0.2623622715473175\nEpoch 2959, Loss: 0.2623630166053772\nEpoch 2960, Loss: 0.26236337423324585\nEpoch 2961, Loss: 0.2623634934425354\nEpoch 2962, Loss: 0.2623634934425354\nEpoch 2963, Loss: 0.26236313581466675\nEpoch 2964, Loss: 0.26236268877983093\nEpoch 2965, Loss: 0.2623642385005951\nEpoch 2966, Loss: 0.26236191391944885\nEpoch 2967, Loss: 0.26236164569854736\nEpoch 2968, Loss: 0.26236703991889954\nEpoch 2969, Loss: 0.26236197352409363\nEpoch 2970, Loss: 0.26236245036125183\nEpoch 2971, Loss: 0.26236262917518616\nEpoch 2972, Loss: 0.26236268877983093\nEpoch 2973, Loss: 0.2623625099658966\nEpoch 2974, Loss: 0.26236215233802795\nEpoch 2975, Loss: 0.2623637318611145\nEpoch 2976, Loss: 0.2623615860939026\nEpoch 2977, Loss: 0.2623646557331085\nEpoch 2978, Loss: 0.2623622715473175\nEpoch 2979, Loss: 0.2623630166053772\nEpoch 2980, Loss: 0.26236337423324585\nEpoch 2981, Loss: 0.2623635530471802\nEpoch 2982, Loss: 0.2623634338378906\nEpoch 2983, Loss: 0.26236313581466675\nEpoch 2984, Loss: 0.26236268877983093\nEpoch 2985, Loss: 0.26236429810523987\nEpoch 2986, Loss: 0.26236197352409363\nEpoch 2987, Loss: 0.26236164569854736\nEpoch 2988, Loss: 0.2623671591281891\nEpoch 2989, Loss: 0.2623620331287384\nEpoch 2990, Loss: 0.26236245036125183\nEpoch 2991, Loss: 0.26236262917518616\nEpoch 2992, Loss: 0.26236262917518616\nEpoch 2993, Loss: 0.2623625099658966\nEpoch 2994, Loss: 0.2623620927333832\nEpoch 2995, Loss: 0.2623637914657593\nEpoch 2996, Loss: 0.2623615264892578\nEpoch 2997, Loss: 0.2623647153377533\nEpoch 2998, Loss: 0.2623622715473175\nEpoch 2999, Loss: 0.2623630166053772\nEpoch 3000, Loss: 0.2623634338378906\nEpoch 3001, Loss: 0.2623635530471802\nEpoch 3002, Loss: 0.2623634338378906\nEpoch 3003, Loss: 0.26236313581466675\nEpoch 3004, Loss: 0.26236262917518616\nEpoch 3005, Loss: 0.26236429810523987\nEpoch 3006, Loss: 0.26236197352409363\nEpoch 3007, Loss: 0.26236164569854736\nEpoch 3008, Loss: 0.26236727833747864\nEpoch 3009, Loss: 0.26236197352409363\nEpoch 3010, Loss: 0.26236245036125183\nEpoch 3011, Loss: 0.26236268877983093\nEpoch 3012, Loss: 0.26236268877983093\nEpoch 3013, Loss: 0.2623625099658966\nEpoch 3014, Loss: 0.2623620927333832\nEpoch 3015, Loss: 0.26236385107040405\nEpoch 3016, Loss: 0.2623615264892578\nEpoch 3017, Loss: 0.26236483454704285\nEpoch 3018, Loss: 0.2623622715473175\nEpoch 3019, Loss: 0.2623629570007324\nEpoch 3020, Loss: 0.2623634338378906\nEpoch 3021, Loss: 0.2623635530471802\nEpoch 3022, Loss: 0.2623634338378906\nEpoch 3023, Loss: 0.26236313581466675\nEpoch 3024, Loss: 0.26236262917518616\nEpoch 3025, Loss: 0.2623644173145294\nEpoch 3026, Loss: 0.26236191391944885\nEpoch 3027, Loss: 0.26236164569854736\nEpoch 3028, Loss: 0.2623673975467682\nEpoch 3029, Loss: 0.26236191391944885\nEpoch 3030, Loss: 0.26236245036125183\nEpoch 3031, Loss: 0.26236262917518616\nEpoch 3032, Loss: 0.26236268877983093\nEpoch 3033, Loss: 0.26236245036125183\nEpoch 3034, Loss: 0.2623620927333832\nEpoch 3035, Loss: 0.26236391067504883\nEpoch 3036, Loss: 0.26236146688461304\nEpoch 3037, Loss: 0.2623648941516876\nEpoch 3038, Loss: 0.2623623311519623\nEpoch 3039, Loss: 0.2623630166053772\nEpoch 3040, Loss: 0.26236337423324585\nEpoch 3041, Loss: 0.2623635530471802\nEpoch 3042, Loss: 0.2623634338378906\nEpoch 3043, Loss: 0.2623631954193115\nEpoch 3044, Loss: 0.26236268877983093\nEpoch 3045, Loss: 0.2623644173145294\nEpoch 3046, Loss: 0.26236191391944885\nEpoch 3047, Loss: 0.26236164569854736\nEpoch 3048, Loss: 0.26236751675605774\nEpoch 3049, Loss: 0.26236197352409363\nEpoch 3050, Loss: 0.2623625099658966\nEpoch 3051, Loss: 0.26236268877983093\nEpoch 3052, Loss: 0.26236268877983093\nEpoch 3053, Loss: 0.2623625099658966\nEpoch 3054, Loss: 0.2623620927333832\nEpoch 3055, Loss: 0.26236391067504883\nEpoch 3056, Loss: 0.2623615264892578\nEpoch 3057, Loss: 0.2623649537563324\nEpoch 3058, Loss: 0.2623623311519623\nEpoch 3059, Loss: 0.2623630166053772\nEpoch 3060, Loss: 0.2623634338378906\nEpoch 3061, Loss: 0.26236361265182495\nEpoch 3062, Loss: 0.2623634934425354\nEpoch 3063, Loss: 0.2623631954193115\nEpoch 3064, Loss: 0.26236268877983093\nEpoch 3065, Loss: 0.2623644769191742\nEpoch 3066, Loss: 0.26236191391944885\nEpoch 3067, Loss: 0.26236164569854736\nEpoch 3068, Loss: 0.2623625695705414\nEpoch 3069, Loss: 0.2623631954193115\nEpoch 3070, Loss: 0.2623635530471802\nEpoch 3071, Loss: 0.2623637318611145\nEpoch 3072, Loss: 0.2623634934425354\nEpoch 3073, Loss: 0.26236313581466675\nEpoch 3074, Loss: 0.2623631954193115\nEpoch 3075, Loss: 0.26236262917518616\nEpoch 3076, Loss: 0.2623623311519623\nEpoch 3077, Loss: 0.26236286759376526\nEpoch 3078, Loss: 0.2623628079891205\nEpoch 3079, Loss: 0.26236337423324585\nEpoch 3080, Loss: 0.26236361265182495\nEpoch 3081, Loss: 0.2623636722564697\nEpoch 3082, Loss: 0.26236337423324585\nEpoch 3083, Loss: 0.2623629570007324\nEpoch 3084, Loss: 0.26236268877983093\nEpoch 3085, Loss: 0.2623623311519623\nEpoch 3086, Loss: 0.2623620331287384\nEpoch 3087, Loss: 0.26236557960510254\nEpoch 3088, Loss: 0.26236239075660706\nEpoch 3089, Loss: 0.2623629570007324\nEpoch 3090, Loss: 0.2623631954193115\nEpoch 3091, Loss: 0.2623631954193115\nEpoch 3092, Loss: 0.2623630166053772\nEpoch 3093, Loss: 0.2623625099658966\nEpoch 3094, Loss: 0.2623634934425354\nEpoch 3095, Loss: 0.26236197352409363\nEpoch 3096, Loss: 0.26236197352409363\nEpoch 3097, Loss: 0.26236268877983093\nEpoch 3098, Loss: 0.26236337423324585\nEpoch 3099, Loss: 0.26236385107040405\nEpoch 3100, Loss: 0.2623639702796936\nEpoch 3101, Loss: 0.26236385107040405\nEpoch 3102, Loss: 0.2623634934425354\nEpoch 3103, Loss: 0.2623630166053772\nEpoch 3104, Loss: 0.2623651325702667\nEpoch 3105, Loss: 0.26236215233802795\nEpoch 3106, Loss: 0.2623618543148041\nEpoch 3107, Loss: 0.26236575841903687\nEpoch 3108, Loss: 0.2623620927333832\nEpoch 3109, Loss: 0.2623625695705414\nEpoch 3110, Loss: 0.2623628079891205\nEpoch 3111, Loss: 0.2623627483844757\nEpoch 3112, Loss: 0.2623625695705414\nEpoch 3113, Loss: 0.2623625099658966\nEpoch 3114, Loss: 0.2623622715473175\nEpoch 3115, Loss: 0.26236215233802795\nEpoch 3116, Loss: 0.26236477494239807\nEpoch 3117, Loss: 0.2623628079891205\nEpoch 3118, Loss: 0.2623634934425354\nEpoch 3119, Loss: 0.2623637914657593\nEpoch 3120, Loss: 0.26236385107040405\nEpoch 3121, Loss: 0.2623636722564697\nEpoch 3122, Loss: 0.2623633146286011\nEpoch 3123, Loss: 0.2623627483844757\nEpoch 3124, Loss: 0.2623642385005951\nEpoch 3125, Loss: 0.2623618543148041\nEpoch 3126, Loss: 0.26236268877983093\nEpoch 3127, Loss: 0.26236239075660706\nEpoch 3128, Loss: 0.262363076210022\nEpoch 3129, Loss: 0.26236337423324585\nEpoch 3130, Loss: 0.2623634934425354\nEpoch 3131, Loss: 0.2623632550239563\nEpoch 3132, Loss: 0.2623629570007324\nEpoch 3133, Loss: 0.2623642385005951\nEpoch 3134, Loss: 0.2623623311519623\nEpoch 3135, Loss: 0.26236215233802795\nEpoch 3136, Loss: 0.2623639702796936\nEpoch 3137, Loss: 0.26236262917518616\nEpoch 3138, Loss: 0.2623631954193115\nEpoch 3139, Loss: 0.2623634934425354\nEpoch 3140, Loss: 0.2623635530471802\nEpoch 3141, Loss: 0.2623632550239563\nEpoch 3142, Loss: 0.26236286759376526\nEpoch 3143, Loss: 0.26236313581466675\nEpoch 3144, Loss: 0.2623622715473175\nEpoch 3145, Loss: 0.2623620331287384\nEpoch 3146, Loss: 0.26236599683761597\nEpoch 3147, Loss: 0.2623623311519623\nEpoch 3148, Loss: 0.2623629570007324\nEpoch 3149, Loss: 0.2623632550239563\nEpoch 3150, Loss: 0.2623632550239563\nEpoch 3151, Loss: 0.2623630166053772\nEpoch 3152, Loss: 0.26236262917518616\nEpoch 3153, Loss: 0.26236337423324585\nEpoch 3154, Loss: 0.26236197352409363\nEpoch 3155, Loss: 0.26236191391944885\nEpoch 3156, Loss: 0.2623627483844757\nEpoch 3157, Loss: 0.2623634934425354\nEpoch 3158, Loss: 0.26236391067504883\nEpoch 3159, Loss: 0.26236408948898315\nEpoch 3160, Loss: 0.26236391067504883\nEpoch 3161, Loss: 0.26236361265182495\nEpoch 3162, Loss: 0.2623630166053772\nEpoch 3163, Loss: 0.26236507296562195\nEpoch 3164, Loss: 0.26236221194267273\nEpoch 3165, Loss: 0.26236191391944885\nEpoch 3166, Loss: 0.2623659372329712\nEpoch 3167, Loss: 0.2623620927333832\nEpoch 3168, Loss: 0.26236262917518616\nEpoch 3169, Loss: 0.2623628079891205\nEpoch 3170, Loss: 0.2623628079891205\nEpoch 3171, Loss: 0.26236262917518616\nEpoch 3172, Loss: 0.26236262917518616\nEpoch 3173, Loss: 0.2623622715473175\nEpoch 3174, Loss: 0.26236215233802795\nEpoch 3175, Loss: 0.2623651325702667\nEpoch 3176, Loss: 0.2623627483844757\nEpoch 3177, Loss: 0.2623634934425354\nEpoch 3178, Loss: 0.26236385107040405\nEpoch 3179, Loss: 0.26236385107040405\nEpoch 3180, Loss: 0.2623636722564697\nEpoch 3181, Loss: 0.2623632550239563\nEpoch 3182, Loss: 0.26236268877983093\nEpoch 3183, Loss: 0.2623644173145294\nEpoch 3184, Loss: 0.2623617947101593\nEpoch 3185, Loss: 0.2623631954193115\nEpoch 3186, Loss: 0.2623623311519623\nEpoch 3187, Loss: 0.2623630166053772\nEpoch 3188, Loss: 0.26236337423324585\nEpoch 3189, Loss: 0.2623634934425354\nEpoch 3190, Loss: 0.2623632550239563\nEpoch 3191, Loss: 0.26236286759376526\nEpoch 3192, Loss: 0.2623644769191742\nEpoch 3193, Loss: 0.26236239075660706\nEpoch 3194, Loss: 0.26236215233802795\nEpoch 3195, Loss: 0.26236435770988464\nEpoch 3196, Loss: 0.26236262917518616\nEpoch 3197, Loss: 0.2623631954193115\nEpoch 3198, Loss: 0.2623634934425354\nEpoch 3199, Loss: 0.2623634934425354\nEpoch 3200, Loss: 0.2623633146286011\nEpoch 3201, Loss: 0.26236286759376526\nEpoch 3202, Loss: 0.2623632550239563\nEpoch 3203, Loss: 0.26236221194267273\nEpoch 3204, Loss: 0.26236197352409363\nEpoch 3205, Loss: 0.2623663544654846\nEpoch 3206, Loss: 0.26236239075660706\nEpoch 3207, Loss: 0.2623630166053772\nEpoch 3208, Loss: 0.2623633146286011\nEpoch 3209, Loss: 0.2623633146286011\nEpoch 3210, Loss: 0.262363076210022\nEpoch 3211, Loss: 0.26236262917518616\nEpoch 3212, Loss: 0.2623635530471802\nEpoch 3213, Loss: 0.2623620331287384\nEpoch 3214, Loss: 0.26236215233802795\nEpoch 3215, Loss: 0.2623627483844757\nEpoch 3216, Loss: 0.2623634934425354\nEpoch 3217, Loss: 0.2623639702796936\nEpoch 3218, Loss: 0.26236408948898315\nEpoch 3219, Loss: 0.2623640298843384\nEpoch 3220, Loss: 0.2623636722564697\nEpoch 3221, Loss: 0.262363076210022\nEpoch 3222, Loss: 0.2623652517795563\nEpoch 3223, Loss: 0.26236215233802795\nEpoch 3224, Loss: 0.2623617947101593\nEpoch 3225, Loss: 0.2623663544654846\nEpoch 3226, Loss: 0.2623620927333832\nEpoch 3227, Loss: 0.26236262917518616\nEpoch 3228, Loss: 0.26236286759376526\nEpoch 3229, Loss: 0.2623628079891205\nEpoch 3230, Loss: 0.2623625695705414\nEpoch 3231, Loss: 0.2623627483844757\nEpoch 3232, Loss: 0.2623622715473175\nEpoch 3233, Loss: 0.26236215233802795\nEpoch 3234, Loss: 0.26236552000045776\nEpoch 3235, Loss: 0.26236286759376526\nEpoch 3236, Loss: 0.2623635530471802\nEpoch 3237, Loss: 0.26236385107040405\nEpoch 3238, Loss: 0.26236391067504883\nEpoch 3239, Loss: 0.2623637318611145\nEpoch 3240, Loss: 0.2623633146286011\nEpoch 3241, Loss: 0.2623627483844757\nEpoch 3242, Loss: 0.26236453652381897\nEpoch 3243, Loss: 0.2623617947101593\nEpoch 3244, Loss: 0.2623634338378906\nEpoch 3245, Loss: 0.26236239075660706\nEpoch 3246, Loss: 0.2623630166053772\nEpoch 3247, Loss: 0.2623634338378906\nEpoch 3248, Loss: 0.2623634934425354\nEpoch 3249, Loss: 0.2623633146286011\nEpoch 3250, Loss: 0.26236286759376526\nEpoch 3251, Loss: 0.26236459612846375\nEpoch 3252, Loss: 0.2623623311519623\nEpoch 3253, Loss: 0.26236215233802795\nEpoch 3254, Loss: 0.2623646557331085\nEpoch 3255, Loss: 0.2623625695705414\nEpoch 3256, Loss: 0.2623631954193115\nEpoch 3257, Loss: 0.2623635530471802\nEpoch 3258, Loss: 0.26236361265182495\nEpoch 3259, Loss: 0.2623633146286011\nEpoch 3260, Loss: 0.26236286759376526\nEpoch 3261, Loss: 0.2623634934425354\nEpoch 3262, Loss: 0.2623622715473175\nEpoch 3263, Loss: 0.26236197352409363\nEpoch 3264, Loss: 0.26236674189567566\nEpoch 3265, Loss: 0.26236239075660706\nEpoch 3266, Loss: 0.2623630166053772\nEpoch 3267, Loss: 0.2623633146286011\nEpoch 3268, Loss: 0.2623632550239563\nEpoch 3269, Loss: 0.262363076210022\nEpoch 3270, Loss: 0.26236262917518616\nEpoch 3271, Loss: 0.2623637318611145\nEpoch 3272, Loss: 0.26236197352409363\nEpoch 3273, Loss: 0.26236239075660706\nEpoch 3274, Loss: 0.2623627483844757\nEpoch 3275, Loss: 0.2623635530471802\nEpoch 3276, Loss: 0.2623640298843384\nEpoch 3277, Loss: 0.26236414909362793\nEpoch 3278, Loss: 0.2623640298843384\nEpoch 3279, Loss: 0.26236361265182495\nEpoch 3280, Loss: 0.262363076210022\nEpoch 3281, Loss: 0.262365460395813\nEpoch 3282, Loss: 0.26236215233802795\nEpoch 3283, Loss: 0.2623617947101593\nEpoch 3284, Loss: 0.26236674189567566\nEpoch 3285, Loss: 0.2623620927333832\nEpoch 3286, Loss: 0.2623625695705414\nEpoch 3287, Loss: 0.26236286759376526\nEpoch 3288, Loss: 0.2623628079891205\nEpoch 3289, Loss: 0.26236262917518616\nEpoch 3290, Loss: 0.26236286759376526\nEpoch 3291, Loss: 0.2623622715473175\nEpoch 3292, Loss: 0.2623620927333832\nEpoch 3293, Loss: 0.2623658776283264\nEpoch 3294, Loss: 0.26236286759376526\nEpoch 3295, Loss: 0.2623634934425354\nEpoch 3296, Loss: 0.2623639702796936\nEpoch 3297, Loss: 0.2623639702796936\nEpoch 3298, Loss: 0.2623637318611145\nEpoch 3299, Loss: 0.26236337423324585\nEpoch 3300, Loss: 0.2623627483844757\nEpoch 3301, Loss: 0.26236477494239807\nEpoch 3302, Loss: 0.2623617351055145\nEpoch 3303, Loss: 0.2623637318611145\nEpoch 3304, Loss: 0.2623623311519623\nEpoch 3305, Loss: 0.262363076210022\nEpoch 3306, Loss: 0.2623634338378906\nEpoch 3307, Loss: 0.2623634934425354\nEpoch 3308, Loss: 0.2623633146286011\nEpoch 3309, Loss: 0.26236286759376526\nEpoch 3310, Loss: 0.26236483454704285\nEpoch 3311, Loss: 0.26236239075660706\nEpoch 3312, Loss: 0.2623620331287384\nEpoch 3313, Loss: 0.2623650133609772\nEpoch 3314, Loss: 0.2623625695705414\nEpoch 3315, Loss: 0.2623632550239563\nEpoch 3316, Loss: 0.2623634934425354\nEpoch 3317, Loss: 0.26236361265182495\nEpoch 3318, Loss: 0.26236337423324585\nEpoch 3319, Loss: 0.26236286759376526\nEpoch 3320, Loss: 0.2623635530471802\nEpoch 3321, Loss: 0.26236221194267273\nEpoch 3322, Loss: 0.26236197352409363\nEpoch 3323, Loss: 0.2623671591281891\nEpoch 3324, Loss: 0.26236239075660706\nEpoch 3325, Loss: 0.2623630166053772\nEpoch 3326, Loss: 0.2623633146286011\nEpoch 3327, Loss: 0.2623633146286011\nEpoch 3328, Loss: 0.262363076210022\nEpoch 3329, Loss: 0.26236262917518616\nEpoch 3330, Loss: 0.26236385107040405\nEpoch 3331, Loss: 0.26236197352409363\nEpoch 3332, Loss: 0.26236268877983093\nEpoch 3333, Loss: 0.2623628079891205\nEpoch 3334, Loss: 0.26236361265182495\nEpoch 3335, Loss: 0.2623640298843384\nEpoch 3336, Loss: 0.2623642385005951\nEpoch 3337, Loss: 0.26236408948898315\nEpoch 3338, Loss: 0.2623636722564697\nEpoch 3339, Loss: 0.262363076210022\nEpoch 3340, Loss: 0.26236313581466675\nEpoch 3341, Loss: 0.2623627483844757\nEpoch 3342, Loss: 0.2623623311519623\nEpoch 3343, Loss: 0.2623681426048279\nEpoch 3344, Loss: 0.26236245036125183\nEpoch 3345, Loss: 0.26236286759376526\nEpoch 3346, Loss: 0.2623630166053772\nEpoch 3347, Loss: 0.2623629570007324\nEpoch 3348, Loss: 0.26236262917518616\nEpoch 3349, Loss: 0.2623620927333832\nEpoch 3350, Loss: 0.2623647153377533\nEpoch 3351, Loss: 0.26236164569854736\nEpoch 3352, Loss: 0.2623625695705414\nEpoch 3353, Loss: 0.2623634934425354\nEpoch 3354, Loss: 0.26236414909362793\nEpoch 3355, Loss: 0.2623644173145294\nEpoch 3356, Loss: 0.2623644173145294\nEpoch 3357, Loss: 0.26236408948898315\nEpoch 3358, Loss: 0.2623637318611145\nEpoch 3359, Loss: 0.26236361265182495\nEpoch 3360, Loss: 0.2623634338378906\nEpoch 3361, Loss: 0.2623630166053772\nEpoch 3362, Loss: 0.262363076210022\nEpoch 3363, Loss: 0.2623631954193115\nEpoch 3364, Loss: 0.2623636722564697\nEpoch 3365, Loss: 0.26236385107040405\nEpoch 3366, Loss: 0.2623637318611145\nEpoch 3367, Loss: 0.2623633146286011\nEpoch 3368, Loss: 0.26236268877983093\nEpoch 3369, Loss: 0.26236414909362793\nEpoch 3370, Loss: 0.2623618543148041\nEpoch 3371, Loss: 0.26236337423324585\nEpoch 3372, Loss: 0.2623625099658966\nEpoch 3373, Loss: 0.2623633146286011\nEpoch 3374, Loss: 0.2623637318611145\nEpoch 3375, Loss: 0.26236385107040405\nEpoch 3376, Loss: 0.2623636722564697\nEpoch 3377, Loss: 0.2623633146286011\nEpoch 3378, Loss: 0.26236459612846375\nEpoch 3379, Loss: 0.26236262917518616\nEpoch 3380, Loss: 0.26236239075660706\nEpoch 3381, Loss: 0.262363076210022\nEpoch 3382, Loss: 0.2623629570007324\nEpoch 3383, Loss: 0.2623635530471802\nEpoch 3384, Loss: 0.26236385107040405\nEpoch 3385, Loss: 0.26236391067504883\nEpoch 3386, Loss: 0.2623636722564697\nEpoch 3387, Loss: 0.26236313581466675\nEpoch 3388, Loss: 0.2623639702796936\nEpoch 3389, Loss: 0.26236239075660706\nEpoch 3390, Loss: 0.2623620927333832\nEpoch 3391, Loss: 0.2623658776283264\nEpoch 3392, Loss: 0.2623625099658966\nEpoch 3393, Loss: 0.26236313581466675\nEpoch 3394, Loss: 0.2623634934425354\nEpoch 3395, Loss: 0.2623634338378906\nEpoch 3396, Loss: 0.2623631954193115\nEpoch 3397, Loss: 0.26236268877983093\nEpoch 3398, Loss: 0.26236435770988464\nEpoch 3399, Loss: 0.2623620331287384\nEpoch 3400, Loss: 0.2623620927333832\nEpoch 3401, Loss: 0.26236286759376526\nEpoch 3402, Loss: 0.2623636722564697\nEpoch 3403, Loss: 0.26236414909362793\nEpoch 3404, Loss: 0.26236429810523987\nEpoch 3405, Loss: 0.26236414909362793\nEpoch 3406, Loss: 0.2623637318611145\nEpoch 3407, Loss: 0.2623634934425354\nEpoch 3408, Loss: 0.2623630166053772\nEpoch 3409, Loss: 0.2623627483844757\nEpoch 3410, Loss: 0.2623622715473175\nEpoch 3411, Loss: 0.2623683214187622\nEpoch 3412, Loss: 0.26236239075660706\nEpoch 3413, Loss: 0.2623629570007324\nEpoch 3414, Loss: 0.26236313581466675\nEpoch 3415, Loss: 0.2623629570007324\nEpoch 3416, Loss: 0.26236262917518616\nEpoch 3417, Loss: 0.2623620927333832\nEpoch 3418, Loss: 0.2623648941516876\nEpoch 3419, Loss: 0.2623618543148041\nEpoch 3420, Loss: 0.2623625695705414\nEpoch 3421, Loss: 0.2623635530471802\nEpoch 3422, Loss: 0.2623642385005951\nEpoch 3423, Loss: 0.26236453652381897\nEpoch 3424, Loss: 0.2623644769191742\nEpoch 3425, Loss: 0.2623642385005951\nEpoch 3426, Loss: 0.26236385107040405\nEpoch 3427, Loss: 0.2623637318611145\nEpoch 3428, Loss: 0.2623634934425354\nEpoch 3429, Loss: 0.2623629570007324\nEpoch 3430, Loss: 0.2623634934425354\nEpoch 3431, Loss: 0.2623631954193115\nEpoch 3432, Loss: 0.2623636722564697\nEpoch 3433, Loss: 0.26236385107040405\nEpoch 3434, Loss: 0.2623636722564697\nEpoch 3435, Loss: 0.26236337423324585\nEpoch 3436, Loss: 0.26236268877983093\nEpoch 3437, Loss: 0.26236435770988464\nEpoch 3438, Loss: 0.2623617947101593\nEpoch 3439, Loss: 0.26236391067504883\nEpoch 3440, Loss: 0.2623625099658966\nEpoch 3441, Loss: 0.2623633146286011\nEpoch 3442, Loss: 0.2623637318611145\nEpoch 3443, Loss: 0.26236391067504883\nEpoch 3444, Loss: 0.2623636722564697\nEpoch 3445, Loss: 0.2623632550239563\nEpoch 3446, Loss: 0.26236483454704285\nEpoch 3447, Loss: 0.2623625695705414\nEpoch 3448, Loss: 0.2623623311519623\nEpoch 3449, Loss: 0.2623635530471802\nEpoch 3450, Loss: 0.26236286759376526\nEpoch 3451, Loss: 0.26236361265182495\nEpoch 3452, Loss: 0.26236391067504883\nEpoch 3453, Loss: 0.26236391067504883\nEpoch 3454, Loss: 0.2623636722564697\nEpoch 3455, Loss: 0.26236313581466675\nEpoch 3456, Loss: 0.26236408948898315\nEpoch 3457, Loss: 0.26236239075660706\nEpoch 3458, Loss: 0.2623620331287384\nEpoch 3459, Loss: 0.2623664140701294\nEpoch 3460, Loss: 0.2623625099658966\nEpoch 3461, Loss: 0.2623631954193115\nEpoch 3462, Loss: 0.2623634934425354\nEpoch 3463, Loss: 0.2623634934425354\nEpoch 3464, Loss: 0.2623631954193115\nEpoch 3465, Loss: 0.26236268877983093\nEpoch 3466, Loss: 0.26236459612846375\nEpoch 3467, Loss: 0.26236197352409363\nEpoch 3468, Loss: 0.26236239075660706\nEpoch 3469, Loss: 0.26236286759376526\nEpoch 3470, Loss: 0.2623636722564697\nEpoch 3471, Loss: 0.2623642385005951\nEpoch 3472, Loss: 0.26236435770988464\nEpoch 3473, Loss: 0.26236414909362793\nEpoch 3474, Loss: 0.2623637318611145\nEpoch 3475, Loss: 0.26236361265182495\nEpoch 3476, Loss: 0.26236313581466675\nEpoch 3477, Loss: 0.2623628079891205\nEpoch 3478, Loss: 0.2623622715473175\nEpoch 3479, Loss: 0.2623688578605652\nEpoch 3480, Loss: 0.26236239075660706\nEpoch 3481, Loss: 0.2623628079891205\nEpoch 3482, Loss: 0.262363076210022\nEpoch 3483, Loss: 0.2623630166053772\nEpoch 3484, Loss: 0.26236262917518616\nEpoch 3485, Loss: 0.26236215233802795\nEpoch 3486, Loss: 0.2623620927333832\nEpoch 3487, Loss: 0.262363076210022\nEpoch 3488, Loss: 0.2623632550239563\nEpoch 3489, Loss: 0.26236408948898315\nEpoch 3490, Loss: 0.2623646557331085\nEpoch 3491, Loss: 0.2623648941516876\nEpoch 3492, Loss: 0.2623647153377533\nEpoch 3493, Loss: 0.26236429810523987\nEpoch 3494, Loss: 0.2623636722564697\nEpoch 3495, Loss: 0.2623652517795563\nEpoch 3496, Loss: 0.26236262917518616\nEpoch 3497, Loss: 0.26236215233802795\nEpoch 3498, Loss: 0.26236483454704285\nEpoch 3499, Loss: 0.26236245036125183\nEpoch 3500, Loss: 0.262363076210022\nEpoch 3501, Loss: 0.2623633146286011\nEpoch 3502, Loss: 0.2623633146286011\nEpoch 3503, Loss: 0.2623629570007324\nEpoch 3504, Loss: 0.26236361265182495\nEpoch 3505, Loss: 0.2623625099658966\nEpoch 3506, Loss: 0.2623623311519623\nEpoch 3507, Loss: 0.26236477494239807\nEpoch 3508, Loss: 0.2623630166053772\nEpoch 3509, Loss: 0.2623637914657593\nEpoch 3510, Loss: 0.2623642385005951\nEpoch 3511, Loss: 0.26236429810523987\nEpoch 3512, Loss: 0.2623639702796936\nEpoch 3513, Loss: 0.2623634934425354\nEpoch 3514, Loss: 0.262363076210022\nEpoch 3515, Loss: 0.2623627483844757\nEpoch 3516, Loss: 0.26236239075660706\nEpoch 3517, Loss: 0.26236459612846375\nEpoch 3518, Loss: 0.26236286759376526\nEpoch 3519, Loss: 0.2623635530471802\nEpoch 3520, Loss: 0.26236385107040405\nEpoch 3521, Loss: 0.2623637914657593\nEpoch 3522, Loss: 0.2623635530471802\nEpoch 3523, Loss: 0.2623630166053772\nEpoch 3524, Loss: 0.2623647153377533\nEpoch 3525, Loss: 0.26236215233802795\nEpoch 3526, Loss: 0.2623618543148041\nEpoch 3527, Loss: 0.2623678147792816\nEpoch 3528, Loss: 0.26236239075660706\nEpoch 3529, Loss: 0.2623630166053772\nEpoch 3530, Loss: 0.2623633146286011\nEpoch 3531, Loss: 0.2623633146286011\nEpoch 3532, Loss: 0.26236313581466675\nEpoch 3533, Loss: 0.2623625099658966\nEpoch 3534, Loss: 0.2623651921749115\nEpoch 3535, Loss: 0.2623618543148041\nEpoch 3536, Loss: 0.2623630166053772\nEpoch 3537, Loss: 0.2623628079891205\nEpoch 3538, Loss: 0.2623636722564697\nEpoch 3539, Loss: 0.2623642385005951\nEpoch 3540, Loss: 0.2623644173145294\nEpoch 3541, Loss: 0.26236429810523987\nEpoch 3542, Loss: 0.2623637914657593\nEpoch 3543, Loss: 0.26236361265182495\nEpoch 3544, Loss: 0.26236313581466675\nEpoch 3545, Loss: 0.26236286759376526\nEpoch 3546, Loss: 0.2623623311519623\nEpoch 3547, Loss: 0.2623690962791443\nEpoch 3548, Loss: 0.26236245036125183\nEpoch 3549, Loss: 0.2623630166053772\nEpoch 3550, Loss: 0.26236313581466675\nEpoch 3551, Loss: 0.262363076210022\nEpoch 3552, Loss: 0.26236262917518616\nEpoch 3553, Loss: 0.2623620331287384\nEpoch 3554, Loss: 0.2623651921749115\nEpoch 3555, Loss: 0.2623622715473175\nEpoch 3556, Loss: 0.2623625695705414\nEpoch 3557, Loss: 0.26236361265182495\nEpoch 3558, Loss: 0.26236435770988464\nEpoch 3559, Loss: 0.26236459612846375\nEpoch 3560, Loss: 0.26236459612846375\nEpoch 3561, Loss: 0.26236429810523987\nEpoch 3562, Loss: 0.26236408948898315\nEpoch 3563, Loss: 0.2623637318611145\nEpoch 3564, Loss: 0.2623634934425354\nEpoch 3565, Loss: 0.2623630166053772\nEpoch 3566, Loss: 0.26236429810523987\nEpoch 3567, Loss: 0.26236313581466675\nEpoch 3568, Loss: 0.2623636722564697\nEpoch 3569, Loss: 0.26236385107040405\nEpoch 3570, Loss: 0.2623637318611145\nEpoch 3571, Loss: 0.2623633146286011\nEpoch 3572, Loss: 0.26236268877983093\nEpoch 3573, Loss: 0.2623648941516876\nEpoch 3574, Loss: 0.26236164569854736\nEpoch 3575, Loss: 0.2623648941516876\nEpoch 3576, Loss: 0.26236245036125183\nEpoch 3577, Loss: 0.2623633146286011\nEpoch 3578, Loss: 0.2623637318611145\nEpoch 3579, Loss: 0.26236385107040405\nEpoch 3580, Loss: 0.2623636722564697\nEpoch 3581, Loss: 0.2623632550239563\nEpoch 3582, Loss: 0.2623652517795563\nEpoch 3583, Loss: 0.2623625695705414\nEpoch 3584, Loss: 0.2623623311519623\nEpoch 3585, Loss: 0.26236435770988464\nEpoch 3586, Loss: 0.26236286759376526\nEpoch 3587, Loss: 0.26236361265182495\nEpoch 3588, Loss: 0.2623639702796936\nEpoch 3589, Loss: 0.2623639702796936\nEpoch 3590, Loss: 0.2623636722564697\nEpoch 3591, Loss: 0.2623631954193115\nEpoch 3592, Loss: 0.26236435770988464\nEpoch 3593, Loss: 0.2623623311519623\nEpoch 3594, Loss: 0.2623620927333832\nEpoch 3595, Loss: 0.2623673975467682\nEpoch 3596, Loss: 0.2623625099658966\nEpoch 3597, Loss: 0.2623631954193115\nEpoch 3598, Loss: 0.2623634934425354\nEpoch 3599, Loss: 0.2623635530471802\nEpoch 3600, Loss: 0.2623632550239563\nEpoch 3601, Loss: 0.2623627483844757\nEpoch 3602, Loss: 0.2623648941516876\nEpoch 3603, Loss: 0.26236197352409363\nEpoch 3604, Loss: 0.26236313581466675\nEpoch 3605, Loss: 0.2623628079891205\nEpoch 3606, Loss: 0.2623637914657593\nEpoch 3607, Loss: 0.26236435770988464\nEpoch 3608, Loss: 0.2623644769191742\nEpoch 3609, Loss: 0.26236429810523987\nEpoch 3610, Loss: 0.2623637914657593\nEpoch 3611, Loss: 0.2623639702796936\nEpoch 3612, Loss: 0.26236313581466675\nEpoch 3613, Loss: 0.2623627483844757\nEpoch 3614, Loss: 0.26236221194267273\nEpoch 3615, Loss: 0.2623700201511383\nEpoch 3616, Loss: 0.2623623311519623\nEpoch 3617, Loss: 0.26236286759376526\nEpoch 3618, Loss: 0.2623630166053772\nEpoch 3619, Loss: 0.2623629570007324\nEpoch 3620, Loss: 0.2623625695705414\nEpoch 3621, Loss: 0.26236245036125183\nEpoch 3622, Loss: 0.2623620331287384\nEpoch 3623, Loss: 0.26236385107040405\nEpoch 3624, Loss: 0.2623631954193115\nEpoch 3625, Loss: 0.26236414909362793\nEpoch 3626, Loss: 0.26236483454704285\nEpoch 3627, Loss: 0.2623650133609772\nEpoch 3628, Loss: 0.26236483454704285\nEpoch 3629, Loss: 0.2623644173145294\nEpoch 3630, Loss: 0.26236361265182495\nEpoch 3631, Loss: 0.26236575841903687\nEpoch 3632, Loss: 0.2623625695705414\nEpoch 3633, Loss: 0.2623620927333832\nEpoch 3634, Loss: 0.26236575841903687\nEpoch 3635, Loss: 0.26236239075660706\nEpoch 3636, Loss: 0.262363076210022\nEpoch 3637, Loss: 0.2623633146286011\nEpoch 3638, Loss: 0.2623633146286011\nEpoch 3639, Loss: 0.2623629570007324\nEpoch 3640, Loss: 0.2623639702796936\nEpoch 3641, Loss: 0.26236245036125183\nEpoch 3642, Loss: 0.2623623311519623\nEpoch 3643, Loss: 0.26236557960510254\nEpoch 3644, Loss: 0.262363076210022\nEpoch 3645, Loss: 0.26236385107040405\nEpoch 3646, Loss: 0.26236429810523987\nEpoch 3647, Loss: 0.26236435770988464\nEpoch 3648, Loss: 0.26236408948898315\nEpoch 3649, Loss: 0.26236361265182495\nEpoch 3650, Loss: 0.26236337423324585\nEpoch 3651, Loss: 0.2623627483844757\nEpoch 3652, Loss: 0.26236239075660706\nEpoch 3653, Loss: 0.26236552000045776\nEpoch 3654, Loss: 0.26236286759376526\nEpoch 3655, Loss: 0.2623635530471802\nEpoch 3656, Loss: 0.26236391067504883\nEpoch 3657, Loss: 0.26236385107040405\nEpoch 3658, Loss: 0.2623634934425354\nEpoch 3659, Loss: 0.2623629570007324\nEpoch 3660, Loss: 0.2623651325702667\nEpoch 3661, Loss: 0.26236215233802795\nEpoch 3662, Loss: 0.2623617947101593\nEpoch 3663, Loss: 0.2623688578605652\nEpoch 3664, Loss: 0.2623622715473175\nEpoch 3665, Loss: 0.2623629570007324\nEpoch 3666, Loss: 0.2623633146286011\nEpoch 3667, Loss: 0.26236337423324585\nEpoch 3668, Loss: 0.262363076210022\nEpoch 3669, Loss: 0.26236268877983093\nEpoch 3670, Loss: 0.26236268877983093\nEpoch 3671, Loss: 0.2623625695705414\nEpoch 3672, Loss: 0.2623647153377533\nEpoch 3673, Loss: 0.26236337423324585\nEpoch 3674, Loss: 0.2623642385005951\nEpoch 3675, Loss: 0.2623647153377533\nEpoch 3676, Loss: 0.26236477494239807\nEpoch 3677, Loss: 0.2623644769191742\nEpoch 3678, Loss: 0.26236385107040405\nEpoch 3679, Loss: 0.262363076210022\nEpoch 3680, Loss: 0.26236629486083984\nEpoch 3681, Loss: 0.26236191391944885\nEpoch 3682, Loss: 0.26236429810523987\nEpoch 3683, Loss: 0.26236429810523987\nEpoch 3684, Loss: 0.26236599683761597\nEpoch 3685, Loss: 0.26236727833747864\nEpoch 3686, Loss: 0.26236802339553833\nEpoch 3687, Loss: 0.2623681426048279\nEpoch 3688, Loss: 0.26236769556999207\nEpoch 3689, Loss: 0.2623668611049652\nEpoch 3690, Loss: 0.2623656392097473\nEpoch 3691, Loss: 0.262371301651001\nEpoch 3692, Loss: 0.2623646557331085\nEpoch 3693, Loss: 0.26236477494239807\nEpoch 3694, Loss: 0.2623644769191742\nEpoch 3695, Loss: 0.26236391067504883\nEpoch 3696, Loss: 0.2623631954193115\nEpoch 3697, Loss: 0.2623623311519623\nEpoch 3698, Loss: 0.2623661160469055\nEpoch 3699, Loss: 0.26236599683761597\nEpoch 3700, Loss: 0.2623629570007324\nEpoch 3701, Loss: 0.26236477494239807\nEpoch 3702, Loss: 0.26236623525619507\nEpoch 3703, Loss: 0.2623671591281891\nEpoch 3704, Loss: 0.2623675763607025\nEpoch 3705, Loss: 0.2623676359653473\nEpoch 3706, Loss: 0.2623671591281891\nEpoch 3707, Loss: 0.2623663544654846\nEpoch 3708, Loss: 0.2623651921749115\nEpoch 3709, Loss: 0.2623661756515503\nEpoch 3710, Loss: 0.2623644173145294\nEpoch 3711, Loss: 0.26236459612846375\nEpoch 3712, Loss: 0.2623644173145294\nEpoch 3713, Loss: 0.2623639702796936\nEpoch 3714, Loss: 0.2623633146286011\nEpoch 3715, Loss: 0.26236414909362793\nEpoch 3716, Loss: 0.2623623311519623\nEpoch 3717, Loss: 0.26236191391944885\nEpoch 3718, Loss: 0.2623671591281891\nEpoch 3719, Loss: 0.26236239075660706\nEpoch 3720, Loss: 0.26236313581466675\nEpoch 3721, Loss: 0.2623635530471802\nEpoch 3722, Loss: 0.2623636722564697\nEpoch 3723, Loss: 0.2623644769191742\nEpoch 3724, Loss: 0.26236385107040405\nEpoch 3725, Loss: 0.26236385107040405\nEpoch 3726, Loss: 0.26236361265182495\nEpoch 3727, Loss: 0.26236313581466675\nEpoch 3728, Loss: 0.26236605644226074\nEpoch 3729, Loss: 0.2623634338378906\nEpoch 3730, Loss: 0.2623640298843384\nEpoch 3731, Loss: 0.26236435770988464\nEpoch 3732, Loss: 0.26236429810523987\nEpoch 3733, Loss: 0.26236385107040405\nEpoch 3734, Loss: 0.2623632550239563\nEpoch 3735, Loss: 0.26236459612846375\nEpoch 3736, Loss: 0.26236221194267273\nEpoch 3737, Loss: 0.2623618543148041\nEpoch 3738, Loss: 0.2623692452907562\nEpoch 3739, Loss: 0.2623622715473175\nEpoch 3740, Loss: 0.2623630166053772\nEpoch 3741, Loss: 0.26236337423324585\nEpoch 3742, Loss: 0.26236337423324585\nEpoch 3743, Loss: 0.262363076210022\nEpoch 3744, Loss: 0.2623630166053772\nEpoch 3745, Loss: 0.2623627483844757\nEpoch 3746, Loss: 0.2623625695705414\nEpoch 3747, Loss: 0.26236483454704285\nEpoch 3748, Loss: 0.2623634338378906\nEpoch 3749, Loss: 0.26236435770988464\nEpoch 3750, Loss: 0.26236477494239807\nEpoch 3751, Loss: 0.2623648941516876\nEpoch 3752, Loss: 0.26236459612846375\nEpoch 3753, Loss: 0.2623640298843384\nEpoch 3754, Loss: 0.2623631954193115\nEpoch 3755, Loss: 0.2623661160469055\nEpoch 3756, Loss: 0.26236197352409363\nEpoch 3757, Loss: 0.2623634934425354\nEpoch 3758, Loss: 0.26236262917518616\nEpoch 3759, Loss: 0.26236337423324585\nEpoch 3760, Loss: 0.26236385107040405\nEpoch 3761, Loss: 0.26236391067504883\nEpoch 3762, Loss: 0.2623636722564697\nEpoch 3763, Loss: 0.26236391067504883\nEpoch 3764, Loss: 0.2623633146286011\nEpoch 3765, Loss: 0.26236313581466675\nEpoch 3766, Loss: 0.26236268877983093\nEpoch 3767, Loss: 0.26236820220947266\nEpoch 3768, Loss: 0.2623630166053772\nEpoch 3769, Loss: 0.2623636722564697\nEpoch 3770, Loss: 0.2623639702796936\nEpoch 3771, Loss: 0.26236385107040405\nEpoch 3772, Loss: 0.2623634338378906\nEpoch 3773, Loss: 0.2623627483844757\nEpoch 3774, Loss: 0.26236453652381897\nEpoch 3775, Loss: 0.2623617947101593\nEpoch 3776, Loss: 0.2623652517795563\nEpoch 3777, Loss: 0.26236268877983093\nEpoch 3778, Loss: 0.2623635530471802\nEpoch 3779, Loss: 0.26236408948898315\nEpoch 3780, Loss: 0.26236429810523987\nEpoch 3781, Loss: 0.2623640298843384\nEpoch 3782, Loss: 0.2623635530471802\nEpoch 3783, Loss: 0.26236557960510254\nEpoch 3784, Loss: 0.2623627483844757\nEpoch 3785, Loss: 0.26236245036125183\nEpoch 3786, Loss: 0.26236408948898315\nEpoch 3787, Loss: 0.2623630166053772\nEpoch 3788, Loss: 0.2623637914657593\nEpoch 3789, Loss: 0.2623642385005951\nEpoch 3790, Loss: 0.2623642385005951\nEpoch 3791, Loss: 0.26236385107040405\nEpoch 3792, Loss: 0.2623632550239563\nEpoch 3793, Loss: 0.262365460395813\nEpoch 3794, Loss: 0.2623623311519623\nEpoch 3795, Loss: 0.26236197352409363\nEpoch 3796, Loss: 0.2623681426048279\nEpoch 3797, Loss: 0.26236245036125183\nEpoch 3798, Loss: 0.2623631954193115\nEpoch 3799, Loss: 0.2623634934425354\nEpoch 3800, Loss: 0.2623635530471802\nEpoch 3801, Loss: 0.2623632550239563\nEpoch 3802, Loss: 0.2623630166053772\nEpoch 3803, Loss: 0.2623627483844757\nEpoch 3804, Loss: 0.2623625695705414\nEpoch 3805, Loss: 0.26236483454704285\nEpoch 3806, Loss: 0.2623634934425354\nEpoch 3807, Loss: 0.26236435770988464\nEpoch 3808, Loss: 0.26236477494239807\nEpoch 3809, Loss: 0.26236483454704285\nEpoch 3810, Loss: 0.2623644769191742\nEpoch 3811, Loss: 0.26236385107040405\nEpoch 3812, Loss: 0.2623637318611145\nEpoch 3813, Loss: 0.26236286759376526\nEpoch 3814, Loss: 0.2623625099658966\nEpoch 3815, Loss: 0.2623651921749115\nEpoch 3816, Loss: 0.2623630166053772\nEpoch 3817, Loss: 0.2623636722564697\nEpoch 3818, Loss: 0.2623639702796936\nEpoch 3819, Loss: 0.26236391067504883\nEpoch 3820, Loss: 0.26236361265182495\nEpoch 3821, Loss: 0.2623633146286011\nEpoch 3822, Loss: 0.2623630166053772\nEpoch 3823, Loss: 0.2623628079891205\nEpoch 3824, Loss: 0.26236286759376526\nEpoch 3825, Loss: 0.26236361265182495\nEpoch 3826, Loss: 0.2623644173145294\nEpoch 3827, Loss: 0.2623648941516876\nEpoch 3828, Loss: 0.2623648941516876\nEpoch 3829, Loss: 0.26236459612846375\nEpoch 3830, Loss: 0.26236391067504883\nEpoch 3831, Loss: 0.26236477494239807\nEpoch 3832, Loss: 0.2623629570007324\nEpoch 3833, Loss: 0.26236245036125183\nEpoch 3834, Loss: 0.26236435770988464\nEpoch 3835, Loss: 0.26236286759376526\nEpoch 3836, Loss: 0.2623636722564697\nEpoch 3837, Loss: 0.2623639702796936\nEpoch 3838, Loss: 0.2623639702796936\nEpoch 3839, Loss: 0.26236361265182495\nEpoch 3840, Loss: 0.2623637318611145\nEpoch 3841, Loss: 0.26236313581466675\nEpoch 3842, Loss: 0.26236286759376526\nEpoch 3843, Loss: 0.26236239075660706\nEpoch 3844, Loss: 0.2623703181743622\nEpoch 3845, Loss: 0.26236268877983093\nEpoch 3846, Loss: 0.2623633146286011\nEpoch 3847, Loss: 0.2623635530471802\nEpoch 3848, Loss: 0.2623634934425354\nEpoch 3849, Loss: 0.26236313581466675\nEpoch 3850, Loss: 0.26236245036125183\nEpoch 3851, Loss: 0.26236599683761597\nEpoch 3852, Loss: 0.2623615264892578\nEpoch 3853, Loss: 0.2623663544654846\nEpoch 3854, Loss: 0.2623625099658966\nEpoch 3855, Loss: 0.2623634338378906\nEpoch 3856, Loss: 0.2623640298843384\nEpoch 3857, Loss: 0.2623642385005951\nEpoch 3858, Loss: 0.26236408948898315\nEpoch 3859, Loss: 0.26236361265182495\nEpoch 3860, Loss: 0.2623656988143921\nEpoch 3861, Loss: 0.26236286759376526\nEpoch 3862, Loss: 0.2623625099658966\nEpoch 3863, Loss: 0.26236385107040405\nEpoch 3864, Loss: 0.2623632550239563\nEpoch 3865, Loss: 0.2623640298843384\nEpoch 3866, Loss: 0.2623644173145294\nEpoch 3867, Loss: 0.2623644769191742\nEpoch 3868, Loss: 0.26236408948898315\nEpoch 3869, Loss: 0.2623634338378906\nEpoch 3870, Loss: 0.2623651325702667\nEpoch 3871, Loss: 0.26236239075660706\nEpoch 3872, Loss: 0.2623620927333832\nEpoch 3873, Loss: 0.26236826181411743\nEpoch 3874, Loss: 0.2623625099658966\nEpoch 3875, Loss: 0.2623632550239563\nEpoch 3876, Loss: 0.2623636722564697\nEpoch 3877, Loss: 0.26236361265182495\nEpoch 3878, Loss: 0.2623633146286011\nEpoch 3879, Loss: 0.262363076210022\nEpoch 3880, Loss: 0.2623627483844757\nEpoch 3881, Loss: 0.2623625695705414\nEpoch 3882, Loss: 0.2623653709888458\nEpoch 3883, Loss: 0.2623634338378906\nEpoch 3884, Loss: 0.26236435770988464\nEpoch 3885, Loss: 0.26236477494239807\nEpoch 3886, Loss: 0.26236483454704285\nEpoch 3887, Loss: 0.2623644769191742\nEpoch 3888, Loss: 0.26236385107040405\nEpoch 3889, Loss: 0.26236408948898315\nEpoch 3890, Loss: 0.2623627483844757\nEpoch 3891, Loss: 0.26236239075660706\nEpoch 3892, Loss: 0.2623659372329712\nEpoch 3893, Loss: 0.26236286759376526\nEpoch 3894, Loss: 0.2623636722564697\nEpoch 3895, Loss: 0.2623639702796936\nEpoch 3896, Loss: 0.26236391067504883\nEpoch 3897, Loss: 0.26236361265182495\nEpoch 3898, Loss: 0.2623634934425354\nEpoch 3899, Loss: 0.2623630166053772\nEpoch 3900, Loss: 0.2623627483844757\nEpoch 3901, Loss: 0.26236337423324585\nEpoch 3902, Loss: 0.26236361265182495\nEpoch 3903, Loss: 0.2623644769191742\nEpoch 3904, Loss: 0.2623649537563324\nEpoch 3905, Loss: 0.2623649537563324\nEpoch 3906, Loss: 0.2623646557331085\nEpoch 3907, Loss: 0.2623639702796936\nEpoch 3908, Loss: 0.2623649537563324\nEpoch 3909, Loss: 0.2623629570007324\nEpoch 3910, Loss: 0.26236245036125183\nEpoch 3911, Loss: 0.2623648941516876\nEpoch 3912, Loss: 0.2623629570007324\nEpoch 3913, Loss: 0.2623637318611145\nEpoch 3914, Loss: 0.2623640298843384\nEpoch 3915, Loss: 0.2623640298843384\nEpoch 3916, Loss: 0.2623636722564697\nEpoch 3917, Loss: 0.26236385107040405\nEpoch 3918, Loss: 0.262363076210022\nEpoch 3919, Loss: 0.26236286759376526\nEpoch 3920, Loss: 0.2623628079891205\nEpoch 3921, Loss: 0.26236361265182495\nEpoch 3922, Loss: 0.26236459612846375\nEpoch 3923, Loss: 0.2623650133609772\nEpoch 3924, Loss: 0.2623650133609772\nEpoch 3925, Loss: 0.2623646557331085\nEpoch 3926, Loss: 0.2623639702796936\nEpoch 3927, Loss: 0.2623648941516876\nEpoch 3928, Loss: 0.2623629570007324\nEpoch 3929, Loss: 0.2623625099658966\nEpoch 3930, Loss: 0.26236483454704285\nEpoch 3931, Loss: 0.2623629570007324\nEpoch 3932, Loss: 0.2623637318611145\nEpoch 3933, Loss: 0.2623640298843384\nEpoch 3934, Loss: 0.26236408948898315\nEpoch 3935, Loss: 0.2623636722564697\nEpoch 3936, Loss: 0.2623637914657593\nEpoch 3937, Loss: 0.262363076210022\nEpoch 3938, Loss: 0.26236286759376526\nEpoch 3939, Loss: 0.26236313581466675\nEpoch 3940, Loss: 0.26236361265182495\nEpoch 3941, Loss: 0.26236459612846375\nEpoch 3942, Loss: 0.2623650133609772\nEpoch 3943, Loss: 0.2623650133609772\nEpoch 3944, Loss: 0.2623646557331085\nEpoch 3945, Loss: 0.2623639702796936\nEpoch 3946, Loss: 0.2623648941516876\nEpoch 3947, Loss: 0.26236286759376526\nEpoch 3948, Loss: 0.26236245036125183\nEpoch 3949, Loss: 0.2623651921749115\nEpoch 3950, Loss: 0.26236286759376526\nEpoch 3951, Loss: 0.2623637318611145\nEpoch 3952, Loss: 0.26236408948898315\nEpoch 3953, Loss: 0.2623640298843384\nEpoch 3954, Loss: 0.2623636722564697\nEpoch 3955, Loss: 0.26236385107040405\nEpoch 3956, Loss: 0.262363076210022\nEpoch 3957, Loss: 0.2623627483844757\nEpoch 3958, Loss: 0.26236337423324585\nEpoch 3959, Loss: 0.2623636722564697\nEpoch 3960, Loss: 0.26236459612846375\nEpoch 3961, Loss: 0.2623650133609772\nEpoch 3962, Loss: 0.2623650133609772\nEpoch 3963, Loss: 0.2623647153377533\nEpoch 3964, Loss: 0.2623639702796936\nEpoch 3965, Loss: 0.2623649537563324\nEpoch 3966, Loss: 0.26236286759376526\nEpoch 3967, Loss: 0.26236245036125183\nEpoch 3968, Loss: 0.262365460395813\nEpoch 3969, Loss: 0.26236286759376526\nEpoch 3970, Loss: 0.2623637318611145\nEpoch 3971, Loss: 0.26236408948898315\nEpoch 3972, Loss: 0.2623640298843384\nEpoch 3973, Loss: 0.2623636722564697\nEpoch 3974, Loss: 0.26236385107040405\nEpoch 3975, Loss: 0.262363076210022\nEpoch 3976, Loss: 0.2623628079891205\nEpoch 3977, Loss: 0.2623634934425354\nEpoch 3978, Loss: 0.26236361265182495\nEpoch 3979, Loss: 0.26236453652381897\nEpoch 3980, Loss: 0.2623650133609772\nEpoch 3981, Loss: 0.26236507296562195\nEpoch 3982, Loss: 0.2623647153377533\nEpoch 3983, Loss: 0.2623639702796936\nEpoch 3984, Loss: 0.26236507296562195\nEpoch 3985, Loss: 0.26236286759376526\nEpoch 3986, Loss: 0.26236239075660706\nEpoch 3987, Loss: 0.262365460395813\nEpoch 3988, Loss: 0.26236286759376526\nEpoch 3989, Loss: 0.2623636722564697\nEpoch 3990, Loss: 0.2623640298843384\nEpoch 3991, Loss: 0.2623639702796936\nEpoch 3992, Loss: 0.2623636722564697\nEpoch 3993, Loss: 0.2623639702796936\nEpoch 3994, Loss: 0.262363076210022\nEpoch 3995, Loss: 0.2623628079891205\nEpoch 3996, Loss: 0.26236361265182495\nEpoch 3997, Loss: 0.26236361265182495\nEpoch 3998, Loss: 0.26236459612846375\nEpoch 3999, Loss: 0.2623650133609772\nEpoch 4000, Loss: 0.26236507296562195\nEpoch 4001, Loss: 0.2623647153377533\nEpoch 4002, Loss: 0.2623639702796936\nEpoch 4003, Loss: 0.2623651325702667\nEpoch 4004, Loss: 0.26236286759376526\nEpoch 4005, Loss: 0.26236239075660706\nEpoch 4006, Loss: 0.2623656988143921\nEpoch 4007, Loss: 0.2623629570007324\nEpoch 4008, Loss: 0.2623637318611145\nEpoch 4009, Loss: 0.26236408948898315\nEpoch 4010, Loss: 0.2623640298843384\nEpoch 4011, Loss: 0.2623636722564697\nEpoch 4012, Loss: 0.2623640298843384\nEpoch 4013, Loss: 0.26236313581466675\nEpoch 4014, Loss: 0.2623627483844757\nEpoch 4015, Loss: 0.2623636722564697\nEpoch 4016, Loss: 0.26236361265182495\nEpoch 4017, Loss: 0.26236459612846375\nEpoch 4018, Loss: 0.26236507296562195\nEpoch 4019, Loss: 0.26236507296562195\nEpoch 4020, Loss: 0.2623647153377533\nEpoch 4021, Loss: 0.2623639702796936\nEpoch 4022, Loss: 0.2623651921749115\nEpoch 4023, Loss: 0.26236286759376526\nEpoch 4024, Loss: 0.26236239075660706\nEpoch 4025, Loss: 0.26236581802368164\nEpoch 4026, Loss: 0.26236286759376526\nEpoch 4027, Loss: 0.2623636722564697\nEpoch 4028, Loss: 0.26236408948898315\nEpoch 4029, Loss: 0.2623640298843384\nEpoch 4030, Loss: 0.26236361265182495\nEpoch 4031, Loss: 0.2623640298843384\nEpoch 4032, Loss: 0.2623630166053772\nEpoch 4033, Loss: 0.2623627483844757\nEpoch 4034, Loss: 0.2623637914657593\nEpoch 4035, Loss: 0.26236361265182495\nEpoch 4036, Loss: 0.26236459612846375\nEpoch 4037, Loss: 0.26236507296562195\nEpoch 4038, Loss: 0.26236507296562195\nEpoch 4039, Loss: 0.2623647153377533\nEpoch 4040, Loss: 0.2623640298843384\nEpoch 4041, Loss: 0.2623651921749115\nEpoch 4042, Loss: 0.26236286759376526\nEpoch 4043, Loss: 0.26236239075660706\nEpoch 4044, Loss: 0.2623659372329712\nEpoch 4045, Loss: 0.2623629570007324\nEpoch 4046, Loss: 0.2623636722564697\nEpoch 4047, Loss: 0.2623640298843384\nEpoch 4048, Loss: 0.2623640298843384\nEpoch 4049, Loss: 0.26236361265182495\nEpoch 4050, Loss: 0.26236408948898315\nEpoch 4051, Loss: 0.262363076210022\nEpoch 4052, Loss: 0.2623628079891205\nEpoch 4053, Loss: 0.2623639702796936\nEpoch 4054, Loss: 0.2623636722564697\nEpoch 4055, Loss: 0.26236459612846375\nEpoch 4056, Loss: 0.26236507296562195\nEpoch 4057, Loss: 0.26236507296562195\nEpoch 4058, Loss: 0.2623647153377533\nEpoch 4059, Loss: 0.2623640298843384\nEpoch 4060, Loss: 0.26236531138420105\nEpoch 4061, Loss: 0.26236286759376526\nEpoch 4062, Loss: 0.26236239075660706\nEpoch 4063, Loss: 0.26236605644226074\nEpoch 4064, Loss: 0.2623629570007324\nEpoch 4065, Loss: 0.2623637318611145\nEpoch 4066, Loss: 0.26236408948898315\nEpoch 4067, Loss: 0.2623640298843384\nEpoch 4068, Loss: 0.26236361265182495\nEpoch 4069, Loss: 0.26236408948898315\nEpoch 4070, Loss: 0.26236313581466675\nEpoch 4071, Loss: 0.2623628079891205\nEpoch 4072, Loss: 0.2623639702796936\nEpoch 4073, Loss: 0.2623636722564697\nEpoch 4074, Loss: 0.26236459612846375\nEpoch 4075, Loss: 0.26236507296562195\nEpoch 4076, Loss: 0.2623651325702667\nEpoch 4077, Loss: 0.26236477494239807\nEpoch 4078, Loss: 0.2623639702796936\nEpoch 4079, Loss: 0.26236531138420105\nEpoch 4080, Loss: 0.2623628079891205\nEpoch 4081, Loss: 0.26236239075660706\nEpoch 4082, Loss: 0.26236623525619507\nEpoch 4083, Loss: 0.26236286759376526\nEpoch 4084, Loss: 0.2623636722564697\nEpoch 4085, Loss: 0.26236408948898315\nEpoch 4086, Loss: 0.2623640298843384\nEpoch 4087, Loss: 0.2623636722564697\nEpoch 4088, Loss: 0.2623642385005951\nEpoch 4089, Loss: 0.262363076210022\nEpoch 4090, Loss: 0.2623627483844757\nEpoch 4091, Loss: 0.26236414909362793\nEpoch 4092, Loss: 0.2623636722564697\nEpoch 4093, Loss: 0.26236459612846375\nEpoch 4094, Loss: 0.2623651325702667\nEpoch 4095, Loss: 0.2623651325702667\nEpoch 4096, Loss: 0.26236477494239807\nEpoch 4097, Loss: 0.2623640298843384\nEpoch 4098, Loss: 0.2623653709888458\nEpoch 4099, Loss: 0.2623628079891205\nEpoch 4100, Loss: 0.26236239075660706\nEpoch 4101, Loss: 0.2623663544654846\nEpoch 4102, Loss: 0.26236286759376526\nEpoch 4103, Loss: 0.2623637318611145\nEpoch 4104, Loss: 0.26236408948898315\nEpoch 4105, Loss: 0.2623640298843384\nEpoch 4106, Loss: 0.26236361265182495\nEpoch 4107, Loss: 0.2623642385005951\nEpoch 4108, Loss: 0.2623630166053772\nEpoch 4109, Loss: 0.2623627483844757\nEpoch 4110, Loss: 0.26236429810523987\nEpoch 4111, Loss: 0.26236361265182495\nEpoch 4112, Loss: 0.26236459612846375\nEpoch 4113, Loss: 0.2623651325702667\nEpoch 4114, Loss: 0.2623651325702667\nEpoch 4115, Loss: 0.2623647153377533\nEpoch 4116, Loss: 0.2623640298843384\nEpoch 4117, Loss: 0.26236552000045776\nEpoch 4118, Loss: 0.26236286759376526\nEpoch 4119, Loss: 0.2623623311519623\nEpoch 4120, Loss: 0.26236653327941895\nEpoch 4121, Loss: 0.26236286759376526\nEpoch 4122, Loss: 0.2623637318611145\nEpoch 4123, Loss: 0.26236408948898315\nEpoch 4124, Loss: 0.2623640298843384\nEpoch 4125, Loss: 0.2623636722564697\nEpoch 4126, Loss: 0.26236429810523987\nEpoch 4127, Loss: 0.2623630166053772\nEpoch 4128, Loss: 0.2623627483844757\nEpoch 4129, Loss: 0.2623644173145294\nEpoch 4130, Loss: 0.26236361265182495\nEpoch 4131, Loss: 0.2623646557331085\nEpoch 4132, Loss: 0.26236507296562195\nEpoch 4133, Loss: 0.2623651325702667\nEpoch 4134, Loss: 0.26236483454704285\nEpoch 4135, Loss: 0.2623640298843384\nEpoch 4136, Loss: 0.26236557960510254\nEpoch 4137, Loss: 0.26236286759376526\nEpoch 4138, Loss: 0.26236239075660706\nEpoch 4139, Loss: 0.2623666524887085\nEpoch 4140, Loss: 0.26236286759376526\nEpoch 4141, Loss: 0.2623637318611145\nEpoch 4142, Loss: 0.26236408948898315\nEpoch 4143, Loss: 0.26236408948898315\nEpoch 4144, Loss: 0.2623636722564697\nEpoch 4145, Loss: 0.26236435770988464\nEpoch 4146, Loss: 0.2623630166053772\nEpoch 4147, Loss: 0.2623627483844757\nEpoch 4148, Loss: 0.26236453652381897\nEpoch 4149, Loss: 0.26236361265182495\nEpoch 4150, Loss: 0.2623646557331085\nEpoch 4151, Loss: 0.2623651325702667\nEpoch 4152, Loss: 0.2623651921749115\nEpoch 4153, Loss: 0.26236477494239807\nEpoch 4154, Loss: 0.2623640298843384\nEpoch 4155, Loss: 0.26236557960510254\nEpoch 4156, Loss: 0.26236286759376526\nEpoch 4157, Loss: 0.26236239075660706\nEpoch 4158, Loss: 0.2623668611049652\nEpoch 4159, Loss: 0.26236286759376526\nEpoch 4160, Loss: 0.2623636722564697\nEpoch 4161, Loss: 0.26236408948898315\nEpoch 4162, Loss: 0.2623640298843384\nEpoch 4163, Loss: 0.2623636722564697\nEpoch 4164, Loss: 0.2623644173145294\nEpoch 4165, Loss: 0.2623630166053772\nEpoch 4166, Loss: 0.2623627483844757\nEpoch 4167, Loss: 0.2623647153377533\nEpoch 4168, Loss: 0.26236361265182495\nEpoch 4169, Loss: 0.2623646557331085\nEpoch 4170, Loss: 0.2623651325702667\nEpoch 4171, Loss: 0.2623651325702667\nEpoch 4172, Loss: 0.26236477494239807\nEpoch 4173, Loss: 0.2623639702796936\nEpoch 4174, Loss: 0.2623656988143921\nEpoch 4175, Loss: 0.26236286759376526\nEpoch 4176, Loss: 0.26236239075660706\nEpoch 4177, Loss: 0.26236698031425476\nEpoch 4178, Loss: 0.26236286759376526\nEpoch 4179, Loss: 0.2623637318611145\nEpoch 4180, Loss: 0.26236408948898315\nEpoch 4181, Loss: 0.2623640298843384\nEpoch 4182, Loss: 0.2623636722564697\nEpoch 4183, Loss: 0.2623644769191742\nEpoch 4184, Loss: 0.2623630166053772\nEpoch 4185, Loss: 0.2623627483844757\nEpoch 4186, Loss: 0.26236483454704285\nEpoch 4187, Loss: 0.26236361265182495\nEpoch 4188, Loss: 0.2623647153377533\nEpoch 4189, Loss: 0.2623651921749115\nEpoch 4190, Loss: 0.2623651921749115\nEpoch 4191, Loss: 0.26236483454704285\nEpoch 4192, Loss: 0.2623640298843384\nEpoch 4193, Loss: 0.2623656988143921\nEpoch 4194, Loss: 0.2623628079891205\nEpoch 4195, Loss: 0.2623623311519623\nEpoch 4196, Loss: 0.2623671591281891\nEpoch 4197, Loss: 0.26236286759376526\nEpoch 4198, Loss: 0.2623637318611145\nEpoch 4199, Loss: 0.26236408948898315\nEpoch 4200, Loss: 0.26236408948898315\nEpoch 4201, Loss: 0.2623636722564697\nEpoch 4202, Loss: 0.26236453652381897\nEpoch 4203, Loss: 0.2623630166053772\nEpoch 4204, Loss: 0.2623627483844757\nEpoch 4205, Loss: 0.26236483454704285\nEpoch 4206, Loss: 0.2623636722564697\nEpoch 4207, Loss: 0.2623646557331085\nEpoch 4208, Loss: 0.2623651921749115\nEpoch 4209, Loss: 0.2623651921749115\nEpoch 4210, Loss: 0.26236477494239807\nEpoch 4211, Loss: 0.2623640298843384\nEpoch 4212, Loss: 0.26236575841903687\nEpoch 4213, Loss: 0.26236286759376526\nEpoch 4214, Loss: 0.2623623311519623\nEpoch 4215, Loss: 0.26236727833747864\nEpoch 4216, Loss: 0.26236286759376526\nEpoch 4217, Loss: 0.2623637318611145\nEpoch 4218, Loss: 0.26236408948898315\nEpoch 4219, Loss: 0.26236408948898315\nEpoch 4220, Loss: 0.2623636722564697\nEpoch 4221, Loss: 0.26236459612846375\nEpoch 4222, Loss: 0.2623630166053772\nEpoch 4223, Loss: 0.2623627483844757\nEpoch 4224, Loss: 0.26236507296562195\nEpoch 4225, Loss: 0.26236361265182495\nEpoch 4226, Loss: 0.2623647153377533\nEpoch 4227, Loss: 0.2623651921749115\nEpoch 4228, Loss: 0.2623652517795563\nEpoch 4229, Loss: 0.26236483454704285\nEpoch 4230, Loss: 0.26236408948898315\nEpoch 4231, Loss: 0.2623658776283264\nEpoch 4232, Loss: 0.2623628079891205\nEpoch 4233, Loss: 0.2623623311519623\nEpoch 4234, Loss: 0.2623673975467682\nEpoch 4235, Loss: 0.2623628079891205\nEpoch 4236, Loss: 0.2623637318611145\nEpoch 4237, Loss: 0.26236414909362793\nEpoch 4238, Loss: 0.2623640298843384\nEpoch 4239, Loss: 0.2623636722564697\nEpoch 4240, Loss: 0.26236459612846375\nEpoch 4241, Loss: 0.2623630166053772\nEpoch 4242, Loss: 0.26236268877983093\nEpoch 4243, Loss: 0.2623651325702667\nEpoch 4244, Loss: 0.2623636722564697\nEpoch 4245, Loss: 0.2623647153377533\nEpoch 4246, Loss: 0.2623651921749115\nEpoch 4247, Loss: 0.26236531138420105\nEpoch 4248, Loss: 0.26236483454704285\nEpoch 4249, Loss: 0.26236408948898315\nEpoch 4250, Loss: 0.2623658776283264\nEpoch 4251, Loss: 0.2623628079891205\nEpoch 4252, Loss: 0.2623623311519623\nEpoch 4253, Loss: 0.2623675763607025\nEpoch 4254, Loss: 0.2623628079891205\nEpoch 4255, Loss: 0.2623637318611145\nEpoch 4256, Loss: 0.26236408948898315\nEpoch 4257, Loss: 0.2623640298843384\nEpoch 4258, Loss: 0.2623636722564697\nEpoch 4259, Loss: 0.26236459612846375\nEpoch 4260, Loss: 0.2623630166053772\nEpoch 4261, Loss: 0.2623627483844757\nEpoch 4262, Loss: 0.2623652517795563\nEpoch 4263, Loss: 0.2623636722564697\nEpoch 4264, Loss: 0.2623647153377533\nEpoch 4265, Loss: 0.2623651921749115\nEpoch 4266, Loss: 0.2623652517795563\nEpoch 4267, Loss: 0.26236483454704285\nEpoch 4268, Loss: 0.26236408948898315\nEpoch 4269, Loss: 0.2623659372329712\nEpoch 4270, Loss: 0.2623628079891205\nEpoch 4271, Loss: 0.2623622715473175\nEpoch 4272, Loss: 0.26236769556999207\nEpoch 4273, Loss: 0.26236286759376526\nEpoch 4274, Loss: 0.2623637318611145\nEpoch 4275, Loss: 0.26236408948898315\nEpoch 4276, Loss: 0.2623640298843384\nEpoch 4277, Loss: 0.2623636722564697\nEpoch 4278, Loss: 0.2623647153377533\nEpoch 4279, Loss: 0.2623630166053772\nEpoch 4280, Loss: 0.2623627483844757\nEpoch 4281, Loss: 0.2623653709888458\nEpoch 4282, Loss: 0.2623636722564697\nEpoch 4283, Loss: 0.2623647153377533\nEpoch 4284, Loss: 0.2623652517795563\nEpoch 4285, Loss: 0.2623652517795563\nEpoch 4286, Loss: 0.2623648941516876\nEpoch 4287, Loss: 0.26236408948898315\nEpoch 4288, Loss: 0.26236599683761597\nEpoch 4289, Loss: 0.26236286759376526\nEpoch 4290, Loss: 0.2623622715473175\nEpoch 4291, Loss: 0.2623678743839264\nEpoch 4292, Loss: 0.26236286759376526\nEpoch 4293, Loss: 0.2623637318611145\nEpoch 4294, Loss: 0.26236414909362793\nEpoch 4295, Loss: 0.26236408948898315\nEpoch 4296, Loss: 0.2623636722564697\nEpoch 4297, Loss: 0.26236477494239807\nEpoch 4298, Loss: 0.2623630166053772\nEpoch 4299, Loss: 0.2623627483844757\nEpoch 4300, Loss: 0.26236552000045776\nEpoch 4301, Loss: 0.2623636722564697\nEpoch 4302, Loss: 0.2623647153377533\nEpoch 4303, Loss: 0.2623652517795563\nEpoch 4304, Loss: 0.2623651921749115\nEpoch 4305, Loss: 0.26236483454704285\nEpoch 4306, Loss: 0.26236408948898315\nEpoch 4307, Loss: 0.2623661160469055\nEpoch 4308, Loss: 0.2623628079891205\nEpoch 4309, Loss: 0.2623623311519623\nEpoch 4310, Loss: 0.2623680830001831\nEpoch 4311, Loss: 0.26236286759376526\nEpoch 4312, Loss: 0.2623637318611145\nEpoch 4313, Loss: 0.26236414909362793\nEpoch 4314, Loss: 0.26236408948898315\nEpoch 4315, Loss: 0.2623636722564697\nEpoch 4316, Loss: 0.26236477494239807\nEpoch 4317, Loss: 0.2623630166053772\nEpoch 4318, Loss: 0.26236268877983093\nEpoch 4319, Loss: 0.2623656988143921\nEpoch 4320, Loss: 0.2623636722564697\nEpoch 4321, Loss: 0.26236477494239807\nEpoch 4322, Loss: 0.2623652517795563\nEpoch 4323, Loss: 0.26236531138420105\nEpoch 4324, Loss: 0.2623648941516876\nEpoch 4325, Loss: 0.26236408948898315\nEpoch 4326, Loss: 0.2623661756515503\nEpoch 4327, Loss: 0.2623627483844757\nEpoch 4328, Loss: 0.2623622715473175\nEpoch 4329, Loss: 0.2623681426048279\nEpoch 4330, Loss: 0.2623628079891205\nEpoch 4331, Loss: 0.2623637318611145\nEpoch 4332, Loss: 0.26236414909362793\nEpoch 4333, Loss: 0.26236408948898315\nEpoch 4334, Loss: 0.2623636722564697\nEpoch 4335, Loss: 0.26236483454704285\nEpoch 4336, Loss: 0.2623630166053772\nEpoch 4337, Loss: 0.26236268877983093\nEpoch 4338, Loss: 0.26236581802368164\nEpoch 4339, Loss: 0.2623636722564697\nEpoch 4340, Loss: 0.26236477494239807\nEpoch 4341, Loss: 0.2623652517795563\nEpoch 4342, Loss: 0.26236531138420105\nEpoch 4343, Loss: 0.2623648941516876\nEpoch 4344, Loss: 0.26236408948898315\nEpoch 4345, Loss: 0.26236623525619507\nEpoch 4346, Loss: 0.2623627483844757\nEpoch 4347, Loss: 0.2623622715473175\nEpoch 4348, Loss: 0.2623683214187622\nEpoch 4349, Loss: 0.2623627483844757\nEpoch 4350, Loss: 0.2623637318611145\nEpoch 4351, Loss: 0.26236408948898315\nEpoch 4352, Loss: 0.26236408948898315\nEpoch 4353, Loss: 0.2623636722564697\nEpoch 4354, Loss: 0.26236483454704285\nEpoch 4355, Loss: 0.2623630166053772\nEpoch 4356, Loss: 0.26236268877983093\nEpoch 4357, Loss: 0.2623658776283264\nEpoch 4358, Loss: 0.2623636722564697\nEpoch 4359, Loss: 0.26236477494239807\nEpoch 4360, Loss: 0.26236531138420105\nEpoch 4361, Loss: 0.26236531138420105\nEpoch 4362, Loss: 0.2623648941516876\nEpoch 4363, Loss: 0.26236414909362793\nEpoch 4364, Loss: 0.26236629486083984\nEpoch 4365, Loss: 0.2623628079891205\nEpoch 4366, Loss: 0.26236221194267273\nEpoch 4367, Loss: 0.26236850023269653\nEpoch 4368, Loss: 0.2623628079891205\nEpoch 4369, Loss: 0.2623637318611145\nEpoch 4370, Loss: 0.26236414909362793\nEpoch 4371, Loss: 0.26236408948898315\nEpoch 4372, Loss: 0.2623636722564697\nEpoch 4373, Loss: 0.2623649537563324\nEpoch 4374, Loss: 0.2623630166053772\nEpoch 4375, Loss: 0.26236262917518616\nEpoch 4376, Loss: 0.26236599683761597\nEpoch 4377, Loss: 0.2623636722564697\nEpoch 4378, Loss: 0.2623647153377533\nEpoch 4379, Loss: 0.2623653709888458\nEpoch 4380, Loss: 0.2623653709888458\nEpoch 4381, Loss: 0.2623648941516876\nEpoch 4382, Loss: 0.26236408948898315\nEpoch 4383, Loss: 0.26236629486083984\nEpoch 4384, Loss: 0.2623627483844757\nEpoch 4385, Loss: 0.26236221194267273\nEpoch 4386, Loss: 0.2623686194419861\nEpoch 4387, Loss: 0.2623627483844757\nEpoch 4388, Loss: 0.2623636722564697\nEpoch 4389, Loss: 0.26236408948898315\nEpoch 4390, Loss: 0.26236408948898315\nEpoch 4391, Loss: 0.2623636722564697\nEpoch 4392, Loss: 0.2623650133609772\nEpoch 4393, Loss: 0.2623630166053772\nEpoch 4394, Loss: 0.26236268877983093\nEpoch 4395, Loss: 0.2623661160469055\nEpoch 4396, Loss: 0.2623636722564697\nEpoch 4397, Loss: 0.26236477494239807\nEpoch 4398, Loss: 0.26236531138420105\nEpoch 4399, Loss: 0.2623653709888458\nEpoch 4400, Loss: 0.2623649537563324\nEpoch 4401, Loss: 0.26236408948898315\nEpoch 4402, Loss: 0.2623664140701294\nEpoch 4403, Loss: 0.2623628079891205\nEpoch 4404, Loss: 0.26236221194267273\nEpoch 4405, Loss: 0.2623687982559204\nEpoch 4406, Loss: 0.2623628079891205\nEpoch 4407, Loss: 0.2623637318611145\nEpoch 4408, Loss: 0.26236414909362793\nEpoch 4409, Loss: 0.26236408948898315\nEpoch 4410, Loss: 0.2623636722564697\nEpoch 4411, Loss: 0.26236507296562195\nEpoch 4412, Loss: 0.2623630166053772\nEpoch 4413, Loss: 0.26236268877983093\nEpoch 4414, Loss: 0.26236629486083984\nEpoch 4415, Loss: 0.2623636722564697\nEpoch 4416, Loss: 0.26236477494239807\nEpoch 4417, Loss: 0.2623653709888458\nEpoch 4418, Loss: 0.2623653709888458\nEpoch 4419, Loss: 0.2623649537563324\nEpoch 4420, Loss: 0.26236408948898315\nEpoch 4421, Loss: 0.2623664140701294\nEpoch 4422, Loss: 0.2623627483844757\nEpoch 4423, Loss: 0.2623622715473175\nEpoch 4424, Loss: 0.26236891746520996\nEpoch 4425, Loss: 0.2623627483844757\nEpoch 4426, Loss: 0.2623637318611145\nEpoch 4427, Loss: 0.26236414909362793\nEpoch 4428, Loss: 0.26236408948898315\nEpoch 4429, Loss: 0.2623637318611145\nEpoch 4430, Loss: 0.26236507296562195\nEpoch 4431, Loss: 0.2623630166053772\nEpoch 4432, Loss: 0.26236262917518616\nEpoch 4433, Loss: 0.2623663544654846\nEpoch 4434, Loss: 0.2623636722564697\nEpoch 4435, Loss: 0.26236483454704285\nEpoch 4436, Loss: 0.26236531138420105\nEpoch 4437, Loss: 0.2623653709888458\nEpoch 4438, Loss: 0.2623649537563324\nEpoch 4439, Loss: 0.26236408948898315\nEpoch 4440, Loss: 0.26236647367477417\nEpoch 4441, Loss: 0.2623627483844757\nEpoch 4442, Loss: 0.26236221194267273\nEpoch 4443, Loss: 0.2623690962791443\nEpoch 4444, Loss: 0.2623627483844757\nEpoch 4445, Loss: 0.2623636722564697\nEpoch 4446, Loss: 0.2623642385005951\nEpoch 4447, Loss: 0.26236408948898315\nEpoch 4448, Loss: 0.2623637318611145\nEpoch 4449, Loss: 0.2623651325702667\nEpoch 4450, Loss: 0.2623629570007324\nEpoch 4451, Loss: 0.26236262917518616\nEpoch 4452, Loss: 0.26236653327941895\nEpoch 4453, Loss: 0.2623636722564697\nEpoch 4454, Loss: 0.26236483454704285\nEpoch 4455, Loss: 0.26236531138420105\nEpoch 4456, Loss: 0.262365460395813\nEpoch 4457, Loss: 0.2623649537563324\nEpoch 4458, Loss: 0.26236414909362793\nEpoch 4459, Loss: 0.2623665928840637\nEpoch 4460, Loss: 0.2623627483844757\nEpoch 4461, Loss: 0.26236221194267273\nEpoch 4462, Loss: 0.262369304895401\nEpoch 4463, Loss: 0.2623627483844757\nEpoch 4464, Loss: 0.2623636722564697\nEpoch 4465, Loss: 0.26236414909362793\nEpoch 4466, Loss: 0.26236408948898315\nEpoch 4467, Loss: 0.2623636722564697\nEpoch 4468, Loss: 0.2623651921749115\nEpoch 4469, Loss: 0.2623630166053772\nEpoch 4470, Loss: 0.2623625695705414\nEpoch 4471, Loss: 0.2623666524887085\nEpoch 4472, Loss: 0.2623637318611145\nEpoch 4473, Loss: 0.26236477494239807\nEpoch 4474, Loss: 0.2623653709888458\nEpoch 4475, Loss: 0.262365460395813\nEpoch 4476, Loss: 0.2623649537563324\nEpoch 4477, Loss: 0.26236414909362793\nEpoch 4478, Loss: 0.2623666524887085\nEpoch 4479, Loss: 0.2623627483844757\nEpoch 4480, Loss: 0.26236215233802795\nEpoch 4481, Loss: 0.26236942410469055\nEpoch 4482, Loss: 0.2623627483844757\nEpoch 4483, Loss: 0.2623637318611145\nEpoch 4484, Loss: 0.26236414909362793\nEpoch 4485, Loss: 0.26236408948898315\nEpoch 4486, Loss: 0.2623636722564697\nEpoch 4487, Loss: 0.2623652517795563\nEpoch 4488, Loss: 0.2623630166053772\nEpoch 4489, Loss: 0.26236262917518616\nEpoch 4490, Loss: 0.26236680150032043\nEpoch 4491, Loss: 0.2623636722564697\nEpoch 4492, Loss: 0.26236483454704285\nEpoch 4493, Loss: 0.2623653709888458\nEpoch 4494, Loss: 0.262365460395813\nEpoch 4495, Loss: 0.2623649537563324\nEpoch 4496, Loss: 0.26236408948898315\nEpoch 4497, Loss: 0.26236674189567566\nEpoch 4498, Loss: 0.2623627483844757\nEpoch 4499, Loss: 0.26236221194267273\n\n\n\nour_svm.w, our_svm.b\n\n(Parameter containing:\n tensor([[ 0.1670],\n         [-0.4376]], requires_grad=True),\n Parameter containing:\n tensor([0.9858], requires_grad=True))\n\n\n\nsklearn_svm = svm.SVC(kernel='linear', C=1)\nsklearn_svm.fit(X, y)\nsklearn_svm.coef_, sklearn_svm.intercept_\n\n(array([[ 0.1761835 , -0.95555852]]), array([2.47220356]))"
  },
  {
    "objectID": "notebooks/rl-2.html",
    "href": "notebooks/rl-2.html",
    "title": "Reinforcement Learning 2 Q Learning",
    "section": "",
    "text": "Reference\n\nDetailed Explanation and Python Implementation of Q-Learning Algorithm in OpenAI Gym (Cart-Pole)\n\n\nBasic Imports\nhttps://www.gymlibrary.dev/environments/classic_control/mountain_car/\n\nimport matplotlib.pyplot as plt\nimport torch\nimport gymnasium as gym\nimport pandas as pd\n\nimport numpy as np\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\nenv = gym.make('MountainCar-v0', render_mode='human')\n\n\nenv.reset(seed=42)\nfor i in range(100):\n    env.render()\n    action = env.action_space.sample()\n    observation, reward, terminated, truncated, info = env.step(action)\n    print(i, observation, reward, terminated, truncated, info)\n    if terminated:\n        break\n    \n\n0 [-0.4457913  -0.00058252] -1.0 False False {}\n1 [-0.4469521  -0.00116079] -1.0 False False {}\n2 [-0.44768268 -0.00073059] -1.0 False False {}\n3 [-4.4797775e-01 -2.9505082e-04] -1.0 False False {}\n4 [-0.4488351  -0.00085736] -1.0 False False {}\n5 [-4.4924849e-01 -4.1339413e-04] -1.0 False False {}\n6 [-0.4502149  -0.00096641] -1.0 False False {}\n7 [-0.45072725 -0.00051235] -1.0 False False {}\n8 [-0.4517818  -0.00105455] -1.0 False False {}\n9 [-0.45437083 -0.00258902] -1.0 False False {}\n10 [-0.45647532 -0.0021045 ] -1.0 False False {}\n11 [-0.46007985 -0.00360453] -1.0 False False {}\n12 [-0.4631579  -0.00307805] -1.0 False False {}\n13 [-0.46568677 -0.00252887] -1.0 False False {}\n14 [-0.4686478  -0.00296103] -1.0 False False {}\n15 [-0.4720191 -0.0033713] -1.0 False False {}\n16 [-0.47577572 -0.0037566 ] -1.0 False False {}\n17 [-0.47888976 -0.00311405] -1.0 False False {}\n18 [-0.4813381  -0.00244836] -1.0 False False {}\n19 [-0.48410258 -0.00276447] -1.0 False False {}\n20 [-0.48616257 -0.00206   ] -1.0 False False {}\n21 [-0.48750275 -0.00134018] -1.0 False False {}\n22 [-0.48911312 -0.00161037] -1.0 False False {}\n23 [-0.48998168 -0.00086855] -1.0 False False {}\n24 [-4.9010193e-01 -1.2025227e-04] -1.0 False False {}\n25 [-0.491473   -0.00137106] -1.0 False False {}\n26 [-0.4930846  -0.00161163] -1.0 False False {}\n27 [-0.49492478 -0.00184016] -1.0 False False {}\n28 [-0.49797973 -0.00305495] -1.0 False False {}\n29 [-0.5002266  -0.00224691] -1.0 False False {}\n30 [-0.5026487  -0.00242205] -1.0 False False {}\n31 [-0.5062278  -0.00357908] -1.0 False False {}\n32 [-0.50993705 -0.0037093 ] -1.0 False False {}\n33 [-0.5137488  -0.00381173] -1.0 False False {}\n34 [-0.5186344 -0.0048856] -1.0 False False {}\n35 [-0.52355725 -0.00492283] -1.0 False False {}\n36 [-0.52748036 -0.00392314] -1.0 False False {}\n37 [-0.5303744  -0.00289403] -1.0 False False {}\n38 [-0.5342176  -0.00384322] -1.0 False False {}\n39 [-0.5389812  -0.00476359] -1.0 False False {}\n40 [-0.54362947 -0.00464826] -1.0 False False {}\n41 [-0.5491276  -0.00549812] -1.0 False False {}\n42 [-0.5554344  -0.00630684] -1.0 False False {}\n43 [-0.56250286 -0.00706844] -1.0 False False {}\n44 [-0.5692802  -0.00677732] -1.0 False False {}\n45 [-0.57671595 -0.00743578] -1.0 False False {}\n46 [-0.5837551  -0.00703908] -1.0 False False {}\n47 [-0.5913454  -0.00759036] -1.0 False False {}\n48 [-0.5974312  -0.00608575] -1.0 False False {}\n49 [-0.60396767 -0.00653652] -1.0 False False {}\n50 [-0.61090726 -0.00693958] -1.0 False False {}\n51 [-0.6171995  -0.00629223] -1.0 False False {}\n52 [-0.6227989  -0.00559941] -1.0 False False {}\n53 [-0.6276652  -0.00486634] -1.0 False False {}\n54 [-0.6317637  -0.00409846] -1.0 False False {}\n55 [-0.63606507 -0.00430139] -1.0 False False {}\n56 [-0.6395389 -0.0034738] -1.0 False False {}\n57 [-0.64116055 -0.00162168] -1.0 False False {}\n58 [-0.6429187  -0.00175813] -1.0 False False {}\n59 [-0.6438009  -0.00088222] -1.0 False False {}\n60 [-0.64280105  0.00099988] -1.0 False False {}\n61 [-0.6399261   0.00287497] -1.0 False False {}\n62 [-0.63719624  0.00272982] -1.0 False False {}\n63 [-0.6326308   0.00456541] -1.0 False False {}\n64 [-0.6282622   0.00436864] -1.0 False False {}\n65 [-0.62212145  0.00614078] -1.0 False False {}\n66 [-0.61425245  0.00786899] -1.0 False False {}\n67 [-0.6047119   0.00954054] -1.0 False False {}\n68 [-0.593569   0.0111429] -1.0 False False {}\n69 [-0.58090514  0.01266384] -1.0 False False {}\n70 [-0.56681365  0.01409152] -1.0 False False {}\n71 [-0.55139893  0.01541472] -1.0 False False {}\n72 [-0.5357759   0.01562298] -1.0 False False {}\n73 [-0.5190616   0.01671429] -1.0 False False {}\n74 [-0.5023814   0.01668026] -1.0 False False {}\n75 [-0.48586014  0.01652124] -1.0 False False {}\n76 [-0.47062132  0.01523881] -1.0 False False {}\n77 [-0.45477816  0.01584315] -1.0 False False {}\n78 [-0.43944752  0.01533065] -1.0 False False {}\n79 [-0.42474133  0.0147062 ] -1.0 False False {}\n80 [-0.41176575  0.01297559] -1.0 False False {}\n81 [-0.39861324  0.01315249] -1.0 False False {}\n82 [-0.38637635  0.01223691] -1.0 False False {}\n83 [-0.3761398   0.01023656] -1.0 False False {}\n84 [-0.36597344  0.01016634] -1.0 False False {}\n85 [-0.35694572  0.00902772] -1.0 False False {}\n86 [-0.3481165   0.00882924] -1.0 False False {}\n87 [-0.3405434   0.00757308] -1.0 False False {}\n88 [-0.33327526  0.00726814] -1.0 False False {}\n89 [-0.32835823  0.00491702] -1.0 False False {}\n90 [-0.3238232   0.00453502] -1.0 False False {}\n91 [-0.32169843  0.0021248 ] -1.0 False False {}\n92 [-3.2199696e-01 -2.9854459e-04] -1.0 False False {}\n93 [-0.322717   -0.00072005] -1.0 False False {}\n94 [-0.32585412 -0.00313711] -1.0 False False {}\n95 [-0.32938883 -0.00353472] -1.0 False False {}\n96 [-0.3352991  -0.00591028] -1.0 False False {}\n97 [-0.34154773 -0.0062486 ] -1.0 False False {}\n98 [-0.34809482 -0.00654711] -1.0 False False {}\n99 [-0.35689825 -0.00880341] -1.0 False False {}\n\n\n\nnbins_pos = 3\nnbins_vel = 3\n\npos_bins = np.linspace(env.observation_space.low[0], env.observation_space.high[0], nbins_pos)\nvel_bins = np.linspace(env.observation_space.low[1], env.observation_space.high[1], nbins_vel)\n\n\npos_bins, vel_bins\n\n(array([-1.20000005, -0.30000001,  0.60000002]), array([-0.07,  0.  ,  0.07]))\n\n\n\n# Create a Q-table in pandas with multi-index\n# index in the form of (pos, vel)\n# columns are the actions\nn_actions = env.action_space.n\n\n\n# Create labels for bins with desired format\npos_labels = [f'{pos1:.2f} &lt; pos &lt; {pos2:.2f}' for pos1, pos2 in zip(pos_bins[:-1], pos_bins[1:])]\nvel_labels = [f'{vel1:.2f} &lt; vel &lt; {vel2:.2f}' for vel1, vel2 in zip(vel_bins[:-1], vel_bins[1:])]\n\n# Create MultiIndex for the Q-table\nindex = pd.MultiIndex.from_product([pos_labels, vel_labels], names=['pos', 'vel'])\n\n\n\ncolumns = range(n_actions)\n\n# Create Q-table with MultiIndex\nq_table = pd.DataFrame(0, index=index, columns=columns)\n\n\nq_table\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\npos\nvel\n\n\n\n\n\n\n\n-1.20 &lt; pos &lt; -0.30\n-0.07 &lt; vel &lt; 0.00\n0\n0\n0\n\n\n0.00 &lt; vel &lt; 0.07\n0\n0\n0\n\n\n-0.30 &lt; pos &lt; 0.60\n-0.07 &lt; vel &lt; 0.00\n0\n0\n0\n\n\n0.00 &lt; vel &lt; 0.07\n0\n0\n0\n\n\n\n\n\n\n\n\npos_bins\n\narray([-1.20000005, -0.30000001,  0.60000002])\n\n\n\n# adding -inft to the first and +inft to the last bin\npos_bins = np.concatenate([[-np.inf], pos_bins, [np.inf]])\nvel_bins = np.concatenate([[-np.inf], vel_bins, [np.inf]])\n\n# Create labels for bins with desired format\npos_labels = [f'{pos1:.2f} &lt; pos &lt; {pos2:.2f}' for pos1, pos2 in zip(pos_bins[:-1], pos_bins[1:])]\nvel_labels = [f'{vel1:.2f} &lt; vel &lt; {vel2:.2f}' for vel1, vel2 in zip(vel_bins[:-1], vel_bins[1:])]\n\n\n# Create MultiIndex for the Q-table\nindex = pd.MultiIndex.from_product([pos_labels, vel_labels], names=['pos', 'vel'])\n\n\n\ncolumns = range(n_actions)\n\n# Create Q-table with MultiIndex\nq_table = pd.DataFrame(0, index=index, columns=columns)\n\n\nq_table\n\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\npos\nvel\n\n\n\n\n\n\n\n-inf &lt; pos &lt; -1.20\n-inf &lt; vel &lt; -0.07\n0\n0\n0\n\n\n-0.07 &lt; vel &lt; 0.00\n0\n0\n0\n\n\n0.00 &lt; vel &lt; 0.07\n0\n0\n0\n\n\n0.07 &lt; vel &lt; inf\n0\n0\n0\n\n\n-1.20 &lt; pos &lt; -0.30\n-inf &lt; vel &lt; -0.07\n0\n0\n0\n\n\n-0.07 &lt; vel &lt; 0.00\n0\n0\n0\n\n\n0.00 &lt; vel &lt; 0.07\n0\n0\n0\n\n\n0.07 &lt; vel &lt; inf\n0\n0\n0\n\n\n-0.30 &lt; pos &lt; 0.60\n-inf &lt; vel &lt; -0.07\n0\n0\n0\n\n\n-0.07 &lt; vel &lt; 0.00\n0\n0\n0\n\n\n0.00 &lt; vel &lt; 0.07\n0\n0\n0\n\n\n0.07 &lt; vel &lt; inf\n0\n0\n0\n\n\n0.60 &lt; pos &lt; inf\n-inf &lt; vel &lt; -0.07\n0\n0\n0\n\n\n-0.07 &lt; vel &lt; 0.00\n0\n0\n0\n\n\n0.00 &lt; vel &lt; 0.07\n0\n0\n0\n\n\n0.07 &lt; vel &lt; inf\n0\n0\n0\n\n\n\n\n\n\n\n\npos_bins[1:-1]\n\narray([-1.20000005, -0.30000001,  0.60000002])\n\n\n\nnp.digitize([-1.3, -1.20, -1.1, 0.6, 5.0], pos_bins[1:-1])\n\narray([0, 1, 1, 2, 3])\n\n\n\n# Usually, we will create1a the multi-dim array in numpy\n\n# Create a Q-table in numpy\nq_table_np = np.arange(nbins_pos * nbins_vel * n_actions).reshape(nbins_pos, nbins_vel, n_actions)\nprint(q_table_np.shape)\n\n(3, 3, 3)\n\n\n\nq_table_np\n\narray([[[ 0,  1,  2],\n        [ 3,  4,  5],\n        [ 6,  7,  8]],\n\n       [[ 9, 10, 11],\n        [12, 13, 14],\n        [15, 16, 17]],\n\n       [[18, 19, 20],\n        [21, 22, 23],\n        [24, 25, 26]]])\n\n\n\ndef discretize_pos(pos):\n    return np.digitize(pos, pos_bins[1:-1])\n\ndef discretize_vel(vel):\n    return np.digitize(vel, vel_bins[1:-1])\n\nfrom ipywidgets import interact\n\n@interact(pos=(-1.3, 0.9, 0.01), vel=(-0.08, 0.09, 0.01))\ndef show_discretize(pos, vel):\n    print(f\"Position index {discretize_pos(pos)}, Velocity index {discretize_vel(vel)}, Value: {q_table_np[discretize_pos(pos), discretize_vel(vel)]}\")\n\n\n\n\n\n# Use untrained Q-table to play the game\n\nobservation,  _  = env.reset(seed=42)\n\nfor i in range(100):\n    env.render()\n    pos, vel = observation\n    pos_idx = discretize_pos(pos)\n    vel_idx = discretize_vel(vel)\n    action = np.argmax(q_table_np[pos_idx, vel_idx])\n    observation, reward, terminated, truncated, info = env.step(action)\n    print(i, observation, reward, terminated, truncated, info, action)\n    if terminated:\n        break\n    \n\n0 [-4.4479132e-01  4.1747934e-04] -1.0 False False {} 2\n1 [-0.4439594   0.00083191] -1.0 False False {} 2\n2 [-0.4427191   0.00124029] -1.0 False False {} 2\n3 [-0.4410795   0.00163962] -1.0 False False {} 2\n4 [-0.43905246  0.00202703] -1.0 False False {} 2\n5 [-0.43665275  0.00239971] -1.0 False False {} 2\n6 [-0.43389776  0.00275498] -1.0 False False {} 2\n7 [-0.43080744  0.00309032] -1.0 False False {} 2\n8 [-0.4274041   0.00340333] -1.0 False False {} 2\n9 [-0.42371225  0.00369185] -1.0 False False {} 2\n10 [-0.4197584   0.00395386] -1.0 False False {} 2\n11 [-0.41557083  0.00418759] -1.0 False False {} 2\n12 [-0.41117933  0.00439149] -1.0 False False {} 2\n13 [-0.40661508  0.00456424] -1.0 False False {} 2\n14 [-0.40191033  0.00470476] -1.0 False False {} 2\n15 [-0.3970981   0.00481224] -1.0 False False {} 2\n16 [-0.392212    0.00488609] -1.0 False False {} 2\n17 [-0.38728598  0.00492601] -1.0 False False {} 2\n18 [-0.38235408  0.00493192] -1.0 False False {} 2\n19 [-0.37745008  0.004904  ] -1.0 False False {} 2\n20 [-0.3726074   0.00484267] -1.0 False False {} 2\n21 [-0.36785883  0.00474856] -1.0 False False {} 2\n22 [-0.36323628  0.00462255] -1.0 False False {} 2\n23 [-0.3587706   0.00446569] -1.0 False False {} 2\n24 [-0.35449135  0.00427925] -1.0 False False {} 2\n25 [-0.3504267   0.00406465] -1.0 False False {} 2\n26 [-0.3466032  0.0038235] -1.0 False False {} 2\n27 [-0.34304565  0.00355754] -1.0 False False {} 2\n28 [-0.33977702  0.00326864] -1.0 False False {} 2\n29 [-0.33681822  0.0029588 ] -1.0 False False {} 2\n30 [-0.3341881   0.00263011] -1.0 False False {} 2\n31 [-0.33190334  0.00228476] -1.0 False False {} 2\n32 [-0.32997838  0.00192499] -1.0 False False {} 2\n33 [-0.32842523  0.00155313] -1.0 False False {} 2\n34 [-0.3272537   0.00117154] -1.0 False False {} 2\n35 [-0.32647103  0.00078265] -1.0 False False {} 2\n36 [-0.32608217  0.00038887] -1.0 False False {} 2\n37 [-3.2608950e-01 -7.3227225e-06] -1.0 False False {} 2\n38 [-0.32649297 -0.00040347] -1.0 False False {} 2\n39 [-0.3272901  -0.00079711] -1.0 False False {} 2\n40 [-0.32847586 -0.00118578] -1.0 False False {} 2\n41 [-0.3300429  -0.00156705] -1.0 False False {} 2\n42 [-0.33198142 -0.0019385 ] -1.0 False False {} 2\n43 [-0.33427918 -0.00229778] -1.0 False False {} 2\n44 [-0.33692175 -0.00264256] -1.0 False False {} 2\n45 [-0.33989236 -0.00297059] -1.0 False False {} 2\n46 [-0.34317204 -0.0032797 ] -1.0 False False {} 2\n47 [-0.34673983 -0.00356778] -1.0 False False {} 2\n48 [-0.35057268 -0.00383286] -1.0 False False {} 2\n49 [-0.35464573 -0.00407306] -1.0 False False {} 2\n50 [-0.35893238 -0.00428664] -1.0 False False {} 2\n51 [-0.3634044  -0.00447202] -1.0 False False {} 2\n52 [-0.36803216 -0.00462776] -1.0 False False {} 2\n53 [-0.37278476 -0.00475261] -1.0 False False {} 2\n54 [-0.3776303  -0.00484552] -1.0 False False {} 2\n55 [-0.3825359  -0.00490563] -1.0 False False {} 2\n56 [-0.38746822 -0.0049323 ] -1.0 False False {} 2\n57 [-0.39239335 -0.00492514] -1.0 False False {} 2\n58 [-0.39727733 -0.00488397] -1.0 False False {} 2\n59 [-0.40208617 -0.00480886] -1.0 False False {} 2\n60 [-0.40678635 -0.00470015] -1.0 False False {} 2\n61 [-0.41134477 -0.00455843] -1.0 False False {} 2\n62 [-0.41572928 -0.00438451] -1.0 False False {} 2\n63 [-0.41990876 -0.00417948] -1.0 False False {} 2\n64 [-0.42385343 -0.00394468] -1.0 False False {} 2\n65 [-0.4275351  -0.00368165] -1.0 False False {} 2\n66 [-0.43092728 -0.0033922 ] -1.0 False False {} 2\n67 [-0.4340056  -0.00307832] -1.0 False False {} 2\n68 [-0.4367478 -0.0027422] -1.0 False False {} 2\n69 [-0.43913403 -0.00238624] -1.0 False False {} 2\n70 [-0.441147   -0.00201297] -1.0 False False {} 2\n71 [-0.4427721  -0.00162507] -1.0 False False {} 2\n72 [-0.4439974  -0.00122535] -1.0 False False {} 2\n73 [-0.44481412 -0.0008167 ] -1.0 False False {} 2\n74 [-4.452162e-01 -4.020977e-04] -1.0 False False {} 2\n75 [-4.4520080e-01  1.5435844e-05] -1.0 False False {} 2\n76 [-4.4476792e-01  4.3285682e-04] -1.0 False False {} 2\n77 [-0.44392082  0.00084712] -1.0 False False {} 2\n78 [-0.4426656   0.00125521] -1.0 False False {} 2\n79 [-0.44101143  0.00165416] -1.0 False False {} 2\n80 [-0.43897036  0.00204107] -1.0 False False {} 2\n81 [-0.4365572   0.00241315] -1.0 False False {} 2\n82 [-0.4337895   0.00276774] -1.0 False False {} 2\n83 [-0.4306872   0.00310229] -1.0 False False {} 2\n84 [-0.42727277  0.00341444] -1.0 False False {} 2\n85 [-0.42357075  0.00370201] -1.0 False False {} 2\n86 [-0.41960773  0.003963  ] -1.0 False False {} 2\n87 [-0.41541207  0.00419566] -1.0 False False {} 2\n88 [-0.41101363  0.00439843] -1.0 False False {} 2\n89 [-0.40644366  0.00457001] -1.0 False False {} 2\n90 [-0.40173432  0.00470932] -1.0 False False {} 2\n91 [-0.39691874  0.00481556] -1.0 False False {} 2\n92 [-0.3920306   0.00488817] -1.0 False False {} 2\n93 [-0.38710377  0.00492683] -1.0 False False {} 2\n94 [-0.38217226  0.00493149] -1.0 False False {} 2\n95 [-0.37726995  0.00490233] -1.0 False False {} 2\n96 [-0.37243018  0.00483977] -1.0 False False {} 2\n97 [-0.3676857   0.00474447] -1.0 False False {} 2\n98 [-0.3630684  0.0046173] -1.0 False False {} 2\n99 [-0.35860908  0.00445932] -1.0 False False {} 2\n\n\n\n# Train the Q-table\n\n# Hyperparameters\nalpha = 0.2 # learning rate\ngamma = 0.99 # discount factor\n\n# Exploration settings\nepsilon = 0.8 # exploration rate\n\n# Number of episodes\nn_episodes = 4000\n\n# Number of steps per episode\nn_steps = 200\n\n# Create a Q-table in numpy\nnbins_pos = 25\nnbins_vel = 25\n\npos_bins = np.linspace(env.observation_space.low[0], env.observation_space.high[0], nbins_pos)\nvel_bins = np.linspace(env.observation_space.low[1], env.observation_space.high[1], nbins_vel)\n\npos_bins = np.concatenate([[-np.inf], pos_bins, [np.inf]])\nvel_bins = np.concatenate([[-np.inf], vel_bins, [np.inf]])\n\nq_table_np = np.random.randn(nbins_pos, nbins_vel, n_actions)\n\ndef discretize_pos(pos):\n    return np.digitize(pos, pos_bins[1:-1])\n\ndef discretize_vel(vel):\n    return np.digitize(vel, vel_bins[1:-1])\n    \n\n\n# loop\n# disable rendering\nenv = gym.make('MountainCar-v0', render_mode=None)\nrewards = np.zeros(n_episodes)\nfor episode in range(n_episodes):\n    epsilon = epsilon * 0.98\n    if episode % 100 == 0:\n        print(f\"Episode {episode}\")\n    observation, _ = env.reset(seed=episode)\n    cumulative_reward = 0\n    for step in range(n_steps):\n        # discretize the observation\n        pos, vel = observation\n        pos_idx = discretize_pos(pos)\n        vel_idx = discretize_vel(vel)\n        \n        # select the action\n        if np.random.rand() &lt; epsilon:\n            action = env.action_space.sample()\n        else:\n            action = np.argmax(q_table_np[pos_idx, vel_idx])\n        \n        # take the action\n        next_observation, reward, terminated, truncated, info = env.step(action)\n        \n        cumulative_reward += reward\n        # discretize the next observation\n        next_pos, next_vel = next_observation\n        next_pos_idx = discretize_pos(next_pos)\n        next_vel_idx = discretize_vel(next_vel)\n        \n        # update the q-table\n        q_table_np[pos_idx, vel_idx, action] += alpha * (reward + gamma * np.max(q_table_np[next_pos_idx, next_vel_idx]) - q_table_np[pos_idx, vel_idx, action])\n        \n        observation = next_observation\n        # rewards update\n        rewards[episode] = cumulative_reward\n        \n        if terminated:\n            print(f\"Episode {episode} terminated after {step} steps\")\n            break\n\nEpisode 0\nEpisode 100\nEpisode 200\nEpisode 300\nEpisode 400\nEpisode 500\nEpisode 504 terminated after 196 steps\nEpisode 588 terminated after 179 steps\nEpisode 589 terminated after 170 steps\nEpisode 590 terminated after 160 steps\nEpisode 593 terminated after 161 steps\nEpisode 596 terminated after 159 steps\nEpisode 597 terminated after 164 steps\nEpisode 598 terminated after 172 steps\nEpisode 599 terminated after 156 steps\nEpisode 600\nEpisode 601 terminated after 161 steps\nEpisode 613 terminated after 161 steps\nEpisode 615 terminated after 163 steps\nEpisode 620 terminated after 152 steps\nEpisode 628 terminated after 156 steps\nEpisode 630 terminated after 153 steps\nEpisode 631 terminated after 157 steps\nEpisode 632 terminated after 195 steps\nEpisode 633 terminated after 191 steps\nEpisode 634 terminated after 154 steps\nEpisode 635 terminated after 158 steps\nEpisode 636 terminated after 174 steps\nEpisode 637 terminated after 157 steps\nEpisode 638 terminated after 181 steps\nEpisode 700\nEpisode 766 terminated after 194 steps\nEpisode 769 terminated after 169 steps\nEpisode 770 terminated after 165 steps\nEpisode 772 terminated after 198 steps\nEpisode 773 terminated after 192 steps\nEpisode 774 terminated after 194 steps\nEpisode 775 terminated after 193 steps\nEpisode 776 terminated after 197 steps\nEpisode 782 terminated after 192 steps\nEpisode 786 terminated after 164 steps\nEpisode 788 terminated after 197 steps\nEpisode 789 terminated after 198 steps\nEpisode 791 terminated after 160 steps\nEpisode 798 terminated after 163 steps\nEpisode 800\nEpisode 801 terminated after 189 steps\nEpisode 803 terminated after 146 steps\nEpisode 805 terminated after 192 steps\nEpisode 813 terminated after 195 steps\nEpisode 815 terminated after 155 steps\nEpisode 817 terminated after 195 steps\nEpisode 818 terminated after 191 steps\nEpisode 825 terminated after 194 steps\nEpisode 828 terminated after 197 steps\nEpisode 850 terminated after 191 steps\nEpisode 852 terminated after 165 steps\nEpisode 854 terminated after 167 steps\nEpisode 855 terminated after 165 steps\nEpisode 859 terminated after 190 steps\nEpisode 860 terminated after 160 steps\nEpisode 862 terminated after 161 steps\nEpisode 863 terminated after 159 steps\nEpisode 864 terminated after 192 steps\nEpisode 869 terminated after 162 steps\nEpisode 871 terminated after 191 steps\nEpisode 872 terminated after 157 steps\nEpisode 876 terminated after 169 steps\nEpisode 878 terminated after 161 steps\nEpisode 880 terminated after 199 steps\nEpisode 886 terminated after 172 steps\nEpisode 888 terminated after 157 steps\nEpisode 890 terminated after 158 steps\nEpisode 893 terminated after 180 steps\nEpisode 896 terminated after 170 steps\nEpisode 900\nEpisode 906 terminated after 177 steps\nEpisode 907 terminated after 169 steps\nEpisode 908 terminated after 168 steps\nEpisode 911 terminated after 178 steps\nEpisode 924 terminated after 171 steps\nEpisode 975 terminated after 162 steps\nEpisode 1000\nEpisode 1006 terminated after 162 steps\nEpisode 1020 terminated after 159 steps\nEpisode 1046 terminated after 189 steps\nEpisode 1054 terminated after 199 steps\nEpisode 1058 terminated after 198 steps\nEpisode 1067 terminated after 159 steps\nEpisode 1084 terminated after 163 steps\nEpisode 1085 terminated after 165 steps\nEpisode 1099 terminated after 187 steps\nEpisode 1100\nEpisode 1100 terminated after 187 steps\nEpisode 1102 terminated after 149 steps\nEpisode 1103 terminated after 187 steps\nEpisode 1104 terminated after 192 steps\nEpisode 1105 terminated after 152 steps\nEpisode 1114 terminated after 191 steps\nEpisode 1115 terminated after 199 steps\nEpisode 1120 terminated after 191 steps\nEpisode 1121 terminated after 185 steps\nEpisode 1122 terminated after 162 steps\nEpisode 1123 terminated after 160 steps\nEpisode 1140 terminated after 191 steps\nEpisode 1141 terminated after 183 steps\nEpisode 1142 terminated after 187 steps\nEpisode 1153 terminated after 191 steps\nEpisode 1158 terminated after 183 steps\nEpisode 1159 terminated after 162 steps\nEpisode 1160 terminated after 181 steps\nEpisode 1178 terminated after 187 steps\nEpisode 1183 terminated after 198 steps\nEpisode 1188 terminated after 197 steps\nEpisode 1191 terminated after 198 steps\nEpisode 1193 terminated after 192 steps\nEpisode 1196 terminated after 196 steps\nEpisode 1200\nEpisode 1202 terminated after 196 steps\nEpisode 1203 terminated after 190 steps\nEpisode 1206 terminated after 189 steps\nEpisode 1207 terminated after 185 steps\nEpisode 1209 terminated after 182 steps\nEpisode 1210 terminated after 184 steps\nEpisode 1211 terminated after 189 steps\nEpisode 1212 terminated after 184 steps\nEpisode 1213 terminated after 182 steps\nEpisode 1216 terminated after 190 steps\nEpisode 1217 terminated after 183 steps\nEpisode 1222 terminated after 192 steps\nEpisode 1223 terminated after 190 steps\nEpisode 1231 terminated after 159 steps\nEpisode 1234 terminated after 161 steps\nEpisode 1238 terminated after 165 steps\nEpisode 1243 terminated after 196 steps\nEpisode 1248 terminated after 196 steps\nEpisode 1249 terminated after 195 steps\nEpisode 1253 terminated after 156 steps\nEpisode 1254 terminated after 185 steps\nEpisode 1255 terminated after 198 steps\nEpisode 1257 terminated after 197 steps\nEpisode 1258 terminated after 156 steps\nEpisode 1259 terminated after 195 steps\nEpisode 1261 terminated after 194 steps\nEpisode 1262 terminated after 161 steps\nEpisode 1264 terminated after 161 steps\nEpisode 1266 terminated after 161 steps\nEpisode 1268 terminated after 170 steps\nEpisode 1270 terminated after 161 steps\nEpisode 1271 terminated after 162 steps\nEpisode 1273 terminated after 163 steps\nEpisode 1276 terminated after 197 steps\nEpisode 1278 terminated after 163 steps\nEpisode 1280 terminated after 169 steps\nEpisode 1281 terminated after 168 steps\nEpisode 1282 terminated after 165 steps\nEpisode 1283 terminated after 156 steps\nEpisode 1284 terminated after 196 steps\nEpisode 1285 terminated after 197 steps\nEpisode 1287 terminated after 159 steps\nEpisode 1289 terminated after 194 steps\nEpisode 1290 terminated after 163 steps\nEpisode 1291 terminated after 188 steps\nEpisode 1292 terminated after 157 steps\nEpisode 1293 terminated after 156 steps\nEpisode 1294 terminated after 197 steps\nEpisode 1297 terminated after 167 steps\nEpisode 1300\nEpisode 1300 terminated after 167 steps\nEpisode 1307 terminated after 172 steps\nEpisode 1308 terminated after 163 steps\nEpisode 1311 terminated after 161 steps\nEpisode 1313 terminated after 169 steps\nEpisode 1315 terminated after 171 steps\nEpisode 1316 terminated after 164 steps\nEpisode 1317 terminated after 174 steps\nEpisode 1318 terminated after 169 steps\nEpisode 1319 terminated after 194 steps\nEpisode 1320 terminated after 195 steps\nEpisode 1321 terminated after 170 steps\nEpisode 1322 terminated after 165 steps\nEpisode 1323 terminated after 171 steps\nEpisode 1326 terminated after 186 steps\nEpisode 1334 terminated after 170 steps\nEpisode 1335 terminated after 159 steps\nEpisode 1345 terminated after 160 steps\nEpisode 1346 terminated after 159 steps\nEpisode 1348 terminated after 157 steps\nEpisode 1349 terminated after 175 steps\nEpisode 1350 terminated after 156 steps\nEpisode 1351 terminated after 163 steps\nEpisode 1352 terminated after 164 steps\nEpisode 1353 terminated after 162 steps\nEpisode 1354 terminated after 173 steps\nEpisode 1355 terminated after 169 steps\nEpisode 1356 terminated after 189 steps\nEpisode 1357 terminated after 194 steps\nEpisode 1366 terminated after 171 steps\nEpisode 1389 terminated after 164 steps\nEpisode 1390 terminated after 167 steps\nEpisode 1393 terminated after 161 steps\nEpisode 1400\nEpisode 1406 terminated after 198 steps\nEpisode 1417 terminated after 198 steps\nEpisode 1420 terminated after 194 steps\nEpisode 1427 terminated after 197 steps\nEpisode 1430 terminated after 198 steps\nEpisode 1437 terminated after 199 steps\nEpisode 1445 terminated after 197 steps\nEpisode 1449 terminated after 194 steps\nEpisode 1452 terminated after 196 steps\nEpisode 1454 terminated after 195 steps\nEpisode 1455 terminated after 189 steps\nEpisode 1456 terminated after 189 steps\nEpisode 1461 terminated after 187 steps\nEpisode 1462 terminated after 167 steps\nEpisode 1463 terminated after 192 steps\nEpisode 1464 terminated after 189 steps\nEpisode 1466 terminated after 186 steps\nEpisode 1469 terminated after 190 steps\nEpisode 1471 terminated after 159 steps\nEpisode 1474 terminated after 185 steps\nEpisode 1475 terminated after 188 steps\nEpisode 1480 terminated after 156 steps\nEpisode 1483 terminated after 190 steps\nEpisode 1484 terminated after 156 steps\nEpisode 1485 terminated after 195 steps\nEpisode 1487 terminated after 160 steps\nEpisode 1488 terminated after 193 steps\nEpisode 1489 terminated after 192 steps\nEpisode 1490 terminated after 194 steps\nEpisode 1491 terminated after 195 steps\nEpisode 1492 terminated after 166 steps\nEpisode 1493 terminated after 191 steps\nEpisode 1494 terminated after 152 steps\nEpisode 1496 terminated after 155 steps\nEpisode 1497 terminated after 159 steps\nEpisode 1498 terminated after 191 steps\nEpisode 1499 terminated after 158 steps\nEpisode 1500\nEpisode 1500 terminated after 190 steps\nEpisode 1501 terminated after 188 steps\nEpisode 1502 terminated after 153 steps\nEpisode 1503 terminated after 152 steps\nEpisode 1504 terminated after 153 steps\nEpisode 1505 terminated after 159 steps\nEpisode 1506 terminated after 196 steps\nEpisode 1507 terminated after 153 steps\nEpisode 1508 terminated after 193 steps\nEpisode 1509 terminated after 194 steps\nEpisode 1510 terminated after 155 steps\nEpisode 1511 terminated after 181 steps\nEpisode 1512 terminated after 158 steps\nEpisode 1513 terminated after 156 steps\nEpisode 1514 terminated after 156 steps\nEpisode 1515 terminated after 156 steps\nEpisode 1516 terminated after 184 steps\nEpisode 1517 terminated after 184 steps\nEpisode 1518 terminated after 158 steps\nEpisode 1519 terminated after 189 steps\nEpisode 1520 terminated after 153 steps\nEpisode 1521 terminated after 153 steps\nEpisode 1522 terminated after 154 steps\nEpisode 1523 terminated after 154 steps\nEpisode 1524 terminated after 156 steps\nEpisode 1525 terminated after 157 steps\nEpisode 1526 terminated after 182 steps\nEpisode 1527 terminated after 177 steps\nEpisode 1528 terminated after 184 steps\nEpisode 1530 terminated after 158 steps\nEpisode 1531 terminated after 151 steps\nEpisode 1532 terminated after 155 steps\nEpisode 1533 terminated after 187 steps\nEpisode 1534 terminated after 166 steps\nEpisode 1535 terminated after 154 steps\nEpisode 1536 terminated after 152 steps\nEpisode 1537 terminated after 180 steps\nEpisode 1538 terminated after 169 steps\nEpisode 1539 terminated after 143 steps\nEpisode 1540 terminated after 142 steps\nEpisode 1541 terminated after 150 steps\nEpisode 1542 terminated after 145 steps\nEpisode 1543 terminated after 147 steps\nEpisode 1544 terminated after 148 steps\nEpisode 1545 terminated after 164 steps\nEpisode 1546 terminated after 170 steps\nEpisode 1547 terminated after 193 steps\nEpisode 1548 terminated after 177 steps\nEpisode 1549 terminated after 152 steps\nEpisode 1550 terminated after 157 steps\nEpisode 1551 terminated after 151 steps\nEpisode 1552 terminated after 147 steps\nEpisode 1553 terminated after 148 steps\nEpisode 1554 terminated after 159 steps\nEpisode 1555 terminated after 149 steps\nEpisode 1556 terminated after 185 steps\nEpisode 1557 terminated after 153 steps\nEpisode 1558 terminated after 181 steps\nEpisode 1559 terminated after 153 steps\nEpisode 1561 terminated after 152 steps\nEpisode 1562 terminated after 184 steps\nEpisode 1564 terminated after 177 steps\nEpisode 1565 terminated after 156 steps\nEpisode 1566 terminated after 153 steps\nEpisode 1567 terminated after 152 steps\nEpisode 1569 terminated after 189 steps\nEpisode 1570 terminated after 147 steps\nEpisode 1573 terminated after 154 steps\nEpisode 1574 terminated after 153 steps\nEpisode 1575 terminated after 194 steps\nEpisode 1576 terminated after 153 steps\nEpisode 1578 terminated after 198 steps\nEpisode 1579 terminated after 188 steps\nEpisode 1582 terminated after 185 steps\nEpisode 1583 terminated after 191 steps\nEpisode 1584 terminated after 156 steps\nEpisode 1585 terminated after 157 steps\nEpisode 1586 terminated after 156 steps\nEpisode 1587 terminated after 165 steps\nEpisode 1588 terminated after 171 steps\nEpisode 1593 terminated after 195 steps\nEpisode 1594 terminated after 163 steps\nEpisode 1595 terminated after 157 steps\nEpisode 1596 terminated after 168 steps\nEpisode 1598 terminated after 153 steps\nEpisode 1599 terminated after 162 steps\nEpisode 1600\nEpisode 1600 terminated after 156 steps\nEpisode 1601 terminated after 194 steps\nEpisode 1603 terminated after 198 steps\nEpisode 1604 terminated after 162 steps\nEpisode 1606 terminated after 194 steps\nEpisode 1608 terminated after 152 steps\nEpisode 1612 terminated after 195 steps\nEpisode 1630 terminated after 189 steps\nEpisode 1632 terminated after 163 steps\nEpisode 1635 terminated after 182 steps\nEpisode 1638 terminated after 165 steps\nEpisode 1639 terminated after 188 steps\nEpisode 1640 terminated after 163 steps\nEpisode 1641 terminated after 172 steps\nEpisode 1642 terminated after 164 steps\nEpisode 1643 terminated after 153 steps\nEpisode 1644 terminated after 191 steps\nEpisode 1645 terminated after 164 steps\nEpisode 1646 terminated after 153 steps\nEpisode 1647 terminated after 151 steps\nEpisode 1649 terminated after 144 steps\nEpisode 1650 terminated after 148 steps\nEpisode 1663 terminated after 163 steps\nEpisode 1690 terminated after 196 steps\nEpisode 1692 terminated after 191 steps\nEpisode 1699 terminated after 196 steps\nEpisode 1700\nEpisode 1700 terminated after 196 steps\nEpisode 1709 terminated after 194 steps\nEpisode 1711 terminated after 198 steps\nEpisode 1713 terminated after 191 steps\nEpisode 1717 terminated after 197 steps\nEpisode 1720 terminated after 195 steps\nEpisode 1723 terminated after 193 steps\nEpisode 1725 terminated after 192 steps\nEpisode 1727 terminated after 198 steps\nEpisode 1729 terminated after 198 steps\nEpisode 1732 terminated after 195 steps\nEpisode 1733 terminated after 199 steps\nEpisode 1736 terminated after 198 steps\nEpisode 1753 terminated after 192 steps\nEpisode 1755 terminated after 190 steps\nEpisode 1757 terminated after 192 steps\nEpisode 1758 terminated after 192 steps\nEpisode 1760 terminated after 187 steps\nEpisode 1761 terminated after 192 steps\nEpisode 1766 terminated after 190 steps\nEpisode 1768 terminated after 190 steps\nEpisode 1769 terminated after 187 steps\nEpisode 1772 terminated after 185 steps\nEpisode 1776 terminated after 192 steps\nEpisode 1792 terminated after 194 steps\nEpisode 1800\nEpisode 1804 terminated after 198 steps\nEpisode 1806 terminated after 189 steps\nEpisode 1815 terminated after 189 steps\nEpisode 1819 terminated after 187 steps\nEpisode 1820 terminated after 194 steps\nEpisode 1823 terminated after 186 steps\nEpisode 1824 terminated after 188 steps\nEpisode 1827 terminated after 194 steps\nEpisode 1832 terminated after 184 steps\nEpisode 1834 terminated after 153 steps\nEpisode 1837 terminated after 151 steps\nEpisode 1838 terminated after 180 steps\nEpisode 1842 terminated after 156 steps\nEpisode 1844 terminated after 153 steps\nEpisode 1848 terminated after 183 steps\nEpisode 1849 terminated after 150 steps\nEpisode 1850 terminated after 153 steps\nEpisode 1851 terminated after 181 steps\nEpisode 1853 terminated after 187 steps\nEpisode 1854 terminated after 155 steps\nEpisode 1858 terminated after 170 steps\nEpisode 1859 terminated after 146 steps\nEpisode 1860 terminated after 151 steps\nEpisode 1861 terminated after 154 steps\nEpisode 1862 terminated after 167 steps\nEpisode 1863 terminated after 144 steps\nEpisode 1864 terminated after 152 steps\nEpisode 1865 terminated after 151 steps\nEpisode 1867 terminated after 149 steps\nEpisode 1868 terminated after 194 steps\nEpisode 1869 terminated after 151 steps\nEpisode 1870 terminated after 186 steps\nEpisode 1872 terminated after 153 steps\nEpisode 1873 terminated after 188 steps\nEpisode 1874 terminated after 149 steps\nEpisode 1875 terminated after 183 steps\nEpisode 1876 terminated after 150 steps\nEpisode 1878 terminated after 147 steps\nEpisode 1879 terminated after 151 steps\nEpisode 1880 terminated after 143 steps\nEpisode 1881 terminated after 152 steps\nEpisode 1882 terminated after 142 steps\nEpisode 1883 terminated after 143 steps\nEpisode 1884 terminated after 186 steps\nEpisode 1885 terminated after 178 steps\nEpisode 1886 terminated after 187 steps\nEpisode 1887 terminated after 147 steps\nEpisode 1888 terminated after 140 steps\nEpisode 1889 terminated after 147 steps\nEpisode 1890 terminated after 185 steps\nEpisode 1893 terminated after 146 steps\nEpisode 1894 terminated after 145 steps\nEpisode 1896 terminated after 141 steps\nEpisode 1897 terminated after 186 steps\nEpisode 1898 terminated after 143 steps\nEpisode 1899 terminated after 149 steps\nEpisode 1900\nEpisode 1900 terminated after 184 steps\nEpisode 1901 terminated after 149 steps\nEpisode 1904 terminated after 148 steps\nEpisode 1906 terminated after 193 steps\nEpisode 1907 terminated after 146 steps\nEpisode 1908 terminated after 182 steps\nEpisode 1911 terminated after 178 steps\nEpisode 1913 terminated after 191 steps\nEpisode 1918 terminated after 194 steps\nEpisode 1920 terminated after 149 steps\nEpisode 1921 terminated after 145 steps\nEpisode 1922 terminated after 191 steps\nEpisode 1923 terminated after 151 steps\nEpisode 1926 terminated after 198 steps\nEpisode 1928 terminated after 147 steps\nEpisode 1929 terminated after 196 steps\nEpisode 1930 terminated after 151 steps\nEpisode 1931 terminated after 144 steps\nEpisode 1933 terminated after 190 steps\nEpisode 1934 terminated after 187 steps\nEpisode 1937 terminated after 185 steps\nEpisode 1942 terminated after 198 steps\nEpisode 1943 terminated after 187 steps\nEpisode 1946 terminated after 147 steps\nEpisode 1948 terminated after 194 steps\nEpisode 1949 terminated after 144 steps\nEpisode 1952 terminated after 145 steps\nEpisode 1953 terminated after 172 steps\nEpisode 1954 terminated after 143 steps\nEpisode 1955 terminated after 189 steps\nEpisode 1956 terminated after 190 steps\nEpisode 1957 terminated after 147 steps\nEpisode 1958 terminated after 187 steps\nEpisode 1960 terminated after 147 steps\nEpisode 1961 terminated after 142 steps\nEpisode 1962 terminated after 195 steps\nEpisode 1963 terminated after 152 steps\nEpisode 1964 terminated after 144 steps\nEpisode 1965 terminated after 193 steps\nEpisode 1966 terminated after 140 steps\nEpisode 1968 terminated after 153 steps\nEpisode 1969 terminated after 142 steps\nEpisode 1970 terminated after 144 steps\nEpisode 1971 terminated after 143 steps\nEpisode 1972 terminated after 146 steps\nEpisode 1974 terminated after 147 steps\nEpisode 1975 terminated after 144 steps\nEpisode 1976 terminated after 155 steps\nEpisode 1977 terminated after 180 steps\nEpisode 1978 terminated after 150 steps\nEpisode 1979 terminated after 174 steps\nEpisode 1980 terminated after 186 steps\nEpisode 1981 terminated after 144 steps\nEpisode 1982 terminated after 144 steps\nEpisode 1983 terminated after 158 steps\nEpisode 1984 terminated after 183 steps\nEpisode 1985 terminated after 177 steps\nEpisode 1986 terminated after 184 steps\nEpisode 1987 terminated after 185 steps\nEpisode 1988 terminated after 151 steps\nEpisode 1989 terminated after 190 steps\nEpisode 1990 terminated after 155 steps\nEpisode 1991 terminated after 184 steps\nEpisode 1992 terminated after 177 steps\nEpisode 1996 terminated after 158 steps\nEpisode 1997 terminated after 160 steps\nEpisode 1999 terminated after 159 steps\nEpisode 2000\nEpisode 2000 terminated after 174 steps\nEpisode 2004 terminated after 160 steps\nEpisode 2010 terminated after 188 steps\nEpisode 2011 terminated after 161 steps\nEpisode 2012 terminated after 190 steps\nEpisode 2013 terminated after 180 steps\nEpisode 2014 terminated after 162 steps\nEpisode 2015 terminated after 169 steps\nEpisode 2016 terminated after 159 steps\nEpisode 2017 terminated after 165 steps\nEpisode 2018 terminated after 165 steps\nEpisode 2019 terminated after 188 steps\nEpisode 2020 terminated after 163 steps\nEpisode 2021 terminated after 162 steps\nEpisode 2022 terminated after 177 steps\nEpisode 2023 terminated after 196 steps\nEpisode 2024 terminated after 165 steps\nEpisode 2025 terminated after 161 steps\nEpisode 2026 terminated after 196 steps\nEpisode 2028 terminated after 146 steps\nEpisode 2029 terminated after 169 steps\nEpisode 2030 terminated after 143 steps\nEpisode 2031 terminated after 149 steps\nEpisode 2033 terminated after 192 steps\nEpisode 2035 terminated after 199 steps\nEpisode 2036 terminated after 175 steps\nEpisode 2037 terminated after 162 steps\nEpisode 2038 terminated after 164 steps\nEpisode 2039 terminated after 165 steps\nEpisode 2040 terminated after 166 steps\nEpisode 2041 terminated after 161 steps\nEpisode 2042 terminated after 162 steps\nEpisode 2043 terminated after 164 steps\nEpisode 2044 terminated after 160 steps\nEpisode 2045 terminated after 167 steps\nEpisode 2046 terminated after 140 steps\nEpisode 2047 terminated after 162 steps\nEpisode 2048 terminated after 164 steps\nEpisode 2049 terminated after 154 steps\nEpisode 2050 terminated after 158 steps\nEpisode 2051 terminated after 167 steps\nEpisode 2052 terminated after 152 steps\nEpisode 2053 terminated after 169 steps\nEpisode 2054 terminated after 179 steps\nEpisode 2055 terminated after 167 steps\nEpisode 2056 terminated after 167 steps\nEpisode 2057 terminated after 162 steps\nEpisode 2058 terminated after 165 steps\nEpisode 2059 terminated after 172 steps\nEpisode 2060 terminated after 159 steps\nEpisode 2061 terminated after 180 steps\nEpisode 2062 terminated after 167 steps\nEpisode 2063 terminated after 177 steps\nEpisode 2064 terminated after 169 steps\nEpisode 2065 terminated after 162 steps\nEpisode 2067 terminated after 165 steps\nEpisode 2068 terminated after 169 steps\nEpisode 2069 terminated after 175 steps\nEpisode 2070 terminated after 178 steps\nEpisode 2071 terminated after 159 steps\nEpisode 2072 terminated after 180 steps\nEpisode 2073 terminated after 168 steps\nEpisode 2074 terminated after 199 steps\nEpisode 2075 terminated after 164 steps\nEpisode 2077 terminated after 170 steps\nEpisode 2078 terminated after 166 steps\nEpisode 2079 terminated after 163 steps\nEpisode 2082 terminated after 163 steps\nEpisode 2083 terminated after 172 steps\nEpisode 2084 terminated after 168 steps\nEpisode 2085 terminated after 160 steps\nEpisode 2090 terminated after 165 steps\nEpisode 2092 terminated after 166 steps\nEpisode 2093 terminated after 172 steps\nEpisode 2095 terminated after 166 steps\nEpisode 2097 terminated after 199 steps\nEpisode 2098 terminated after 146 steps\nEpisode 2099 terminated after 164 steps\nEpisode 2100\nEpisode 2100 terminated after 171 steps\nEpisode 2102 terminated after 198 steps\nEpisode 2103 terminated after 167 steps\nEpisode 2104 terminated after 167 steps\nEpisode 2105 terminated after 169 steps\nEpisode 2106 terminated after 159 steps\nEpisode 2108 terminated after 161 steps\nEpisode 2109 terminated after 194 steps\nEpisode 2110 terminated after 164 steps\nEpisode 2111 terminated after 165 steps\nEpisode 2112 terminated after 196 steps\nEpisode 2114 terminated after 161 steps\nEpisode 2115 terminated after 158 steps\nEpisode 2116 terminated after 160 steps\nEpisode 2118 terminated after 164 steps\nEpisode 2119 terminated after 196 steps\nEpisode 2120 terminated after 195 steps\nEpisode 2121 terminated after 158 steps\nEpisode 2122 terminated after 167 steps\nEpisode 2123 terminated after 196 steps\nEpisode 2127 terminated after 160 steps\nEpisode 2128 terminated after 163 steps\nEpisode 2129 terminated after 158 steps\nEpisode 2130 terminated after 163 steps\nEpisode 2131 terminated after 195 steps\nEpisode 2132 terminated after 154 steps\nEpisode 2133 terminated after 191 steps\nEpisode 2134 terminated after 163 steps\nEpisode 2135 terminated after 149 steps\nEpisode 2136 terminated after 152 steps\nEpisode 2139 terminated after 163 steps\nEpisode 2141 terminated after 191 steps\nEpisode 2142 terminated after 189 steps\nEpisode 2143 terminated after 158 steps\nEpisode 2144 terminated after 187 steps\nEpisode 2145 terminated after 162 steps\nEpisode 2146 terminated after 192 steps\nEpisode 2147 terminated after 166 steps\nEpisode 2149 terminated after 194 steps\nEpisode 2150 terminated after 158 steps\nEpisode 2151 terminated after 161 steps\nEpisode 2152 terminated after 158 steps\nEpisode 2153 terminated after 159 steps\nEpisode 2154 terminated after 162 steps\nEpisode 2155 terminated after 192 steps\nEpisode 2156 terminated after 164 steps\nEpisode 2157 terminated after 155 steps\nEpisode 2158 terminated after 157 steps\nEpisode 2159 terminated after 194 steps\nEpisode 2160 terminated after 154 steps\nEpisode 2161 terminated after 160 steps\nEpisode 2162 terminated after 157 steps\nEpisode 2163 terminated after 154 steps\nEpisode 2164 terminated after 159 steps\nEpisode 2165 terminated after 155 steps\nEpisode 2166 terminated after 169 steps\nEpisode 2167 terminated after 158 steps\nEpisode 2168 terminated after 160 steps\nEpisode 2169 terminated after 155 steps\nEpisode 2170 terminated after 150 steps\nEpisode 2171 terminated after 154 steps\nEpisode 2172 terminated after 157 steps\nEpisode 2173 terminated after 161 steps\nEpisode 2174 terminated after 154 steps\nEpisode 2176 terminated after 155 steps\nEpisode 2177 terminated after 196 steps\nEpisode 2178 terminated after 189 steps\nEpisode 2179 terminated after 190 steps\nEpisode 2180 terminated after 192 steps\nEpisode 2181 terminated after 149 steps\nEpisode 2182 terminated after 156 steps\nEpisode 2183 terminated after 198 steps\nEpisode 2184 terminated after 159 steps\nEpisode 2185 terminated after 153 steps\nEpisode 2186 terminated after 162 steps\nEpisode 2187 terminated after 189 steps\nEpisode 2189 terminated after 158 steps\nEpisode 2190 terminated after 157 steps\nEpisode 2191 terminated after 193 steps\nEpisode 2192 terminated after 198 steps\nEpisode 2193 terminated after 156 steps\nEpisode 2194 terminated after 151 steps\nEpisode 2195 terminated after 151 steps\nEpisode 2196 terminated after 156 steps\nEpisode 2197 terminated after 159 steps\nEpisode 2198 terminated after 158 steps\nEpisode 2199 terminated after 151 steps\nEpisode 2200\nEpisode 2200 terminated after 152 steps\nEpisode 2201 terminated after 155 steps\nEpisode 2202 terminated after 150 steps\nEpisode 2203 terminated after 169 steps\nEpisode 2204 terminated after 155 steps\nEpisode 2205 terminated after 158 steps\nEpisode 2206 terminated after 152 steps\nEpisode 2207 terminated after 151 steps\nEpisode 2208 terminated after 153 steps\nEpisode 2209 terminated after 162 steps\nEpisode 2210 terminated after 155 steps\nEpisode 2211 terminated after 160 steps\nEpisode 2212 terminated after 156 steps\nEpisode 2213 terminated after 160 steps\nEpisode 2214 terminated after 184 steps\nEpisode 2215 terminated after 154 steps\nEpisode 2216 terminated after 151 steps\nEpisode 2217 terminated after 157 steps\nEpisode 2218 terminated after 186 steps\nEpisode 2219 terminated after 156 steps\nEpisode 2220 terminated after 153 steps\nEpisode 2221 terminated after 150 steps\nEpisode 2222 terminated after 162 steps\nEpisode 2223 terminated after 189 steps\nEpisode 2224 terminated after 154 steps\nEpisode 2225 terminated after 195 steps\nEpisode 2226 terminated after 154 steps\nEpisode 2227 terminated after 153 steps\nEpisode 2228 terminated after 147 steps\nEpisode 2229 terminated after 188 steps\nEpisode 2230 terminated after 156 steps\nEpisode 2231 terminated after 154 steps\nEpisode 2233 terminated after 153 steps\nEpisode 2234 terminated after 160 steps\nEpisode 2235 terminated after 141 steps\nEpisode 2236 terminated after 144 steps\nEpisode 2237 terminated after 147 steps\nEpisode 2238 terminated after 139 steps\nEpisode 2239 terminated after 146 steps\nEpisode 2240 terminated after 146 steps\nEpisode 2241 terminated after 147 steps\nEpisode 2242 terminated after 151 steps\nEpisode 2243 terminated after 149 steps\nEpisode 2244 terminated after 149 steps\nEpisode 2245 terminated after 150 steps\nEpisode 2246 terminated after 149 steps\nEpisode 2248 terminated after 156 steps\nEpisode 2249 terminated after 154 steps\nEpisode 2250 terminated after 144 steps\nEpisode 2251 terminated after 158 steps\nEpisode 2256 terminated after 158 steps\nEpisode 2257 terminated after 161 steps\nEpisode 2259 terminated after 157 steps\nEpisode 2261 terminated after 154 steps\nEpisode 2262 terminated after 152 steps\nEpisode 2263 terminated after 148 steps\nEpisode 2264 terminated after 148 steps\nEpisode 2265 terminated after 150 steps\nEpisode 2266 terminated after 150 steps\nEpisode 2267 terminated after 147 steps\nEpisode 2268 terminated after 150 steps\nEpisode 2270 terminated after 195 steps\nEpisode 2271 terminated after 148 steps\nEpisode 2273 terminated after 148 steps\nEpisode 2274 terminated after 151 steps\nEpisode 2275 terminated after 156 steps\nEpisode 2276 terminated after 148 steps\nEpisode 2277 terminated after 157 steps\nEpisode 2278 terminated after 149 steps\nEpisode 2280 terminated after 156 steps\nEpisode 2282 terminated after 152 steps\nEpisode 2283 terminated after 147 steps\nEpisode 2284 terminated after 152 steps\nEpisode 2285 terminated after 192 steps\nEpisode 2286 terminated after 199 steps\nEpisode 2287 terminated after 198 steps\nEpisode 2288 terminated after 147 steps\nEpisode 2289 terminated after 157 steps\nEpisode 2291 terminated after 192 steps\nEpisode 2292 terminated after 159 steps\nEpisode 2294 terminated after 166 steps\nEpisode 2300\nEpisode 2300 terminated after 168 steps\nEpisode 2301 terminated after 182 steps\nEpisode 2315 terminated after 191 steps\nEpisode 2318 terminated after 152 steps\nEpisode 2320 terminated after 187 steps\nEpisode 2324 terminated after 154 steps\nEpisode 2328 terminated after 167 steps\nEpisode 2330 terminated after 162 steps\nEpisode 2331 terminated after 164 steps\nEpisode 2332 terminated after 199 steps\nEpisode 2333 terminated after 157 steps\nEpisode 2334 terminated after 158 steps\nEpisode 2335 terminated after 157 steps\nEpisode 2336 terminated after 161 steps\nEpisode 2337 terminated after 155 steps\nEpisode 2338 terminated after 117 steps\nEpisode 2339 terminated after 160 steps\nEpisode 2341 terminated after 156 steps\nEpisode 2342 terminated after 155 steps\nEpisode 2343 terminated after 154 steps\nEpisode 2344 terminated after 155 steps\nEpisode 2345 terminated after 153 steps\nEpisode 2349 terminated after 168 steps\nEpisode 2350 terminated after 165 steps\nEpisode 2351 terminated after 158 steps\nEpisode 2352 terminated after 150 steps\nEpisode 2353 terminated after 154 steps\nEpisode 2355 terminated after 153 steps\nEpisode 2356 terminated after 168 steps\nEpisode 2357 terminated after 166 steps\nEpisode 2358 terminated after 159 steps\nEpisode 2359 terminated after 126 steps\nEpisode 2360 terminated after 147 steps\nEpisode 2361 terminated after 164 steps\nEpisode 2362 terminated after 152 steps\nEpisode 2364 terminated after 155 steps\nEpisode 2366 terminated after 122 steps\nEpisode 2367 terminated after 122 steps\nEpisode 2368 terminated after 157 steps\nEpisode 2369 terminated after 198 steps\nEpisode 2377 terminated after 196 steps\nEpisode 2378 terminated after 196 steps\nEpisode 2380 terminated after 194 steps\nEpisode 2393 terminated after 195 steps\nEpisode 2400\nEpisode 2405 terminated after 195 steps\nEpisode 2408 terminated after 194 steps\nEpisode 2409 terminated after 191 steps\nEpisode 2410 terminated after 193 steps\nEpisode 2412 terminated after 192 steps\nEpisode 2413 terminated after 194 steps\nEpisode 2416 terminated after 192 steps\nEpisode 2421 terminated after 198 steps\nEpisode 2422 terminated after 192 steps\nEpisode 2423 terminated after 189 steps\nEpisode 2427 terminated after 190 steps\nEpisode 2434 terminated after 188 steps\nEpisode 2437 terminated after 193 steps\nEpisode 2444 terminated after 188 steps\nEpisode 2450 terminated after 183 steps\nEpisode 2451 terminated after 183 steps\nEpisode 2452 terminated after 186 steps\nEpisode 2455 terminated after 183 steps\nEpisode 2456 terminated after 186 steps\nEpisode 2457 terminated after 179 steps\nEpisode 2458 terminated after 170 steps\nEpisode 2460 terminated after 171 steps\nEpisode 2462 terminated after 185 steps\nEpisode 2463 terminated after 186 steps\nEpisode 2464 terminated after 178 steps\nEpisode 2466 terminated after 181 steps\nEpisode 2468 terminated after 175 steps\nEpisode 2469 terminated after 186 steps\nEpisode 2472 terminated after 178 steps\nEpisode 2474 terminated after 186 steps\nEpisode 2476 terminated after 183 steps\nEpisode 2485 terminated after 176 steps\nEpisode 2489 terminated after 176 steps\nEpisode 2490 terminated after 177 steps\nEpisode 2491 terminated after 188 steps\nEpisode 2492 terminated after 186 steps\nEpisode 2496 terminated after 193 steps\nEpisode 2499 terminated after 183 steps\nEpisode 2500\nEpisode 2503 terminated after 196 steps\nEpisode 2504 terminated after 193 steps\nEpisode 2505 terminated after 191 steps\nEpisode 2507 terminated after 193 steps\nEpisode 2509 terminated after 184 steps\nEpisode 2510 terminated after 188 steps\nEpisode 2511 terminated after 189 steps\nEpisode 2512 terminated after 180 steps\nEpisode 2513 terminated after 190 steps\nEpisode 2514 terminated after 187 steps\nEpisode 2515 terminated after 192 steps\nEpisode 2516 terminated after 180 steps\nEpisode 2517 terminated after 191 steps\nEpisode 2518 terminated after 183 steps\nEpisode 2519 terminated after 172 steps\nEpisode 2534 terminated after 188 steps\nEpisode 2537 terminated after 194 steps\nEpisode 2544 terminated after 192 steps\nEpisode 2548 terminated after 188 steps\nEpisode 2549 terminated after 186 steps\nEpisode 2552 terminated after 194 steps\nEpisode 2558 terminated after 194 steps\nEpisode 2559 terminated after 195 steps\nEpisode 2561 terminated after 195 steps\nEpisode 2563 terminated after 183 steps\nEpisode 2564 terminated after 197 steps\nEpisode 2565 terminated after 155 steps\nEpisode 2567 terminated after 190 steps\nEpisode 2568 terminated after 189 steps\nEpisode 2570 terminated after 173 steps\nEpisode 2571 terminated after 191 steps\nEpisode 2572 terminated after 161 steps\nEpisode 2573 terminated after 190 steps\nEpisode 2574 terminated after 192 steps\nEpisode 2575 terminated after 188 steps\nEpisode 2577 terminated after 178 steps\nEpisode 2578 terminated after 176 steps\nEpisode 2579 terminated after 181 steps\nEpisode 2580 terminated after 163 steps\nEpisode 2582 terminated after 156 steps\nEpisode 2583 terminated after 179 steps\nEpisode 2584 terminated after 180 steps\nEpisode 2585 terminated after 179 steps\nEpisode 2586 terminated after 175 steps\nEpisode 2587 terminated after 178 steps\nEpisode 2588 terminated after 182 steps\nEpisode 2589 terminated after 177 steps\nEpisode 2590 terminated after 170 steps\nEpisode 2591 terminated after 163 steps\nEpisode 2592 terminated after 161 steps\nEpisode 2593 terminated after 159 steps\nEpisode 2594 terminated after 182 steps\nEpisode 2595 terminated after 180 steps\nEpisode 2596 terminated after 185 steps\nEpisode 2597 terminated after 177 steps\nEpisode 2599 terminated after 191 steps\nEpisode 2600\nEpisode 2600 terminated after 176 steps\nEpisode 2601 terminated after 185 steps\nEpisode 2602 terminated after 182 steps\nEpisode 2603 terminated after 183 steps\nEpisode 2604 terminated after 176 steps\nEpisode 2605 terminated after 159 steps\nEpisode 2606 terminated after 172 steps\nEpisode 2607 terminated after 178 steps\nEpisode 2608 terminated after 180 steps\nEpisode 2609 terminated after 161 steps\nEpisode 2610 terminated after 160 steps\nEpisode 2611 terminated after 157 steps\nEpisode 2612 terminated after 175 steps\nEpisode 2613 terminated after 153 steps\nEpisode 2614 terminated after 159 steps\nEpisode 2615 terminated after 171 steps\nEpisode 2616 terminated after 185 steps\nEpisode 2617 terminated after 159 steps\nEpisode 2618 terminated after 173 steps\nEpisode 2619 terminated after 199 steps\nEpisode 2620 terminated after 197 steps\nEpisode 2621 terminated after 176 steps\nEpisode 2622 terminated after 178 steps\nEpisode 2623 terminated after 180 steps\nEpisode 2624 terminated after 178 steps\nEpisode 2625 terminated after 157 steps\nEpisode 2626 terminated after 180 steps\nEpisode 2627 terminated after 162 steps\nEpisode 2628 terminated after 189 steps\nEpisode 2629 terminated after 163 steps\nEpisode 2630 terminated after 191 steps\nEpisode 2631 terminated after 185 steps\nEpisode 2632 terminated after 173 steps\nEpisode 2633 terminated after 184 steps\nEpisode 2634 terminated after 156 steps\nEpisode 2635 terminated after 175 steps\nEpisode 2636 terminated after 179 steps\nEpisode 2637 terminated after 166 steps\nEpisode 2638 terminated after 173 steps\nEpisode 2639 terminated after 197 steps\nEpisode 2640 terminated after 163 steps\nEpisode 2641 terminated after 181 steps\nEpisode 2642 terminated after 184 steps\nEpisode 2643 terminated after 182 steps\nEpisode 2644 terminated after 173 steps\nEpisode 2645 terminated after 177 steps\nEpisode 2646 terminated after 163 steps\nEpisode 2647 terminated after 162 steps\nEpisode 2648 terminated after 183 steps\nEpisode 2651 terminated after 184 steps\nEpisode 2652 terminated after 199 steps\nEpisode 2653 terminated after 176 steps\nEpisode 2654 terminated after 170 steps\nEpisode 2655 terminated after 172 steps\nEpisode 2656 terminated after 194 steps\nEpisode 2657 terminated after 198 steps\nEpisode 2661 terminated after 182 steps\nEpisode 2662 terminated after 171 steps\nEpisode 2663 terminated after 170 steps\nEpisode 2664 terminated after 176 steps\nEpisode 2665 terminated after 174 steps\nEpisode 2666 terminated after 174 steps\nEpisode 2667 terminated after 170 steps\nEpisode 2670 terminated after 192 steps\nEpisode 2671 terminated after 190 steps\nEpisode 2673 terminated after 192 steps\nEpisode 2675 terminated after 157 steps\nEpisode 2676 terminated after 116 steps\nEpisode 2677 terminated after 176 steps\nEpisode 2678 terminated after 156 steps\nEpisode 2679 terminated after 156 steps\nEpisode 2680 terminated after 161 steps\nEpisode 2682 terminated after 180 steps\nEpisode 2683 terminated after 176 steps\nEpisode 2684 terminated after 174 steps\nEpisode 2685 terminated after 154 steps\nEpisode 2686 terminated after 158 steps\nEpisode 2687 terminated after 180 steps\nEpisode 2688 terminated after 158 steps\nEpisode 2689 terminated after 160 steps\nEpisode 2690 terminated after 160 steps\nEpisode 2691 terminated after 174 steps\nEpisode 2692 terminated after 160 steps\nEpisode 2693 terminated after 174 steps\nEpisode 2694 terminated after 184 steps\nEpisode 2695 terminated after 159 steps\nEpisode 2696 terminated after 158 steps\nEpisode 2697 terminated after 160 steps\nEpisode 2698 terminated after 166 steps\nEpisode 2699 terminated after 112 steps\nEpisode 2700\nEpisode 2700 terminated after 160 steps\nEpisode 2701 terminated after 173 steps\nEpisode 2702 terminated after 110 steps\nEpisode 2703 terminated after 162 steps\nEpisode 2704 terminated after 180 steps\nEpisode 2705 terminated after 155 steps\nEpisode 2706 terminated after 168 steps\nEpisode 2707 terminated after 180 steps\nEpisode 2708 terminated after 154 steps\nEpisode 2709 terminated after 185 steps\nEpisode 2710 terminated after 168 steps\nEpisode 2711 terminated after 158 steps\nEpisode 2712 terminated after 158 steps\nEpisode 2714 terminated after 159 steps\nEpisode 2715 terminated after 169 steps\nEpisode 2716 terminated after 180 steps\nEpisode 2717 terminated after 171 steps\nEpisode 2718 terminated after 115 steps\nEpisode 2719 terminated after 167 steps\nEpisode 2720 terminated after 166 steps\nEpisode 2721 terminated after 112 steps\nEpisode 2722 terminated after 198 steps\nEpisode 2723 terminated after 159 steps\nEpisode 2724 terminated after 159 steps\nEpisode 2725 terminated after 156 steps\nEpisode 2726 terminated after 110 steps\nEpisode 2727 terminated after 110 steps\nEpisode 2728 terminated after 176 steps\nEpisode 2729 terminated after 156 steps\nEpisode 2730 terminated after 155 steps\nEpisode 2731 terminated after 178 steps\nEpisode 2732 terminated after 154 steps\nEpisode 2733 terminated after 158 steps\nEpisode 2734 terminated after 175 steps\nEpisode 2735 terminated after 160 steps\nEpisode 2736 terminated after 196 steps\nEpisode 2737 terminated after 155 steps\nEpisode 2738 terminated after 184 steps\nEpisode 2739 terminated after 190 steps\nEpisode 2740 terminated after 164 steps\nEpisode 2741 terminated after 165 steps\nEpisode 2742 terminated after 168 steps\nEpisode 2743 terminated after 183 steps\nEpisode 2744 terminated after 158 steps\nEpisode 2745 terminated after 184 steps\nEpisode 2746 terminated after 166 steps\nEpisode 2747 terminated after 176 steps\nEpisode 2748 terminated after 163 steps\nEpisode 2749 terminated after 168 steps\nEpisode 2750 terminated after 168 steps\nEpisode 2751 terminated after 176 steps\nEpisode 2752 terminated after 154 steps\nEpisode 2753 terminated after 167 steps\nEpisode 2754 terminated after 172 steps\nEpisode 2755 terminated after 156 steps\nEpisode 2756 terminated after 170 steps\nEpisode 2757 terminated after 111 steps\nEpisode 2758 terminated after 167 steps\nEpisode 2759 terminated after 169 steps\nEpisode 2760 terminated after 155 steps\nEpisode 2761 terminated after 163 steps\nEpisode 2762 terminated after 110 steps\nEpisode 2763 terminated after 170 steps\nEpisode 2764 terminated after 162 steps\nEpisode 2765 terminated after 156 steps\nEpisode 2766 terminated after 161 steps\nEpisode 2767 terminated after 198 steps\nEpisode 2768 terminated after 148 steps\nEpisode 2769 terminated after 152 steps\nEpisode 2770 terminated after 170 steps\nEpisode 2771 terminated after 178 steps\nEpisode 2772 terminated after 176 steps\nEpisode 2773 terminated after 170 steps\nEpisode 2774 terminated after 175 steps\nEpisode 2777 terminated after 144 steps\nEpisode 2778 terminated after 190 steps\nEpisode 2779 terminated after 156 steps\nEpisode 2787 terminated after 188 steps\nEpisode 2800\nEpisode 2800 terminated after 173 steps\nEpisode 2814 terminated after 190 steps\nEpisode 2815 terminated after 193 steps\nEpisode 2816 terminated after 197 steps\nEpisode 2824 terminated after 195 steps\nEpisode 2829 terminated after 185 steps\nEpisode 2849 terminated after 198 steps\nEpisode 2864 terminated after 158 steps\nEpisode 2866 terminated after 125 steps\nEpisode 2867 terminated after 184 steps\nEpisode 2868 terminated after 170 steps\nEpisode 2870 terminated after 164 steps\nEpisode 2871 terminated after 137 steps\nEpisode 2872 terminated after 140 steps\nEpisode 2873 terminated after 129 steps\nEpisode 2874 terminated after 187 steps\nEpisode 2875 terminated after 173 steps\nEpisode 2876 terminated after 197 steps\nEpisode 2878 terminated after 187 steps\nEpisode 2880 terminated after 176 steps\nEpisode 2881 terminated after 162 steps\nEpisode 2882 terminated after 128 steps\nEpisode 2883 terminated after 195 steps\nEpisode 2884 terminated after 168 steps\nEpisode 2886 terminated after 187 steps\nEpisode 2887 terminated after 172 steps\nEpisode 2891 terminated after 182 steps\nEpisode 2892 terminated after 131 steps\nEpisode 2893 terminated after 180 steps\nEpisode 2894 terminated after 150 steps\nEpisode 2895 terminated after 177 steps\nEpisode 2896 terminated after 182 steps\nEpisode 2897 terminated after 172 steps\nEpisode 2899 terminated after 123 steps\nEpisode 2900\nEpisode 2900 terminated after 168 steps\nEpisode 2901 terminated after 168 steps\nEpisode 2902 terminated after 124 steps\nEpisode 2904 terminated after 166 steps\nEpisode 2905 terminated after 170 steps\nEpisode 2908 terminated after 171 steps\nEpisode 2912 terminated after 170 steps\nEpisode 2914 terminated after 175 steps\nEpisode 2915 terminated after 183 steps\nEpisode 2916 terminated after 175 steps\nEpisode 2917 terminated after 132 steps\nEpisode 2918 terminated after 161 steps\nEpisode 2919 terminated after 135 steps\nEpisode 2920 terminated after 130 steps\nEpisode 2922 terminated after 126 steps\nEpisode 2924 terminated after 169 steps\nEpisode 2926 terminated after 169 steps\nEpisode 2928 terminated after 123 steps\nEpisode 2932 terminated after 124 steps\nEpisode 2934 terminated after 166 steps\nEpisode 2936 terminated after 162 steps\nEpisode 2937 terminated after 161 steps\nEpisode 2939 terminated after 157 steps\nEpisode 2940 terminated after 163 steps\nEpisode 2941 terminated after 150 steps\nEpisode 2942 terminated after 154 steps\nEpisode 2944 terminated after 155 steps\nEpisode 2946 terminated after 162 steps\nEpisode 2948 terminated after 125 steps\nEpisode 2949 terminated after 163 steps\nEpisode 2950 terminated after 153 steps\nEpisode 2951 terminated after 125 steps\nEpisode 2952 terminated after 158 steps\nEpisode 2953 terminated after 158 steps\nEpisode 2954 terminated after 157 steps\nEpisode 2955 terminated after 162 steps\nEpisode 2956 terminated after 161 steps\nEpisode 2959 terminated after 157 steps\nEpisode 2960 terminated after 153 steps\nEpisode 2961 terminated after 155 steps\nEpisode 2962 terminated after 155 steps\nEpisode 2964 terminated after 155 steps\nEpisode 2965 terminated after 168 steps\nEpisode 2966 terminated after 157 steps\nEpisode 2968 terminated after 119 steps\nEpisode 2970 terminated after 156 steps\nEpisode 2971 terminated after 123 steps\nEpisode 2972 terminated after 158 steps\nEpisode 2973 terminated after 158 steps\nEpisode 2976 terminated after 159 steps\nEpisode 2977 terminated after 146 steps\nEpisode 2978 terminated after 153 steps\nEpisode 2979 terminated after 124 steps\nEpisode 2980 terminated after 155 steps\nEpisode 2981 terminated after 157 steps\nEpisode 2982 terminated after 158 steps\nEpisode 2983 terminated after 156 steps\nEpisode 2985 terminated after 122 steps\nEpisode 2986 terminated after 157 steps\nEpisode 2987 terminated after 155 steps\nEpisode 2988 terminated after 160 steps\nEpisode 2989 terminated after 125 steps\nEpisode 2990 terminated after 124 steps\nEpisode 2991 terminated after 155 steps\nEpisode 2992 terminated after 155 steps\nEpisode 2993 terminated after 124 steps\nEpisode 2994 terminated after 125 steps\nEpisode 2995 terminated after 155 steps\nEpisode 2996 terminated after 157 steps\nEpisode 2997 terminated after 128 steps\nEpisode 2998 terminated after 149 steps\nEpisode 2999 terminated after 153 steps\nEpisode 3000\nEpisode 3000 terminated after 148 steps\nEpisode 3001 terminated after 151 steps\nEpisode 3002 terminated after 134 steps\nEpisode 3003 terminated after 128 steps\nEpisode 3004 terminated after 152 steps\nEpisode 3005 terminated after 131 steps\nEpisode 3007 terminated after 151 steps\nEpisode 3008 terminated after 154 steps\nEpisode 3009 terminated after 149 steps\nEpisode 3010 terminated after 199 steps\nEpisode 3011 terminated after 147 steps\nEpisode 3014 terminated after 151 steps\nEpisode 3016 terminated after 145 steps\nEpisode 3018 terminated after 150 steps\nEpisode 3019 terminated after 148 steps\nEpisode 3020 terminated after 149 steps\nEpisode 3025 terminated after 146 steps\nEpisode 3026 terminated after 148 steps\nEpisode 3029 terminated after 154 steps\nEpisode 3030 terminated after 146 steps\nEpisode 3032 terminated after 146 steps\nEpisode 3038 terminated after 150 steps\nEpisode 3044 terminated after 148 steps\nEpisode 3045 terminated after 146 steps\nEpisode 3046 terminated after 151 steps\nEpisode 3049 terminated after 161 steps\nEpisode 3053 terminated after 150 steps\nEpisode 3055 terminated after 146 steps\nEpisode 3056 terminated after 186 steps\nEpisode 3057 terminated after 149 steps\nEpisode 3059 terminated after 148 steps\nEpisode 3060 terminated after 158 steps\nEpisode 3062 terminated after 157 steps\nEpisode 3063 terminated after 151 steps\nEpisode 3064 terminated after 152 steps\nEpisode 3065 terminated after 195 steps\nEpisode 3066 terminated after 191 steps\nEpisode 3067 terminated after 148 steps\nEpisode 3068 terminated after 192 steps\nEpisode 3069 terminated after 146 steps\nEpisode 3071 terminated after 147 steps\nEpisode 3073 terminated after 144 steps\nEpisode 3074 terminated after 147 steps\nEpisode 3076 terminated after 144 steps\nEpisode 3077 terminated after 188 steps\nEpisode 3078 terminated after 144 steps\nEpisode 3079 terminated after 143 steps\nEpisode 3082 terminated after 151 steps\nEpisode 3083 terminated after 184 steps\nEpisode 3084 terminated after 147 steps\nEpisode 3085 terminated after 176 steps\nEpisode 3086 terminated after 150 steps\nEpisode 3087 terminated after 149 steps\nEpisode 3088 terminated after 154 steps\nEpisode 3089 terminated after 148 steps\nEpisode 3090 terminated after 186 steps\nEpisode 3091 terminated after 154 steps\nEpisode 3092 terminated after 144 steps\nEpisode 3093 terminated after 150 steps\nEpisode 3096 terminated after 148 steps\nEpisode 3097 terminated after 155 steps\nEpisode 3099 terminated after 150 steps\nEpisode 3100\nEpisode 3100 terminated after 152 steps\nEpisode 3101 terminated after 144 steps\nEpisode 3102 terminated after 152 steps\nEpisode 3103 terminated after 188 steps\nEpisode 3104 terminated after 145 steps\nEpisode 3106 terminated after 186 steps\nEpisode 3110 terminated after 153 steps\nEpisode 3114 terminated after 192 steps\nEpisode 3116 terminated after 185 steps\nEpisode 3117 terminated after 183 steps\nEpisode 3118 terminated after 183 steps\nEpisode 3119 terminated after 181 steps\nEpisode 3122 terminated after 176 steps\nEpisode 3125 terminated after 182 steps\nEpisode 3132 terminated after 199 steps\nEpisode 3133 terminated after 185 steps\nEpisode 3141 terminated after 178 steps\nEpisode 3143 terminated after 178 steps\nEpisode 3144 terminated after 180 steps\nEpisode 3147 terminated after 183 steps\nEpisode 3152 terminated after 176 steps\nEpisode 3153 terminated after 183 steps\nEpisode 3154 terminated after 187 steps\nEpisode 3161 terminated after 184 steps\nEpisode 3162 terminated after 178 steps\nEpisode 3163 terminated after 184 steps\nEpisode 3166 terminated after 179 steps\nEpisode 3172 terminated after 184 steps\nEpisode 3173 terminated after 185 steps\nEpisode 3174 terminated after 184 steps\nEpisode 3175 terminated after 181 steps\nEpisode 3176 terminated after 184 steps\nEpisode 3178 terminated after 195 steps\nEpisode 3180 terminated after 181 steps\nEpisode 3181 terminated after 176 steps\nEpisode 3182 terminated after 191 steps\nEpisode 3183 terminated after 186 steps\nEpisode 3184 terminated after 180 steps\nEpisode 3185 terminated after 184 steps\nEpisode 3186 terminated after 190 steps\nEpisode 3187 terminated after 189 steps\nEpisode 3188 terminated after 180 steps\nEpisode 3189 terminated after 178 steps\nEpisode 3190 terminated after 188 steps\nEpisode 3191 terminated after 190 steps\nEpisode 3192 terminated after 196 steps\nEpisode 3193 terminated after 191 steps\nEpisode 3194 terminated after 188 steps\nEpisode 3195 terminated after 186 steps\nEpisode 3196 terminated after 188 steps\nEpisode 3197 terminated after 195 steps\nEpisode 3198 terminated after 185 steps\nEpisode 3199 terminated after 180 steps\nEpisode 3200\nEpisode 3200 terminated after 176 steps\nEpisode 3201 terminated after 186 steps\nEpisode 3202 terminated after 189 steps\nEpisode 3204 terminated after 191 steps\nEpisode 3205 terminated after 192 steps\nEpisode 3206 terminated after 191 steps\nEpisode 3207 terminated after 187 steps\nEpisode 3208 terminated after 184 steps\nEpisode 3209 terminated after 182 steps\nEpisode 3211 terminated after 168 steps\nEpisode 3212 terminated after 156 steps\nEpisode 3213 terminated after 170 steps\nEpisode 3214 terminated after 183 steps\nEpisode 3215 terminated after 183 steps\nEpisode 3216 terminated after 185 steps\nEpisode 3217 terminated after 185 steps\nEpisode 3218 terminated after 181 steps\nEpisode 3219 terminated after 158 steps\nEpisode 3220 terminated after 193 steps\nEpisode 3221 terminated after 184 steps\nEpisode 3222 terminated after 154 steps\nEpisode 3223 terminated after 154 steps\nEpisode 3224 terminated after 182 steps\nEpisode 3225 terminated after 153 steps\nEpisode 3226 terminated after 185 steps\nEpisode 3227 terminated after 182 steps\nEpisode 3228 terminated after 187 steps\nEpisode 3229 terminated after 171 steps\nEpisode 3230 terminated after 185 steps\nEpisode 3231 terminated after 182 steps\nEpisode 3232 terminated after 187 steps\nEpisode 3233 terminated after 188 steps\nEpisode 3234 terminated after 164 steps\nEpisode 3235 terminated after 179 steps\nEpisode 3236 terminated after 183 steps\nEpisode 3237 terminated after 179 steps\nEpisode 3238 terminated after 180 steps\nEpisode 3239 terminated after 156 steps\nEpisode 3241 terminated after 180 steps\nEpisode 3242 terminated after 183 steps\nEpisode 3243 terminated after 182 steps\nEpisode 3244 terminated after 160 steps\nEpisode 3245 terminated after 177 steps\nEpisode 3246 terminated after 158 steps\nEpisode 3247 terminated after 165 steps\nEpisode 3248 terminated after 178 steps\nEpisode 3249 terminated after 183 steps\nEpisode 3250 terminated after 185 steps\nEpisode 3251 terminated after 163 steps\nEpisode 3252 terminated after 186 steps\nEpisode 3253 terminated after 186 steps\nEpisode 3254 terminated after 186 steps\nEpisode 3255 terminated after 181 steps\nEpisode 3256 terminated after 182 steps\nEpisode 3257 terminated after 180 steps\nEpisode 3258 terminated after 183 steps\nEpisode 3259 terminated after 183 steps\nEpisode 3260 terminated after 181 steps\nEpisode 3261 terminated after 183 steps\nEpisode 3262 terminated after 169 steps\nEpisode 3263 terminated after 185 steps\nEpisode 3264 terminated after 157 steps\nEpisode 3265 terminated after 159 steps\nEpisode 3266 terminated after 176 steps\nEpisode 3267 terminated after 164 steps\nEpisode 3268 terminated after 162 steps\nEpisode 3269 terminated after 154 steps\nEpisode 3270 terminated after 180 steps\nEpisode 3271 terminated after 181 steps\nEpisode 3272 terminated after 174 steps\nEpisode 3273 terminated after 175 steps\nEpisode 3274 terminated after 154 steps\nEpisode 3275 terminated after 184 steps\nEpisode 3276 terminated after 158 steps\nEpisode 3277 terminated after 181 steps\nEpisode 3278 terminated after 180 steps\nEpisode 3279 terminated after 153 steps\nEpisode 3280 terminated after 181 steps\nEpisode 3281 terminated after 188 steps\nEpisode 3282 terminated after 179 steps\nEpisode 3283 terminated after 182 steps\nEpisode 3284 terminated after 153 steps\nEpisode 3285 terminated after 176 steps\nEpisode 3286 terminated after 165 steps\nEpisode 3287 terminated after 181 steps\nEpisode 3288 terminated after 183 steps\nEpisode 3289 terminated after 156 steps\nEpisode 3290 terminated after 170 steps\nEpisode 3291 terminated after 178 steps\nEpisode 3292 terminated after 155 steps\nEpisode 3293 terminated after 184 steps\nEpisode 3294 terminated after 181 steps\nEpisode 3295 terminated after 181 steps\nEpisode 3296 terminated after 186 steps\nEpisode 3297 terminated after 187 steps\nEpisode 3298 terminated after 170 steps\nEpisode 3299 terminated after 171 steps\nEpisode 3300\nEpisode 3300 terminated after 156 steps\nEpisode 3301 terminated after 183 steps\nEpisode 3302 terminated after 190 steps\nEpisode 3303 terminated after 166 steps\nEpisode 3304 terminated after 154 steps\nEpisode 3305 terminated after 161 steps\nEpisode 3306 terminated after 188 steps\nEpisode 3307 terminated after 180 steps\nEpisode 3308 terminated after 187 steps\nEpisode 3309 terminated after 153 steps\nEpisode 3310 terminated after 156 steps\nEpisode 3311 terminated after 190 steps\nEpisode 3312 terminated after 190 steps\nEpisode 3313 terminated after 154 steps\nEpisode 3314 terminated after 152 steps\nEpisode 3315 terminated after 169 steps\nEpisode 3316 terminated after 164 steps\nEpisode 3317 terminated after 161 steps\nEpisode 3318 terminated after 188 steps\nEpisode 3319 terminated after 165 steps\nEpisode 3320 terminated after 187 steps\nEpisode 3321 terminated after 157 steps\nEpisode 3322 terminated after 183 steps\nEpisode 3323 terminated after 182 steps\nEpisode 3324 terminated after 191 steps\nEpisode 3325 terminated after 184 steps\nEpisode 3326 terminated after 180 steps\nEpisode 3327 terminated after 159 steps\nEpisode 3328 terminated after 184 steps\nEpisode 3329 terminated after 180 steps\nEpisode 3330 terminated after 155 steps\nEpisode 3331 terminated after 178 steps\nEpisode 3332 terminated after 178 steps\nEpisode 3333 terminated after 180 steps\nEpisode 3334 terminated after 166 steps\nEpisode 3335 terminated after 168 steps\nEpisode 3336 terminated after 187 steps\nEpisode 3337 terminated after 180 steps\nEpisode 3338 terminated after 183 steps\nEpisode 3339 terminated after 154 steps\nEpisode 3340 terminated after 181 steps\nEpisode 3341 terminated after 183 steps\nEpisode 3342 terminated after 179 steps\nEpisode 3343 terminated after 182 steps\nEpisode 3344 terminated after 183 steps\nEpisode 3345 terminated after 185 steps\nEpisode 3346 terminated after 155 steps\nEpisode 3347 terminated after 185 steps\nEpisode 3348 terminated after 182 steps\nEpisode 3349 terminated after 178 steps\nEpisode 3350 terminated after 178 steps\nEpisode 3351 terminated after 181 steps\nEpisode 3352 terminated after 151 steps\nEpisode 3353 terminated after 175 steps\nEpisode 3354 terminated after 167 steps\nEpisode 3355 terminated after 183 steps\nEpisode 3357 terminated after 191 steps\nEpisode 3358 terminated after 170 steps\nEpisode 3359 terminated after 187 steps\nEpisode 3360 terminated after 165 steps\nEpisode 3361 terminated after 163 steps\nEpisode 3362 terminated after 168 steps\nEpisode 3364 terminated after 151 steps\nEpisode 3365 terminated after 152 steps\nEpisode 3366 terminated after 188 steps\nEpisode 3367 terminated after 188 steps\nEpisode 3368 terminated after 162 steps\nEpisode 3369 terminated after 183 steps\nEpisode 3370 terminated after 189 steps\nEpisode 3371 terminated after 163 steps\nEpisode 3372 terminated after 195 steps\nEpisode 3373 terminated after 183 steps\nEpisode 3374 terminated after 188 steps\nEpisode 3375 terminated after 154 steps\nEpisode 3376 terminated after 163 steps\nEpisode 3377 terminated after 187 steps\nEpisode 3378 terminated after 156 steps\nEpisode 3379 terminated after 181 steps\nEpisode 3380 terminated after 186 steps\nEpisode 3381 terminated after 160 steps\nEpisode 3382 terminated after 151 steps\nEpisode 3383 terminated after 185 steps\nEpisode 3384 terminated after 182 steps\nEpisode 3385 terminated after 171 steps\nEpisode 3386 terminated after 187 steps\nEpisode 3387 terminated after 187 steps\nEpisode 3388 terminated after 188 steps\nEpisode 3389 terminated after 183 steps\nEpisode 3390 terminated after 152 steps\nEpisode 3391 terminated after 180 steps\nEpisode 3392 terminated after 163 steps\nEpisode 3393 terminated after 155 steps\nEpisode 3394 terminated after 187 steps\nEpisode 3395 terminated after 188 steps\nEpisode 3396 terminated after 187 steps\nEpisode 3397 terminated after 166 steps\nEpisode 3398 terminated after 154 steps\nEpisode 3400\nEpisode 3401 terminated after 153 steps\nEpisode 3402 terminated after 165 steps\nEpisode 3403 terminated after 154 steps\nEpisode 3404 terminated after 189 steps\nEpisode 3405 terminated after 180 steps\nEpisode 3406 terminated after 189 steps\nEpisode 3407 terminated after 184 steps\nEpisode 3408 terminated after 152 steps\nEpisode 3409 terminated after 157 steps\nEpisode 3410 terminated after 169 steps\nEpisode 3411 terminated after 181 steps\nEpisode 3412 terminated after 173 steps\nEpisode 3413 terminated after 187 steps\nEpisode 3414 terminated after 191 steps\nEpisode 3415 terminated after 168 steps\nEpisode 3416 terminated after 171 steps\nEpisode 3417 terminated after 152 steps\nEpisode 3418 terminated after 173 steps\nEpisode 3419 terminated after 185 steps\nEpisode 3420 terminated after 185 steps\nEpisode 3421 terminated after 177 steps\nEpisode 3422 terminated after 174 steps\nEpisode 3423 terminated after 182 steps\nEpisode 3424 terminated after 156 steps\nEpisode 3427 terminated after 174 steps\nEpisode 3430 terminated after 153 steps\nEpisode 3431 terminated after 197 steps\nEpisode 3432 terminated after 148 steps\nEpisode 3437 terminated after 185 steps\nEpisode 3439 terminated after 172 steps\nEpisode 3441 terminated after 152 steps\nEpisode 3442 terminated after 155 steps\nEpisode 3443 terminated after 165 steps\nEpisode 3444 terminated after 152 steps\nEpisode 3445 terminated after 157 steps\nEpisode 3446 terminated after 150 steps\nEpisode 3447 terminated after 150 steps\nEpisode 3449 terminated after 172 steps\nEpisode 3451 terminated after 164 steps\nEpisode 3454 terminated after 151 steps\nEpisode 3456 terminated after 164 steps\nEpisode 3457 terminated after 147 steps\nEpisode 3458 terminated after 158 steps\nEpisode 3460 terminated after 181 steps\nEpisode 3461 terminated after 162 steps\nEpisode 3462 terminated after 175 steps\nEpisode 3469 terminated after 170 steps\nEpisode 3470 terminated after 176 steps\nEpisode 3496 terminated after 190 steps\nEpisode 3500\nEpisode 3502 terminated after 193 steps\nEpisode 3503 terminated after 189 steps\nEpisode 3504 terminated after 146 steps\nEpisode 3509 terminated after 188 steps\nEpisode 3512 terminated after 168 steps\nEpisode 3516 terminated after 152 steps\nEpisode 3525 terminated after 193 steps\nEpisode 3526 terminated after 192 steps\nEpisode 3527 terminated after 161 steps\nEpisode 3528 terminated after 158 steps\nEpisode 3529 terminated after 149 steps\nEpisode 3530 terminated after 155 steps\nEpisode 3531 terminated after 193 steps\nEpisode 3532 terminated after 150 steps\nEpisode 3533 terminated after 155 steps\nEpisode 3534 terminated after 148 steps\nEpisode 3535 terminated after 154 steps\nEpisode 3536 terminated after 154 steps\nEpisode 3537 terminated after 197 steps\nEpisode 3538 terminated after 125 steps\nEpisode 3539 terminated after 152 steps\nEpisode 3540 terminated after 160 steps\nEpisode 3541 terminated after 123 steps\nEpisode 3542 terminated after 151 steps\nEpisode 3543 terminated after 123 steps\nEpisode 3544 terminated after 152 steps\nEpisode 3545 terminated after 155 steps\nEpisode 3547 terminated after 189 steps\nEpisode 3548 terminated after 154 steps\nEpisode 3549 terminated after 162 steps\nEpisode 3550 terminated after 156 steps\nEpisode 3551 terminated after 159 steps\nEpisode 3552 terminated after 194 steps\nEpisode 3553 terminated after 149 steps\nEpisode 3554 terminated after 190 steps\nEpisode 3555 terminated after 147 steps\nEpisode 3556 terminated after 156 steps\nEpisode 3557 terminated after 153 steps\nEpisode 3558 terminated after 129 steps\nEpisode 3559 terminated after 157 steps\nEpisode 3560 terminated after 159 steps\nEpisode 3561 terminated after 161 steps\nEpisode 3562 terminated after 165 steps\nEpisode 3563 terminated after 157 steps\nEpisode 3564 terminated after 156 steps\nEpisode 3565 terminated after 132 steps\nEpisode 3566 terminated after 166 steps\nEpisode 3567 terminated after 159 steps\nEpisode 3568 terminated after 168 steps\nEpisode 3569 terminated after 129 steps\nEpisode 3570 terminated after 122 steps\nEpisode 3571 terminated after 175 steps\nEpisode 3572 terminated after 164 steps\nEpisode 3573 terminated after 129 steps\nEpisode 3575 terminated after 171 steps\nEpisode 3576 terminated after 172 steps\nEpisode 3577 terminated after 128 steps\nEpisode 3578 terminated after 178 steps\nEpisode 3579 terminated after 149 steps\nEpisode 3580 terminated after 185 steps\nEpisode 3581 terminated after 123 steps\nEpisode 3582 terminated after 165 steps\nEpisode 3583 terminated after 186 steps\nEpisode 3585 terminated after 197 steps\nEpisode 3586 terminated after 126 steps\nEpisode 3587 terminated after 153 steps\nEpisode 3588 terminated after 149 steps\nEpisode 3590 terminated after 159 steps\nEpisode 3591 terminated after 124 steps\nEpisode 3592 terminated after 158 steps\nEpisode 3593 terminated after 166 steps\nEpisode 3594 terminated after 124 steps\nEpisode 3595 terminated after 164 steps\nEpisode 3596 terminated after 198 steps\nEpisode 3597 terminated after 160 steps\nEpisode 3599 terminated after 174 steps\nEpisode 3600\nEpisode 3600 terminated after 190 steps\nEpisode 3606 terminated after 198 steps\nEpisode 3609 terminated after 152 steps\nEpisode 3615 terminated after 197 steps\nEpisode 3616 terminated after 198 steps\nEpisode 3623 terminated after 196 steps\nEpisode 3626 terminated after 190 steps\nEpisode 3629 terminated after 152 steps\nEpisode 3636 terminated after 176 steps\nEpisode 3639 terminated after 193 steps\nEpisode 3642 terminated after 197 steps\nEpisode 3643 terminated after 193 steps\nEpisode 3646 terminated after 193 steps\nEpisode 3647 terminated after 196 steps\nEpisode 3649 terminated after 197 steps\nEpisode 3650 terminated after 195 steps\nEpisode 3658 terminated after 193 steps\nEpisode 3667 terminated after 193 steps\nEpisode 3668 terminated after 192 steps\nEpisode 3670 terminated after 187 steps\nEpisode 3674 terminated after 193 steps\nEpisode 3679 terminated after 194 steps\nEpisode 3681 terminated after 190 steps\nEpisode 3682 terminated after 196 steps\nEpisode 3697 terminated after 191 steps\nEpisode 3700\nEpisode 3701 terminated after 195 steps\nEpisode 3703 terminated after 189 steps\nEpisode 3704 terminated after 185 steps\nEpisode 3705 terminated after 194 steps\nEpisode 3717 terminated after 198 steps\nEpisode 3733 terminated after 190 steps\nEpisode 3739 terminated after 192 steps\nEpisode 3743 terminated after 186 steps\nEpisode 3746 terminated after 194 steps\nEpisode 3750 terminated after 193 steps\nEpisode 3756 terminated after 190 steps\nEpisode 3762 terminated after 186 steps\nEpisode 3764 terminated after 186 steps\nEpisode 3766 terminated after 184 steps\nEpisode 3770 terminated after 184 steps\nEpisode 3773 terminated after 186 steps\nEpisode 3779 terminated after 174 steps\nEpisode 3781 terminated after 153 steps\nEpisode 3782 terminated after 178 steps\nEpisode 3784 terminated after 157 steps\nEpisode 3785 terminated after 182 steps\nEpisode 3786 terminated after 157 steps\nEpisode 3787 terminated after 178 steps\nEpisode 3788 terminated after 164 steps\nEpisode 3789 terminated after 184 steps\nEpisode 3790 terminated after 186 steps\nEpisode 3791 terminated after 156 steps\nEpisode 3792 terminated after 186 steps\nEpisode 3793 terminated after 156 steps\nEpisode 3794 terminated after 154 steps\nEpisode 3795 terminated after 153 steps\nEpisode 3796 terminated after 154 steps\nEpisode 3797 terminated after 156 steps\nEpisode 3798 terminated after 149 steps\nEpisode 3799 terminated after 179 steps\nEpisode 3800\nEpisode 3800 terminated after 152 steps\nEpisode 3801 terminated after 153 steps\nEpisode 3802 terminated after 176 steps\nEpisode 3803 terminated after 174 steps\nEpisode 3804 terminated after 184 steps\nEpisode 3805 terminated after 150 steps\nEpisode 3806 terminated after 188 steps\nEpisode 3807 terminated after 178 steps\nEpisode 3808 terminated after 186 steps\nEpisode 3809 terminated after 172 steps\nEpisode 3810 terminated after 154 steps\nEpisode 3811 terminated after 151 steps\nEpisode 3812 terminated after 175 steps\nEpisode 3813 terminated after 187 steps\nEpisode 3814 terminated after 154 steps\nEpisode 3815 terminated after 155 steps\nEpisode 3816 terminated after 190 steps\nEpisode 3817 terminated after 185 steps\nEpisode 3818 terminated after 154 steps\nEpisode 3819 terminated after 150 steps\nEpisode 3821 terminated after 192 steps\nEpisode 3822 terminated after 151 steps\nEpisode 3823 terminated after 189 steps\nEpisode 3824 terminated after 153 steps\nEpisode 3825 terminated after 153 steps\nEpisode 3826 terminated after 184 steps\nEpisode 3827 terminated after 176 steps\nEpisode 3828 terminated after 184 steps\nEpisode 3829 terminated after 149 steps\nEpisode 3830 terminated after 151 steps\nEpisode 3831 terminated after 151 steps\nEpisode 3832 terminated after 153 steps\nEpisode 3833 terminated after 150 steps\nEpisode 3834 terminated after 149 steps\nEpisode 3835 terminated after 150 steps\nEpisode 3836 terminated after 154 steps\nEpisode 3837 terminated after 154 steps\nEpisode 3838 terminated after 149 steps\nEpisode 3839 terminated after 150 steps\nEpisode 3840 terminated after 151 steps\nEpisode 3841 terminated after 148 steps\nEpisode 3842 terminated after 185 steps\nEpisode 3843 terminated after 174 steps\nEpisode 3844 terminated after 150 steps\nEpisode 3845 terminated after 190 steps\nEpisode 3846 terminated after 149 steps\nEpisode 3847 terminated after 152 steps\nEpisode 3848 terminated after 195 steps\nEpisode 3849 terminated after 151 steps\nEpisode 3850 terminated after 153 steps\nEpisode 3851 terminated after 150 steps\nEpisode 3852 terminated after 173 steps\nEpisode 3853 terminated after 147 steps\nEpisode 3854 terminated after 147 steps\nEpisode 3855 terminated after 151 steps\nEpisode 3856 terminated after 151 steps\nEpisode 3857 terminated after 171 steps\nEpisode 3858 terminated after 180 steps\nEpisode 3859 terminated after 149 steps\nEpisode 3860 terminated after 189 steps\nEpisode 3861 terminated after 151 steps\nEpisode 3862 terminated after 183 steps\nEpisode 3863 terminated after 187 steps\nEpisode 3864 terminated after 151 steps\nEpisode 3865 terminated after 146 steps\nEpisode 3866 terminated after 146 steps\nEpisode 3867 terminated after 185 steps\nEpisode 3868 terminated after 187 steps\nEpisode 3869 terminated after 146 steps\nEpisode 3870 terminated after 146 steps\nEpisode 3871 terminated after 173 steps\nEpisode 3872 terminated after 148 steps\nEpisode 3873 terminated after 149 steps\nEpisode 3874 terminated after 148 steps\nEpisode 3875 terminated after 187 steps\nEpisode 3876 terminated after 152 steps\nEpisode 3877 terminated after 146 steps\nEpisode 3878 terminated after 192 steps\nEpisode 3879 terminated after 152 steps\nEpisode 3880 terminated after 186 steps\nEpisode 3881 terminated after 150 steps\nEpisode 3882 terminated after 181 steps\nEpisode 3884 terminated after 186 steps\nEpisode 3885 terminated after 178 steps\nEpisode 3886 terminated after 146 steps\nEpisode 3887 terminated after 187 steps\nEpisode 3888 terminated after 152 steps\nEpisode 3889 terminated after 166 steps\nEpisode 3890 terminated after 186 steps\nEpisode 3891 terminated after 185 steps\nEpisode 3892 terminated after 190 steps\nEpisode 3893 terminated after 150 steps\nEpisode 3894 terminated after 151 steps\nEpisode 3895 terminated after 151 steps\nEpisode 3896 terminated after 183 steps\nEpisode 3897 terminated after 175 steps\nEpisode 3898 terminated after 146 steps\nEpisode 3899 terminated after 147 steps\nEpisode 3900\nEpisode 3900 terminated after 146 steps\nEpisode 3901 terminated after 165 steps\nEpisode 3902 terminated after 152 steps\nEpisode 3903 terminated after 143 steps\nEpisode 3904 terminated after 143 steps\nEpisode 3905 terminated after 188 steps\nEpisode 3906 terminated after 145 steps\nEpisode 3907 terminated after 169 steps\nEpisode 3908 terminated after 185 steps\nEpisode 3909 terminated after 146 steps\nEpisode 3910 terminated after 162 steps\nEpisode 3911 terminated after 187 steps\nEpisode 3912 terminated after 143 steps\nEpisode 3913 terminated after 151 steps\nEpisode 3914 terminated after 143 steps\nEpisode 3915 terminated after 165 steps\nEpisode 3916 terminated after 149 steps\nEpisode 3917 terminated after 148 steps\nEpisode 3918 terminated after 183 steps\nEpisode 3919 terminated after 168 steps\nEpisode 3920 terminated after 192 steps\nEpisode 3921 terminated after 147 steps\nEpisode 3922 terminated after 148 steps\nEpisode 3923 terminated after 149 steps\nEpisode 3924 terminated after 189 steps\nEpisode 3925 terminated after 150 steps\nEpisode 3926 terminated after 151 steps\nEpisode 3927 terminated after 150 steps\nEpisode 3928 terminated after 182 steps\nEpisode 3929 terminated after 172 steps\nEpisode 3930 terminated after 145 steps\nEpisode 3931 terminated after 190 steps\nEpisode 3932 terminated after 144 steps\nEpisode 3933 terminated after 143 steps\nEpisode 3934 terminated after 172 steps\nEpisode 3935 terminated after 189 steps\nEpisode 3936 terminated after 186 steps\nEpisode 3937 terminated after 152 steps\nEpisode 3938 terminated after 147 steps\nEpisode 3939 terminated after 185 steps\nEpisode 3940 terminated after 146 steps\nEpisode 3941 terminated after 147 steps\nEpisode 3942 terminated after 146 steps\nEpisode 3943 terminated after 185 steps\nEpisode 3944 terminated after 146 steps\nEpisode 3945 terminated after 185 steps\nEpisode 3946 terminated after 183 steps\nEpisode 3947 terminated after 145 steps\nEpisode 3948 terminated after 144 steps\nEpisode 3949 terminated after 149 steps\nEpisode 3950 terminated after 167 steps\nEpisode 3951 terminated after 148 steps\nEpisode 3952 terminated after 152 steps\nEpisode 3953 terminated after 150 steps\nEpisode 3954 terminated after 180 steps\nEpisode 3955 terminated after 182 steps\nEpisode 3956 terminated after 178 steps\nEpisode 3957 terminated after 186 steps\nEpisode 3958 terminated after 188 steps\nEpisode 3959 terminated after 147 steps\nEpisode 3960 terminated after 146 steps\nEpisode 3961 terminated after 151 steps\nEpisode 3962 terminated after 179 steps\nEpisode 3963 terminated after 151 steps\nEpisode 3964 terminated after 148 steps\nEpisode 3965 terminated after 169 steps\nEpisode 3966 terminated after 148 steps\nEpisode 3967 terminated after 170 steps\nEpisode 3968 terminated after 177 steps\nEpisode 3969 terminated after 150 steps\nEpisode 3970 terminated after 182 steps\nEpisode 3971 terminated after 178 steps\nEpisode 3972 terminated after 146 steps\nEpisode 3973 terminated after 181 steps\nEpisode 3974 terminated after 178 steps\nEpisode 3975 terminated after 179 steps\nEpisode 3976 terminated after 176 steps\nEpisode 3977 terminated after 147 steps\nEpisode 3978 terminated after 185 steps\nEpisode 3979 terminated after 147 steps\nEpisode 3980 terminated after 150 steps\nEpisode 3981 terminated after 150 steps\nEpisode 3982 terminated after 148 steps\nEpisode 3983 terminated after 187 steps\nEpisode 3984 terminated after 152 steps\nEpisode 3985 terminated after 149 steps\nEpisode 3986 terminated after 188 steps\nEpisode 3987 terminated after 147 steps\nEpisode 3988 terminated after 188 steps\nEpisode 3989 terminated after 179 steps\nEpisode 3990 terminated after 152 steps\nEpisode 3991 terminated after 156 steps\nEpisode 3992 terminated after 152 steps\nEpisode 3993 terminated after 181 steps\nEpisode 3994 terminated after 147 steps\nEpisode 3995 terminated after 152 steps\nEpisode 3996 terminated after 173 steps\nEpisode 3997 terminated after 148 steps\nEpisode 3998 terminated after 150 steps\nEpisode 3999 terminated after 148 steps\n\n\n\nq_table_np\n\narray([[[-6.17936588e-01, -1.98903383e+00,  1.15120248e+00],\n        [-4.53764806e-01,  1.34908758e-01, -1.07268580e+00],\n        [ 6.36300619e-02, -6.49255742e-01,  3.43143288e-01],\n        ...,\n        [-1.47426748e+00, -1.92654619e-01, -1.01827323e+00],\n        [-1.23244092e+00, -1.08382756e+00, -1.66590846e+00],\n        [ 6.18682191e-01, -6.31722662e-01,  1.89562638e+00]],\n\n       [[ 1.44884462e+00, -1.08582448e+00, -6.17532955e-01],\n        [-2.60033696e-01, -1.50735907e+00, -7.36880075e-01],\n        [-4.33453026e+00,  7.21930130e-01, -8.94114558e-01],\n        ...,\n        [ 1.39149714e+00, -4.35242186e-01, -1.44402614e+00],\n        [ 1.47319582e+00, -9.81043937e-01, -1.16663769e+00],\n        [ 1.54574724e-01,  2.01528043e-01, -7.87255597e-01]],\n\n       [[-9.42959906e-01,  3.57988275e-01,  6.76234740e-02],\n        [-1.75824035e-01, -3.90116835e-01,  2.22525056e+00],\n        [-2.61460860e+01, -2.57651012e+01, -2.65800774e+01],\n        ...,\n        [-4.34968569e-01, -1.87249267e+00,  1.76290733e-01],\n        [-1.68751403e+00, -3.76561730e-01,  8.76169482e-01],\n        [-5.96034401e-01, -2.85341434e+00,  4.82285164e-01]],\n\n       ...,\n\n       [[-5.43106444e-01, -1.35838659e+00, -3.17955211e-01],\n        [ 3.65419766e-01,  5.30288952e-01,  6.60925958e-01],\n        [ 1.92307652e+00, -6.83889095e-01,  3.84116355e-01],\n        ...,\n        [ 8.16187611e-03,  2.16478945e-01,  2.19802520e+00],\n        [-1.48308991e+00,  1.67206495e+00,  3.82901283e-01],\n        [ 3.10720595e-01, -8.04125951e-01, -6.11653944e-01]],\n\n       [[-7.42037528e-01,  6.87114205e-01, -1.01632588e+00],\n        [-9.40280627e-01,  1.68347169e-01, -2.91297623e-01],\n        [ 7.75168521e-01, -1.28795146e+00,  1.42890256e+00],\n        ...,\n        [-6.04564766e-01,  1.43782213e+00,  3.30899649e-03],\n        [ 2.28905749e-01,  9.51891150e-01, -1.82367227e-01],\n        [ 1.45109916e+00, -6.93703178e-01,  1.00332049e+00]],\n\n       [[-7.46471478e-01, -1.42855882e+00, -6.77811701e-01],\n        [ 2.50420687e-01,  6.78487091e-01,  1.29496745e+00],\n        [-7.72925285e-01, -5.73864364e-01, -1.46103255e+00],\n        ...,\n        [-5.23042989e-01, -1.38253290e+00,  1.49749978e-01],\n        [-9.71722146e-02, -1.17977164e+00, -2.98167619e-01],\n        [-1.11982445e+00, -9.15361681e-01,  1.40031848e+00]]])\n\n\n\npd.Series(rewards).plot()\n\n\n\n\n\n\n\n\n\n# plot smoothed rewards\npd.Series(rewards).rolling(100).mean().plot()\n\n\n\n\n\n\n\n\n\nenv = gym.make('MountainCar-v0', render_mode='human')\nobservation,  _  = env.reset(seed=42)\n\nfor i in range(300):\n    env.render()\n    pos, vel = observation\n    pos_idx = discretize_pos(pos)\n    vel_idx = discretize_vel(vel)\n    action = np.argmax(q_table_np[pos_idx, vel_idx])\n    observation, reward, terminated, truncated, info = env.step(action)\n    print(i, observation, reward, terminated, truncated, info, action)\n    if terminated:\n        break\n\n0 [-0.4457913  -0.00058252] -1.0 False False {} 1\n1 [-0.4479521  -0.00216079] -1.0 False False {} 0\n2 [-0.4516754  -0.00372328] -1.0 False False {} 0\n3 [-0.45693392 -0.00525853] -1.0 False False {} 0\n4 [-0.46368912 -0.00675519] -1.0 False False {} 0\n5 [-0.47189122 -0.0082021 ] -1.0 False False {} 0\n6 [-0.4814796  -0.00958835] -1.0 False False {} 0\n7 [-0.49238297 -0.01090341] -1.0 False False {} 0\n8 [-0.5045202  -0.01213718] -1.0 False False {} 0\n9 [-0.5178004 -0.0132802] -1.0 False False {} 0\n10 [-0.53212404 -0.01432368] -1.0 False False {} 0\n11 [-0.5473838  -0.01525975] -1.0 False False {} 0\n12 [-0.5634653  -0.01608151] -1.0 False False {} 0\n13 [-0.58024853 -0.01678323] -1.0 False False {} 0\n14 [-0.5976089 -0.0173604] -1.0 False False {} 0\n15 [-0.6154188  -0.01780987] -1.0 False False {} 0\n16 [-0.6335487  -0.01812989] -1.0 False False {} 0\n17 [-0.6518688  -0.01832014] -1.0 False False {} 0\n18 [-0.67025054 -0.01838169] -1.0 False False {} 0\n19 [-0.68856746 -0.01831694] -1.0 False False {} 0\n20 [-0.70669705 -0.01812956] -1.0 False False {} 0\n21 [-0.7245214  -0.01782435] -1.0 False False {} 0\n22 [-0.7419284  -0.01740704] -1.0 False False {} 0\n23 [-0.7568126  -0.01488416] -1.0 False False {} 2\n24 [-0.7690869 -0.0122743] -1.0 False False {} 2\n25 [-0.778682   -0.00959512] -1.0 False False {} 2\n26 [-0.78654534 -0.00786333] -1.0 False False {} 1\n27 [-0.79263484 -0.00608949] -1.0 False False {} 1\n28 [-0.7969186  -0.00428376] -1.0 False False {} 1\n29 [-0.79837453 -0.00145597] -1.0 False False {} 2\n30 [-0.7969953   0.00137926] -1.0 False False {} 2\n31 [-0.79278785  0.00420745] -1.0 False False {} 2\n32 [-0.7857739   0.00701397] -1.0 False False {} 2\n33 [-0.7759901   0.00978372] -1.0 False False {} 2\n34 [-0.76348925  0.0125009 ] -1.0 False False {} 2\n35 [-0.7483405   0.01514874] -1.0 False False {} 2\n36 [-0.73063105  0.01770948] -1.0 False False {} 2\n37 [-0.71046674  0.02016429] -1.0 False False {} 2\n38 [-0.6879732   0.02249354] -1.0 False False {} 2\n39 [-0.6632962   0.02467699] -1.0 False False {} 2\n40 [-0.6366019   0.02669432] -1.0 False False {} 2\n41 [-0.6080762  0.0285257] -1.0 False False {} 2\n42 [-0.57792366  0.03015252] -1.0 False False {} 2\n43 [-0.5463655   0.03155815] -1.0 False False {} 2\n44 [-0.51363677  0.03272877] -1.0 False False {} 2\n45 [-0.4799827   0.03365407] -1.0 False False {} 2\n46 [-0.4456548   0.03432788] -1.0 False False {} 2\n47 [-0.4109062   0.03474861] -1.0 False False {} 2\n48 [-0.37598678  0.03491943] -1.0 False False {} 2\n49 [-0.3411386   0.03484817] -1.0 False False {} 2\n50 [-0.30659157  0.03454704] -1.0 False False {} 2\n51 [-0.27255952  0.03403204] -1.0 False False {} 2\n52 [-0.23923728  0.03332225] -1.0 False False {} 2\n53 [-0.20679832  0.03243897] -1.0 False False {} 2\n54 [-0.17639346  0.03040484] -1.0 False False {} 1\n55 [-0.14814667  0.02824679] -1.0 False False {} 1\n56 [-0.12115702  0.02698966] -1.0 False False {} 2\n57 [-0.09550402  0.02565299] -1.0 False False {} 2\n58 [-0.07124913  0.0242549 ] -1.0 False False {} 2\n59 [-0.04843733  0.02281179] -1.0 False False {} 2\n60 [-0.02709919  0.02133814] -1.0 False False {} 2\n61 [-0.00725279  0.0198464 ] -1.0 False False {} 2\n62 [0.01109421 0.01834699] -1.0 False False {} 2\n63 [0.02694258 0.01584838] -1.0 False False {} 1\n64 [0.04029912 0.01335654] -1.0 False False {} 1\n65 [0.05117391 0.01087479] -1.0 False False {} 1\n66 [0.0585781  0.00740419] -1.0 False False {} 0\n67 [0.06252079 0.00394269] -1.0 False False {} 0\n68 [0.06300733 0.00048654] -1.0 False False {} 0\n69 [ 0.0600384  -0.00296893] -1.0 False False {} 0\n70 [ 0.05360991 -0.00642849] -1.0 False False {} 0\n71 [ 0.04471368 -0.00889623] -1.0 False False {} 1\n72 [ 0.03333991 -0.01137377] -1.0 False False {} 1\n73 [ 0.01947864 -0.01386127] -1.0 False False {} 1\n74 [ 0.00412164 -0.01535701] -1.0 False False {} 2\n75 [-0.01273518 -0.01685682] -1.0 False False {} 2\n76 [-0.03309017 -0.02035499] -1.0 False False {} 0\n77 [-0.05693285 -0.02384268] -1.0 False False {} 0\n78 [-0.08323916 -0.02630631] -1.0 False False {} 1\n79 [-0.11296792 -0.02972876] -1.0 False False {} 0\n80 [-0.14405449 -0.03108656] -1.0 False False {} 2\n81 [-0.1764112  -0.03235672] -1.0 False False {} 2\n82 [-0.21092589 -0.0345147 ] -1.0 False False {} 1\n83 [-0.24745657 -0.03653067] -1.0 False False {} 1\n84 [-0.2868294  -0.03937284] -1.0 False False {} 0\n85 [-0.32883242 -0.04200301] -1.0 False False {} 0\n86 [-0.37221447 -0.04338205] -1.0 False False {} 1\n87 [-0.41669327 -0.0444788 ] -1.0 False False {} 1\n88 [-0.4619602  -0.04526692] -1.0 False False {} 1\n89 [-0.5086868  -0.04672658] -1.0 False False {} 0\n90 [-0.5565252  -0.04783838] -1.0 False False {} 0\n91 [-0.60511696 -0.04859183] -1.0 False False {} 0\n92 [-0.6541035  -0.04898652] -1.0 False False {} 0\n93 [-0.703136   -0.04903255] -1.0 False False {} 0\n94 [-0.75188625 -0.04875019] -1.0 False False {} 0\n95 [-0.79905504 -0.04716877] -1.0 False False {} 1\n96 [-0.8443851  -0.04533008] -1.0 False False {} 1\n97 [-0.8886638  -0.04427873] -1.0 False False {} 0\n98 [-0.93172    -0.04305618] -1.0 False False {} 0\n99 [-0.97242475 -0.04070471] -1.0 False False {} 1\n100 [-1.0106921  -0.03826734] -1.0 False False {} 1\n101 [-1.0474744  -0.03678232] -1.0 False False {} 0\n102 [-1.0827568  -0.03528232] -1.0 False False {} 0\n103 [-1.1155533  -0.03279654] -1.0 False False {} 1\n104 [-1.1469022  -0.03134892] -1.0 False False {} 0\n105 [-1.1758621  -0.02895992] -1.0 False False {} 1\n106 [-1.2  0. ] -1.0 False False {} 2\n107 [-1.1977581  0.0022419] -1.0 False False {} 1\n108 [-1.1932669   0.00449118] -1.0 False False {} 1\n109 [-1.186512    0.00675497] -1.0 False False {} 1\n110 [-1.1764722   0.01003978] -1.0 False False {} 2\n111 [-1.163118    0.01335412] -1.0 False False {} 2\n112 [-1.1464136   0.01670446] -1.0 False False {} 2\n113 [-1.126319    0.02009453] -1.0 False False {} 2\n114 [-1.1047946   0.02152443] -1.0 False False {} 0\n115 [-1.0798074   0.02498721] -1.0 False False {} 2\n116 [-1.0513321   0.02847525] -1.0 False False {} 2\n117 [-1.0193571   0.03197506] -1.0 False False {} 2\n118 [-0.9838908   0.03546635] -1.0 False False {} 2\n119 [-0.9459694  0.0379214] -1.0 False False {} 1\n120 [-0.9046624  0.041307 ] -1.0 False False {} 2\n121 [-0.8600805  0.0445819] -1.0 False False {} 2\n122 [-0.8123822   0.04769824] -1.0 False False {} 2\n123 [-0.76177907  0.05060317] -1.0 False False {} 2\n124 [-0.7085377   0.05324135] -1.0 False False {} 2\n125 [-0.6549794   0.05355831] -1.0 False False {} 0\n126 [-0.5994611   0.05551836] -1.0 False False {} 2\n127 [-0.5423786   0.05708242] -1.0 False False {} 2\n128 [-0.48415545  0.0582232 ] -1.0 False False {} 2\n129 [-0.42522737  0.05892806] -1.0 False False {} 2\n130 [-0.36602643  0.05920094] -1.0 False False {} 2\n131 [-0.30696377  0.05906267] -1.0 False False {} 2\n132 [-0.24841388  0.05854989] -1.0 False False {} 2\n133 [-0.19070129  0.05771258] -1.0 False False {} 2\n134 [-0.13409062  0.05661067] -1.0 False False {} 2\n135 [-0.07878038  0.05531024] -1.0 False False {} 2\n136 [-0.02490064  0.05387974] -1.0 False False {} 2\n137 [0.02748607 0.05238671] -1.0 False False {} 2\n138 [0.07838127 0.0508952 ] -1.0 False False {} 2\n139 [0.12584527 0.047464  ] -1.0 False False {} 0\n140 [0.16998534 0.04414006] -1.0 False False {} 0\n141 [0.21294348 0.04295815] -1.0 False False {} 2\n142 [0.25489464 0.04195116] -1.0 False False {} 2\n143 [0.29604182 0.04114716] -1.0 False False {} 2\n144 [0.33661178 0.04056999] -1.0 False False {} 2\n145 [0.37685177 0.04023999] -1.0 False False {} 2\n146 [0.41602638 0.0391746 ] -1.0 False False {} 1\n147 [0.4544081  0.03838173] -1.0 False False {} 1\n148 [0.49127463 0.03686652] -1.0 False False {} 0\n149 [0.5268991  0.03562447] -1.0 True False {} 0\n\n\n\nThe Kernel crashed while executing code in the current cell or a previous cell. \n\nPlease review the code in the cell(s) to identify a possible cause of the failure. \n\nClick &lt;a href='https://aka.ms/vscodeJupyterKernelCrash'&gt;here&lt;/a&gt; for more info. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\nReference: https://omarelb.github.io/dqn-investigation/\n\n# Deep Q-learning\n\n\nclass QNetwork(torch.nn.Module):\n    def __init__(self, n_inputs, n_outputs, n_hidden=128):\n        super(QNetwork, self).__init__()\n        self.fc1 = torch.nn.Linear(n_inputs, n_hidden)\n        self.fc2 = torch.nn.Linear(n_hidden, n_outputs)\n        \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n    \nn_inputs = env.observation_space.shape[0]\nn_outputs = env.action_space.n\n\nq_network = QNetwork(n_inputs, n_outputs)\n\n\nq_network\n\nQNetwork(\n  (fc1): Linear(in_features=2, out_features=128, bias=True)\n  (fc2): Linear(in_features=128, out_features=3, bias=True)\n)\n\n\n\nwith torch.no_grad():\n    q_values = q_network(torch.tensor([0.1, 0.2], dtype=torch.float32))\n    print(q_values)\n\ntensor([-0.0065,  0.1707,  0.0062])\n\n\n\n# Train the Q-network\n\n# Hyperparameters\nalpha = 0.2 # learning rate\ngamma = 0.99 # discount factor\n\n# Exploration settings\nepsilon = 0.2 # exploration rate\neps_mult = 0.99\n\n# Number of episodes\nn_episodes = 2000\n\n# Number of steps per episode\nn_steps = 300\n\n\n\nenv = gym.make('MountainCar-v0', render_mode=None)\n\nobs, _ = env.reset(seed=42)\n\nq_network = QNetwork(n_inputs, n_outputs)\n\npos, vel = obs\npos_idx = discretize_pos(pos)\nvel_idx = discretize_vel(vel)\nprint(observation)\n\nwith torch.no_grad():\n    q_values = q_network(torch.tensor([pos, vel], dtype=torch.float32))\n    print(q_values)\n\n    action = torch.argmax(q_values).item()\n    print(action)\n\n    next_observation, reward, terminated, truncated, info = env.step(action)\n    print(next_observation)\n\n    next_pos, next_vel = next_observation\n\n    next_pos_idx = discretize_pos(next_pos)\n    next_vel_idx = discretize_vel(next_vel)\n\n    next_q_values = q_network(torch.tensor([next_pos, next_vel], dtype=torch.float32))\n    print(next_q_values)\n\n    # Compute the target\n    target = reward + gamma * torch.max(next_q_values)\n    print(target)\n\n# Compute the loss\ncriterion = torch.nn.MSELoss()\nloss = criterion(q_values[action], target)\nprint(loss)\n\n\n\n[-0.68420756 -0.01730118]\ntensor([ 0.1212, -0.1776,  0.1786])\n2\n[-4.4479132e-01  4.1747934e-04]\ntensor([ 0.1211, -0.1776,  0.1787])\ntensor(-0.8231)\ntensor(1.0035)\n\n\n\n\noptimizer = torch.optim.Adam(q_network.parameters(), lr=alpha)\n\nfor episode in range(n_episodes):\n    epsilon = epsilon * eps_mult\n    if episode % 100 == 0:\n        print(f\"Episode {episode}\")\n    observation, _ = env.reset(seed=1)\n    cumulative_reward = 0\n    for step in range(n_steps):\n        # discretize the observation\n        pos, vel = observation\n        pos_idx = discretize_pos(pos)\n        vel_idx = discretize_vel(vel)\n        \n        # select the action\n        if np.random.rand() &lt; epsilon:\n            action = env.action_space.sample()\n        else:\n            q_values = q_network(torch.tensor([pos, vel], dtype=torch.float32))\n            action = torch.argmax(q_values).item()\n        \n        # take the action\n        next_observation, reward, terminated, truncated, info = env.step(action)\n        \n        cumulative_reward += reward\n        # discretize the next observation\n        next_pos, next_vel = next_observation\n        next_pos_idx = discretize_pos(next_pos)\n        next_vel_idx = discretize_vel(next_vel)\n        \n        # Compute Q-values for the next state\n        with torch.no_grad():\n            next_q_values = q_network(torch.tensor([next_pos, next_vel], dtype=torch.float32))\n            max_next_q_value = torch.max(next_q_values).item()\n        \n        target_q_value = reward + gamma * max_next_q_value\n        \n        # Compute loss and update Q-network\n        q_value = q_network(torch.tensor([pos, vel], dtype=torch.float32))[action]\n        criterion = torch.nn.MSELoss()\n        loss = criterion(q_value, torch.tensor(target_q_value, dtype=torch.float32))\n        print(loss.item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        observation = next_observation\n        # rewards update\n        rewards[episode] = cumulative_reward\n\n        \n        if terminated:\n            print(f\"Episode {episode} terminated after {step} steps\")\n            break\n\nEpisode 0\n1.0048069953918457\n1.0014567375183105\n0.9958764910697937\n0.9182038307189941\n0.8115965723991394\n0.63067626953125\n0.4421957731246948\n0.222701296210289\n0.1335132122039795\n0.03360588848590851\n0.0016043486539274454\n0.06032954156398773\n0.10956689715385437\n0.08843399584293365\n0.036764971911907196\n0.003027551807463169\n0.006071371491998434\n0.03167363628745079\n0.05209045484662056\n0.05149663984775543\n26067.0234375\n0.020529165863990784\n0.019284943118691444\n0.01451992616057396\n0.008877977728843689\n0.004237228073179722\n0.001369189703837037\n3689.034912109375\n200.40049743652344\n0.0010999157093465328\n2.06069671548903e-05\n0.002101067453622818\n0.008885168470442295\n0.019762517884373665\n0.03410563990473747\n0.051299456506967545\n0.07075944542884827\n0.09196017682552338\n0.11443059146404266\n0.2135302722454071\n1061.5845947265625\n0.35540324449539185\n0.4385024905204773\n2.2119390964508057\n0.5501630306243896\n0.6136103272438049\n62.149166107177734\n0.7008147239685059\n0.7459989786148071\n0.7844885587692261\n0.8161461353302002\n0.8079355359077454\n11.735383033752441\n0.8425405621528625\n0.8601585030555725\n0.8744558691978455\n0.8858668804168701\n0.89484041929245\n0.9017441868782043\n0.9068735837936401\n0.9104544520378113\n0.9126688838005066\n0.9136549234390259\n22.528955459594727\n21.10693359375\n0.9221128225326538\n0.9201486110687256\n0.9172738194465637\n0.9135172963142395\n28.91239356994629\n0.9009637236595154\n0.8957247138023376\n0.8897145986557007\n0.8829682469367981\n0.8755138516426086\n0.867385745048523\n0.8586211204528809\n0.8492619395256042\n0.8393563032150269\n0.8289539217948914\n0.818106472492218\n0.8420364260673523\n0.8452581763267517\n0.8479573130607605\n0.8501735925674438\n0.8519507646560669\n0.8533157110214233\n0.8542920351028442\n0.8549039363861084\n0.8551614284515381\n0.8550767302513123\n0.8546534776687622\n0.8538954257965088\n0.8527960181236267\n0.5181078910827637\n0.8351424336433411\n0.8339384198188782\n0.8324307203292847\n0.8306166529655457\n0.8284920454025269\n0.8260563611984253\n0.8233006000518799\n0.8202125430107117\n0.8167924284934998\n0.8130285143852234\n0.00946256797760725\n0.7074952721595764\n0.6875628232955933\n0.6680741310119629\n0.6491575837135315\n0.010133844800293446\n0.6226768493652344\n0.6061031818389893\n0.5905121564865112\n0.013254135847091675\n0.5808330774307251\n0.5682294368743896\n0.5568295121192932\n0.5467037558555603\n0.5378998517990112\n0.13113203644752502\n0.5339943170547485\n0.5397647619247437\n0.543150007724762\n0.5417367815971375\n0.541486918926239\n0.5282825231552124\n0.1626611202955246\n0.5473513007164001\n0.5545318722724915\n0.6287177801132202\n0.6405863761901855\n0.6424058079719543\n0.6359159350395203\n0.578769326210022\n0.5639973282814026\n0.20727494359016418\n0.5249177813529968\n0.5160287022590637\n0.49465301632881165\n1.6578389406204224\n0.566173791885376\n0.6446887254714966\n3.447338104248047\n0.7390631437301636\n0.7837555408477783\n0.8029047250747681\n34.42030715942383\n3.5606000423431396\n0.9760288596153259\n1.0388840436935425\n3.7759339809417725\n1.072930932044983\n1.1045341491699219\n1.1177312135696411\n1.1973824501037598\n1.2144906520843506\n1.184635043144226\n1.2342448234558105\n1.287087082862854\n1.3653899431228638\n1.4400018453598022\n1.3697348833084106\n1.2765662670135498\n1.1720798015594482\n1.0941269397735596\n1.0166243314743042\n0.924038827419281\n0.8152710199356079\n0.7463482618331909\n0.07717791944742203\n0.4869456887245178\n0.30334317684173584\n0.14025332033634186\n0.0306768249720335\n0.00020354636944830418\n0.019882824271917343\n0.08833419531583786\n0.2003059685230255\n0.3414388597011566\n0.48998504877090454\n0.621380090713501\n0.7141947150230408\n0.7549545764923096\n0.7403754591941833\n0.6765599846839905\n0.5763630867004395\n0.456144243478775\n0.33228179812431335\n0.2186906635761261\n0.1252431571483612\n0.05737763270735741\n0.016427552327513695\n0.0004951127339154482\n0.005796344485133886\n0.027137253433465958\n0.05929398909211159\n0.09754183888435364\n0.13798674941062927\n0.17771950364112854\n0.21481546759605408\n0.24822930991649628\n2940.632568359375\n0.35349100828170776\n0.4081454277038574\n0.45835229754447937\n0.5049440860748291\n0.5489243865013123\n0.5912716388702393\n0.6329079270362854\n0.6746003031730652\n0.7169989347457886\n0.7606263756752014\n0.805864155292511\n0.851112961769104\n0.8998361825942993\n0.9507410526275635\n1.0035966634750366\n1.0581567287445068\n1.1420928239822388\n1.2452329397201538\n1.3473215103149414\n1.4442713260650635\n1.5303895473480225\n1.60977303981781\n1.6817858219146729\n1.7456586360931396\n1.8006134033203125\n1.845811128616333\n1.8804264068603516\n144.0146942138672\n1.844096064567566\n1.8292278051376343\n1.809690237045288\n1.776503086090088\n1.7347337007522583\n1.684993028640747\n1.1734265089035034\n1.0867496728897095\n0.9949100613594055\n0.9060201048851013\n0.8257963061332703\n0.7453269958496094\n0.6609809398651123\n0.5673570036888123\n0.7136016488075256\n0.6398889422416687\n691.3258056640625\n0.39360135793685913\n0.07814531028270721\n0.08164729177951813\n0.09932269155979156\n0.12702062726020813\n0.1532098948955536\n3128.6982421875\n0.2630188465118408\n0.3107008635997772\n0.39648208022117615\n0.45336729288101196\n0.49007850885391235\n2533.0625\n0.6128858923912048\n0.6627312898635864\n0.7118167281150818\n0.639194130897522\n0.6848539113998413\n2.527656316757202\n38.997779846191406\n0.3087991178035736\n0.6807946562767029\n0.7545204162597656\n0.6552304029464722\n0.8755919337272644\n0.9300628900527954\n0.982782244682312\n0.9687613248825073\n1.0691131353378296\n1.0780832767486572\n1.0766698122024536\n1.0969276428222656\n1.0774726867675781\n1.0751389265060425\n1.0110520124435425\n0.8765778541564941\n0.6194000840187073\n0.4812657833099365\n0.33369502425193787\n0.2790739834308624\n12.684856414794922\n0.2908516526222229\n0.4264437258243561\n0.5511820912361145\n0.6731754541397095\n0.8049842715263367\n545.3660278320312\n1.030138611793518\n1.1640650033950806\n1.3088256120681763\n1.350746512413025\n492.9010009765625\n0.38391634821891785\n0.44368860125541687\n0.49779972434043884\n0.5457663536071777\n0.609416127204895\n114.84229278564453\n0.7250807285308838\n0.7669440507888794\n0.7813073396682739\n0.8024501800537109\n0.8205753564834595\n0.5882841944694519\n0.7402154803276062\n0.778099775314331\n0.8091918230056763\n0.8350064754486084\n0.8567950129508972\n0.8755664825439453\n0.8921247720718384\n102.75447845458984\n0.9190875291824341\n0.9309282898902893\n0.9394422769546509\n0.9457013607025146\n0.9922849535942078\n1.0409846305847168\n1.0406723022460938\n1.0403172969818115\n1.039923906326294\n1.0394954681396484\n1.039035677909851\n1.038547158241272\n1.038033127784729\n1.0374953746795654\n0.7967777252197266\n1.036433458328247\n1.035905361175537\n1.0353550910949707\n0.6108056902885437\n0.5676560401916504\n1.0338069200515747\n1.0333144664764404\n1.0327955484390259\n1.0322531461715698\n1.0316892862319946\n1.0311057567596436\n1.0305049419403076\n1.0298881530761719\n1.0292569398880005\n1.028612732887268\n1.0279566049575806\n1.0272895097732544\n1.0266129970550537\n0.5162349939346313\n0.5566445589065552\n1.0247529745101929\n1.0241752862930298\n1.0235786437988281\n1.0229648351669312\n1.0223356485366821\n1.0216920375823975\n1.02103590965271\n1.020368218421936\n1.0196901559829712\n1.0190317630767822\n1.0188626050949097\n0.9435514211654663\n1.0183418989181519\n1.0179976224899292\n1.0175658464431763\n1.0170546770095825\n1.0164718627929688\n1.0158246755599976\n1.0151193141937256\n1.0145231485366821\n1.0141942501068115\n0.08548671007156372\n1.0134834051132202\n1.0131018161773682\n1.012679934501648\n1.0122216939926147\n1.0117310285568237\n1.0112104415893555\n1.0106631517410278\n1.0100916624069214\n1.0096499919891357\n1.0093084573745728\n1.0088764429092407\n1.0083630084991455\n1.0077757835388184\n1.0073035955429077\n0.9699271321296692\n1.00654935836792\n1.0061479806900024\n1.0057072639465332\n1.0052307844161987\n1.004722237586975\n1.004184603691101\n1.0036205053329468\n0.6694735884666443\n1.0025171041488647\n0.7368310689926147\n1.0017911195755005\n1.0013549327850342\n1.000859022140503\n1.0004642009735107\n1.000028371810913\n0.9995657801628113\n0.9991292357444763\n0.998816967010498\n0.9987285733222961\n0.9985952377319336\n0.9984219074249268\n0.9982122778892517\n0.9979698061943054\n0.9976977109909058\n0.9070425629615784\n0.9971296191215515\n0.9968329668045044\n0.9965119957923889\n0.9961689710617065\n0.9959237575531006\n0.9750918745994568\n0.9954706430435181\n0.9951478838920593\n0.9920316338539124\n0.9525390267372131\n0.9945463538169861\n0.9943752884864807\n0.9941386580467224\n0.9938428401947021\n0.993493914604187\n0.8282284140586853\n0.9930750131607056\n0.9442638158798218\n0.9927597641944885\n0.9084575176239014\n0.9923993945121765\n0.992190420627594\n0.9919467568397522\n0.9916722774505615\n0.9913696050643921\n0.8820803761482239\n0.9907458424568176\n0.9904240369796753\n0.9900786280632019\n0.9897118806838989\n0.8632120490074158\n0.9889776706695557\n0.9886084198951721\n0.9883031249046326\n0.9084314703941345\n0.9879090189933777\n0.9876559376716614\n0.9873433113098145\n0.9869768619537354\n0.9865755438804626\n0.9863318800926208\n0.9860562682151794\n0.9848976135253906\n0.9854913353919983\n0.9852293729782104\n0.9849498867988586\n0.9846416115760803\n0.9844247698783875\n0.9841768145561218\n0.983867347240448\n0.9836011528968811\n0.9832914471626282\n0.9830702543258667\n0.9828343987464905\n0.9825645685195923\n0.9822645783424377\n0.9452266693115234\n0.9817841649055481\n0.9815391898155212\n0.9812318682670593\n0.9809204339981079\n0.9806684851646423\n0.9665892124176025\n0.9801275134086609\n0.9798389673233032\n0.9795213341712952\n0.9791773557662964\n0.9789683222770691\n0.9787095785140991\n0.9784020185470581\n0.9781445264816284\n0.9778248071670532\n0.9774497151374817\n0.9771947264671326\n0.9769658446311951\n0.9766477346420288\n0.9763556122779846\n0.9760599732398987\n0.9758034944534302\n0.9755100607872009\n0.9752484560012817\n0.9749779105186462\n0.9746754765510559\n0.9743967652320862\n0.9741042852401733\n0.973822832107544\n0.973534882068634\n0.966282308101654\n0.9730080366134644\n0.9716938138008118\n0.972536563873291\n0.9722170233726501\n0.9718698263168335\n0.9715800881385803\n0.9713051319122314\n0.9710422158241272\n0.9707152247428894\n0.9704770445823669\n0.9702064990997314\n0.9699028730392456\n0.9695793390274048\n0.9692699313163757\n0.9689901471138\n0.968704342842102\n0.9684288501739502\n0.9681327939033508\n0.9678555130958557\n0.9675454497337341\n0.9672614932060242\n0.9669473171234131\n0.966737687587738\n0.9664226770401001\n0.9443001747131348\n0.965951144695282\n0.9657093286514282\n0.9654305577278137\n0.9651192426681519\n0.9647777676582336\n0.9644098877906799\n0.9591569304466248\n0.9638555645942688\n0.963606059551239\n0.963289201259613\n0.9629113078117371\n0.962628960609436\n0.9623576402664185\n0.9621075987815857\n0.9618213772773743\n0.9615022540092468\n0.9612294435501099\n0.9608811736106873\n0.9471459984779358\n0.9603005051612854\n0.9600865244865417\n0.9504370093345642\n0.9596202969551086\n0.9593283534049988\n0.9589720368385315\n0.9424108266830444\n0.9585256576538086\n0.9582871198654175\n0.9580103754997253\n0.9576990604400635\n0.9573565721511841\n0.9569861888885498\n0.9565901756286621\n0.956171452999115\n0.9558399319648743\n0.9555863738059998\n0.9553514122962952\n0.9105222821235657\n0.9224833250045776\n0.9546953439712524\n0.9544242024421692\n0.8918054699897766\n0.9537799954414368\n0.9534103274345398\n0.952982485294342\n0.938325047492981\n0.9522569179534912\n0.9194425344467163\n0.9518088698387146\n0.9515544176101685\n0.9512622952461243\n0.9509363174438477\n0.9505792260169983\n0.9501948356628418\n0.9498457908630371\n0.9142169952392578\n0.9493430256843567\n0.9049842357635498\n0.9487530589103699\n0.8878368735313416\n0.9480922222137451\n0.9478541016578674\n0.9476127028465271\n0.9473319053649902\n0.9470149278640747\n0.9466655254364014\n0.9463171362876892\n0.9387491345405579\n0.8532584309577942\n0.9455074071884155\n0.9451926350593567\n0.944812536239624\n0.9444250464439392\n0.9441410303115845\n0.9239320158958435\n0.9435322880744934\n0.9432080984115601\n0.9428515434265137\n0.9426149725914001\n0.9423437118530273\n0.9419461488723755\n0.9415250420570374\n0.9412386417388916\n0.940960168838501\n0.9406614303588867\n0.9403640627861023\n0.9400306940078735\n0.8914704918861389\n0.9395305514335632\n0.9209451079368591\n0.9389519691467285\n0.9386063814163208\n0.938197135925293\n0.9379034042358398\n0.9376198649406433\n0.937299907207489\n0.9369463324546814\n0.936680018901825\n0.9363856315612793\n0.9359639286994934\n0.9261884689331055\n0.9211606979370117\n0.9351443648338318\n0.9348732233047485\n0.9345634579658508\n0.9342195987701416\n0.9338435530662537\n0.9334956407546997\n0.933199942111969\n0.9328328371047974\n0.9324501752853394\n0.9321181774139404\n0.9319593906402588\n0.9316643476486206\n0.9142106175422668\n0.9308613538742065\n0.9306198358535767\n0.9303369522094727\n0.930015504360199\n0.9296600818634033\n0.9294099807739258\n0.9291213154792786\n0.9139443635940552\n0.9284347295761108\n0.928041934967041\n0.9276657700538635\n0.9274187088012695\n0.9224475622177124\n0.9268163442611694\n0.9264326095581055\n0.9261003136634827\n0.925733745098114\n0.925446093082428\n0.9251484274864197\n0.9248640537261963\n0.924483060836792\n0.9241451621055603\n0.9238706231117249\n0.9235919713973999\n0.9232730269432068\n0.9229189157485962\n0.9226220846176147\n0.9223005771636963\n0.9180859923362732\n0.9215561151504517\n0.9212055206298828\n0.9210206270217896\n0.9207003116607666\n0.9202529191970825\n0.9199281930923462\n0.9195861220359802\n0.9088627099990845\n0.9189661741256714\n0.9186891913414001\n0.9183838963508606\n0.9180421233177185\n0.9177021980285645\n0.917357861995697\n0.9170207977294922\n0.9045619368553162\n0.9163679480552673\n0.916022002696991\n0.9156651496887207\n0.9153229594230652\n0.9071043133735657\n0.9147217869758606\n0.9144609570503235\n0.9017966985702515\n0.9139019846916199\n0.8351354598999023\n0.9133185744285583\n0.9130005240440369\n0.8049620389938354\n0.9123263359069824\n0.9119911789894104\n0.9116988182067871\n0.9113582968711853\n0.9110287427902222\n0.9107019901275635\n0.9103343486785889\n0.9099940657615662\n0.9096384048461914\n0.9092727899551392\n0.9088917970657349\n0.908515453338623\n0.9081300497055054\n0.9077693223953247\n0.9076130390167236\n0.9073459506034851\n0.9069408178329468\n0.9064341187477112\n0.9061117768287659\n0.9058167934417725\n0.9054846167564392\n0.9052051305770874\n0.9048558473587036\n0.9044821262359619\n0.9041429162025452\n0.9037675857543945\n0.9034810662269592\n0.903134822845459\n0.8996073007583618\n0.9025277495384216\n0.9021046757698059\n0.9018519520759583\n0.852358341217041\n0.9013041257858276\n0.9010008573532104\n0.9006568789482117\n0.9003591537475586\n0.9000695943832397\n0.8997032046318054\n0.899349570274353\n0.8990294337272644\n0.8986704349517822\n0.898277223110199\n0.8979130387306213\n0.8975479602813721\n0.8971242904663086\n0.89674311876297\n0.8963683843612671\n0.8789527416229248\n0.8960027098655701\n0.8957328200340271\n0.8953213095664978\n0.8948007225990295\n0.8945093750953674\n0.863106369972229\n0.8464685678482056\n0.8936039805412292\n0.8932893872261047\n0.8929352164268494\n0.892544150352478\n0.8921216130256653\n0.8918054699897766\n0.8915173411369324\n0.8911499381065369\n0.8907124996185303\n0.884473443031311\n0.8900007009506226\n0.8896875977516174\n0.8689834475517273\n0.8894124031066895\n0.8891264200210571\n0.8886992931365967\n0.8881446719169617\n0.887741208076477\n0.8874393105506897\n0.8870961666107178\n0.8867144584655762\n0.8862996101379395\n0.7869611978530884\n0.8858830332756042\n0.8856452107429504\n0.8853230476379395\n0.8849237561225891\n0.8844555020332336\n0.8839694857597351\n0.8836835026741028\n0.883353590965271\n0.8829843401908875\n0.8825793266296387\n0.882142186164856\n0.8816863298416138\n0.8813228011131287\n0.8810040950775146\n0.8807132244110107\n0.880280077457428\n0.8799570798873901\n0.8796368837356567\n0.8792746663093567\n0.8790412545204163\n0.878722071647644\n0.8783252239227295\n0.8779042959213257\n0.8775701522827148\n0.8771958351135254\n0.8699900507926941\n0.8648961186408997\n0.8762359023094177\n0.8641902804374695\n0.8756030797958374\n0.8726410865783691\n0.872471809387207\n0.8744701147079468\n0.8741125464439392\n0.8737185001373291\n0.8733298778533936\n0.8730313181877136\n0.8726517558097839\n0.8722429275512695\n0.8718875646591187\n0.8714993596076965\n0.8711405992507935\n0.8685505390167236\n0.8705300688743591\n0.8701786398887634\n0.8697908520698547\n0.8354065418243408\n0.8691612482070923\n0.8688180446624756\n0.8684367537498474\n0.8681719303131104\n0.7975991368293762\n0.782911479473114\n0.8672524690628052\n0.8668928146362305\n0.8664764761924744\n0.8661800026893616\n0.8658391833305359\n0.8654576539993286\n0.7248427867889404\n0.8459860682487488\n0.8646230101585388\n0.8643028736114502\n0.8289434909820557\n0.8635414838790894\n0.863106369972229\n0.862600564956665\n0.8622639775276184\n0.8204664587974548\n0.8616566061973572\n0.8613237738609314\n0.8609485626220703\n0.8605371117591858\n0.8602655529975891\n0.859989583492279\n0.8595625162124634\n0.8590028882026672\n0.8587103486061096\n0.8583728075027466\n0.8579920530319214\n0.768606960773468\n0.8572748899459839\n0.8570170402526855\n0.856671929359436\n0.8562853336334229\n0.8559641242027283\n0.8555997610092163\n0.855213463306427\n0.8548351526260376\n0.8544692397117615\n0.8540708422660828\n0.8536707758903503\n0.8533033728599548\n0.8528761267662048\n0.85248863697052\n0.8520951271057129\n0.851664662361145\n0.8515256643295288\n0.8512440323829651\n0.8508138656616211\n0.8502746820449829\n0.8499221205711365\n0.8275912404060364\n0.849251389503479\n0.8489543795585632\n0.8486108183860779\n0.8482269644737244\n0.8478993773460388\n0.8475560545921326\n0.8471338152885437\n0.8467748165130615\n0.8464220762252808\n0.8460281491279602\n0.8455983996391296\n0.8451354503631592\n0.8448706865310669\n0.844546377658844\n0.8442431688308716\n0.8006793260574341\n0.8435931205749512\n0.8432217240333557\n0.8427734375\n0.8424110412597656\n0.8242215514183044\n0.8415744304656982\n0.841285765171051\n0.8409498929977417\n0.8405895829200745\n0.8317590951919556\n0.839925229549408\n0.8395721316337585\n0.7803820371627808\n0.808096706867218\n0.7361636757850647\n0.8382182717323303\n0.8378829956054688\n0.8375042080879211\n0.8259384632110596\n0.8370102643966675\n0.8367241024971008\n0.8363507986068726\n0.8359008431434631\n0.8353794813156128\n0.8349698781967163\n0.8346527218818665\n0.8342902660369873\n0.8338861465454102\n0.833447277545929\n0.8329737186431885\n0.8324724435806274\n0.8320426940917969\n0.8318930864334106\n0.8316130042076111\n0.8311799764633179\n0.8140349388122559\n0.8305332064628601\n0.8301994800567627\n0.8297824859619141\n0.8227658867835999\n0.8195631504058838\n0.8286882042884827\n0.828382670879364\n0.8280285596847534\n0.8276346325874329\n0.8272008895874023\n0.8267343044281006\n0.8262366652488708\n0.8259193897247314\n0.825581431388855\n0.8251603245735168\n0.82492995262146\n0.8246181011199951\n0.8241540193557739\n0.8235532641410828\n0.8232399821281433\n0.8229354619979858\n0.8225842714309692\n0.8223351836204529\n0.8220031261444092\n0.8215880990028381\n0.8212268352508545\n0.7977516055107117\n0.8205874562263489\n0.8202367424964905\n0.8198429346084595\n0.8194112181663513\n0.8189433813095093\n0.8184446096420288\n0.8179787993431091\n0.8176459074020386\n0.8173268437385559\n0.781369686126709\n0.8167286515235901\n0.8163735866546631\n0.771996796131134\n0.8155413866043091\n0.8151177167892456\n0.814682126045227\n0.8142603635787964\n0.8139162063598633\n0.8136099576950073\n0.8132572770118713\n0.8128600120544434\n0.8124249577522278\n0.8120914697647095\n0.8117271661758423\n0.8112804293632507\n0.8109265565872192\n0.8105384111404419\n0.8102225065231323\n0.7737104892730713\n0.8095976710319519\n0.809251070022583\n0.7123454213142395\n0.8085116744041443\n0.808118999004364\n0.8078944087028503\n0.8075944185256958\n0.8072053790092468\n0.8067376017570496\n0.8063418865203857\n0.806007981300354\n0.8056278824806213\n0.8052050471305847\n0.8047481775283813\n0.8042571544647217\n0.7365515828132629\n0.8036586046218872\n0.8033080697059631\n0.7483863234519958\n0.7670692801475525\n0.8021272420883179\n0.8016899824142456\n0.8011794686317444\n0.8007953763008118\n0.8004626035690308\n0.8000974655151367\n0.7997443079948425\n0.7993588447570801\n0.7990400195121765\n0.7575919032096863\n0.7982031106948853\n0.7978129386901855\n0.7974177002906799\n0.7969936728477478\n0.7965663075447083\n0.7963041663169861\n0.7959757447242737\n0.7955605983734131\n0.7950894236564636\n0.7946608662605286\n0.7943072319030762\n0.7939078211784363\n0.7935373783111572\n0.7932400703430176\n0.7927865386009216\n0.7924231886863708\n0.7921124696731567\n0.7917746901512146\n0.791389524936676\n0.7909772396087646\n0.7905820608139038\n0.790234386920929\n0.7448248267173767\n0.7894936203956604\n0.7890971302986145\n0.7886600494384766\n0.7883501052856445\n0.7879775762557983\n0.7875204682350159\n0.7870923280715942\n0.7657501697540283\n0.786444365978241\n0.7861128449440002\n0.7857341170310974\n0.7711306214332581\n0.7852218747138977\n0.7848213911056519\n0.784272313117981\n0.7837386131286621\n0.7833570837974548\n0.7829350829124451\n0.7826229333877563\n0.7822213768959045\n0.7011484503746033\n0.7816243171691895\n0.7812938094139099\n0.7809162139892578\n0.65704345703125\n0.7801175117492676\n0.6368334293365479\n0.7794740796089172\n0.7791440486907959\n0.7787283062934875\n0.7622924447059631\n0.778144359588623\n0.7778398394584656\n0.7774849534034729\n0.7770863771438599\n0.7766459584236145\n0.7761686444282532\n0.7756595611572266\n0.6576434373855591\n0.7747611403465271\n0.7744203209877014\n0.7739923596382141\n0.7734857201576233\n0.7731301188468933\n0.7727628946304321\n0.7723521590232849\n0.7292933464050293\n0.7717840075492859\n0.7714606523513794\n0.7709799408912659\n0.7705026865005493\n0.7701544761657715\n0.7697611451148987\n0.7693261504173279\n0.7688527703285217\n0.7684531211853027\n0.7681388258934021\n0.7677342891693115\n0.7672480344772339\n0.7669172883033752\n0.7665097713470459\n0.7662092447280884\n0.7658853530883789\n0.7655131816864014\n0.7650977373123169\n0.7646422982215881\n0.7641520500183105\n0.7638219594955444\n0.7634819149971008\n0.7630903124809265\n0.7626821994781494\n0.7623257637023926\n0.7618927955627441\n0.7614200711250305\n0.7610872387886047\n0.7607079148292542\n0.7602854371070862\n0.7549065351486206\n0.7594906687736511\n0.7590967416763306\n0.7586946487426758\n0.7583109140396118\n0.7578991055488586\n0.7532038688659668\n0.757150411605835\n0.7459495067596436\n0.7564037442207336\n0.7560023069381714\n0.7556706666946411\n0.7552910447120667\n0.7548269629478455\n0.7544839978218079\n0.7541112899780273\n0.7536956071853638\n0.7534207701683044\n0.7530251145362854\n0.7524773478507996\n0.7521283030509949\n0.7517479062080383\n0.7514039278030396\n0.7510716319084167\n0.7506617903709412\n0.7502899765968323\n0.7499051094055176\n0.7495021224021912\n0.749056339263916\n0.7487229704856873\n0.748302161693573\n0.7477989792823792\n0.7473949790000916\n0.7471888661384583\n0.7468393445014954\n0.7463334202766418\n0.712689995765686\n0.7457617521286011\n0.7454438805580139\n0.7450750470161438\n0.7446618676185608\n0.7442092895507812\n0.7438473701477051\n0.7435430884361267\n0.7431467175483704\n0.7426667213439941\n0.7421999573707581\n0.7418286204338074\n0.7414114475250244\n0.721215546131134\n0.7405428886413574\n0.7400915622711182\n0.7396010756492615\n0.7392550110816956\n0.7220548987388611\n0.7386434078216553\n0.7381910681724548\n0.7376536130905151\n0.7372326850891113\n0.7368741035461426\n0.6954345107078552\n0.6680008769035339\n0.7359722256660461\n0.7356466054916382\n0.7352720499038696\n0.7348533868789673\n0.7314026355743408\n0.7340230345726013\n0.7336063981056213\n0.7332062125205994\n0.7328126430511475\n0.7323930859565735\n0.731931209564209\n0.7315836548805237\n0.7311497926712036\n0.7306361198425293\n0.7302204370498657\n0.729811429977417\n0.7296419739723206\n0.6923156976699829\n0.729049026966095\n0.7286028861999512\n0.7280087471008301\n0.7274230122566223\n0.7270488739013672\n0.7268130779266357\n0.7264846563339233\n0.7260652780532837\n0.7256038188934326\n0.6251822710037231\n0.7249215841293335\n0.7245497107505798\n0.7241309285163879\n0.7237122058868408\n0.723321259021759\n0.7228865623474121\n0.7224681973457336\n0.7220305800437927\n0.7215979099273682\n0.7211443185806274\n0.7106786370277405\n0.7204091548919678\n0.7202585935592651\n0.7199299931526184\n0.7194413542747498\n0.7189189195632935\n0.7185097932815552\n0.7180151343345642\n0.7176628708839417\n0.7173542976379395\n0.7169941067695618\n0.7165839076042175\n0.7161367535591125\n0.7156493663787842\n0.7153041362762451\n0.7148879766464233\n0.7145397067070007\n0.7141931056976318\n0.7137595415115356\n0.7132423520088196\n0.7129234671592712\n0.7125917673110962\n0.7122102379798889\n0.7117820978164673\n0.7114008069038391\n0.7109857797622681\n0.7105146646499634\n0.7100918889045715\n0.7096371054649353\n0.7092852592468262\n0.6909071207046509\n0.7084935307502747\n0.7081050872802734\n0.7077632546424866\n0.7073476910591125\n0.7069339156150818\n0.7065362334251404\n0.7060922384262085\n0.7056531310081482\n0.7053055167198181\n0.6952245831489563\n0.7045112252235413\n0.7041718363761902\n0.7037397623062134\n0.6960676908493042\n0.7030166983604431\n0.7026777267456055\n0.7022876739501953\n0.7018880844116211\n0.7014663219451904\n0.7010605931282043\n0.7006422877311707\n0.7002080678939819\n0.6997325420379639\n0.6994134783744812\n0.6672418713569641\n0.6987883448600769\n0.6983961462974548\n0.6979403495788574\n0.6955681443214417\n0.6971215605735779\n0.6906360387802124\n0.6869730353355408\n0.6959435939788818\n0.695647656917572\n0.6952945590019226\n0.6948969960212708\n0.6013311743736267\n0.6940577030181885\n0.6936160326004028\n0.693212628364563\n0.6928791999816895\n0.6924537420272827\n0.6919618248939514\n0.6915620565414429\n0.6911180019378662\n0.6907057762145996\n0.6902461647987366\n0.6898120641708374\n0.6894952654838562\n0.6891404986381531\n0.688633918762207\n0.6816526055335999\n0.687874436378479\n0.6874663233757019\n0.6871785521507263\n0.6657501459121704\n0.6864988207817078\n0.6860848665237427\n0.6855856776237488\n0.6850709319114685\n0.6846920847892761\n0.6842691898345947\n0.6839820742607117\n0.683597207069397\n0.6830517053604126\n0.682673454284668\n0.682317316532135\n0.6819424033164978\n0.6816179752349854\n0.5846667885780334\n0.6809221506118774\n0.6805412769317627\n0.6801133751869202\n0.6663011908531189\n0.6795377731323242\n0.6791919469833374\n0.678754985332489\n0.6782365441322327\n0.6776428818702698\n0.6772881150245667\n0.6769365668296814\n0.67653489112854\n0.6760925054550171\n0.6757318377494812\n0.6529697179794312\n0.6751048564910889\n0.6385104060173035\n0.6742243766784668\n0.6736544370651245\n0.6731222867965698\n0.6727905869483948\n0.6723682284355164\n0.6720085740089417\n0.671639621257782\n0.6712363958358765\n0.670823872089386\n0.6704583764076233\n0.6700149178504944\n0.6695902943611145\n0.6691752076148987\n0.6687166094779968\n0.6683641076087952\n0.6468376517295837\n0.6676470041275024\n0.6672793030738831\n0.6669023036956787\n0.6664382219314575\n0.6659275889396667\n0.6555029153823853\n0.6652460098266602\n0.664928674697876\n0.6645616888999939\n0.6641450524330139\n0.6636881232261658\n0.6633400917053223\n0.662979781627655\n0.662529468536377\n0.6619986295700073\n0.6616603136062622\n0.6612818241119385\n0.660860002040863\n0.6604321002960205\n0.6600663661956787\n0.6596015691757202\n0.6591771841049194\n0.6587157845497131\n0.6583040952682495\n0.6579450964927673\n0.6576078534126282\n0.6571346521377563\n0.6567265391349792\n0.656364917755127\n0.6560125946998596\n0.6556141376495361\n0.6551694273948669\n0.6548699140548706\n0.6544748544692993\n0.6539965867996216\n0.6345447301864624\n0.6531916856765747\n0.6528217792510986\n0.6524057388305664\n0.6519467234611511\n0.6515371203422546\n0.6510845422744751\n0.6492083072662354\n0.6503399014472961\n0.6481298804283142\n0.6249606013298035\n0.6493558287620544\n0.6490331292152405\n0.5614590644836426\n0.5917733311653137\n0.6480193734169006\n0.5150399208068848\n0.5665900707244873\n0.6470524072647095\n0.6467087864875793\n0.6463161706924438\n0.6458777189254761\n0.5489187240600586\n0.6449674963951111\n0.6444988250732422\n0.6439905762672424\n0.6434488296508789\n0.6428767442703247\n0.6425678730010986\n0.642295777797699\n0.6419258713722229\n0.6414675116539001\n0.6412323117256165\n0.6408657431602478\n0.6403436660766602\n0.6396753191947937\n0.639260470867157\n0.6389768719673157\n0.6386841535568237\n0.6383366584777832\n0.637940526008606\n0.6374987959861755\n0.6371790170669556\n0.6368167400360107\n0.6363662481307983\n0.6358460187911987\n0.6354445219039917\n0.6350006461143494\n0.6345356106758118\n0.6340829730033875\n0.6336334943771362\n0.6329261660575867\n0.6330323815345764\n0.6327046155929565\n0.6322131752967834\n0.6315763592720032\n0.6312580704689026\n0.6239025592803955\n0.513595461845398\n0.6305006146430969\n0.6301765441894531\n0.6297617554664612\n0.6292592883110046\n0.6289809346199036\n0.6286784410476685\n0.6283215880393982\n0.6279194951057434\n0.6274722218513489\n0.6269857883453369\n0.6050463914871216\n0.6114696860313416\n0.6255699396133423\n0.6251053810119629\n0.6246048212051392\n0.6241828203201294\n0.6237368583679199\n0.6232187747955322\n0.6229146718978882\n0.6224420666694641\n0.6220929622650146\n0.621722936630249\n0.6213079690933228\n0.6209982633590698\n0.6206225752830505\n0.6201568245887756\n0.6197303533554077\n0.5973738431930542\n0.6189978122711182\n0.6186017394065857\n0.6181907653808594\n0.6177979111671448\n0.6174081563949585\n0.6170035600662231\n0.6165571808815002\n0.6160690784454346\n0.6156648993492126\n0.6153207421302795\n0.6149467825889587\n0.6144921183586121\n0.6140795350074768\n0.6136192679405212\n0.6132577657699585\n0.6128485798835754\n0.6125320792198181\n0.6121231317520142\n0.6116307377815247\n0.611290693283081\n0.6109119653701782\n0.6106138229370117\n0.6101995706558228\n0.6097526550292969\n0.6093595623970032\n0.6089189052581787\n0.6084397435188293\n0.6081422567367554\n0.5748263001441956\n0.6074849963188171\n0.6070777177810669\n0.5472553372383118\n0.6064982414245605\n0.606106162071228\n0.6055569052696228\n0.6050671935081482\n0.6046488881111145\n0.604150652885437\n0.6037948727607727\n0.6034985184669495\n0.6031458973884583\n0.6027489900588989\n0.6023048162460327\n0.6018223166465759\n0.6013045907020569\n0.600875735282898\n0.5899435877799988\n0.5418954491615295\n0.513130784034729\n0.6000805497169495\n0.5997408032417297\n0.5992445945739746\n0.5986009836196899\n0.5980993509292603\n0.5371460914611816\n0.5226953029632568\n0.597161591053009\n0.5967842936515808\n0.596318781375885\n0.5957739353179932\n0.5953470468521118\n0.5950733423233032\n0.39269959926605225\n0.37558066844940186\n0.50579833984375\n0.5939350724220276\n0.3106115460395813\n0.5933619737625122\n0.5930328965187073\n0.5926539897918701\n0.5922312140464783\n0.591764509677887\n0.5912598967552185\n0.5907232165336609\n0.317525178194046\n0.5899670124053955\n0.5896916389465332\n0.589316725730896\n0.5888541340827942\n0.5883156657218933\n0.5877656936645508\n0.5874118804931641\n0.5870084762573242\n0.586561381816864\n0.5860735774040222\n0.5855538845062256\n0.4945993721485138\n0.5848272442817688\n0.5844393372535706\n0.5280510783195496\n0.5835385322570801\n0.5830286741256714\n0.5826704502105713\n0.5823240280151367\n0.5819252729415894\n0.5815324783325195\n0.5811282396316528\n0.5807299017906189\n0.5803259015083313\n0.5539354681968689\n0.5794747471809387\n0.5792221426963806\n0.5788940787315369\n0.5784733295440674\n0.5779685974121094\n0.5776438117027283\n0.5772959589958191\n0.5769018530845642\n0.5764615535736084\n0.5759808421134949\n0.5755177140235901\n0.5751386880874634\n0.5746729969978333\n0.5743000507354736\n0.5738722681999207\n0.5734822154045105\n0.5730518698692322\n0.5727111101150513\n0.5723358988761902\n0.5719001889228821\n0.571479082107544\n0.5711129307746887\n0.5707526206970215\n0.5703405737876892\n0.5699689984321594\n0.5695658922195435\n0.5654335021972656\n0.5421397686004639\n0.568489670753479\n0.568138837814331\n0.5578433275222778\n0.5676443576812744\n0.5672765374183655\n0.5667508840560913\n0.5662197470664978\n0.5658466219902039\n0.5654277801513672\n0.5649660229682922\n0.564470112323761\n0.5640059113502502\n0.5637051463127136\n0.5633071064949036\n0.5628262162208557\n0.5624113082885742\n0.5613504648208618\n0.5616134405136108\n0.5555976033210754\n0.5411094427108765\n0.5606675744056702\n0.5602220892906189\n0.5596511960029602\n0.5592859387397766\n0.5588324666023254\n0.5584504008293152\n0.5581055283546448\n0.5577123165130615\n0.5572764873504639\n0.5521596074104309\n0.5565904378890991\n0.556169331073761\n0.5558052062988281\n0.5554526448249817\n0.555012047290802\n0.554489254951477\n0.5541342496871948\n0.553804874420166\n0.5534273982048035\n0.5530046224594116\n0.5526586174964905\n0.5522446036338806\n0.5167468786239624\n0.5514086484909058\n0.5510206818580627\n0.5505903363227844\n0.5501177310943604\n0.5497358441352844\n0.549371063709259\n0.5490261316299438\n0.5485909581184387\n0.5481531023979187\n0.5477408170700073\n0.547387957572937\n0.5469872951507568\n0.5465951561927795\n0.5461750626564026\n0.5345101356506348\n0.545464813709259\n0.5450901985168457\n0.5447860360145569\n0.5443891286849976\n0.5439557433128357\n0.5435647368431091\n0.5431345701217651\n0.5426623225212097\n0.5421538352966309\n0.5419319272041321\n0.5416062474250793\n0.5412694215774536\n0.5408428907394409\n0.5404810905456543\n0.5400548577308655\n0.5395475625991821\n0.5172159075737\n0.5388501286506653\n0.5385589003562927\n0.5382118225097656\n0.47516414523124695\n0.5374648571014404\n0.5370650291442871\n0.5366206169128418\n0.5361372828483582\n0.5356206893920898\n0.5351796746253967\n0.5347918272018433\n0.5345603227615356\n0.534239649772644\n0.5338270664215088\n0.5333338379859924\n0.5327656865119934\n0.5324232578277588\n0.5320308804512024\n0.5317137241363525\n0.5312909483909607\n0.5308489799499512\n0.5304487943649292\n0.5301015973091125\n0.5297322273254395\n0.5292797684669495\n0.4977082312107086\n0.5286388993263245\n0.4689812958240509\n0.5279845595359802\n0.527624249458313\n0.4237302541732788\n0.5268514752388\n0.5264389514923096\n0.5260238647460938\n0.5256946682929993\n0.525274395942688\n0.5248818397521973\n0.3894766569137573\n0.5241608023643494\n0.5237714648246765\n0.5233380794525146\n0.5229324698448181\n0.5225463509559631\n0.5220721960067749\n0.52171391248703\n0.5213227272033691\n0.5208876729011536\n0.5204142332077026\n0.5199905037879944\n0.5196632146835327\n0.5192150473594666\n0.5188330411911011\n0.5155519843101501\n0.5132264494895935\n0.5176714062690735\n0.5067075490951538\n0.4735877215862274\n0.5167715549468994\n0.5164644718170166\n0.5161054134368896\n0.5156999230384827\n0.515253484249115\n0.5148701667785645\n0.5145527124404907\n0.5141478180885315\n0.36746829748153687\n0.5132728815078735\n0.5129067301750183\n0.5124943256378174\n0.5121448040008545\n0.5117353796958923\n0.5112661123275757\n0.5108543634414673\n0.5104018449783325\n0.5099958777427673\n0.5095464587211609\n0.5090999603271484\n0.5088822245597839\n0.5085666179656982\n0.5080960988998413\n0.5075606107711792\n0.5071910619735718\n0.5068895220756531\n0.455330491065979\n0.5061809420585632\n0.5057711601257324\n0.505418598651886\n0.5050308108329773\n0.40537190437316895\n0.5043397545814514\n0.5039659738540649\n0.503551721572876\n0.5031755566596985\n0.5027832388877869\n0.5023803114891052\n0.5019586086273193\n0.5015667676925659\n0.5011184215545654\n0.5007134675979614\n0.5002951622009277\n0.4893951117992401\n0.49980148673057556\n0.4994833171367645\n0.4645039439201355\n0.49858325719833374\n0.49801236391067505\n0.49758443236351013\n0.4394553303718567\n0.4970437288284302\n0.4967506229877472\n0.3378174304962158\n0.49609482288360596\n0.495734840631485\n0.49544480443000793\n0.49512800574302673\n0.49472010135650635\n0.4943391680717468\n0.49399325251579285\n0.49359917640686035\n0.4931624233722687\n0.4927258789539337\n0.4923189580440521\n0.4918827414512634\n0.491457462310791\n0.490994930267334\n0.4848769009113312\n0.4674568772315979\n0.48985955119132996\n0.48947516083717346\n0.4890482425689697\n0.48858150839805603\n0.48846951127052307\n0.48820561170578003\n0.48778456449508667\n0.4167025089263916\n0.4867061376571655\n0.48623788356781006\n0.485892117023468\n0.48549866676330566\n0.4850655198097229\n0.4847281575202942\n0.48440685868263245\n0.48400071263313293\n0.47681957483291626\n0.48307496309280396\n0.4827064871788025\n0.47942036390304565\n0.48195406794548035\n0.4815516173839569\n0.48123136162757874\n0.48076310753822327\n0.4803875684738159\n0.48003333806991577\n0.4796924591064453\n0.47933056950569153\n0.4739448130130768\n0.4786072075366974\n0.47821933031082153\n0.47778943181037903\n0.47733074426651\n0.4769117832183838\n0.4766009747982025\n0.45061415433883667\n0.47588756680488586\n0.4756007492542267\n0.47527197003364563\n0.47489598393440247\n0.4744780957698822\n0.47402098774909973\n0.4735982418060303\n0.4731546640396118\n0.47278738021850586\n0.47246745228767395\n0.4720584750175476\n0.47172045707702637\n0.4713799059391022\n0.47098973393440247\n0.4705657362937927\n0.4701000452041626\n0.4697810113430023\n0.4694019854068756\n0.469143271446228\n0.4443749487400055\n0.46844589710235596\n0.46796560287475586\n0.46749863028526306\n0.4671778678894043\n0.4668129086494446\n0.4124091863632202\n0.466033935546875\n0.46575531363487244\n0.4654247462749481\n0.4650084674358368\n0.3580050766468048\n0.46435055136680603\n0.4640282690525055\n0.46365416049957275\n0.43872734904289246\n0.4628726541996002\n0.46245747804641724\n0.46200621128082275\n0.46159663796424866\n0.4482128322124481\n0.4608660340309143\n0.46043625473976135\n0.45992904901504517\n0.45974281430244446\n0.43152064085006714\n0.45915326476097107\n0.4587191045284271\n0.45818185806274414\n0.4578462541103363\n0.45745915174484253\n0.4570361077785492\n0.4565771818161011\n0.45619577169418335\n0.4558815062046051\n0.4554746448993683\n0.4552069306373596\n0.45480552315711975\n0.45426028966903687\n0.45399293303489685\n0.4536794126033783\n0.4533146321773529\n0.4529602527618408\n0.45261630415916443\n0.4522160291671753\n0.4518415629863739\n0.4514467716217041\n0.45104703307151794\n0.4506320655345917\n0.3894861936569214\n0.449961394071579\n0.3882729709148407\n0.44919919967651367\n0.37648361921310425\n0.448509156703949\n0.44818729162216187\n0.44781962037086487\n0.3280426561832428\n0.44712555408477783\n0.298717737197876\n0.4422566592693329\n0.4461720585823059\n0.4457950294017792\n0.44547414779663086\n0.4451432228088379\n0.24654825031757355\n0.4444257915019989\n0.44404441118240356\n0.44366827607154846\n0.4433126151561737\n0.44289618730545044\n0.4425053000450134\n0.44211968779563904\n0.44169366359710693\n0.4412881135940552\n0.44088780879974365\n0.4404674470424652\n0.4400169253349304\n0.43545377254486084\n0.43922775983810425\n0.4390407204627991\n0.37587061524391174\n0.4385302662849426\n0.4381312429904938\n0.4375961124897003\n0.43729332089424133\n0.4369957149028778\n0.43664780259132385\n0.4362596869468689\n0.38487184047698975\n0.4354487359523773\n0.43502092361450195\n0.43456313014030457\n0.43420109152793884\n0.43387436866760254\n0.43346238136291504\n0.43297529220581055\n0.43264904618263245\n0.4323028326034546\n0.4319918751716614\n0.4316810369491577\n0.4313201904296875\n0.43091943860054016\n0.3927450180053711\n0.4300834834575653\n0.429738312959671\n0.4293282926082611\n0.42890846729278564\n0.40148550271987915\n0.4283340573310852\n0.42801955342292786\n0.4276203215122223\n0.42729610204696655\n0.4269869327545166\n0.42662808299064636\n0.4262295067310333\n0.4257962703704834\n0.4253980815410614\n0.4250299334526062\n0.4245873689651489\n0.4241798222064972\n0.4237973093986511\n0.42351922392845154\n0.4232015013694763\n0.4227350950241089\n0.4223383367061615\n0.42198142409324646\n0.4216444790363312\n0.42130768299102783\n0.420906662940979\n0.420540452003479\n0.42017441987991333\n0.41976404190063477\n0.4193785786628723\n0.4189932942390442\n0.4115400016307831\n0.41846996545791626\n0.41812455654144287\n0.41764122247695923\n0.41724687814712524\n0.4168921411037445\n0.41651782393455505\n0.41620275378227234\n0.4158484637737274\n0.4154500365257263\n0.4151107966899872\n0.41474220156669617\n0.4142952263355255\n0.4139515459537506\n0.41358837485313416\n0.41324007511138916\n0.4128870368003845\n0.4124998450279236\n0.4121275246143341\n0.4117211103439331\n0.4113491177558899\n0.41101646423339844\n0.41059592366218567\n0.41033685207366943\n0.4099264442920685\n0.4096333980560303\n0.4093209505081177\n0.40896469354629517\n0.4085647165775299\n0.40813082456588745\n0.4078189432621002\n0.4074925482273102\n0.40708842873573303\n0.40661153197288513\n0.4063926339149475\n0.4060279428958893\n0.40569257736206055\n0.4053961932659149\n0.4050465226173401\n0.4046630263328552\n0.4042360484600067\n0.4037801921367645\n0.4034118354320526\n0.40307754278182983\n0.4027724266052246\n0.4023900330066681\n0.4019884169101715\n0.401543527841568\n0.4012341797351837\n0.400924950838089\n0.40057235956192017\n0.4002392590045929\n0.39988216757774353\n0.39950111508369446\n0.39913952350616455\n0.3957774043083191\n0.39837831258773804\n0.3979739248752594\n0.39161744713783264\n0.3974302411079407\n0.39709362387657166\n0.3966226279735565\n0.3961902856826782\n0.3958542048931122\n0.39556625485420227\n0.3952304422855377\n0.39483723044395447\n0.3944873511791229\n0.39410409331321716\n0.39374974370002747\n0.39336204528808594\n0.3929841220378876\n0.3925681412220001\n0.39232438802719116\n0.3919851779937744\n0.3916126787662506\n0.39128807187080383\n0.3909206986427307\n0.39051055908203125\n0.3900720775127411\n0.3898100256919861\n0.3894909620285034\n0.3726966977119446\n0.38905301690101624\n0.38869619369506836\n0.388206422328949\n0.3876076936721802\n0.36102887988090515\n0.3870379328727722\n0.3867199718952179\n0.38643062114715576\n0.3861176669597626\n0.3839116096496582\n0.3854779303073883\n0.24377550184726715\n0.3848860561847687\n0.3845595121383667\n0.22355541586875916\n0.38385963439941406\n0.383486270904541\n0.38308480381965637\n0.3827779293060303\n0.3823862373828888\n0.3820796608924866\n0.22633802890777588\n0.3814244270324707\n0.38107582926750183\n0.23348967730998993\n0.3699166178703308\n0.3800121545791626\n0.37965479493141174\n0.37926942110061646\n0.3788983225822449\n0.3785790503025055\n0.37821298837661743\n0.37781426310539246\n0.37738296389579773\n0.33080577850341797\n0.3766334354877472\n0.3763010799884796\n0.37589868903160095\n0.375575989484787\n0.3753095269203186\n0.37496376037597656\n0.374599426984787\n0.37426796555519104\n0.3738900125026703\n0.37347957491874695\n0.3732231855392456\n0.3729109764099121\n0.37251973152160645\n0.372254341840744\n0.37188205122947693\n0.3692997097969055\n0.3711659014225006\n0.37087777256965637\n0.37054795026779175\n0.3701765239238739\n0.3698980510234833\n0.3695918619632721\n0.3692069947719574\n0.36878061294555664\n0.36843785643577576\n0.3680582046508789\n0.36764636635780334\n0.36734113097190857\n0.3669666647911072\n0.3666016459465027\n0.36629220843315125\n0.3658490777015686\n0.3655537962913513\n0.3652448058128357\n0.36489444971084595\n0.3646272122859955\n0.364304780960083\n0.36390426754951477\n0.36355456709861755\n0.36322343349456787\n0.3628556728363037\n0.3473983108997345\n0.3620885908603668\n0.3617902398109436\n0.3614645004272461\n0.361005961894989\n0.3606438934803009\n0.3603736460208893\n0.3439667820930481\n0.35975101590156555\n0.35938045382499695\n0.35906949639320374\n0.35868558287620544\n0.35826534032821655\n0.3477896451950073\n0.3576856255531311\n0.3318246006965637\n0.35707899928092957\n0.3128628432750702\n0.3564501404762268\n0.3561086058616638\n0.355730801820755\n0.3553168475627899\n0.3548940420150757\n0.35458502173423767\n0.35419896245002747\n0.34136754274368286\n0.35369062423706055\n0.3533594608306885\n0.3530057966709137\n0.3527202904224396\n0.35239410400390625\n0.35202735662460327\n0.3516291379928589\n0.351203978061676\n0.35080623626708984\n0.35043126344680786\n0.3502235412597656\n0.3209500014781952\n0.290267676115036\n0.34950149059295654\n0.25046560168266296\n0.34892892837524414\n0.34857749938964844\n0.34815868735313416\n0.34769967198371887\n0.34744778275489807\n0.3471510410308838\n0.3468095064163208\n0.3464321792125702\n0.3322378396987915\n0.3326249420642853\n0.34532392024993896\n0.34495189785957336\n0.3445487320423126\n0.3441144526004791\n0.3437744081020355\n0.343515008687973\n0.34325122833251953\n0.34288033843040466\n0.3425230383872986\n0.342170387506485\n0.34175992012023926\n0.3414834439754486\n0.34121599793434143\n0.34089964628219604\n0.3405478298664093\n0.3401605784893036\n0.3398313820362091\n0.33946678042411804\n0.33906683325767517\n0.3344711363315582\n0.338480681180954\n0.33812567591667175\n0.3377797305583954\n0.3241303861141205\n0.3371680974960327\n0.33683592081069946\n0.33647292852401733\n0.33613666892051697\n0.3358094394207001\n0.3354337215423584\n0.33506709337234497\n0.3347889184951782\n0.3344932198524475\n0.3341226577758789\n0.3284185826778412\n0.3335011303424835\n0.3331311345100403\n0.3327789604663849\n0.3324665427207947\n0.33211469650268555\n0.3318421542644501\n0.3315301835536957\n0.33114373683929443\n0.3307882249355316\n0.33046796917915344\n0.32955196499824524\n0.3299199640750885\n0.3295913636684418\n0.3291360139846802\n0.3287946879863739\n0.3284929096698761\n0.32819125056266785\n0.3279203176498413\n0.32760584354400635\n0.32725220918655396\n0.32695114612579346\n0.3266109526157379\n0.3262055814266205\n0.3258570730686188\n0.3255044221878052\n0.3251258134841919\n0.3248996436595917\n0.32459530234336853\n0.32416948676109314\n0.32386547327041626\n0.3235659599304199\n0.32327091693878174\n0.32294565439224243\n0.3226379156112671\n0.3222956359386444\n0.3219968378543854\n0.3216419517993927\n0.32128292322158813\n0.32094135880470276\n0.32056546211242676\n0.3202761113643646\n0.31996530294418335\n0.31965896487236023\n0.31933122873306274\n0.3043524920940399\n0.3186590075492859\n0.2725430727005005\n0.31812089681625366\n0.317819744348526\n0.3174843490123749\n0.3172694444656372\n0.31697729229927063\n0.31661656498908997\n0.31620028614997864\n0.31589147448539734\n0.3155399560928345\n0.3151586353778839\n0.3148631751537323\n0.31450796127319336\n0.3140887916088104\n0.3014342784881592\n0.3137468099594116\n0.3134819269180298\n0.25180378556251526\n0.3127391040325165\n0.312359482049942\n0.3120909035205841\n0.31178411841392517\n0.3114391267299652\n0.3110603094100952\n0.310651957988739\n0.31028637290000916\n0.30351129174232483\n0.30981481075286865\n0.3095770478248596\n0.30925875902175903\n0.308872789144516\n0.30842772126197815\n0.30817779898643494\n0.3079195022583008\n0.3076232075691223\n0.3072890043258667\n0.30692118406295776\n0.3058907091617584\n0.3063761591911316\n0.30608484148979187\n0.30566713213920593\n0.305156946182251\n0.30488306283950806\n0.3045335114002228\n0.3043019771575928\n0.30403268337249756\n0.30372148752212524\n0.30337679386138916\n0.28865861892700195\n0.30265864729881287\n0.30228519439697266\n0.3020629286766052\n0.3017443120479584\n0.30146360397338867\n0.30116206407546997\n0.30079373717308044\n0.29723092913627625\n0.30018314719200134\n0.29992401599884033\n0.2996232807636261\n0.2992892563343048\n0.2990764379501343\n0.2821536064147949\n0.29846757650375366\n0.29805925488471985\n0.29760125279426575\n0.29727253317832947\n0.29703962802886963\n0.29678604006767273\n0.29646608233451843\n0.2960757315158844\n0.29578107595443726\n0.29549896717071533\n0.29517972469329834\n0.29482749104499817\n0.29450032114982605\n0.29418572783470154\n0.2937886118888855\n0.29352402687072754\n0.2932347357273102\n0.29287540912628174\n0.2925988435745239\n0.29230591654777527\n0.29198014736175537\n0.2917369604110718\n0.29140326380729675\n0.2819834351539612\n0.29073646664619446\n0.2904321253299713\n0.2900950610637665\n0.2898115813732147\n0.28950774669647217\n0.2819105088710785\n0.28882670402526855\n0.2885192632675171\n0.2882283627986908\n0.2878844141960144\n0.2875938415527344\n0.2873729467391968\n0.2870335876941681\n0.28673937916755676\n0.286445289850235\n0.2861146330833435\n0.28578418493270874\n0.2790175676345825\n0.28523382544517517\n0.284903883934021\n0.28451308608055115\n0.2842608392238617\n0.2839680314064026\n0.2836916446685791\n0.2825874090194702\n0.28315141797065735\n0.2827942967414856\n0.2824414074420929\n0.2605965733528137\n0.2818416357040405\n0.28158652782440186\n0.28133153915405273\n0.2810119390487671\n0.2807369828224182\n0.2804621756076813\n0.2801511585712433\n0.27980801463127136\n0.2794328033924103\n0.27920296788215637\n0.2560094892978668\n0.27863484621047974\n0.27829664945602417\n0.2779063880443573\n0.2776409983634949\n0.2772592306137085\n0.26177799701690674\n0.27666500210762024\n0.27647238969802856\n0.27624377608299255\n0.2759711742401123\n0.2756626605987549\n0.2753222584724426\n0.2751181423664093\n0.27485010027885437\n0.27451422810554504\n0.27411460876464844\n0.2594994306564331\n0.2735916078090668\n0.2733282744884491\n0.2730332016944885\n0.2727024257183075\n0.27233996987342834\n0.2719499468803406\n0.27185046672821045\n0.2716197967529297\n0.2712700068950653\n0.2708132266998291\n0.27026164531707764\n0.2700950801372528\n0.2698611915111542\n0.26955610513687134\n0.269207626581192\n0.2689424753189087\n0.268641859292984\n0.2683098018169403\n0.2680569291114807\n0.2677449882030487\n0.2673740088939667\n0.2670624554157257\n0.2667628824710846\n0.2665383219718933\n0.2662666141986847\n0.2658848762512207\n0.2656174302101135\n0.2653343975543976\n0.26502010226249695\n0.26476094126701355\n0.2645018994808197\n0.2641802430152893\n0.26382744312286377\n0.26354146003723145\n0.26321646571159363\n0.2628643214702606\n0.2626062035560608\n0.2624029517173767\n0.26212552189826965\n0.2606666684150696\n0.2615126371383667\n0.2218308448791504\n0.19865331053733826\n0.2607056200504303\n0.26047974824905396\n0.25387537479400635\n0.26010218262672424\n0.25985321402549744\n0.2468741536140442\n0.2592468559741974\n0.2589012384414673\n0.25865286588668823\n0.25838908553123474\n0.2580866515636444\n0.257757306098938\n0.25739720463752747\n0.2570141553878784\n0.2567898631095886\n0.2565038502216339\n0.25615620613098145\n0.2557547986507416\n0.2553420960903168\n0.25505688786506653\n0.2548488676548004\n0.25452545285224915\n0.25424453616142273\n0.2539599537849426\n0.2536486089229584\n0.23790477216243744\n0.2530495524406433\n0.2527349591255188\n0.2524205446243286\n0.2522059381008148\n0.2519301474094391\n0.2515932619571686\n0.2395486831665039\n0.2511151432991028\n0.2508704960346222\n0.2505916357040405\n0.2503739893436432\n0.2500801086425781\n0.2497444748878479\n0.2494547963142395\n0.24913480877876282\n0.24878840148448944\n0.24844983220100403\n0.24817229807376862\n0.23115824162960052\n0.24774299561977386\n0.24746966361999512\n0.2270972728729248\n0.2468324601650238\n0.24163684248924255\n0.2461695671081543\n0.24351942539215088\n0.245620995759964\n0.24532616138458252\n0.245107039809227\n0.23531070351600647\n0.2445332407951355\n0.24421265721321106\n0.24393750727176666\n0.24360601603984833\n0.24333497881889343\n0.2430339902639389\n0.24278581142425537\n0.24255655705928802\n0.23836269974708557\n0.2419557273387909\n0.24167434871196747\n0.2414156198501587\n0.24112705886363983\n0.24082370102405548\n0.24053549766540527\n0.24026241898536682\n0.23997081816196442\n0.22621464729309082\n0.23957107961177826\n0.2392873615026474\n0.2388993799686432\n0.23865333199501038\n0.23840367794036865\n0.2381206452846527\n0.23784522712230682\n0.237599715590477\n0.23728743195533752\n0.23704221844673157\n0.23677855730056763\n0.2364816516637802\n0.23615527153015137\n0.2358068972826004\n0.22204650938510895\n0.23538102209568024\n0.2351182997226715\n0.23479655385017395\n0.2345304638147354\n0.2342238873243332\n0.2338511049747467\n0.23361504077911377\n0.233342245221138\n0.23304013907909393\n0.23283761739730835\n0.23257263004779816\n0.2322489619255066\n0.2319144904613495\n0.23168307542800903\n0.2314334362745285\n0.23114722967147827\n0.23086488246917725\n0.23054607212543488\n0.23027506470680237\n0.23000788688659668\n0.22972622513771057\n0.22947032749652863\n0.22920726239681244\n0.22891514003276825\n0.22867794334888458\n0.22840073704719543\n0.2246353030204773\n0.21684998273849487\n0.2277521789073944\n0.22747555375099182\n0.2270936369895935\n0.201290562748909\n0.22671204805374146\n0.18505777418613434\n0.22630174458026886\n0.22606950998306274\n0.22580479085445404\n0.22550760209560394\n0.10671932250261307\n0.22489573061466217\n0.22470401227474213\n0.22449429333209991\n0.2242196500301361\n0.223891019821167\n0.22358788549900055\n0.1069188043475151\n0.2231227457523346\n0.222866952419281\n0.22257889807224274\n0.22226586937904358\n0.221924290060997\n0.221726655960083\n0.22148242592811584\n0.2211737483739853\n0.22081509232521057\n0.22044239938259125\n0.22022037208080292\n0.22001275420188904\n0.2197122573852539\n0.2194584161043167\n0.21917258203029633\n0.21893692016601562\n0.21860504150390625\n0.21833045780658722\n0.21817363798618317\n0.21794918179512024\n0.21766433119773865\n0.21735477447509766\n0.2171129733324051\n0.21683932840824127\n0.2165374457836151\n0.21627481281757355\n0.21599461138248444\n0.21145880222320557\n0.21556223928928375\n0.2153143584728241\n0.21497109532356262\n0.2146422415971756\n0.21202051639556885\n0.2142394781112671\n0.21399588882923126\n0.20806242525577545\n0.21342451870441437\n0.21314968168735504\n0.21288558840751648\n0.2078188955783844\n0.21232976019382477\n0.2120380699634552\n0.21184489130973816\n0.21159915626049042\n0.2112518548965454\n0.21105903387069702\n0.20358015596866608\n0.21058262884616852\n0.21029913425445557\n0.2100822627544403\n0.20985153317451477\n0.15367163717746735\n0.209352046251297\n0.20908333361148834\n0.20878691971302032\n0.208546444773674\n0.20830610394477844\n0.2080102413892746\n0.20768675208091736\n0.20742259919643402\n0.2071308195590973\n0.20694683492183685\n0.20671436190605164\n0.20638495683670044\n0.18427419662475586\n0.2060246467590332\n0.20579615235328674\n0.2055089920759201\n0.205322265625\n0.20513217151165009\n0.20490416884422302\n0.2046486884355545\n0.20436231791973114\n0.20405203104019165\n0.20374542474746704\n0.20350442826747894\n0.2032085508108139\n0.20290601253509521\n0.2026311755180359\n0.2023324966430664\n0.2020168900489807\n0.20187288522720337\n0.20167069137096405\n0.2013693004846573\n0.20101340115070343\n0.20077061653137207\n0.19205768406391144\n0.20042207837104797\n0.20019671320915222\n0.19991347193717957\n0.19958272576332092\n0.19927948713302612\n0.1990513652563095\n0.19881656765937805\n0.19856151938438416\n0.19834738969802856\n0.17022737860679626\n0.19788895547389984\n0.1976412683725357\n0.1973632425069809\n0.1970617026090622\n0.19674009084701538\n0.1965404748916626\n0.19631730020046234\n0.19604021310806274\n0.19575993716716766\n0.19554395973682404\n0.19531798362731934\n0.1950618028640747\n0.19478222727775574\n0.1945095807313919\n0.1943279355764389\n0.1940959393978119\n0.1938069760799408\n0.18924279510974884\n0.19334711134433746\n0.1931324601173401\n0.1928877830505371\n0.19261646270751953\n0.19231857359409332\n0.19205434620380402\n0.1918638050556183\n0.19160322844982147\n0.17750246822834015\n0.19114264845848083\n0.19087590277194977\n0.19060267508029938\n0.19032631814479828\n0.1877356767654419\n0.17595013976097107\n0.18972434103488922\n0.18950839340686798\n0.18926602602005005\n0.18899394571781158\n0.1302356719970703\n0.18863259255886078\n0.18843384087085724\n0.1881789118051529\n0.18787124752998352\n0.18761339783668518\n0.11448221653699875\n0.18721704185009003\n0.18699923157691956\n0.18674857914447784\n0.1864750236272812\n0.18617534637451172\n0.1858726143836975\n0.1442558914422989\n0.1854419708251953\n0.18518251180648804\n0.1848740130662918\n0.18467724323272705\n0.18445764482021332\n0.1842086911201477\n0.1840188205242157\n0.18379634618759155\n0.1589014232158661\n0.15532998740673065\n0.18308071792125702\n0.1828685998916626\n0.18262724578380585\n0.1823599934577942\n0.18216130137443542\n0.1507427841424942\n0.18166019022464752\n0.18135464191436768\n0.1811143010854721\n0.1808481514453888\n0.18064381182193756\n0.18043634295463562\n0.18017393350601196\n0.17996348440647125\n0.17969495058059692\n0.1794847995042801\n0.17928121984004974\n0.17904548346996307\n0.17878732085227966\n0.1785680204629898\n0.17834563553333282\n0.17807509005069733\n0.17781119048595428\n0.17757320404052734\n0.1773996353149414\n0.15922091901302338\n0.1769532412290573\n0.17667414247989655\n0.17645293474197388\n0.17620626091957092\n0.1759309470653534\n0.17125195264816284\n0.17554713785648346\n0.17527872323989868\n0.17508713901042938\n0.17485415935516357\n0.17457352578639984\n0.17439506947994232\n0.17420394718647003\n0.17398430407047272\n0.17373934388160706\n0.1514604687690735\n0.1732213795185089\n0.1729801446199417\n0.17274858057498932\n0.17249499261379242\n0.17225109040737152\n0.16794663667678833\n0.17188714444637299\n0.17169108986854553\n0.1714414358139038\n0.17114777863025665\n0.17093952000141144\n0.17069357633590698\n0.17047299444675446\n0.17028719186782837\n0.17007002234458923\n0.16982784867286682\n0.15334880352020264\n0.16931888461112976\n0.1690552830696106\n0.1688200980424881\n0.16861014068126678\n0.1683877855539322\n0.16551361978054047\n0.16796539723873138\n0.16771534085273743\n0.16751855611801147\n0.16729068756103516\n0.16703802347183228\n0.15759335458278656\n0.1467323899269104\n0.16655194759368896\n0.16631539165973663\n0.165998175740242\n0.16574959456920624\n0.16553844511508942\n0.16527780890464783\n0.16508865356445312\n0.11627812683582306\n0.16476023197174072\n0.1645713746547699\n0.16435480117797852\n0.1641136258840561\n0.1638479381799698\n0.16224905848503113\n0.10617170482873917\n0.16316922008991241\n0.1629381775856018\n0.16267651319503784\n0.16246117651462555\n0.16222140192985535\n0.1619848757982254\n0.16173318028450012\n0.16150622069835663\n0.16126714646816254\n0.16116605699062347\n0.1609792709350586\n0.16070695221424103\n0.15908396244049072\n0.16019658744335175\n0.16005615890026093\n0.1598578244447708\n0.15965351462364197\n0.1594584733247757\n0.15923918783664703\n0.1590200662612915\n0.15879195928573608\n0.15857921540737152\n0.1583544760942459\n0.15810561180114746\n0.15785089135169983\n0.1576024293899536\n0.15737535059452057\n0.1571333110332489\n0.15695492923259735\n0.1567736268043518\n0.15650790929794312\n0.15624241530895233\n0.1560554951429367\n0.15585362911224365\n0.1556398570537567\n0.1554202139377594\n0.15520673990249634\n0.1517932116985321\n0.15480123460292816\n0.15458817780017853\n0.15435129404067993\n0.15417449176311493\n0.14341099560260773\n0.1537793129682541\n0.1535101681947708\n0.15328307449817657\n0.1530889868736267\n0.1502455472946167\n0.12354523688554764\n0.15245380997657776\n0.15226322412490845\n0.15206977725028992\n0.15186157822608948\n0.15165650844573975\n0.15142782032489777\n0.15122896432876587\n0.15101541578769684\n0.15077832341194153\n0.15051773190498352\n0.15031062066555023\n0.15008589625358582\n0.14997360110282898\n0.14979343116283417\n0.1323171705007553\n0.14929480850696564\n0.1490708589553833\n0.14889417588710785\n0.1486911028623581\n0.1484617292881012\n0.12977078557014465\n0.14800935983657837\n0.1478186398744583\n0.14549295604228973\n0.11293859779834747\n0.1472940444946289\n0.14711841940879822\n0.14691656827926636\n0.09823646396398544\n0.14648407697677612\n0.14625641703605652\n0.14601433277130127\n0.14583073556423187\n0.14560066163539886\n0.14540275931358337\n0.14519627392292023\n0.1449667066335678\n0.1447169929742813\n0.14449359476566315\n0.14433124661445618\n0.14414000511169434\n0.14393731951713562\n0.1437058448791504\n0.14346878230571747\n0.14331567287445068\n0.14314241707324982\n0.14294621348381042\n0.1427241861820221\n0.14248217642307281\n0.14228065311908722\n0.1420879065990448\n0.1418895423412323\n0.14167408645153046\n0.14144156873226166\n0.14125800132751465\n0.14107739925384521\n0.14087115228176117\n0.1406421661376953\n0.140473410487175\n0.14027045667171478\n0.1400248259305954\n0.13965393602848053\n0.1396254301071167\n0.13943448662757874\n0.13926929235458374\n0.13903023302555084\n0.1388169676065445\n0.13862089812755585\n0.12974604964256287\n0.13831712305545807\n0.13814125955104828\n0.1379229873418808\n0.13771340250968933\n0.13754357397556305\n0.13735124468803406\n0.12661580741405487\n0.1369415521621704\n0.13672424852848053\n0.13648737967014313\n0.1362394541501999\n0.13602551817893982\n0.1358707994222641\n0.13569368422031403\n0.13544367253780365\n0.13140569627285004\n0.13507890701293945\n0.13487988710403442\n0.13470622897148132\n0.13452427089214325\n0.13430051505565643\n0.13412721455097198\n0.13394007086753845\n0.13373352587223053\n0.13350483775138855\n0.13328193128108978\n0.13308702409267426\n0.13292564451694489\n0.1327393352985382\n0.11791766434907913\n0.13234493136405945\n0.13217566907405853\n0.1319788098335266\n0.13180148601531982\n0.13160766661167145\n0.1313890963792801\n0.131187304854393\n0.13096632063388824\n0.1307951956987381\n0.13057731091976166\n0.1304284781217575\n0.13024944067001343\n0.12999625504016876\n0.1298450082540512\n0.1296800971031189\n0.12015506625175476\n0.12931495904922485\n0.12912297248840332\n0.12890374660491943\n0.12873397767543793\n0.09554585069417953\n0.12837836146354675\n0.12071637809276581\n0.12798501551151276\n0.12775585055351257\n0.1275704801082611\n0.12739069759845734\n0.1271892637014389\n0.1138918325304985\n0.12688471376895905\n0.12671084702014923\n0.1264665424823761\n0.12627668678760529\n0.1261032372713089\n0.1259082406759262\n0.12569716572761536\n0.12553492188453674\n0.12537819147109985\n0.12517836689949036\n0.12054678797721863\n0.12472528964281082\n0.12453136593103409\n0.12435372918844223\n0.12417084723711014\n0.12400422245264053\n0.11156085133552551\n0.12366057187318802\n0.12347283959388733\n0.12326917797327042\n0.12304427474737167\n0.12286770343780518\n0.12272868305444717\n0.1225309744477272\n0.12238680571317673\n0.12220004200935364\n0.1157635822892189\n0.12176837027072906\n0.12159272283315659\n0.12142251431941986\n0.12122586369514465\n0.12102937698364258\n0.12082773447036743\n0.12064746767282486\n0.12048851698637009\n0.12030850350856781\n0.12014977633953094\n0.11995416134595871\n0.11976926773786545\n0.11958451569080353\n0.11937881261110306\n0.11920489370822906\n0.11902058124542236\n0.11218860745429993\n0.11513624340295792\n0.11855778098106384\n0.10776116698980331\n0.1110466942191124\n0.11799103766679764\n0.11775529384613037\n0.11761395633220673\n0.11745701730251312\n0.1172792837023735\n0.11711212992668152\n0.11695030331611633\n0.11674167215824127\n0.10772109776735306\n0.11646030843257904\n0.11630414426326752\n0.11612728238105774\n0.11592977494001389\n0.1157168596982956\n0.11553525924682617\n0.11537453532218933\n0.1151517778635025\n0.11491888761520386\n0.11474825441837311\n0.11459840834140778\n0.11440736800432205\n0.11426805704832077\n0.11410821229219437\n0.11392787843942642\n0.11372195929288864\n0.11350594460964203\n0.11335691064596176\n0.11318230628967285\n0.11308992654085159\n0.11292578279972076\n0.11269515752792358\n0.11245453357696533\n0.11228573322296143\n0.10729583352804184\n0.11200468987226486\n0.1118464395403862\n0.11166790127754211\n0.08525052666664124\n0.11129599064588547\n0.1111128032207489\n0.11094502359628677\n0.11076212674379349\n0.11058446019887924\n0.11039679497480392\n0.11020928621292114\n0.11001688241958618\n0.10991568118333817\n0.10975386202335358\n0.10953154414892197\n0.10936496406793594\n0.10918337851762772\n0.10900697857141495\n0.10883575677871704\n0.10869485139846802\n0.10849372297525406\n0.10835304111242294\n0.0891432836651802\n0.10250236839056015\n0.10793153941631317\n0.10778120160102844\n0.10761094838380814\n0.10742082446813583\n0.10729583352804184\n0.09630199521780014\n0.10700613260269165\n0.10682651400566101\n0.10661716014146805\n0.10637316107749939\n0.08344689756631851\n0.10609465092420578\n0.10595056414604187\n0.10578672587871552\n0.1056031808257103\n0.07734757661819458\n0.10527122020721436\n0.10511285066604614\n0.10491999983787537\n0.10475202649831772\n0.10456937551498413\n0.10436716675758362\n0.10420948266983032\n0.1040322333574295\n0.10389939695596695\n0.10373223572969437\n0.10352592915296555\n0.10334926098585129\n0.10323646664619446\n0.10307474434375763\n0.10291803628206253\n0.08547789603471756\n0.10214605927467346\n0.10246329009532928\n0.10227777063846588\n0.10210704803466797\n0.10194133222103119\n0.10176115483045578\n0.10159572958946228\n0.1014547273516655\n0.1012798398733139\n0.10108083486557007\n0.10092081129550934\n0.09480678290128708\n0.10067858546972275\n0.09169892221689224\n0.10037379711866379\n0.0864173024892807\n0.0999777764081955\n0.09982344508171082\n0.0878722220659256\n0.09952476620674133\n0.09935636073350906\n0.09919770061969757\n0.0904693603515625\n0.09885677695274353\n0.09871769696474075\n0.09852602332830429\n0.09759426862001419\n0.0982484221458435\n0.09809543192386627\n0.09794733673334122\n0.0977802649140358\n0.0976419448852539\n0.09748466312885284\n0.09730847179889679\n0.09711816906929016\n0.09697556495666504\n0.09683306515216827\n0.09668117761611938\n0.09652941673994064\n0.09636355936527252\n0.09615998715162277\n0.08433965593576431\n0.09595663100481033\n0.09583377838134766\n0.09569212049245834\n0.09553170204162598\n0.09539498388767242\n0.09523952007293701\n0.09504655003547668\n0.0948866754770279\n0.0947081446647644\n0.07374519854784012\n0.09437035024166107\n0.09418294578790665\n0.09400507807731628\n0.09384608268737793\n0.09371057152748108\n0.09356115758419037\n0.09339320659637451\n0.09323473274707794\n0.09309035539627075\n0.0929274782538414\n0.09278798848390579\n0.08929368853569031\n0.09253252297639847\n0.07106012105941772\n0.09225423634052277\n0.055390357971191406\n0.09199950844049454\n0.0918792188167572\n0.09174051135778427\n0.028966858983039856\n0.09148649871349335\n0.0913480892777443\n0.0912051796913147\n0.09107159078121185\n0.09091969579458237\n0.09077252447605133\n0.09061169624328613\n0.024437909945845604\n0.09033631533384323\n0.09019419550895691\n0.0900338813662529\n0.08985540270805359\n0.08973195403814316\n0.08958574384450912\n0.08290121704339981\n0.055537693202495575\n0.08910228312015533\n0.06442964822053909\n0.08877009898424149\n0.08865194022655487\n0.07790875434875488\n0.08840223401784897\n0.08826618641614914\n0.08810758590698242\n0.08794008195400238\n0.0878179520368576\n0.08768687397241592\n0.08749719709157944\n0.08730322122573853\n0.08721307665109634\n0.08709145337343216\n0.0869339182972908\n0.08678101748228073\n0.08665071427822113\n0.0865025520324707\n0.08633658289909363\n0.08617973327636719\n0.08331471681594849\n0.08588434010744095\n0.08579046279191971\n0.08565196394920349\n0.08546451479196548\n0.08530399948358536\n0.08517926186323166\n0.08505461364984512\n0.0849122703075409\n0.08476559817790985\n0.08461906015872955\n0.08448595553636551\n0.07852326333522797\n0.08421119302511215\n0.06986525654792786\n0.06942632794380188\n0.08380872756242752\n0.08367184549570084\n0.08351743966341019\n0.08334995061159134\n0.0832398533821106\n0.08309903740882874\n0.08290121704339981\n0.08277825266122818\n0.08265537023544312\n0.08250628411769867\n0.08240112662315369\n0.0822829082608223\n0.0821472778916359\n0.08199865370988846\n0.08183269947767258\n0.08166255056858063\n0.08152307569980621\n0.08135325461626053\n0.08121839165687561\n0.08110102266073227\n0.08097505569458008\n0.07848906517028809\n0.08071473985910416\n0.08058907091617584\n0.08044618368148804\n0.08029045909643173\n0.08016080409288406\n0.0800442025065422\n0.07989317178726196\n0.07971644401550293\n0.07958725094795227\n0.07945816218852997\n0.07931198924779892\n0.06382299959659576\n0.07911872863769531\n0.07899859547615051\n0.0788271427154541\n0.07865586876869202\n0.07853609323501587\n0.0783950462937355\n0.07824131846427917\n0.07807068526744843\n0.07797691226005554\n0.0778704285621643\n0.0639386996626854\n0.07760240882635117\n0.07744945585727692\n0.07731787115335464\n0.07718639820814133\n0.07700421661138535\n0.07685185968875885\n0.07673768699169159\n0.07661093771457672\n0.0765053853392601\n0.04952146112918854\n0.07626079022884369\n0.07611759006977081\n0.07602079212665558\n0.07590724527835846\n0.03752369061112404\n0.06279407441616058\n0.07555451989173889\n0.07543294131755829\n0.07529470324516296\n0.07513986527919769\n0.0749768316745758\n0.0748223140835762\n0.07470132410526276\n0.07455126196146011\n0.07437638193368912\n0.07428901642560959\n0.07418092340230942\n0.07405629754066467\n0.06735536456108093\n0.07384467869997025\n0.07372447848320007\n0.07359196245670319\n0.07344301789999008\n0.062012672424316406\n0.07313733547925949\n0.0729806050658226\n0.07285699248313904\n0.07244160771369934\n0.07262241840362549\n0.07248267531394958\n0.07237180322408676\n0.06773606687784195\n0.06955504417419434\n0.07201921194791794\n0.0719209685921669\n0.07180234789848328\n0.07167156785726547\n0.07152458280324936\n0.07136551290750504\n0.0711984857916832\n0.06175076216459274\n0.07104791700839996\n0.07094627618789673\n0.07082033902406693\n0.07066205888986588\n0.07048370689153671\n0.07038651406764984\n0.07025298476219177\n0.07009938359260559\n0.07000648975372314\n0.0698934942483902\n0.06976446509361267\n0.06962347775697708\n0.06947056204080582\n0.06933388859033585\n0.06919334083795547\n0.06911308318376541\n0.0690128356218338\n0.06888462603092194\n0.06872852146625519\n0.06860856711864471\n0.0685046911239624\n0.06838493049144745\n0.06824932992458344\n0.0681019201874733\n0.06797853112220764\n0.06785526126623154\n0.06771621108055115\n0.06760111451148987\n0.06746232509613037\n0.05960091948509216\n0.05490303412079811\n0.06720100343227386\n0.06710610538721085\n0.0669994205236435\n0.06687309592962265\n0.06673505902290344\n0.06658535450696945\n0.06642401963472366\n0.06628251820802689\n0.06616079807281494\n0.06605878472328186\n0.06596077978610992\n0.06583935022354126\n0.06569065153598785\n0.06553822010755539\n0.06543278694152832\n0.06531184911727905\n0.06522998213768005\n0.0605170801281929\n0.06498858332633972\n0.06483308225870132\n0.06472045928239822\n0.06461181491613388\n0.0645148828625679\n0.06441028416156769\n0.06429029256105423\n0.06416267901659012\n0.06405449658632278\n0.0639386996626854\n0.0637960210442543\n0.06366889923810959\n0.06354960054159164\n0.06341888010501862\n0.06336509436368942\n0.050686128437519073\n0.06316552311182022\n0.06302753835916519\n0.06284762173891068\n0.06275202333927155\n0.06264886260032654\n0.06253433972597122\n0.0624542310833931\n0.029177598655223846\n0.062271326780319214\n0.062157150357961655\n0.06201647222042084\n0.061936698853969574\n0.06184559315443039\n0.06173938885331154\n0.061618123203516006\n0.06148562580347061\n0.06134193390607834\n0.06118708476424217\n0.061104074120521545\n0.06099850684404373\n0.060870442539453506\n0.060716189444065094\n0.049185868352651596\n0.041935086250305176\n0.03585202619433403\n0.0605771541595459\n0.06049831584095955\n0.06037452444434166\n0.06021341308951378\n0.060015130788087845\n0.05979105457663536\n0.033729080110788345\n0.05963072553277016\n0.03221360966563225\n0.05945944786071777\n0.024124057963490486\n0.059273555874824524\n0.017746731638908386\n0.05907682701945305\n0.058969322592020035\n0.058850809931755066\n0.05872132256627083\n0.058588284999132156\n0.05851075053215027\n0.0584111362695694\n0.05828581750392914\n0.05814223736524582\n0.05803558602929115\n0.05490303412079811\n0.05780790373682976\n0.057690564543008804\n0.05755870044231415\n0.05741967260837555\n0.05738311633467674\n0.05730272829532623\n0.05718225613236427\n0.04023410752415657\n0.057029105722904205\n0.0569416880607605\n0.05682886764407158\n0.056694358587265015\n0.05653823912143707\n0.056367844343185425\n0.05629540979862213\n0.05621941015124321\n0.056136228144168854\n0.05603865534067154\n0.05594117194414139\n0.055850982666015625\n0.05573204532265663\n0.055620431900024414\n0.055505335330963135\n0.05538317561149597\n0.05526115000247955\n0.05512851104140282\n0.05193038284778595\n0.05492448806762695\n0.05487443506717682\n0.054802969098091125\n0.05470656603574753\n0.05458885431289673\n0.05444634333252907\n0.05430401861667633\n0.05418674275279045\n0.05407668650150299\n0.04371274262666702\n0.05392421409487724\n0.030012326315045357\n0.05376134440302849\n0.05366939678788185\n0.016970770433545113\n0.05347162112593651\n0.05336582288146019\n0.053312961012125015\n0.053235478699207306\n0.05313342809677124\n0.05301390960812569\n0.052883997559547424\n0.05279280245304108\n0.013147413730621338\n0.05259665101766586\n0.05248822644352913\n0.052372924983501434\n0.052271708846092224\n0.052170585840940475\n0.05205215513706207\n0.05194081366062164\n0.051836539059877396\n0.0517219603061676\n0.0516248382627964\n0.04536140710115433\n0.05147932842373848\n0.046561941504478455\n0.05127181485295296\n0.05114060267806053\n0.05106126889586449\n0.05095788091421127\n0.050854600965976715\n0.05077548697590828\n0.05068269371986389\n0.05057969316840172\n0.05046650767326355\n0.05038769915699959\n0.050291839987039566\n0.045657627284526825\n0.05007307603955269\n0.049946822226047516\n0.04989227280020714\n0.04980369657278061\n0.04968118295073509\n0.04959619417786598\n0.04949769377708435\n0.04803531989455223\n0.04929082840681076\n0.049182482063770294\n0.04907425493001938\n0.04898978769779205\n0.04888177290558815\n0.04878398776054382\n0.048699766397476196\n0.048608891665935516\n0.048508018255233765\n0.04839717969298363\n0.048323359340429306\n0.046723417937755585\n0.04816918447613716\n0.04808549955487251\n0.047978486865758896\n0.04785490036010742\n0.04776815325021744\n0.03777535259723663\n0.04762152582406998\n0.04753831773996353\n0.04744521155953407\n0.04733891412615776\n0.04722941666841507\n0.04713992774486542\n0.04703066125512123\n0.04486231133341789\n0.04683232679963112\n0.04673001542687416\n0.046627819538116455\n0.04655535891652107\n0.046459928154945374\n0.04592537507414818\n0.04624967277050018\n0.04615128040313721\n0.04605299234390259\n0.04597770795226097\n0.045882873237133026\n0.04577181488275528\n0.04569676145911217\n0.04462991654872894\n0.045517534017562866\n0.04541341960430145\n0.0453321635723114\n0.04354064166545868\n0.045163385570049286\n0.045072633773088455\n0.04296625033020973\n0.04487847164273262\n0.04477832093834877\n0.04471699148416519\n0.04463636130094528\n0.04453648254275322\n0.04441741481423378\n0.04434669390320778\n0.04425997659564018\n0.04418296739459038\n0.044096410274505615\n0.044013142585754395\n0.043907564133405685\n0.04382766783237457\n0.043738268315792084\n0.04363621026277542\n0.043543823063373566\n0.04347062110900879\n0.04337523132562637\n0.04326407611370087\n0.037805017083883286\n0.04312770813703537\n0.04304852336645126\n0.042975738644599915\n0.042893536388874054\n0.04281456768512726\n0.03709632530808449\n0.04265684634447098\n0.02215616963803768\n0.042496271431446075\n0.025657355785369873\n0.04232972115278244\n0.042241863906383514\n0.042141567915678024\n0.04203513637185097\n0.041922587901353836\n0.0418819822371006\n0.04181955009698868\n0.04173846170306206\n0.04163876548409462\n0.04152363911271095\n0.041414882987737656\n0.04133728891611099\n0.041234973818063736\n0.04116373881697655\n0.041083287447690964\n0.040993645787239075\n0.04089484363794327\n0.04080232232809067\n0.04071914777159691\n0.040642205625772476\n0.040546901524066925\n0.040470123291015625\n0.040402621030807495\n0.04032598063349724\n0.0364234559237957\n0.04016374051570892\n0.04007510840892792\n0.03998047113418579\n0.03991337865591049\n0.039840247482061386\n0.03974892944097519\n0.039639487862586975\n0.039575714617967606\n0.03949379920959473\n0.03942105546593666\n0.039345353841781616\n0.03926367312669754\n0.039179060608148575\n0.03908548876643181\n0.03763905540108681\n0.038934800773859024\n0.03884151950478554\n0.03877238556742668\n0.03868529945611954\n0.038601312786340714\n0.03849945217370987\n0.03314315527677536\n0.038373805582523346\n0.03831106051802635\n0.038236431777477264\n0.03815293312072754\n0.011241448111832142\n0.03805761784315109\n0.03799215704202652\n0.037911899387836456\n0.037810951471328735\n0.037721991539001465\n0.037656817585229874\n0.03757987171411514\n0.03749413788318634\n0.037402600049972534\n0.03729938715696335\n0.0372169204056263\n0.03714042156934738\n0.027945028617978096\n0.036958325654268265\n0.036861587315797806\n0.034802377223968506\n0.03674156963825226\n0.03666848689317703\n0.03658963739871979\n0.036519620567560196\n0.036440931260585785\n0.03635650873184204\n0.036260560154914856\n0.03618215024471283\n0.03612122684717178\n0.036057453602552414\n0.03597636893391609\n0.03587803244590759\n0.03360029682517052\n0.03575674816966057\n0.0356731191277504\n0.03560110554099083\n0.035532042384147644\n0.035457298159599304\n0.03537402302026749\n0.03528224676847458\n0.03519631549715996\n0.03170623257756233\n0.035070471465587616\n0.03499336168169975\n0.03490208089351654\n0.03481946140527725\n0.03475115820765495\n0.03469429165124893\n0.03461191803216934\n0.03454381972551346\n0.034470126032829285\n0.03439084812998772\n0.03430318087339401\n0.03425233066082001\n0.03418741002678871\n0.03410563990473747\n0.03401271253824234\n0.03395926579833031\n0.033883385360240936\n0.025313889607787132\n0.03376271575689316\n0.0337066650390625\n0.017294321209192276\n0.02295592986047268\n0.03353040665388107\n0.03346617519855499\n0.03339363634586334\n0.03331282362341881\n0.03322654217481613\n0.03313204273581505\n0.0330321304500103\n0.032965607941150665\n0.03289638087153435\n0.03285211697220802\n0.03280235081911087\n0.0232086181640625\n0.032675351947546005\n0.03259816765785217\n0.032507315278053284\n0.03240560367703438\n0.03234794735908508\n0.03229856491088867\n0.0322355255484581\n0.03216706961393356\n0.03208775445818901\n0.03200307860970497\n0.031956691294908524\n0.03189126029610634\n0.03180139884352684\n0.02856830321252346\n0.03163020312786102\n0.03156239539384842\n0.031494658440351486\n0.031429700553417206\n0.03135670721530914\n0.03128649294376373\n0.031210966408252716\n0.03113822638988495\n0.031068257987499237\n0.030995681881904602\n0.030939294025301933\n0.030872231349349022\n0.030807919800281525\n0.024008069187402725\n0.03068484365940094\n0.030618058517575264\n0.028303276747465134\n0.03050868585705757\n0.03045274317264557\n0.03038620948791504\n0.03031177818775177\n0.030232133343815804\n0.030144644901156425\n0.030078448355197906\n0.030017614364624023\n0.029954198747873306\n0.02554504945874214\n0.02984338440001011\n0.029777521267533302\n0.02969595231115818\n0.02962237223982811\n0.029569871723651886\n0.029506931081414223\n0.029438823461532593\n0.02936033345758915\n0.02927933633327484\n0.029211491346359253\n0.023390289396047592\n0.024166736751794815\n0.015208200551569462\n0.029089046642184258\n0.02903442084789276\n0.028956472873687744\n0.028857888653874397\n0.02875947207212448\n0.02869998663663864\n0.02863023430109024\n0.028545096516609192\n0.02571849524974823\n0.028447216376662254\n0.0284060537815094\n0.028357211500406265\n0.028298143297433853\n0.02823144569993019\n0.02815970405936241\n0.028080381453037262\n0.027996066957712173\n0.02793482504785061\n0.027876198291778564\n0.02158302441239357\n0.02772355079650879\n0.027637235820293427\n0.024562105536460876\n0.027530796825885773\n0.027475126087665558\n0.027424564585089684\n0.027366476133465767\n0.027303406968712807\n0.027240410447120667\n0.027180003002285957\n0.027114635333418846\n0.026641834527254105\n0.026989150792360306\n0.026921510696411133\n0.02278977632522583\n0.026781490072607994\n0.026729077100753784\n0.026664255186915398\n0.023939553648233414\n0.026579607278108597\n0.026519935578107834\n0.02644544094800949\n0.026393357664346695\n0.026336373761296272\n0.02626955509185791\n0.0261978842318058\n0.022706925868988037\n0.02608686313033104\n0.02332499250769615\n0.025958865880966187\n0.02588270977139473\n0.02365708351135254\n0.025779709219932556\n0.0257258377969265\n0.025669578462839127\n0.025620706379413605\n0.02555968426167965\n0.025503605604171753\n0.025445155799388885\n0.025381911545991898\n0.025311462581157684\n0.025260508060455322\n0.02519991621375084\n0.025139395147562027\n0.02508619800209999\n0.0250113345682621\n0.02495827153325081\n0.0249076746404171\n0.024859534576535225\n0.0248018279671669\n0.024746589362621307\n0.02469141036272049\n0.02462911047041416\n0.02420945279300213\n0.024521468207240105\n0.024461768567562103\n0.024392610415816307\n0.024337828159332275\n0.024285485967993736\n0.024221325293183327\n0.024171480908989906\n0.02411457896232605\n0.022299490869045258\n0.022151626646518707\n0.02394191548228264\n0.023882925510406494\n0.023826364427804947\n0.023767517879605293\n0.02371109463274479\n0.023652389645576477\n0.023614857345819473\n0.023558614775538445\n0.023486068472266197\n0.023429978638887405\n0.023376289755105972\n0.023329652845859528\n0.023273751139640808\n0.02321326732635498\n0.023159828037023544\n0.023120367899537086\n0.02306240051984787\n0.022999877110123634\n0.022942060604691505\n0.02290278673171997\n0.02285432070493698\n0.022801294922828674\n0.022741427645087242\n0.022677043452858925\n0.022619634866714478\n0.02257605269551277\n0.018978914245963097\n0.022479861974716187\n0.022424988448619843\n0.022381596267223358\n0.018163830041885376\n0.019433557987213135\n0.022222084924578667\n0.02215389907360077\n0.018161773681640625\n0.02207447960972786\n0.02203369140625\n0.021986152976751328\n0.021931884810328484\n0.021873172372579575\n0.021807774901390076\n0.021744728088378906\n0.021699748933315277\n0.019576337188482285\n0.021591991186141968\n0.017655376344919205\n0.021522540599107742\n0.021477794274687767\n0.015285450033843517\n0.021354977041482925\n0.021301494911313057\n0.021252527832984924\n0.018788103014230728\n0.02113923244178295\n0.021083805710077286\n0.021037302911281586\n0.020984221249818802\n0.020926790311932564\n0.0196896530687809\n0.020860621705651283\n0.02080996334552765\n0.0207417830824852\n0.020695660263299942\n0.02064739540219307\n0.020599186420440674\n0.02055978588759899\n0.020513866096735\n0.020459264516830444\n0.020402558147907257\n0.020363345742225647\n0.020317645743489265\n0.020258963108062744\n0.02019169181585312\n0.020146183669567108\n0.020115874707698822\n0.020072614774107933\n0.020018605515360832\n0.0199754498898983\n0.019923726096749306\n0.019869916141033173\n0.019818328320980072\n0.019758228212594986\n0.019706785678863525\n0.019676808267831802\n0.0196382999420166\n0.019587013870477676\n0.019529396668076515\n0.019486773759126663\n0.012519976124167442\n0.019405914470553398\n0.01936342567205429\n0.019312500953674316\n0.019257405772805214\n0.01920027658343315\n0.019143233075737953\n0.019098922610282898\n0.019063090905547142\n0.01901887357234955\n0.01895579695701599\n0.01891380362212658\n0.018871856853365898\n0.018832050263881683\n0.018211159855127335\n0.01874420791864395\n0.018698275089263916\n0.018646148964762688\n0.018608663231134415\n0.01856081932783127\n0.018504733219742775\n0.018452877178788185\n0.01591433770954609\n0.018382471054792404\n0.018343184143304825\n0.01829361915588379\n0.01825442723929882\n0.018209101632237434\n0.018157660961151123\n0.01810629479587078\n0.018052950501441956\n0.018003778532147408\n0.01795467548072338\n0.017919933423399925\n0.01787298358976841\n0.017832208424806595\n0.01779147982597351\n0.017746731638908386\n0.01769392006099224\n0.01764523983001709\n0.01760675013065338\n0.017560213804244995\n0.017519796267151833\n0.017471356317400932\n0.017437085509300232\n0.013007817789912224\n0.01736261509358883\n0.017322424799203873\n0.017278270795941353\n0.015459501184523106\n0.011170377023518085\n0.010384797118604183\n0.017106210812926292\n0.017064325511455536\n0.017026474699378014\n0.016986677423119545\n0.016944939270615578\n0.016901269555091858\n0.01686161756515503\n0.01681409776210785\n0.010282423347234726\n0.016735047101974487\n0.016695592552423477\n0.016650276258587837\n0.016601089388132095\n0.016555901616811752\n0.016518618911504745\n0.016477461904287338\n0.016442224383354187\n0.016397252678871155\n0.016350392252206802\n0.01631334237754345\n0.01627049408853054\n0.016223816201090813\n0.016190791502594948\n0.01615392230451107\n0.016115158796310425\n0.016068704426288605\n0.016022315248847008\n0.01425454393029213\n0.01596056856215\n0.015927813947200775\n0.015889322385191917\n0.015847034752368927\n0.015800967812538147\n0.01575113832950592\n0.015712860971689224\n0.015666989609599113\n0.015626907348632812\n0.015584971755743027\n0.015560217201709747\n0.011710558086633682\n0.015489871613681316\n0.015442430973052979\n0.015396956354379654\n0.015361003577709198\n0.015321314334869385\n0.015276018530130386\n0.015245858579874039\n0.015211964026093483\n0.015168709680438042\n0.015118011273443699\n0.015080511569976807\n0.015043058432638645\n0.015018738806247711\n0.014981362968683243\n0.014932842925190926\n0.014893711544573307\n0.014850912615656853\n0.01480631809681654\n0.014769206754863262\n0.014733994379639626\n0.014693275094032288\n0.014654459431767464\n0.014619383960962296\n0.014576980844140053\n0.013354677706956863\n0.014514410868287086\n0.014481340534985065\n0.014448307454586029\n0.01441164780408144\n0.01437869481742382\n0.014336641877889633\n0.014298300258815289\n0.011379383504390717\n0.014225410297513008\n0.014187217690050602\n0.014145446941256523\n0.014116425067186356\n0.014078378677368164\n0.014040384441614151\n0.014004246331751347\n0.013964548707008362\n0.013924907892942429\n0.01388352457433939\n0.013847589492797852\n0.0138081144541502\n0.013779440894722939\n0.013747218996286392\n0.013702528551220894\n0.012076647020876408\n0.013634738512337208\n0.006061863619834185\n0.0035268403589725494\n0.01356356218457222\n0.01353691890835762\n0.013506755232810974\n0.013476625084877014\n0.01344829797744751\n0.013411164283752441\n0.011606761254370213\n0.013361731544137001\n0.013333525508642197\n0.013301829807460308\n0.013264898210763931\n0.013224508613348007\n0.013180676847696304\n0.01313342060893774\n0.013084502890706062\n0.013051360845565796\n0.003952160477638245\n0.012976511381566525\n0.01293482817709446\n0.012901876121759415\n0.012868966907262802\n0.012830913066864014\n0.012803273275494576\n0.012777388095855713\n0.011138145811855793\n0.010613061487674713\n0.012670673429965973\n0.012626055628061295\n0.012596924789249897\n0.012567827478051186\n0.012537055648863316\n0.01250632107257843\n0.01247732900083065\n0.01244326401501894\n0.012409244664013386\n0.012373575009405613\n0.012339651584625244\n0.01230408251285553\n0.010291708633303642\n0.010419034399092197\n0.012016355991363525\n0.012179151177406311\n0.012147177010774612\n0.01211356557905674\n0.012080000713467598\n0.012046482414007187\n0.012009666301310062\n0.011977915652096272\n0.011944539844989777\n0.011914541013538837\n0.011881252750754356\n0.011851334013044834\n0.011818134225904942\n0.011789951473474503\n0.01175683829933405\n0.011722119525074959\n0.011699002236127853\n0.011664369143545628\n0.011634724214673042\n0.01160347368568182\n0.008810533210635185\n0.007094442844390869\n0.011526349931955338\n0.0034924896899610758\n0.011473987251520157\n0.011446218006312847\n0.011421743780374527\n0.011392408981919289\n0.011366365477442741\n0.011335478164255619\n0.01130463182926178\n0.01127544790506363\n0.010739190503954887\n0.0014974027872085571\n0.011184895411133766\n0.011152643710374832\n0.011118830181658268\n0.003478976432234049\n0.011069007217884064\n0.011040129698812962\n0.011004885658621788\n0.010968100279569626\n0.010939355008304119\n0.008870790712535381\n0.010878792963922024\n0.010846986435353756\n0.010819986462593079\n0.010793020948767662\n0.010764505714178085\n0.01073444727808237\n0.010704430751502514\n0.010677609592676163\n0.010498618707060814\n0.010619350709021091\n0.010589495301246643\n0.010562818497419357\n0.010536175221204758\n0.010506438091397285\n0.010473618283867836\n0.010451767593622208\n0.010425264947116375\n0.010395684279501438\n0.010363038629293442\n0.010339751839637756\n0.01031184196472168\n0.010280875489115715\n0.010249955579638481\n0.009020883589982986\n0.010197498835623264\n0.010174399241805077\n0.010151324793696404\n0.010125206783413887\n0.010096054524183273\n0.010070007294416428\n0.01004093512892723\n0.010013432241976261\n0.009982917457818985\n0.004654247313737869\n0.009167851880192757\n0.009908352978527546\n0.009879515506327152\n0.009846176952123642\n0.009811383672058582\n0.009784197434782982\n0.009755540639162064\n0.009731441736221313\n0.009702863171696663\n0.00967582780867815\n0.009645831771194935\n0.009618875570595264\n0.009593451395630836\n0.00956656876951456\n0.009541214443743229\n0.009512916207313538\n0.009487632662057877\n0.00946089904755354\n0.0094356844201684\n0.009406063705682755\n0.00938092265278101\n0.008875102736055851\n0.009332215413451195\n0.009304230101406574\n0.009280695579946041\n0.009257190860807896\n0.009229317307472229\n0.009198559448122978\n0.009176620282232761\n0.00914886873215437\n0.009124074131250381\n0.00909931305795908\n0.00907167885452509\n0.009046989493072033\n0.009023782797157764\n0.008997710421681404\n0.008968786336481571\n0.00791365746408701\n0.008928369730710983\n0.008900996297597885\n0.00886504352092743\n0.008842071518301964\n0.008821994997560978\n0.00879907887428999\n0.008777622133493423\n0.008756191469728947\n0.00873050931841135\n0.008702018298208714\n0.00867925863713026\n0.008655108511447906\n0.008625323884189129\n0.008597004227340221\n0.008571556769311428\n0.008543326519429684\n0.008532047271728516\n0.00850951112806797\n0.008479977026581764\n0.008444885723292828\n0.008421064354479313\n0.0084028709679842\n0.008381903171539307\n0.008358171209692955\n0.008330294862389565\n0.00830246414989233\n0.008278844878077507\n0.007534174248576164\n0.008230323903262615\n0.008206807076931\n0.008180564269423485\n0.008158497512340546\n0.00814059004187584\n0.008115828037261963\n0.008092476055026054\n0.008066416718065739\n0.008044504560530186\n0.005606247112154961\n0.008006229996681213\n0.007985763251781464\n0.00796123780310154\n0.007935390807688236\n0.007912300527095795\n0.007890598848462105\n0.007864866405725479\n0.007836475037038326\n0.007813528180122375\n0.007796003948897123\n0.007774462457746267\n0.007744891569018364\n0.007723420858383179\n0.007705997675657272\n0.007683243602514267\n0.007664530072361231\n0.007644505240023136\n0.007620509713888168\n0.007595222443342209\n0.007576616480946541\n0.007554054260253906\n0.007527553476393223\n0.007499777711927891\n0.007479969412088394\n0.007460187189280987\n0.007439115084707737\n0.007418072782456875\n0.007394435815513134\n0.007368216756731272\n0.00735119916498661\n0.005805641878396273\n0.007063630968332291\n0.007297657895833254\n0.007276816759258509\n0.007250807248055935\n0.007231330499053001\n0.0072054024785757065\n0.00717693567276001\n0.0071601406671106815\n0.00714078638702631\n0.00711759552359581\n0.007097013294696808\n0.0070764608681201935\n0.007052093744277954\n0.007029047701507807\n0.007008593995124102\n0.006995825562626123\n0.006974145770072937\n0.006946139968931675\n0.006929617375135422\n0.006911845877766609\n0.00655008852481842\n0.006880167871713638\n0.0068624597042799\n0.006839726120233536\n0.006813250947743654\n0.006795629393309355\n0.0030755973421037197\n0.00676045473664999\n0.00674164853990078\n0.00672161765396595\n0.00669786985963583\n0.006672917399555445\n0.006645523477345705\n0.006630605086684227\n0.00661818590015173\n0.006602058187127113\n0.0065809981897473335\n0.0065661524422466755\n0.0065451497212052345\n0.006516787689179182\n0.005560640245676041\n0.005582280457019806\n0.006463928148150444\n0.006444314494729042\n0.006421062164008617\n0.006407619453966618\n0.0063917506486177444\n0.006372246891260147\n0.0063527729362249374\n0.006329686380922794\n0.006305430084466934\n0.005327600985765457\n0.0062691327184438705\n0.006252230145037174\n0.006230531260371208\n0.0062112752348184586\n0.006190848536789417\n0.006170455366373062\n0.006154883187264204\n0.006135744508355856\n0.006115442141890526\n0.006099939811974764\n0.004508625715970993\n0.006066616624593735\n0.00604642927646637\n0.006023906636983156\n0.005532230716198683\n0.005990792065858841\n0.00597308948636055\n0.00595305860042572\n0.005942467600107193\n0.004723206162452698\n0.005909577943384647\n0.0058873118832707405\n0.005862751044332981\n0.005846405401825905\n0.004170941188931465\n0.005814946722239256\n0.005799829959869385\n0.005783572793006897\n0.005765020847320557\n0.005747655406594276\n0.005731471348553896\n0.005714156664907932\n0.0022454489953815937\n0.005677306093275547\n0.005662369541823864\n0.0056451596319675446\n0.0030183233320713043\n0.00560967531055212\n0.005593686830252409\n0.005575442221015692\n0.005292020738124847\n0.005545858293771744\n0.0041483063250780106\n0.005514086689800024\n0.005498235113918781\n0.005483536049723625\n0.005464344285428524\n0.005445186048746109\n0.005424937233328819\n0.0054069701582193375\n0.005393514409661293\n0.005377837456762791\n0.005361065734177828\n0.00534208957105875\n0.005325373727828264\n0.0053086839616298676\n0.005293130874633789\n0.0052753835916519165\n0.005259878933429718\n0.005242187529802322\n0.0052278353832662106\n0.005211299285292625\n0.005196989513933659\n0.004304037429392338\n0.0051640416495501995\n0.0051497966051101685\n0.005134478211402893\n0.00511699914932251\n0.005084306001663208\n0.004346184432506561\n0.00507560558617115\n0.005060398019850254\n0.005041962023824453\n0.005021397024393082\n0.00500843022018671\n0.004994401708245277\n0.004981469828635454\n0.0049664038233459\n0.004949213471263647\n0.004937412217259407\n0.004920272156596184\n0.004899957217276096\n0.004882882349193096\n0.004864772781729698\n0.004856262356042862\n0.004844572860747576\n0.003314467379823327\n0.004814880900084972\n0.004796897992491722\n0.004781058989465237\n0.0047652460634708405\n0.004744202829897404\n0.004733698442578316\n0.004721108824014664\n0.004234248772263527\n0.004694934468716383\n0.004680308513343334\n0.004663621075451374\n0.004651125054806471\n0.0015860656276345253\n0.0046230703592300415\n0.004253135062754154\n0.004593031946569681\n0.002092682756483555\n0.004564122296869755\n0.004550730809569359\n0.004535303916782141\n0.004520928952842951\n0.004505552351474762\n0.00449020229279995\n0.004194625653326511\n0.004460599273443222\n0.004446343518793583\n0.00443312618881464\n0.004418914206326008\n0.004404725041240454\n0.00388815114274621\n0.00438146386295557\n0.003719704458490014\n0.004356249701231718\n0.00434216158464551\n0.004327092319726944\n0.0043100458569824696\n0.003879592753946781\n0.004288035444915295\n0.004279047250747681\n0.0042670778930187225\n0.004252139944583178\n0.004241202026605606\n0.0042253173887729645\n0.004087598063051701\n0.004000268876552582\n0.0041748839430511\n0.004162076860666275\n0.0041483063250780106\n0.004135540220886469\n0.004120834171772003\n0.0041120233945548534\n0.004097359254956245\n0.004084671847522259\n0.004072003997862339\n0.004059355705976486\n0.004043815657496452\n0.004034118261188269\n0.004022496752440929\n0.004007027484476566\n0.003989660181105137\n0.003979065455496311\n0.003968484699726105\n0.003956958185881376\n0.0021003680303692818\n0.003932998515665531\n0.003922479227185249\n0.0039100656285882\n0.003896719077602029\n0.0038824444636702538\n0.0038681961596012115\n0.0038530267775058746\n0.003840723540633917\n0.003827495966106653\n0.003817118937149644\n0.0038048732094466686\n0.003748609684407711\n0.0037766890600323677\n0.003716913051903248\n0.0037514129653573036\n0.003161659697070718\n0.0037308803293854\n0.003719704458490014\n0.00370668713003397\n0.003695547580718994\n0.0036816466599702835\n0.003668696153908968\n0.0036557684652507305\n0.0036465483717620373\n0.0036364197731018066\n0.002300737425684929\n0.003611618187278509\n0.0035987915471196175\n0.0035878154449164867\n0.0035759436432272196\n0.0035622697323560715\n0.0035504403058439493\n0.0035377228632569313\n0.0035250282380729914\n0.0035168793983757496\n0.003506029024720192\n0.0034906864166259766\n0.0034807766787707806\n0.0034708811435848475\n0.0034609995782375336\n0.0034493396524339914\n0.0034403838217258453\n0.0034287585876882076\n0.003415369428694248\n0.003404676914215088\n0.0033940011635422707\n0.003381567308679223\n0.0033682705834507942\n0.003354116342961788\n0.0033488161861896515\n0.0033391101751476526\n0.003325897268950939\n0.003315345849841833\n0.003303057048469782\n0.0026246546767652035\n0.002537831198424101\n0.0032724349293857813\n0.0032602259889245033\n0.003127433592453599\n0.00323500856757164\n0.0032254690304398537\n0.0028229141607880592\n0.003204704262316227\n0.0031934846192598343\n0.003181423991918564\n0.0031702453270554543\n0.0031608017161488533\n0.0031496593728661537\n0.003140246495604515\n0.0031291404739022255\n0.003118906170129776\n0.0031103903893381357\n0.0030993372201919556\n0.003088303841650486\n0.0030755973421037197\n0.0030662959907203913\n0.0030578523874282837\n0.0030477354303002357\n0.0030384762212634087\n0.003027551807463169\n0.003014971036463976\n0.0030040889978408813\n0.0029957315418869257\n0.00298571796156466\n0.0028603302780538797\n0.0029649101197719574\n0.002954118885099888\n0.0029433472082018852\n0.0029317690059542656\n0.0029226879123598337\n0.0029127972666174173\n0.0029021012596786022\n0.0028914250433444977\n0.0028832259122282267\n0.0028725843876600266\n0.0028627789579331875\n0.0028529902920126915\n0.002844032132998109\n0.002832651138305664\n0.0028245358262211084\n0.0025155881885439157\n0.002806723117828369\n0.001925160177052021\n0.002788160927593708\n0.002779305214062333\n0.0027704634703695774\n0.0027616359293460846\n0.0024517124984413385\n0.002744822297245264\n0.002735237590968609\n0.002724873134866357\n0.0027137333527207375\n0.0027018231339752674\n0.002692313864827156\n0.002686774590983987\n0.0020124949514865875\n0.002671767957508564\n0.0026607373729348183\n0.00264658872038126\n0.002638744655996561\n0.002631695009768009\n0.0026238730642944574\n0.0026160627603530884\n0.002608264097943902\n0.002599698957055807\n0.00258959480561316\n0.002581835724413395\n0.002572540193796158\n0.0025617165956646204\n0.0025516864843666553\n0.0025432149413973093\n0.0025332211516797543\n0.0025224806740880013\n0.0022159016225486994\n0.0025102337822318077\n0.0025033580604940653\n0.00249268114566803\n0.0024865902960300446\n0.0024789872113615274\n0.0013199544046074152\n0.0024615442380309105\n0.0016080178320407867\n0.0024419003166258335\n0.0024313554167747498\n0.0019311904907226562\n0.002419332042336464\n0.0024133315309882164\n0.0024058413691818714\n0.002397615695372224\n0.002387912478297949\n0.0023782290518283844\n0.0023670801892876625\n0.0023633698001503944\n0.0023559576366096735\n0.002345600165426731\n0.0023330538533627987\n0.002322746906429529\n0.002017974853515625\n0.002310995478183031\n0.002302933717146516\n0.002293424215167761\n0.0022868523374199867\n0.0022802897728979588\n0.0022722817957401276\n0.002263561822474003\n0.001781330443918705\n0.0022454489953815937\n0.002236059168353677\n0.002225968986749649\n0.0022194944322109222\n0.002211594022810459\n0.002205140423029661\n0.0021979808807373047\n0.002189404796808958\n0.0021801330149173737\n0.002171591855585575\n0.002165196929126978\n0.0021595205180346966\n0.00121725769713521\n0.0021453620865941048\n0.002138300333172083\n0.0010243398137390614\n0.002122102538123727\n0.0021157809533178806\n0.0021080675069242716\n0.002099668839946389\n0.0020919847302138805\n0.0020836181938648224\n0.0020745734218508005\n0.0020704055204987526\n0.002064161468297243\n0.002056542783975601\n0.0020489380694925785\n0.0014604348689317703\n0.002033771015703678\n0.0020255218259990215\n0.002017974853515625\n0.0020104418508708477\n0.0020042890682816505\n0.0019974636379629374\n0.0019892884884029627\n0.0019831680692732334\n0.001975700492039323\n0.0019682468846440315\n0.0019614831544458866\n0.001954056555405259\n0.0019479906186461449\n0.0019405896309763193\n0.001933202613145113\n0.001925160177052021\n0.0019184709526598454\n0.0019117933697998524\n0.001905127428472042\n0.0018984731286764145\n0.0018918304704129696\n0.0018845370505005121\n0.0018792415503412485\n0.0018726326525211334\n0.0018647173419594765\n0.001856161281466484\n0.0018502494785934687\n0.0018240886274725199\n0.0014348900876939297\n0.0018325704149901867\n0.0006334973149932921\n0.0018214827869087458\n0.00181562639772892\n0.00011133315274491906\n0.0014645196497440338\n0.0018026460893452168\n0.0017981140408664942\n0.001102447509765625\n0.001785841304808855\n0.0017787553369998932\n7.88271427154541e-06\n0.0017627030611038208\n0.0017575817182660103\n0.00011441321112215519\n0.0017460859380662441\n0.0017397156916558743\n0.0015492131933569908\n0.001730182208120823\n0.0017251083627343178\n0.0017175115644931793\n0.001709300559014082\n0.0017029978334903717\n0.0016985929105430841\n0.0016929376870393753\n0.0016879187896847725\n0.001682281494140625\n0.0016760288272053003\n0.0016697878018021584\n0.0016629360616207123\n0.001656719483435154\n0.0016498947516083717\n0.001646178076043725\n0.0016412290278822184\n0.0016350531950592995\n0.001627657562494278\n0.0016208929009735584\n0.001225790474563837\n0.0016104662790894508\n0.0016049598343670368\n0.0016012941487133503\n0.0005461024702526629\n0.0015903222374618053\n0.0015830285847187042\n0.0015787817537784576\n0.0015739351511001587\n0.0015684915706515312\n0.0006832110229879618\n0.0015564286150038242\n0.0015498138964176178\n0.0015426138415932655\n0.0015354305505752563\n0.0015324424020946026\n0.001527667511254549\n0.0013612965121865273\n0.0015163570642471313\n0.001510421046987176\n0.0015056806150823832\n0.0011745179072022438\n0.0014932723715901375\n0.0014873817563056946\n0.0014826776459813118\n0.001477980986237526\n0.001472120638936758\n0.0014662719331681728\n0.0014592688530683517\n0.001452864147722721\n0.0010958709754049778\n0.0014464734122157097\n0.00144183449447155\n0.001436624675989151\n0.001429692842066288\n0.0014233533293008804\n0.0014176024124026299\n0.0014101436827331781\n0.001406707800924778\n0.0014027045108377934\n0.0010439674369990826\n0.0013935756869614124\n0.0013884538784623146\n0.0013827739749103785\n0.001377105712890625\n0.0013714490924030542\n0.0013669321779161692\n0.0013607335276901722\n0.0006868052878417075\n0.0013500601053237915\n0.001345018856227398\n0.0013394285924732685\n0.001276522409170866\n0.0013277269899845123\n0.001324948389083147\n0.0013199544046074152\n0.0013133103493601084\n0.0013099946081638336\n0.0013050287961959839\n0.001300072530284524\n0.00045830593444406986\n0.0012918328866362572\n0.0012874491512775421\n0.0012825264129787683\n0.0012776129879057407\n0.001272709108889103\n0.0012672713492065668\n0.0012623872607946396\n0.0004284298629499972\n0.0012537278234958649\n0.0012483308091759682\n0.0012434835080057383\n0.0012386455200612545\n0.0012332811020314693\n0.0012279283255338669\n0.001225256361067295\n0.001221520360559225\n0.0012161931954324245\n0.0012119398452341557\n0.001206633634865284\n0.001200810307636857\n0.0011960561387240887\n0.0009585267398506403\n0.0011886795982718468\n0.0011844746768474579\n0.001179753104224801\n0.0011750408448278904\n0.0011698161251842976\n0.0011646030470728874\n0.0011599212884902954\n0.000763619551435113\n0.0011542118154466152\n0.0011505859438329935\n0.0011459323577582836\n0.0011402575764805079\n0.0011361392680555582\n0.0011325418017804623\n0.0011284374631941319\n0.0011238290462642908\n0.0011187195777893066\n0.0011131125502288342\n0.0011075197253376245\n0.0011029541492462158\n0.0011009280569851398\n0.0010973869357258081\n0.0010918336920440197\n0.0010857917368412018\n0.001081773079931736\n0.0010787639766931534\n0.0010747583582997322\n0.0010697618126869202\n0.0010662712156772614\n0.0007142667309381068\n0.0010593070182949305\n0.0010553377214819193\n0.0010508811101317406\n0.0010459404438734055\n0.0010434745345264673\n0.0010400270111858845\n0.001034621149301529\n0.0010258054826408625\n0.0008255450520664454\n0.0010218995157629251\n0.001018974930047989\n0.001014595851302147\n0.0010107113048434258\n9.943317854776978e-05\n0.001003931276500225\n0.0007261419668793678\n0.0009971740655601025\n0.0009938040748238564\n0.0009894794784486294\n0.0009851644281297922\n0.0009803809225559235\n0.0009765625\n0.0009717999491840601\n0.0009679982904344797\n0.0009642040822654963\n0.0009599445038475096\n0.0009571100235916674\n0.0009523952030576766\n0.0009486317285336554\n0.0008985540480352938\n0.0009415952954441309\n0.0009378532413393259\n0.0009350515902042389\n0.0009308569715358317\n0.0009266717825084925\n0.0009229595307260752\n0.0009187921532429755\n0.0009150957339443266\n0.0009127892553806305\n0.0009091049432754517\n0.0009045100305229425\n0.0009003845625557005\n0.0008976394892670214\n0.0008944422006607056\n0.0008907951414585114\n0.0008876100764609873\n0.0008844307158142328\n0.0008798986673355103\n0.0008762814104557037\n0.0008726716041564941\n0.0008686194778420031\n0.0008645767811685801\n0.0008618868887424469\n0.000858306884765625\n0.0008547343313694\n0.0008516144589520991\n0.0008476115763187408\n0.0008440613746643066\n0.0008405186235904694\n0.0008374248282052577\n0.0008338960469700396\n0.0007619338575750589\n0.0008272996637970209\n0.000823792302981019\n0.0008216039859689772\n0.0008181087323464453\n0.0008141854777932167\n0.0008111405768431723\n0.0008076676749624312\n0.000804202223662287\n0.0008011760655790567\n0.0007977245841175318\n0.000793850573245436\n0.0007908439729362726\n0.0007878430769778788\n0.0007844204665161669\n0.000781005306635052\n0.0007775975973345339\n0.0007746219635009766\n0.0007720759604126215\n0.0007691109203733504\n0.0007648850441910326\n0.0007623551064170897\n0.0007594088092446327\n0.0007568879518657923\n0.0007535333279520273\n0.0007501861546188593\n0.0007468464318662882\n0.000743514159694314\n0.0007401893381029367\n0.0007381151081062853\n0.0007356298738159239\n0.0007323227473534644\n0.0007286111358553171\n0.000725730846170336\n0.0006216475740075111\n0.0007199873798526824\n0.0007167156436480582\n0.0007130438461899757\n0.0007101945229806006\n0.000707350904121995\n0.000527717696968466\n0.0007024894002825022\n0.0007000649347901344\n0.0006780359544791281\n0.0006936201825737953\n0.0006912110839039087\n0.0006880054716020823\n0.00068520667264238\n0.0006820150301791728\n0.0006788308382965624\n0.0006760507822036743\n0.0006728805601596832\n0.0006712982431054115\n0.0004296941333450377\n0.0006665624678134918\n0.0005414767656475306\n0.0006602741777896881\n0.000656750111375004\n0.0006540156900882721\n0.0006512869731523097\n0.000648952613119036\n0.0006458466523326933\n0.0006431350484490395\n0.0006404291489161551\n0.0006377289537340403\n0.0006346500013023615\n0.0006323456764221191\n0.0006300455424934626\n0.0006277495995163918\n0.0006254578474909067\n0.0006224086973816156\n0.0006197468028403819\n0.00061671162256971\n0.0006140619516372681\n0.0006114179850555956\n0.0006087797228246927\n0.0006061471649445593\n0.0004069143906235695\n0.0006023962632752955\n0.0005997775588184595\n0.000597164558712393\n0.0005949293845333159\n0.0005923269782215357\n0.000589730276260525\n0.0005871392786502838\n0.0005845539853908122\n0.00058197439648211\n0.00038266275078058243\n0.0005768323317170143\n0.0005742698558606207\n0.0005291207344271243\n0.0005691620172001421\n0.0005669799284078181\n0.0005644394550472498\n0.0005622664466500282\n0.0005600976292043924\n0.0005579330027103424\n0.0005554129020310938\n0.0004504995886236429\n0.000550747849047184\n0.0005482440465129912\n0.0005457459483295679\n0.0005393484607338905\n0.0005414767656475306\n0.0005393484607338905\n0.0005375780747272074\n0.0005351044237613678\n0.0004807880613952875\n0.0005301742348819971\n0.0005280682817101479\n0.0005263165221549571\n0.0005242182523943484\n0.0005217755679041147\n0.0005196863785386086\n0.000483803974930197\n0.0005158670828677714\n0.0004072222509421408\n0.0005113715305924416\n0.0005086148157715797\n0.0005058655515313148\n0.000503466057125479\n0.0005014138878323138\n0.0005000481032766402\n0.0003734655329026282\n0.0004966417909599841\n0.0004942642990499735\n0.0004912159056402743\n0.0004891888820566237\n0.0004871660494245589\n0.00048514740774407983\n0.00048279762268066406\n0.00048112269723787904\n0.0004791166284121573\n8.270516991615295e-05\n0.0004751170636154711\n0.0004734555259346962\n0.0003761241678148508\n0.0004694797098636627\n0.000467168225441128\n0.00046486244536936283\n0.00046256236964836717\n0.0004605954163707793\n0.0004579793312586844\n0.00045700022019445896\n0.00045537069672718644\n0.0004527695127762854\n0.0004504995886236429\n0.0004488817066885531\n0.00023610168136656284\n0.00044533261097967625\n0.0004437240422703326\n0.00044179760152474046\n0.00044019543565809727\n0.00043827667832374573\n0.00043636211194097996\n0.00043445173650979996\n0.0004325455520302057\n0.00043064355850219727\n0.0004281140863895416\n0.0004265369498170912\n0.00038117176154628396\n0.0004227636964060366\n0.00042088335612788796\n0.00039318116614595056\n0.0004168236628174782\n0.0004155784845352173\n0.00041402463102713227\n0.00041216384852305055\n0.000409998232498765\n0.00040783832082524896\n0.00040660664672032\n0.00027107098139822483\n0.00040322914719581604\n0.00040139281190931797\n0.00039925571763888\n0.0003971243277192116\n0.00039560539880767465\n0.0003940893802791834\n0.0003925762721337378\n0.0003904628101736307\n0.00038895668694749475\n0.0003868530038744211\n0.000385054387152195\n0.00038355874130502343\n0.00038206600584089756\n0.0003802785649895668\n0.0003787922323681414\n0.0003773088101297617\n0.0003755325451493263\n0.0003734655329026282\n0.00037140422500669956\n0.0003699353546835482\n0.0003690554294735193\n0.0003672987222671509\n0.0003655462060123682\n0.0003637978807091713\n0.0003617634647525847\n0.0003600242198444903\n0.00035886705154553056\n0.00035771174589172006\n0.0003559822798706591\n0.00016506854444742203\n0.00035282247699797153\n0.00035110488533973694\n0.0003493914846330881\n0.00034768227487802505\n0.00034626113483682275\n0.00034427642822265625\n0.00034286227310076356\n0.0003411691286601126\n0.0003397613763809204\n0.0003383565344847739\n0.0003366745659150183\n0.00033499678829684854\n0.0003336018417030573\n0.0003322098054923117\n0.0003305432037450373\n0.0003291575703769922\n0.0003277748473919928\n0.00032639503479003906\n0.000325018132571131\n0.0003230953589081764\n0.00032172544160857797\n0.0003203584346920252\n0.00018483982421457767\n0.000318449514452368\n0.00031708949245512486\n0.0003149195108562708\n0.0003138373140245676\n0.00031275697983801365\n0.0003114091814495623\n0.000309795665089041\n0.00030818633968010545\n0.0003068484365940094\n0.00030578020960092545\n0.0003041813615709543\n0.00030258670449256897\n0.00030073156813159585\n0.00024081417359411716\n0.0002211076789535582\n0.00016448093811050057\n0.0002959874109365046\n0.00029467628337442875\n0.00029336806619539857\n0.00023892358876764774\n0.00029180204728618264\n0.00029050023294985294\n0.0002889418974518776\n0.0002866122522391379\n0.00028557988116517663\n0.00028429203666746616\n0.00028300710255280137\n0.00028172507882118225\n1.017027534544468e-05\n0.0002799351350404322\n0.00027891487115994096\n0.0002773879677988589\n0.0002758652553893626\n1.7826096154749393e-06\n0.00027384149143472314\n0.00027283240342512727\n0.00027157366275787354\n0.00027006701566278934\n0.0002688146778382361\n0.0002675652503967285\n0.0002660697791725397\n0.0002648267545737326\n0.0002638344303704798\n0.0002211076789535582\n0.00026210234500467777\n0.0002606222406029701\n0.00025914632715284824\n0.0002579196006990969\n0.00025669578462839127\n0.0002554748789407313\n0.0002545002498663962\n0.0002532845828682184\n0.0002518296241760254\n0.00023892358876764774\n0.00025013746926561\n0.00024893227964639664\n0.0002472499036230147\n0.00024629110703244805\n0.00024509523063898087\n0.000244140625\n0.0002427122090011835\n0.00024176225997507572\n0.0002405774430371821\n0.0002041999832727015\n0.0002386877895332873\n0.0002375105395913124\n0.0002363362000323832\n0.00023516477085649967\n0.00023399625206366181\n0.00023283064365386963\n0.00023190025240182877\n0.0002307398826815188\n0.0002298136823810637\n0.00022865855135023594\n0.00022750633070245385\n0.00022658664966002107\n0.0002254396677017212\n0.00022406713105738163\n0.0002233824343420565\n0.00012137088924646378\n0.00022156169870868325\n0.00022065412485972047\n0.00021974841365590692\n0.00021861889399588108\n0.00021749228471890092\n0.00021659309277310967\n0.00021547172218561172\n0.00021435326198115945\n0.000213014951441437\n0.0002119028940796852\n0.00021101534366607666\n0.00021035090321674943\n0.00020946661243215203\n0.00020814366871491075\n0.00020682491594925523\n0.00020594807574525476\n0.0002050730981864035\n0.00020398199558258057\n0.00015904713654890656\n8.820561924949288e-05\n0.00020159181440249085\n0.0002009423915296793\n0.00017989450134336948\n0.00019900040933862329\n0.0001981403329409659\n0.00019728211918845773\n0.0001964257680810988\n5.031237378716469e-06\n0.00019471865380182862\n0.00019386789062991738\n0.00019280705600976944\n0.00019196048378944397\n0.00019090488785877824\n0.00019006250659003854\n0.00018901214934885502\n0.00018796470249071717\n0.000186920166015625\n0.00018650316633284092\n0.0001858785399235785\n0.00018483982421457767\n0.00015598308527842164\n0.00018339051166549325\n0.00018256489420309663\n0.00018174113938584924\n0.00018091924721375108\n0.00017989450134336948\n0.0001790768001228571\n0.00017846474656835198\n0.00017744698561728\n0.0001764321350492537\n0.00017542019486427307\n0.00017461273819208145\n0.00017360603669658303\n0.0001726022455841303\n0.00017180130816996098\n0.0001710022334009409\n0.000153326487634331\n0.0001696083345450461\n0.0001688143820501864\n0.00016782456077635288\n0.00016703479923307896\n0.0001664437004365027\n0.00016565719852223992\n0.00016467669047415257\n0.00016369909280911088\n0.00016272440552711487\n0.00015238323248922825\n0.00016117095947265625\n8.834898471832275e-05\n0.0001603970304131508\n0.00015981780597940087\n0.00015885476022958755\n0.00015751138562336564\n0.00015693739987909794\n0.00015617371536791325\n0.00015541189350187778\n0.0001167096197605133\n0.0001542726531624794\n0.00015389383770525455\n0.00015313760377466679\n0.00015257165068760514\n0.00015163072384893894\n0.00015088007785379887\n0.00014994438970461488\n0.00014882540563121438\n0.00014789612032473087\n0.00014715478755533695\n0.00014641531743109226\n0.00014586193719878793\n0.0001451257267035544\n0.0001443913788534701\n0.000143658893648535\n0.00014274590648710728\n0.00014183582970872521\n0.00014110986376181245\n0.00014074757928028703\n3.0595401767641306e-05\n0.0001396635198034346\n0.00013912306167185307\n0.00013840408064424992\n0.000137686962261796\n0.0001369717065244913\n0.00013608025619760156\n0.00013554678298532963\n0.00013483711518347263\n0.0001339526497758925\n0.00013342336751520634\n0.00013271928764879704\n7.017538882791996e-05\n0.00013131671585142612\n0.00013096723705530167\n0.00013044389197602868\n0.0001297477283515036\n0.00012888014316558838\n0.00012836098903790116\n0.00012767041334882379\n0.00012680981308221817\n0.00012612342834472656\n0.00012560986215248704\n0.00012492673704400659\n0.00012390554184094071\n0.00012356607476249337\n0.0001018825569190085\n0.00012238160707056522\n0.00012187572428956628\n0.0001212028437294066\n0.00012069940567016602\n0.00012002978473901749\n0.00011936202645301819\n0.00011869613081216812\n0.00011819793144240975\n0.00011770077981054783\n0.00011720467591658235\n0.00011638016439974308\n0.00011572265066206455\n9.00784507393837e-05\n0.0001149033778347075\n0.00011441321112215519\n0.00011376128531992435\n0.00011327356332913041\n0.00011246302165091038\n0.00011181668378412724\n0.00011133315274491906\n0.00011085066944360733\n0.00011020898818969727\n0.0001097289496101439\n0.00010909052798524499\n9.10950475372374e-05\n0.00010781927267089486\n0.00010734447278082371\n0.00010671303607523441\n0.00010624068090692163\n0.00010561250383034348\n0.00010498618939891458\n0.00010467373067513108\n0.00010420591570436954\n0.0001035837922245264\n0.00010311842197552323\n0.0001026540994644165\n0.00010219082469120622\n0.00010157475480809808\n0.00010096054757013917\n0.00010019540786743164\n9.761570254340768e-05\n9.943317854776978e-05\n9.89772379398346e-05\n9.837094694375992e-05\n9.776651859283447e-05\n9.731441969051957e-05\n9.686336852610111e-05\n9.64133650995791e-05\n9.596440941095352e-05\n9.551650146022439e-05\n9.00784507393837e-05\n9.447545744478703e-05\n9.40310419537127e-05\n9.358767420053482e-05\n9.299814701080322e-05\n9.241048246622086e-05\n9.211734868586063e-05\n9.1678521130234e-05\n9.10950475372374e-05\n9.036832489073277e-05\n9.00784507393837e-05\n8.96445126272738e-05\n8.921162225306034e-05\n8.877977961674333e-05\n8.849246660247445e-05\n8.806237019598484e-05\n8.749053813517094e-05\n8.706288645043969e-05\n8.649431401863694e-05\n8.606910705566406e-05\n8.564494783058763e-05\n8.522183634340763e-05\n6.527878576889634e-05\n8.409866131842136e-05\n8.353986777365208e-05\n8.31219949759543e-05\n8.284399518743157e-05\n8.228939259424806e-05\n8.18746630102396e-05\n8.146098116412759e-05\n8.104834705591202e-05\n8.063676068559289e-05\n8.02262220531702e-05\n7.968046702444553e-05\n7.927237311378121e-05\n7.886532694101334e-05\n7.84593285061419e-05\n7.791962707415223e-05\n7.751607336103916e-05\n7.711356738582253e-05\n7.684581214562058e-05\n7.644505240023136e-05\n7.591233588755131e-05\n7.564667612314224e-05\n7.524905959144235e-05\n7.485249079763889e-05\n7.432536222040653e-05\n7.393123814836144e-05\n7.35381618142128e-05\n7.31461332179606e-05\n7.288536289706826e-05\n7.249508053064346e-05\n7.21058459021151e-05\n7.158849621191621e-05\n7.120170630514622e-05\n7.081596413627267e-05\n7.043126970529556e-05\n7.00476230122149e-05\n6.966502405703068e-05\n6.92834728397429e-05\n6.902968743816018e-05\n6.86498824506998e-05\n6.827112520113587e-05\n5.5106880608946085e-05\n5.20909670740366e-05\n6.714113987982273e-05\n2.459273673593998e-05\n6.651744479313493e-05\n6.62687816657126e-05\n6.602058419957757e-05\n6.564916111528873e-05\n6.527878576889634e-05\n6.490945816040039e-05\n4.498520866036415e-06\n6.429624045267701e-05\n6.392970681190491e-05\n6.356422090902925e-05\n6.319978274405003e-05\n6.283639231696725e-05\n6.247404962778091e-05\n5.5788608733564615e-05\n6.199255585670471e-05\n6.175250746309757e-05\n6.139330798760056e-05\n6.091600516811013e-05\n6.055925041437149e-05\n6.03219959884882e-05\n6.008520722389221e-05\n5.9730897191911936e-05\n5.93776348978281e-05\n5.902542034164071e-05\n5.867425352334976e-05\n5.8324134442955256e-05\n4.4565240386873484e-05\n3.0174851417541504e-05\n5.7742930948734283e-05\n5.7395605836063623e-05\n5.7049328461289406e-05\n5.658925510942936e-05\n5.63599169254303e-05\n3.0426832381635904e-05\n5.567469634115696e-05\n1.0121671948581934e-05\n5.5220210924744606e-05\n5.499366670846939e-05\n5.465472349897027e-05\n5.4541975259780884e-05\n5.4204429034143686e-05\n5.386793054640293e-05\n5.353247979655862e-05\n5.330942803993821e-05\n5.29757235199213e-05\n5.2643066737800837e-05\n5.231145769357681e-05\n5.20909670740366e-05\n5.165138281881809e-05\n3.762636333703995e-05\n5.14322891831398e-05\n5.121366120874882e-05\n5.0886592362076044e-05\n5.045213038101792e-05\n3.4511322155594826e-05\n5.0019531045109034e-05\n4.969630390405655e-05\n4.937412450090051e-05\n4.915992030873895e-05\n4.8946181777864695e-05\n4.862644709646702e-05\n4.8413872718811035e-05\n4.5961351133883e-06\n4.777894355356693e-05\n4.631374031305313e-05\n4.7357985749840736e-05\n3.4511322155594826e-05\n4.68344078399241e-05\n4.652165807783604e-05\n4.631374031305313e-05\n4.610628820955753e-05\n4.589930176734924e-05\n4.558969521895051e-05\n4.5178516302257776e-05\n4.497362533584237e-05\n4.4769200030714273e-05\n4.4565240386873484e-05\n4.436174640432e-05\n4.405737854540348e-05\n4.385504871606827e-05\n4.355242708697915e-05\n4.335126141086221e-05\n4.3050386011600494e-05\n4.2950327042490244e-05\n4.2650848627090454e-05\n3.9617589209228754e-05\n4.2253173887729645e-05\n4.195614019408822e-05\n4.175869980826974e-05\n4.146341234445572e-05\n4.126713611185551e-05\n4.10713255405426e-05\n4.077848279848695e-05\n4.0486687794327736e-05\n4.029273986816406e-05\n4.0002691093832254e-05\n3.980990732088685e-05\n3.9617589209228754e-05\n3.9425736758857965e-05\n3.9138831198215485e-05\n3.8948142901062965e-05\n3.866298357024789e-05\n3.847345942631364e-05\n3.8284400943666697e-05\n3.469083458185196e-05\n3.7907680962234735e-05\n3.762636333703995e-05\n3.743940033018589e-05\n3.725290298461914e-05\n3.70668713003397e-05\n2.0124949514865875e-05\n3.6603829357773066e-05\n3.64194274879992e-05\n3.6235491279512644e-05\n3.596046008169651e-05\n3.577768802642822e-05\n3.0091090593487024e-05\n3.541354089975357e-05\n3.5232165828347206e-05\n3.505125641822815e-05\n3.48708126693964e-05\n3.469083458185196e-05\n3.4421740565449e-05\n3.424292663112283e-05\n3.4064578358083963e-05\n3.388669574633241e-05\n3.3620744943618774e-05\n3.344402648508549e-05\n3.326777368783951e-05\n3.3091986551880836e-05\n3.291666507720947e-05\n3.2741809263825417e-05\n2.1588115487247705e-05\n3.230670699849725e-05\n3.213348099961877e-05\n3.19607206620276e-05\n3.1788425985723734e-05\n2.74722115136683e-05\n3.1359726563096046e-05\n3.118906170129776e-05\n8.990115020424128e-06\n3.101886250078678e-05\n3.084912896156311e-05\n3.067986108362675e-05\n3.0426832381635904e-05\n3.025872865691781e-05\n2.9206275939941406e-06\n2.9923918191343546e-05\n2.9840506613254547e-05\n2.967403270304203e-05\n2.9425194952636957e-05\n2.925988519564271e-05\n2.901279367506504e-05\n2.8930662665516138e-05\n2.8766749892383814e-05\n2.8603302780538797e-05\n2.844032132998109e-05\n2.8277805540710688e-05\n2.8115755412727594e-05\n2.795417094603181e-05\n2.779305214062333e-05\n2.763239899650216e-05\n2.74722115136683e-05\n2.7312489692121744e-05\n2.7153233531862497e-05\n2.6994443032890558e-05\n1.8713122699409723e-05\n1.0024814400821924e-05\n2.6678259018808603e-05\n2.6599504053592682e-05\n2.644234336912632e-05\n2.620747545734048e-05\n2.6051478926092386e-05\n2.581835724413395e-05\n2.5663524866104126e-05\n2.033083001151681e-05\n2.550915814936161e-05\n2.5355257093906403e-05\n4.434026777744293e-06\n2.1234736777842045e-06\n4.71482053399086e-07\n2.4972541723400354e-05\n2.691522240638733e-07\n2.474430948495865e-05\n5.93776348978281e-07\n2.4517124984413385e-05\n2.4366250727325678e-05\n2.4290988221764565e-05\n2.4065899197012186e-05\n2.3916421923786402e-05\n2.3767410311847925e-05\n2.3618864361196756e-05\n8.990115020424128e-06\n2.3544766008853912e-05\n2.3396918550133705e-05\n2.3323169443756342e-05\n2.3249536752700806e-05\n2.3102620616555214e-05\n2.295617014169693e-05\n2.2810185328125954e-05\n2.04686657525599e-05\n2.2519612684845924e-05\n1.5139812603592873e-05\n2.2159016225486994e-05\n2.208724617958069e-05\n2.194405533373356e-05\n2.1801330149173737e-05\n2.1235086023807526e-05\n2.1588115487247705e-05\n2.1446554455906153e-05\n2.1305459085851908e-05\n2.1235086023807526e-05\n2.1094689145684242e-05\n2.0954757928848267e-05\n2.088496694341302e-05\n2.0745734218508005e-05\n2.06069671548903e-05\n2.04686657525599e-05\n2.033083001151681e-05\n1.5021301805973053e-05\n2.0124949514865875e-05\n1.9988277927041054e-05\n1.985207200050354e-05\n1.9716331735253334e-05\n1.964863622561097e-05\n1.9513594452291727e-05\n1.937901834025979e-05\n1.9311904907226562e-05\n1.9178027287125587e-05\n1.2748874723911285e-05\n1.8978083971887827e-05\n1.884537050500512e-05\n1.8713122699409723e-05\n1.8647173419594765e-05\n1.851562410593033e-05\n1.83845404535532e-05\n1.825392246246338e-05\n1.818878808990121e-05\n1.805886859074235e-05\n1.799408346414566e-05\n1.7864862456917763e-05\n1.3243930879980326e-05\n1.760781742632389e-05\n1.754384720697999e-05\n7.79726542532444e-06\n1.735263504087925e-05\n1.722574234008789e-05\n1.716247061267495e-05\n1.7036276403814554e-05\n1.6973353922367096e-05\n1.684785820543766e-05\n1.6785284969955683e-05\n1.666048774495721e-05\n1.6536156181246042e-05\n1.6412290278822184e-05\n1.6288890037685633e-05\n1.6227364540100098e-05\n1.610466279089451e-05\n1.5982426702976227e-05\n1.5860656276345253e-05\n1.5799945686012506e-05\n1.5739351511001587e-05\n1.561851240694523e-05\n1.5498138964176178e-05\n1.5438126865774393e-05\n1.5318451914936304e-05\n1.52587890625e-05\n1.5139812603592873e-05\n1.508049899712205e-05\n1.5021301805973053e-05\n1.4903256669640541e-05\n1.4785677194595337e-05\n1.466856338083744e-05\n1.4610181096941233e-05\n1.4493765775114298e-05\n1.4435732737183571e-05\n1.437781611457467e-05\n1.4204764738678932e-05\n1.414731377735734e-05\n1.4089979231357574e-05\n1.3975659385323524e-05\n1.391867408528924e-05\n1.3805052731186152e-05\n1.3748416677117348e-05\n1.4530960470438004e-06\n1.3579207006841898e-05\n1.3523036614060402e-05\n1.341104507446289e-05\n1.3299519196152687e-05\n1.3243930879980326e-05\n1.3133103493601084e-05\n1.3077864423394203e-05\n1.0316784027963877e-05\n1.2967735528945923e-05\n3.5800039768218994e-06\n1.285807229578495e-05\n1.2748874723911285e-05\n3.995606675744057e-06\n1.258595148101449e-05\n1.8235878087580204e-06\n1.2477918062359095e-05\n1.2424075976014137e-05\n1.2316741049289703e-05\n1.2263248208910227e-05\n1.2156611774116755e-05\n1.205044100061059e-05\n1.1997530236840248e-05\n1.1944735888391733e-05\n1.1892057955265045e-05\n4.087632987648249e-06\n1.1734722647815943e-05\n1.1682510375976562e-05\n1.163041451945901e-05\n1.1526572052389383e-05\n1.1423195246607065e-05\n8.67270864546299e-06\n1.1320284102112055e-05\n1.126900315284729e-05\n1.1217838618904352e-05\n1.1115858796983957e-05\n1.10650435090065e-05\n1.0963762179017067e-05\n1.091329613700509e-05\n1.0862946510314941e-05\n1.0762596502900124e-05\n1.0662712156772614e-05\n8.89884540811181e-06\n2.494140062481165e-06\n1.0563293471932411e-05\n1.051375875249505e-05\n1.041503855958581e-05\n1.036585308611393e-05\n1.0316784027963877e-05\n1.0218995157629251e-05\n1.017027534544468e-05\n1.0073184967041016e-05\n1.0024814400821924e-05\n9.928422514349222e-06\n9.880401194095612e-06\n9.832496289163828e-06\n9.784707799553871e-06\n9.737035725265741e-06\n9.642040822654963e-06\n9.594717994332314e-06\n9.500421583652496e-06\n9.453448001295328e-06\n9.406590834259987e-06\n9.359850082546473e-06\n9.266717825084925e-06\n3.3248797990381718e-06\n9.174051228910685e-06\n2.2589811123907566e-06\n9.035924449563026e-06\n8.990115020424128e-06\n8.944422006607056e-06\n8.89884540811181e-06\n8.808041457086802e-06\n8.762814104557037e-06\n8.7177031673491e-06\n8.67270864546299e-06\n8.58306884765625e-06\n8.58306884765625e-06\n8.493894711136818e-06\n8.449482265859842e-06\n8.405186235904694e-06\n8.316943421959877e-06\n8.272996637970209e-06\n8.229166269302368e-06\n8.185452315956354e-06\n8.141854777932167e-06\n8.098373655229807e-06\n8.011760655790567e-06\n7.968628779053688e-06\n7.925613317638636e-06\n7.839931640774012e-06\n7.839931640774012e-06\n7.79726542532444e-06\n7.712282240390778e-06\n7.669965270906687e-06\n7.585680577903986e-06\n7.543712854385376e-06\n3.437104169279337e-06\n7.460126653313637e-06\n7.418508175760508e-06\n7.377006113529205e-06\n7.33562046661973e-06\n7.294351235032082e-06\n7.25319841876626e-06\n7.212162017822266e-06\n7.130438461899757e-06\n5.308771505951881e-06\n5.4147676564753056e-06\n7.008726242929697e-06\n6.968388333916664e-06\n6.928166840225458e-06\n6.848073098808527e-06\n4.895264282822609e-06\n1.682201400399208e-06\n6.768445018678904e-06\n6.728805601596832e-06\n2.384185791015625e-07\n1.026783138513565e-07\n6.6498760133981705e-06\n6.61058584228158e-06\n6.571412086486816e-06\n7.043126970529556e-07\n6.454589311033487e-06\n6.4158812165260315e-06\n6.377289537340403e-06\n6.338814273476601e-06\n6.300455424934626e-06\n6.2622129917144775e-06\n2.236105501651764e-06\n6.224086973816156e-06\n6.186077371239662e-06\n6.148184183984995e-06\n6.072747055441141e-06\n5.2736722864210606e-06\n5.997775588184595e-06\n5.9604644775390625e-06\n5.9604644775390625e-06\n5.923269782215357e-06\n5.886191502213478e-06\n5.849229637533426e-06\n5.7756551541388035e-06\n5.7756551541388035e-06\n3.0258670449256897e-06\n5.702546332031488e-06\n5.666166543960571e-06\n3.493914846330881e-06\n5.5577256716787815e-06\n5.521811544895172e-06\n5.48601383343339e-06\n5.450332537293434e-06\n5.450332537293434e-06\n5.4147676564753056e-06\n5.379319190979004e-06\n5.308771505951881e-06\n5.308771505951881e-06\n5.2736722864210606e-06\n5.238689482212067e-06\n5.16907311975956e-06\n5.1344395615160465e-06\n5.09992241859436e-06\n5.09992241859436e-06\n5.065521690994501e-06\n5.031237378716469e-06\n4.026165697723627e-06\n2.0137522369623184e-06\n4.9290829338133335e-06\n4.895264282822609e-06\n4.861562047153711e-06\n4.827976226806641e-06\n4.794506821781397e-06\n4.76115383207798e-06\n4.72791725769639e-06\n4.694797098636627e-06\n4.661793354898691e-06\n4.628906026482582e-06\n4.5961351133883e-06\n4.563480615615845e-06\n2.446176949888468e-06\n4.5309425331652164e-06\n4.498520866036415e-06\n4.466215614229441e-06\n4.434026777744293e-06\n4.401954356580973e-06\n4.369998350739479e-06\n4.338158760219812e-06\n4.306435585021973e-06\n3.784953150898218e-06\n4.243338480591774e-06\n4.211964551359415e-06\n4.180707037448883e-06\n4.149565938860178e-06\n4.1185412555933e-06\n4.087632987648249e-06\n4.056841135025024e-06\n4.026165697723627e-06\n4.026165697723627e-06\n3.995606675744057e-06\n3.965164069086313e-06\n3.934837877750397e-06\n3.904628101736307e-06\n3.8745347410440445e-06\n3.844557795673609e-06\n3.814697265625e-06\n3.784953150898218e-06\n3.784953150898218e-06\n3.7553254514932632e-06\n3.7258141674101353e-06\n3.6964192986488342e-06\n3.66714084520936e-06\n3.637978807091713e-06\n3.637978807091713e-06\n3.6089331842958927e-06\n3.5800039768218994e-06\n3.551191184669733e-06\n3.5224948078393936e-06\n3.493914846330881e-06\n3.4654513001441956e-06\n3.437104169279337e-06\n3.4088734537363052e-06\n3.4088734537363052e-06\n3.3807591535151005e-06\n3.3527612686157227e-06\n3.3248797990381718e-06\n3.297114744782448e-06\n3.297114744782448e-06\n3.269466105848551e-06\n3.2419338822364807e-06\n3.2145180739462376e-06\n3.1872186809778214e-06\n2.7409405447542667e-06\n3.160035703331232e-06\n3.1329691410064697e-06\n3.1060189940035343e-06\n3.079185262322426e-06\n3.079185262322426e-06\n3.0524679459631443e-06\n3.0258670449256897e-06\n2.999382559210062e-06\n2.9730144888162613e-06\n2.9730144888162613e-06\n2.9467628337442875e-06\n2.9206275939941406e-06\n2.8946087695658207e-06\n2.8687063604593277e-06\n1.5276018530130386e-06\n9.686336852610111e-07\n7.697963155806065e-07\n2.8172507882118225e-06\n2.7916976250708103e-06\n2.766260877251625e-06\n2.7409405447542667e-06\n2.7157366275787354e-06\n2.7157366275787354e-06\n2.690649125725031e-06\n2.6656780391931534e-06\n2.6656780391931534e-06\n2.640823367983103e-06\n2.616085112094879e-06\n2.5914632715284824e-06\n2.5914632715284824e-06\n2.5669578462839127e-06\n2.54256883636117e-06\n2.518296241760254e-06\n2.1907035261392593e-06\n2.494140062481165e-06\n2.470100298523903e-06\n2.446176949888468e-06\n2.446176949888468e-06\n2.4223700165748596e-06\n2.3986794985830784e-06\n2.3986794985830784e-06\n4.926696419715881e-07\n2.3516477085649967e-06\n2.3283064365386963e-06\n2.3283064365386963e-06\n2.305081579834223e-06\n2.2819731384515762e-06\n2.2819731384515762e-06\n2.2589811123907566e-06\n2.236105501651764e-06\n2.236105501651764e-06\n2.213346306234598e-06\n2.1907035261392593e-06\n2.1681771613657475e-06\n2.1681771613657475e-06\n2.1457672119140625e-06\n2.1234736777842045e-06\n2.1234736777842045e-06\n2.1012965589761734e-06\n2.0792358554899693e-06\n2.0792358554899693e-06\n2.057291567325592e-06\n2.0354636944830418e-06\n2.0354636944830418e-06\n2.0137522369623184e-06\n1.992157194763422e-06\n1.992157194763422e-06\n1.9706785678863525e-06\n1.94931635633111e-06\n1.94931635633111e-06\n1.9280705600976944e-06\n1.9069411791861057e-06\n1.9069411791861057e-06\n1.885928213596344e-06\n1.8650316633284092e-06\n1.8650316633284092e-06\n1.8442515283823013e-06\n1.8235878087580204e-06\n1.8235878087580204e-06\n1.8030405044555664e-06\n1.8030405044555664e-06\n1.7826096154749393e-06\n1.7622951418161392e-06\n1.742097083479166e-06\n1.742097083479166e-06\n1.7220154404640198e-06\n1.7220154404640198e-06\n1.7020502127707005e-06\n1.682201400399208e-06\n7.968046702444553e-07\n1.6624690033495426e-06\n1.6624690033495426e-06\n1.642853021621704e-06\n1.6233534552156925e-06\n1.6233534552156925e-06\n1.6039703041315079e-06\n1.6039703041315079e-06\n1.5847035683691502e-06\n1.5655532479286194e-06\n1.5655532479286194e-06\n1.5465193428099155e-06\n1.5465193428099155e-06\n1.5276018530130386e-06\n1.5276018530130386e-06\n1.5088007785379887e-06\n1.029635313898325e-06\n1.4901161193847656e-06\n1.4715478755533695e-06\n1.4530960470438004e-06\n9.837094694375992e-07\n1.4347606338560581e-06\n1.4347606338560581e-06\n1.4165416359901428e-06\n1.3984390534460545e-06\n1.3984390534460545e-06\n1.380452886223793e-06\n1.380452886223793e-06\n1.3625831343233585e-06\n1.3625831343233585e-06\n1.029635313898325e-06\n1.344829797744751e-06\n1.3271928764879704e-06\n1.3096723705530167e-06\n1.3096723705530167e-06\n1.29226827993989e-06\n1.29226827993989e-06\n5.704932846128941e-07\n1.27498060464859e-06\n1.2578093446791172e-06\n1.2578093446791172e-06\n1.2407545000314713e-06\n1.2407545000314713e-06\n1.2238160707056522e-06\n2.3102620616555214e-07\n1.2069940567016602e-06\n1.2069940567016602e-06\n1.190288458019495e-06\n1.1736992746591568e-06\n1.1736992746591568e-06\n1.1572265066206455e-06\n1.1572265066206455e-06\n1.1408701539039612e-06\n1.1408701539039612e-06\n1.1246302165091038e-06\n1.1246302165091038e-06\n1.1085066944360733e-06\n1.1085066944360733e-06\n1.0924995876848698e-06\n1.0924995876848698e-06\n1.0766088962554932e-06\n1.0766088962554932e-06\n1.0608346201479435e-06\n1.0608346201479435e-06\n1.0451767593622208e-06\n1.0451767593622208e-06\n1.029635313898325e-06\n1.029635313898325e-06\n1.014210283756256e-06\n1.014210283756256e-06\n9.989016689360142e-07\n9.989016689360142e-07\n9.837094694375992e-07\n9.837094694375992e-07\n9.686336852610111e-07\n9.686336852610111e-07\n9.5367431640625e-07\n9.5367431640625e-07\n9.388313628733158e-07\n9.388313628733158e-07\n9.241048246622086e-07\n9.241048246622086e-07\n9.094947017729282e-07\n9.094947017729282e-07\n8.950009942054749e-07\n8.950009942054749e-07\n8.806237019598484e-07\n8.806237019598484e-07\n8.663628250360489e-07\n8.663628250360489e-07\n8.663628250360489e-07\n8.522183634340763e-07\n8.522183634340763e-07\n8.381903171539307e-07\n8.381903171539307e-07\n8.24278686195612e-07\n8.24278686195612e-07\n8.104834705591202e-07\n8.104834705591202e-07\n8.104834705591202e-07\n7.968046702444553e-07\n7.832422852516174e-07\n1.341104507446289e-07\n7.832422852516174e-07\n7.697963155806065e-07\n7.697963155806065e-07\n7.697963155806065e-07\n7.564667612314224e-07\n7.564667612314224e-07\n7.432536222040653e-07\n7.432536222040653e-07\n7.301568984985352e-07\n7.301568984985352e-07\n7.171765901148319e-07\n7.171765901148319e-07\n7.171765901148319e-07\n7.043126970529556e-07\n7.043126970529556e-07\n6.915652193129063e-07\n6.915652193129063e-07\n6.789341568946838e-07\n6.789341568946838e-07\n6.664195097982883e-07\n6.664195097982883e-07\n6.664195097982883e-07\n6.540212780237198e-07\n6.540212780237198e-07\n6.417394615709782e-07\n6.417394615709782e-07\n6.295740604400635e-07\n6.295740604400635e-07\n6.295740604400635e-07\n6.175250746309757e-07\n6.175250746309757e-07\n4.3050386011600494e-07\n6.055925041437149e-07\n6.055925041437149e-07\n5.93776348978281e-07\n5.93776348978281e-07\n5.93776348978281e-07\n5.820766091346741e-07\n5.820766091346741e-07\n5.704932846128941e-07\n5.704932846128941e-07\n4.71482053399086e-07\n5.59026375412941e-07\n5.59026375412941e-07\n5.476758815348148e-07\n4.926696419715881e-07\n5.476758815348148e-07\n5.364418029785156e-07\n5.364418029785156e-07\n5.364418029785156e-07\n5.253241397440434e-07\n5.253241397440434e-07\n5.14322891831398e-07\n4.0099257603287697e-07\n5.14322891831398e-07\n1.5739351511001587e-07\n5.034380592405796e-07\n4.926696419715881e-07\n4.926696419715881e-07\n4.926696419715881e-07\n4.926696419715881e-07\n4.820176400244236e-07\n4.820176400244236e-07\n4.820176400244236e-07\n4.71482053399086e-07\n4.71482053399086e-07\n4.71482053399086e-07\n4.6106288209557533e-07\n4.6106288209557533e-07\n4.507601261138916e-07\n4.507601261138916e-07\n4.507601261138916e-07\n4.405737854540348e-07\n3.632740117609501e-07\n4.405737854540348e-07\n4.3050386011600494e-07\n4.3050386011600494e-07\n4.3050386011600494e-07\n4.20550350099802e-07\n4.20550350099802e-07\n4.20550350099802e-07\n4.10713255405426e-07\n4.10713255405426e-07\n2.771266736090183e-07\n3.3620744943618774e-07\n4.0099257603287697e-07\n4.0099257603287697e-07\n1.285807229578495e-07\n3.9138831198215485e-07\n3.9138831198215485e-07\n3.8190046325325966e-07\n3.8190046325325966e-07\n3.8190046325325966e-07\n3.725290298461914e-07\n3.725290298461914e-07\n3.725290298461914e-07\n3.632740117609501e-07\n3.632740117609501e-07\n2.2375024855136871e-07\n3.541354089975357e-07\n3.541354089975357e-07\n3.541354089975357e-07\n3.4511322155594826e-07\n3.4511322155594826e-07\n3.4511322155594826e-07\n3.4511322155594826e-07\n3.3620744943618774e-07\n2.53552570939064e-07\n3.3620744943618774e-07\n3.2741809263825417e-07\n3.2741809263825417e-07\n3.2741809263825417e-07\n3.187451511621475e-07\n3.187451511621475e-07\n3.187451511621475e-07\n3.187451511621475e-07\n2.459273673593998e-07\n3.101886250078678e-07\n3.101886250078678e-07\n3.0174851417541504e-07\n3.0174851417541504e-07\n3.0174851417541504e-07\n2.934248186647892e-07\n2.934248186647892e-07\n2.612941898405552e-07\n2.934248186647892e-07\n2.852175384759903e-07\n2.852175384759903e-07\n2.852175384759903e-07\n2.771266736090183e-07\n2.771266736090183e-07\n2.771266736090183e-07\n2.771266736090183e-07\n2.691522240638733e-07\n1.9581057131290436e-07\n2.691522240638733e-07\n2.612941898405552e-07\n2.612941898405552e-07\n2.612941898405552e-07\n2.612941898405552e-07\n2.53552570939064e-07\n2.53552570939064e-07\n2.53552570939064e-07\n2.53552570939064e-07\n2.459273673593998e-07\n2.459273673593998e-07\n2.459273673593998e-07\n1.4551915228366852e-07\n2.384185791015625e-07\n2.384185791015625e-07\n7.968628779053688e-08\n5.593756213784218e-08\n2.3102620616555214e-07\n2.3102620616555214e-07\n2.3102620616555214e-07\n2.2375024855136871e-07\n2.2375024855136871e-07\n2.2375024855136871e-07\n2.2375024855136871e-07\n2.1659070625901222e-07\n2.1659070625901222e-07\n2.1659070625901222e-07\n2.1659070625901222e-07\n2.0954757928848267e-07\n2.0954757928848267e-07\n2.0954757928848267e-07\n2.0954757928848267e-07\n2.0262086763978004e-07\n2.0262086763978004e-07\n2.0262086763978004e-07\n2.0262086763978004e-07\n1.9581057131290436e-07\n1.9581057131290436e-07\n1.9581057131290436e-07\n1.9581057131290436e-07\n1.891166903078556e-07\n1.891166903078556e-07\n1.891166903078556e-07\n1.891166903078556e-07\n1.891166903078556e-07\n1.825392246246338e-07\n5.960464477539063e-08\n1.825392246246338e-07\n1.825392246246338e-07\n1.760781742632389e-07\n1.760781742632389e-07\n1.760781742632389e-07\n1.760781742632389e-07\n1.6973353922367096e-07\n1.6973353922367096e-07\n1.6973353922367096e-07\n1.6973353922367096e-07\n6.3388142734766e-08\n9.784707799553871e-08\n1.6350531950592995e-07\n1.6350531950592995e-07\n1.6350531950592995e-07\n1.6350531950592995e-07\n1.5739351511001587e-07\n1.5739351511001587e-07\n5.960464477539063e-08\n1.5739351511001587e-07\n1.5739351511001587e-07\n1.5139812603592873e-07\n1.5139812603592873e-07\n1.5139812603592873e-07\n1.5139812603592873e-07\n1.4551915228366852e-07\n2.3283064365386963e-08\n1.4551915228366852e-07\n1.4551915228366852e-07\n1.4551915228366852e-07\n1.3975659385323524e-07\n1.3975659385323524e-07\n1.3975659385323524e-07\n1.3975659385323524e-07\n1.3975659385323524e-07\n7.968628779053688e-08\n6.3388142734766e-08\n1.341104507446289e-07\n1.341104507446289e-07\n1.341104507446289e-07\n1.285807229578495e-07\n1.285807229578495e-07\n1.285807229578495e-07\n1.285807229578495e-07\n1.285807229578495e-07\n1.2316741049289703e-07\n1.2316741049289703e-07\n1.2316741049289703e-07\n1.2316741049289703e-07\n2.1012965589761734e-08\n1.178705133497715e-07\n1.3096723705530167e-08\n1.178705133497715e-07\n1.178705133497715e-07\n1.178705133497715e-07\n7.130438461899757e-08\n1.126900315284729e-07\n1.126900315284729e-07\n1.126900315284729e-07\n1.126900315284729e-07\n1.126900315284729e-07\n1.0762596502900124e-07\n1.0762596502900124e-07\n1.0762596502900124e-07\n1.0762596502900124e-07\n1.0762596502900124e-07\n1.0762596502900124e-07\n1.026783138513565e-07\n1.026783138513565e-07\n1.026783138513565e-07\n1.026783138513565e-07\n1.026783138513565e-07\n9.784707799553871e-08\n9.784707799553871e-08\n9.784707799553871e-08\n9.784707799553871e-08\n9.784707799553871e-08\n9.784707799553871e-08\n9.313225746154785e-08\n9.313225746154785e-08\n9.313225746154785e-08\n9.313225746154785e-08\n9.313225746154785e-08\n9.313225746154785e-08\n8.853385224938393e-08\n1.682201400399208e-08\n8.853385224938393e-08\n8.853385224938393e-08\n8.853385224938393e-08\n8.405186235904694e-08\n8.405186235904694e-08\n8.405186235904694e-08\n8.405186235904694e-08\n5.820766091346741e-11\n8.405186235904694e-08\n8.405186235904694e-08\n7.968628779053688e-08\n7.968628779053688e-08\n7.968628779053688e-08\n7.968628779053688e-08\n5.593756213784218e-08\n7.968628779053688e-08\n7.543712854385376e-08\n4.243338480591774e-08\n7.543712854385376e-08\n7.543712854385376e-08\n7.543712854385376e-08\n7.543712854385376e-08\n7.543712854385376e-08\n7.130438461899757e-08\n7.130438461899757e-08\n7.130438461899757e-08\n7.130438461899757e-08\n7.130438461899757e-08\n7.130438461899757e-08\n6.728805601596832e-08\n4.895264282822609e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.3388142734766e-08\n6.728805601596832e-08\n6.3388142734766e-08\n6.3388142734766e-08\n6.3388142734766e-08\n6.3388142734766e-08\n6.3388142734766e-08\n6.3388142734766e-08\n6.3388142734766e-08\n5.960464477539063e-08\n5.960464477539063e-08\n5.960464477539063e-08\n5.2386894822120667e-08\n5.960464477539063e-08\n5.960464477539063e-08\n5.960464477539063e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n3.637978807091713e-08\n1.3096723705530167e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.243338480591774e-08\n4.243338480591774e-08\n4.243338480591774e-08\n4.243338480591774e-08\n4.243338480591774e-08\n4.243338480591774e-08\n4.243338480591774e-08\n4.243338480591774e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.079185262322426e-08\n3.637978807091713e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n2.3283064365386963e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.3283064365386963e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.5669578462839127e-08\n1.4901161193847656e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n1.4901161193847656e-08\n2.852175384759903e-09\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.5669578462839127e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n1.885928213596344e-08\n1.3096723705530167e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n8.381903171539307e-09\n1.885928213596344e-08\n0.0\n1.885928213596344e-08\n1.885928213596344e-08\n1.682201400399208e-08\n2.3283064365386963e-10\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n5.238689482212067e-10\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n2.852175384759903e-09\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n8.381903171539307e-09\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n5.820766091346741e-09\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n4.71482053399086e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n3.725290298461914e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n4.71482053399086e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n4.71482053399086e-09\n7.043126970529556e-09\n5.820766091346741e-11\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n5.820766091346741e-09\n5.820766091346741e-09\n4.71482053399086e-09\n5.820766091346741e-09\n5.820766091346741e-09\n3.725290298461914e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n4.71482053399086e-09\n4.71482053399086e-09\n2.3283064365386963e-10\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n2.852175384759903e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n2.852175384759903e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n2.0954757928848267e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n2.3283064365386963e-10\n3.725290298461914e-09\n3.725290298461914e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n1.4551915228366852e-09\n2.852175384759903e-09\n9.313225746154785e-10\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.0954757928848267e-09\n1.4551915228366852e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n5.820766091346741e-11\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n1.4551915228366852e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n5.820766091346741e-11\n5.238689482212067e-10\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n5.820766091346741e-11\n0.0\n2.0954757928848267e-09\n2.0954757928848267e-09\n9.313225746154785e-10\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n9.313225746154785e-10\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n5.238689482212067e-10\n2.3283064365386963e-10\n1.4551915228366852e-09\n0.0\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n0.0\n5.820766091346741e-11\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n5.238689482212067e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.820766091346741e-11\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n2.3283064365386963e-10\n5.238689482212067e-10\n2.3283064365386963e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.820766091346741e-11\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n2.3283064365386963e-10\n5.820766091346741e-11\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n2.3283064365386963e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n5.820766091346741e-11\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n0.0\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n5.820766091346741e-11\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n5.820766091346741e-11\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n5.820766091346741e-11\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n5.820766091346741e-11\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.3283064365386963e-10\n0.0\n2.3283064365386963e-10\n0.0\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n1.4551915228366852e-09\n2.852175384759903e-09\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0954757928848267e-09\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n0.0\n0.0\n2.3283064365386963e-10\n2.3283064365386963e-10\n0.0\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n2.3283064365386963e-10\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.3283064365386963e-10\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0954757928848267e-09\n2.3283064365386963e-10\n0.0\n0.0\n0.0\n4.0099257603287697e-07\n7.711356738582253e-05\n1.885928213596344e-06\n1.3984390534460545e-06\n4.0099257603287697e-07\n5.820766091346741e-09\n1.2316741049289703e-07\n1.760781742632389e-07\n0.0006189873092807829\n7.543712854385376e-08\n8.853385224938393e-08\n9.784707799553871e-08\n1.0762596502900124e-07\n1.126900315284729e-07\n1.178705133497715e-07\n1.178705133497715e-07\n1.2316741049289703e-07\n1.285807229578495e-07\n1.4551915228366852e-07\n1.5739351511001587e-07\n1.6973353922367096e-07\n0.0012279283255338669\n1.9581057131290436e-07\n2.0262086763978004e-07\n2.0954757928848267e-07\n2.384185791015625e-07\n3.2741809263825417e-07\n4.0099257603287697e-07\n4.820176400244236e-07\n0.000659098441246897\n6.295740604400635e-07\n6.915652193129063e-07\n7.432536222040653e-07\n7.968046702444553e-07\n8.24278686195612e-07\n8.522183634340763e-07\n8.663628250360489e-07\n8.806237019598484e-07\n8.950009942054749e-07\n8.806237019598484e-07\n8.806237019598484e-07\n8.950009942054749e-07\n9.241048246622086e-07\n9.388313628733158e-07\n0.0038208907935768366\n0.0009345850558020175\n1.2407545000314713e-06\n2.2819731384515762e-06\n3.4654513001441956e-06\n4.694797098636627e-06\n5.923269782215357e-06\n7.049180567264557e-06\n8.098373655229807e-06\n8.990115020424128e-06\n9.737035725265741e-06\n1.0316784027963877e-05\n1.0812713298946619e-05\n1.10650435090065e-05\n1.1217838618904352e-05\n1.126900315284729e-05\n1.1166790500283241e-05\n1.0963762179017067e-05\n1.0662712156772614e-05\n1.0316784027963877e-05\n9.880401194095612e-06\n9.359850082546473e-06\n8.853385224938393e-06\n8.316943421959877e-06\n7.754715625196695e-06\n7.171242032200098e-06\n6.61058584228158e-06\n6.072747055441141e-06\n5.521811544895172e-06\n4.9970694817602634e-06\n4.498520866036415e-06\n4.026165697723627e-06\n3.6089331842958927e-06\n3.1872186809778214e-06\n2.8172507882118225e-06\n2.446176949888468e-06\n2.1457672119140625e-06\n1.8442515283823013e-06\n1.5847035683691502e-06\n1.344829797744751e-06\n1.1408701539039612e-06\n1.1085066944360733e-06\n1.1085066944360733e-06\n1.0924995876848698e-06\n1.0924995876848698e-06\n1.0608346201479435e-06\n1.0451767593622208e-06\n1.014210283756256e-06\n9.989016689360142e-07\n9.686336852610111e-07\n9.388313628733158e-07\n9.094947017729282e-07\n0.0024683624505996704\n8.522183634340763e-07\n8.24278686195612e-07\n1.1572265066206455e-06\n1.5465193428099155e-06\n1.9280705600976944e-06\n0.0039983391761779785\n2.640823367983103e-06\n2.9467628337442875e-06\n3.1872186809778214e-06\n3.4088734537363052e-06\n3.551191184669733e-06\n3.6964192986488342e-06\n4.149565938860178e-06\n4.5309425331652164e-06\n4.861562047153711e-06\n0.0006439092103391886\n5.4147676564753056e-06\n5.593756213784218e-06\n5.7756551541388035e-06\n6.2622129917144775e-06\n6.6498760133981705e-06\n6.928166840225458e-06\n7.089751306921244e-06\n7.171242032200098e-06\n7.171242032200098e-06\n7.089751306921244e-06\n6.968388333916664e-06\n6.768445018678904e-06\n6.61058584228158e-06\n6.61058584228158e-06\n6.53235474601388e-06\n6.454589311033487e-06\n6.338814273476601e-06\n6.186077371239662e-06\n6.03219959884882e-05\n6.6498760133981705e-06\n5.7390425354242325e-06\n5.812384188175201e-06\n0.026961583644151688\n5.812384188175201e-06\n5.7390425354242325e-06\n5.629903171211481e-06\n5.450332537293434e-06\n5.812384188175201e-06\n6.968388333916664e-06\n8.098373655229807e-06\n9.174051228910685e-06\n1.0073184967041016e-05\n1.091329613700509e-05\n1.1682510375976562e-05\n1.2263248208910227e-05\n1.2748874723911285e-05\n1.3188458979129791e-05\n1.3466982636600733e-05\n1.3691897038370371e-05\n1.3805052731186152e-05\n1.3805052731186152e-05\n1.3805052731186152e-05\n1.3691897038370371e-05\n1.3523036614060402e-05\n1.3299519196152687e-05\n1.3077864423394203e-05\n1.2748874723911285e-05\n1.2424075976014137e-05\n0.0004814574494957924\n0.00048011913895606995\n0.00044147693552076817\n0.00070978794246912\n4.72791725769639e-06\n6.224086973816156e-06\n5.1344395615160465e-06\n0.0843573808670044\n0.06631787866353989\n0.005207995418459177\n0.0022209323942661285\n0.0005790332797914743\n0.00041775876889005303\n3.725290298461914e-07\n0.0007678419351577759\n0.0003200853825546801\n49.289886474609375\n2.7312489692121744e-05\n6.175250746309757e-07\n0.0015921483281999826\n0.0022123116068542004\n0.0035814205184578896\n0.00537671847268939\n0.007615182548761368\n0.010307194665074348\n0.013442990370094776\n0.016958845779299736\n0.02074398100376129\n0.024672232568264008\n0.0020289570093154907\n0.0020234622061252594\n0.001990649849176407\n0.001933202613145113\n0.0018502494785934687\n28.543107986450195\n0.0028914250433444977\n0.0029450030997395515\n0.0037085453514009714\n0.20078429579734802\n0.2411607801914215\n1.8472366333007812\n63.539283752441406\n3.2725846767425537\n4.029090404510498\n4.801024436950684\n5.270457744598389\n5.110265254974365\n4.2339768409729\n2.4522902965545654\n0.511699914932251\n0.35337308049201965\n4.983707427978516\n89.90937805175781\n69.25959777832031\n68.69944763183594\n63.000999450683594\n54.1269416809082\n43.58183670043945\n33.00382995605469\n23.657367706298828\n16.158313751220703\n16406.53125\n7.850466728210449\n6.123465061187744\n4.482773780822754\n3.1564834117889404\n0.008837766945362091\n0.008190227672457695\n0.007620509713888168\n0.00711759552359581\n0.006671670824289322\n90.5580062866211\n87.4540023803711\n0.005624542012810707\n0.0053521315567195415\n0.005106089636683464\n0.004885015077888966\n0.004684485029429197\n0.004502480383962393\n0.004546614363789558\n0.0883251279592514\n0.00479478482156992\n0.004902093671262264\n0.004999794997274876\n0.005087570752948523\n0.005166234914213419\n1.1855653524398804\n0.005302015691995621\n0.005359948612749577\n0.005412581842392683\n0.005458706058561802\n0.005500498227775097\n0.0055367713794112206\n0.005569746717810631\n0.005598252639174461\n0.005624542012810707\n0.005646306090056896\n0.005666963290423155\n0.005684206262230873\n0.005699171684682369\n0.00571184977889061\n0.005723387934267521\n0.0057326266542077065\n0.005740716587752104\n1.6958448886871338\n0.005753440782427788\n0.005758071318268776\n0.005761545617133379\n0.005763862282037735\n0.005766179412603378\n0.0057673379778862\n56.25274658203125\n0.0057673379778862\n0.0057673379778862\n0.0057673379778862\n0.005765020847320557\n0.005763862282037735\n0.005761545617133379\n0.00575922941789031\n0.005755756050348282\n0.49270176887512207\n0.005749969277530909\n0.005746498703956604\n0.005743029061704874\n0.005739560350775719\n0.005736093036830425\n0.005731471348553896\n0.005728006362915039\n0.005723387934267521\n0.005718771368265152\n35.02045822143555\n0.0969708114862442\n0.0755922719836235\n0.005702627822756767\n0.005699171684682369\n0.005694564897567034\n26.53171157836914\n24.476871490478516\n0.005684206262230873\n0.005680755712091923\n0.005676156375557184\n0.00567270815372467\n0.005668112076818943\n0.005664666183292866\n0.005675006657838821\n0.005688808858394623\n0.005694564897567034\n0.005692262202501297\n0.0056853569112718105\n0.0031582287047058344\n0.0056600733660161495\n0.005642866715788841\n0.00563599169254303\n0.005632555577903986\n0.0056302654556930065\n0.005626831203699112\n0.005623397883027792\n0.005618821829557419\n0.005615390837192535\n0.005610818043351173\n5.067307472229004\n0.005602820310741663\n0.00559939444065094\n0.0055948281660676\n0.00559026375412941\n3.6972994804382324\n0.0055811405181884766\n3.0579888820648193\n0.005573163740336895\n0.005569746717810631\n0.005565192550420761\n0.005560640245676041\n0.005556089803576469\n0.005551541224122047\n0.005546994507312775\n0.005542449653148651\n0.0055367713794112206\n0.005532230716198683\n0.005526557564735413\n0.005522021092474461\n0.005516353063285351\n0.005511820781975985\n0.005506157875061035\n0.005500498227775097\n0.0054948413744568825\n0.005490317940711975\n0.005484666209667921\n0.005479017272591591\n0.005473371595144272\n0.005467728711664677\n0.005462088622152805\n0.005456451326608658\n0.0054508172906935215\n0.005445186048746109\n0.00543955760076642\n0.35374507308006287\n0.3271562159061432\n0.005428309552371502\n0.005422689951956272\n0.00541594997048378\n0.005412581842392683\n0.005408092401921749\n0.005404726602137089\n0.005400240421295166\n0.0053957561030983925\n0.005390153266489506\n0.005385673139244318\n0.0053811948746442795\n0.00537559948861599\n0.002724873134866357\n0.005366653203964233\n0.005362182855606079\n0.00535659771412611\n0.0053521315567195415\n0.005347667261958122\n0.00534208957105875\n0.057566020637750626\n0.005333171226084232\n0.005327600985765457\n0.005323146935552359\n0.005318694747984409\n0.005313131958246231\n0.0053086839616298676\n0.005303126759827137\n0.00529757235199213\n0.005293130874633789\n0.005287581589072943\n0.00528203509747982\n0.005277600139379501\n0.0049395570531487465\n0.005266521126031876\n0.005262092687189579\n0.0052565596997737885\n0.005252135451883078\n0.0052466075867414474\n0.004663621075451374\n0.00523666525259614\n0.005232249386608601\n0.0052278353832662106\n0.005222320556640625\n0.005217910744249821\n0.0052124010398983955\n0.005206894129514694\n0.0035786814987659454\n0.005196989513933659\n0.005192590411752462\n0.005188193172216415\n0.005182699300348759\n0.003952160477638245\n0.0051739150658249855\n0.005169525742530823\n0.0051640416495501995\n0.0051596565172076225\n0.00515417754650116\n0.0051497966051101685\n0.005144323222339153\n0.005139946471899748\n0.005134478211402893\n0.005129012744873762\n0.005123550072312355\n0.0051191821694374084\n0.005113725084811449\n0.004812763538211584\n0.005103909410536289\n0.005098460242152214\n0.0050941030494868755\n0.005088659003376961\n0.005083218216896057\n0.005078867543488741\n0.004297032952308655\n0.005069085396826267\n0.005063654854893684\n0.00505931256338954\n0.005053887143731117\n0.005049549043178558\n0.005044129211455584\n0.005040878430008888\n0.005038712173700333\n0.0050332979299128056\n0.0050268047489225864\n0.005023559555411339\n0.005020315758883953\n0.005015992093831301\n0.0050127506256103516\n0.00500843022018671\n0.005004111677408218\n0.004999794997274876\n0.004995480179786682\n0.004991167224943638\n0.004812763538211584\n0.004981469828635454\n0.004977162927389145\n0.004972857888787985\n0.004968554712831974\n0.0049642533995211124\n0.004961028695106506\n0.004772622138261795\n0.00410713255405426\n0.004950286820530891\n0.004944920539855957\n0.0049395570531487465\n0.004446343518793583\n0.002852175384759903\n0.004930981434881687\n0.004927767440676689\n0.004924554377794266\n0.004920272156596184\n0.004917061887681484\n0.004912782926112413\n0.004908505827188492\n0.0049042305909097195\n0.004899957217276096\n0.003134263912215829\n0.004892483353614807\n0.004888215102255344\n0.004883948713541031\n0.004879684187471867\n0.0048743560910224915\n0.004870095755904913\n0.004865837283432484\n0.0048605166375637054\n0.004856262356042862\n0.004850947298109531\n0.004846697207540274\n0.004842448979616165\n0.0048371413722634315\n0.004832897335290909\n0.004828655160963535\n0.004824414849281311\n0.004820176400244236\n0.004819117020815611\n0.004812763538211584\n0.004809588193893433\n0.00480535626411438\n0.004801126196980476\n0.004797955043613911\n0.00479478482156992\n0.004790559411048889\n0.004787391517311335\n0.004783169366419315\n0.004778949078172445\n0.0047747306525707245\n0.004770514089614153\n0.0047652460634708405\n0.004761033691465855\n0.004756823182106018\n0.004752614535391331\n0.004557939246296883\n0.004745253827422857\n0.00474105030298233\n0.0047378987073898315\n0.004733698442578316\n0.004729500040411949\n0.004725303500890732\n0.004721108824014664\n0.004716916009783745\n0.00471167778596282\n0.004707489162683487\n0.004703302402049303\n0.004698071628808975\n0.004693889059126377\n0.004689708352088928\n0.004685529507696629\n0.004464676603674889\n0.004681352525949478\n0.004677177406847477\n0.004055467899888754\n0.004666747525334358\n0.004661537241190672\n0.00465841218829155\n0.004654247313737869\n0.00393778458237648\n0.004646963439881802\n0.00464384350925684\n0.004639685153961182\n0.004636567551642656\n0.004632412455976009\n0.004628259222954512\n0.004624107852578163\n0.003203840460628271\n0.004615810699760914\n0.004611664917320013\n0.0046075209975242615\n0.004602343775331974\n0.004598204046487808\n0.004594066180288792\n0.0045909639447927475\n0.0026137218810617924\n0.0023131966590881348\n0.004580630920827389\n0.004575468599796295\n0.004572372883558273\n0.004569278098642826\n0.0045661842450499535\n0.0045630913227796555\n0.004558969289064407\n0.0045548491179943085\n0.004551760386675596\n0.0045476434752345085\n0.0036834985949099064\n0.004539415240287781\n0.004535303916782141\n0.00453119445592165\n0.004527086857706308\n0.004522981122136116\n0.004518877249211073\n0.004513749852776527\n0.00450965017080307\n0.004505552351474762\n0.004501456394791603\n0.004496339242905378\n0.004188698250800371\n0.004489179700613022\n0.00448509119451046\n0.0044820262119174\n0.0044779409654438496\n0.004473857581615448\n0.004469776060432196\n0.004464676603674889\n0.0038378871977329254\n0.004456523805856705\n0.004452450200915337\n0.004448378458619118\n0.004444308578968048\n0.004439224023371935\n0.004435158334672451\n0.004431094508618116\n0.004427032545208931\n0.004422972444444895\n0.004419928416609764\n0.004415871575474739\n0.0042273011058568954\n0.004411816596984863\n0.004404725041240454\n0.004399662837386131\n0.004395615309476852\n0.004392581060528755\n0.00422630924731493\n0.004385504871606827\n0.004382473882287741\n0.004379443824291229\n0.0043754056096076965\n0.004371369257569313\n0.004367334768176079\n0.004363302141427994\n0.004359271377325058\n0.004355242475867271\n0.004350209143012762\n0.004347190260887146\n0.004343166947364807\n0.004338140599429607\n0.004334121476858854\n0.00433010421693325\n0.004326088819652796\n0.00432207528501749\n0.004318063613027334\n0.004314053803682327\n0.004309044219553471\n0.004307040944695473\n0.004302035551518202\n0.004297032952308655\n0.004293032921850681\n0.004289034754037857\n0.004288035444915295\n0.004282042384147644\n0.003585987724363804\n0.004275055602192879\n0.004272063262760639\n0.004269071854650974\n0.004265084862709045\n0.0006262212991714478\n0.004258112050592899\n0.0042551252990961075\n0.004251144826412201\n0.004248160868883133\n0.0042441836558282375\n0.004241202026605606\n0.003952160477638245\n0.004234248772263527\n0.0034305457957088947\n0.004228293430060148\n0.0042253173887729645\n0.004222342278808355\n0.00421837717294693\n0.004214413929730654\n0.004210452549159527\n0.004206493031233549\n0.004202535375952721\n0.004198579583317041\n0.004194625653326511\n0.004189685918390751\n0.004185736179351807\n8.270516991615295e-05\n0.004176856018602848\n0.004172912333160639\n0.004167985171079636\n0.004164045676589012\n0.0041610924527049065\n0.004156172275543213\n0.004153221845626831\n0.004149289336055517\n0.0007610917091369629\n0.004141429904848337\n0.004137502983212471\n0.004133577924221754\n0.0041306354105472565\n0.004126713611185551\n0.004122793674468994\n0.0012381086125969887\n0.004114959388971329\n0.0041120233945548534\n0.0041081104427576065\n0.004105176776647568\n0.004101267084479332\n0.004097359254956245\n0.004093453288078308\n0.00408954918384552\n0.004087598063051701\n0.004082721658051014\n0.0017729680985212326\n0.004075899720191956\n0.004072003997862339\n0.004069083370268345\n0.0014974027872085571\n0.004062272608280182\n0.004058383405208588\n0.004055467899888754\n0.004051581956446171\n0.004047697875648737\n0.004043815657496452\n0.004039935301989317\n0.004036056809127331\n0.004032180178910494\n0.004028305411338806\n0.004024432506412268\n0.004021529108285904\n0.004017659462988377\n0.0015110140666365623\n0.00400992576032877\n0.0013844766654074192\n0.0037691909819841385\n0.004000268876552582\n0.003996409475803375\n0.003993516322225332\n0.003989660181105137\n0.003985805902630091\n0.003981953486800194\n0.003847345942631364\n0.003974254243075848\n0.003971369005739689\n0.0039675235748291016\n0.0039636800065636635\n0.003959838300943375\n0.003955998457968235\n0.003952160477638245\n0.0039483243599534035\n0.003943531773984432\n0.003939699847251177\n0.003935869783163071\n0.003932041581720114\n0.003928215242922306\n0.003924390766769648\n0.003920568153262138\n0.003916747402399778\n0.003911973908543587\n0.003909111488610506\n0.00390625\n0.0039014830254018307\n0.003554987721145153\n0.003895766567438841\n0.003891957923769951\n0.0038891027215868235\n0.0038852973375469446\n0.003881493816152215\n0.0038776921574026346\n0.003323257900774479\n0.0038700944278389215\n0.003866298357024789\n0.0038625041488558054\n0.003858711803331971\n0.003854921320453286\n0.00385018577799201\n0.003847345942631364\n0.003842615056782961\n0.00383883249014616\n0.0038359968457370996\n0.00383221753872931\n0.0038284400943666697\n0.0038246645126491785\n0.0038199475966393948\n0.0038161762058734894\n0.0038114646449685097\n0.0034879823215305805\n0.0036447057500481606\n0.00345651269890368\n0.0038020501378923655\n0.002550915814936161\n0.0032689443323761225\n0.003791707567870617\n0.0017181439325213432\n0.0037823175080120564\n0.0037776269018650055\n0.0037748138420283794\n0.0037720019463449717\n0.003768254304304719\n0.0037654447369277477\n0.003761700354516506\n0.003757957834750414\n0.003754217177629471\n0.0003773088101297617\n0.0037476755678653717\n0.00030020257690921426\n0.0037402063608169556\n0.0037374072708189487\n0.0001344829797744751\n0.0037336768582463264\n0.0037308803293854\n0.003725290298461914\n0.003722496796399355\n0.003719704458490014\n0.003716913051903248\n0.0037141228094697\n0.0037113334983587265\n0.003707616124302149\n0.0037039006128907204\n0.003700186964124441\n0.00017744698561728\n0.0036936926189810038\n0.003689984092488885\n0.0036862774286419153\n0.003682572627440095\n0.003678869688883424\n0.003469083458185196\n0.0036705448292195797\n0.0036668479442596436\n0.0036631529219448566\n0.0036603829357773066\n0.003656691173091531\n0.0036530012730509043\n0.003649313235655427\n0.003645627060905099\n0.00364194274879992\n0.0018865247257053852\n0.0036354996263980865\n0.003631820436567068\n0.003628143109381199\n0.003624467644840479\n0.003620794042944908\n0.0036162047181278467\n0.0035204999148845673\n0.003611618187278509\n0.0036079511046409607\n0.003602453973144293\n0.0035987915471196175\n0.0035951309837400913\n0.0035914722830057144\n0.003588729538023472\n0.0026560169644653797\n0.0035841604694724083\n0.0035786814987659454\n0.0035759436432272196\n0.003572294721379876\n0.003568647662177682\n0.003565002465620637\n0.003561359131708741\n0.0035577176604419947\n0.0035540780518203974\n0.0035504403058439493\n0.0035458956845104694\n0.003543170401826501\n0.0035377228632569313\n0.003534093499183655\n0.0035304659977555275\n0.0035277465358376503\n0.003524122294038534\n0.003519594669342041\n0.0035159746184945107\n0.0035123564302921295\n0.0035087401047348976\n0.003505125641822815\n0.0035015130415558815\n0.003384229727089405\n0.0034951954148709774\n0.0034915879368782043\n0.0034879823215305805\n0.0034852793905884027\n0.00348167703486979\n0.0034780765417963266\n0.0034744779113680124\n0.0034708811435848475\n0.0034672862384468317\n0.003463693195953965\n0.003460102016106248\n0.0034556156024336815\n0.00345202861353755\n0.0034493396524339914\n0.003445755923166871\n0.0033021802082657814\n0.0034376992844045162\n0.003434121608734131\n0.003431439632549882\n0.0034278652165085077\n0.0022868523374199867\n0.003420721972361207\n0.0034180451184511185\n0.003414477687329054\n0.0034109121188521385\n0.0034073484130203724\n0.003404676914215088\n0.003401116468012333\n0.003397557884454727\n0.0033940011635422707\n0.0033904463052749634\n0.0033868933096528053\n0.0033833421766757965\n0.003379792906343937\n0.0033771321177482605\n0.0033726999536156654\n0.0033691562712192535\n0.001527667511254549\n0.003362959250807762\n0.0013327356427907944\n0.003356768051162362\n0.003354116342961788\n0.0033505824394524097\n0.003347933292388916\n0.0033444026485085487\n0.0033408738672733307\n0.003337346948683262\n0.003332940861582756\n0.003329418133944273\n0.0033276574686169624\n0.0033241375349462032\n0.0033197402954101562\n0.003316224552690983\n0.0033127106726169586\n0.0033100764267146587\n0.0033065658062696457\n0.003303057048469782\n0.0032995501533150673\n0.003296045120805502\n0.0032916665077209473\n0.0032881656661629677\n0.0032846666872501373\n0.0027873553335666656\n0.003276800736784935\n0.0032733078114688396\n0.0032698167487978935\n0.0032663275487720966\n0.003262840211391449\n0.0032584837172180414\n0.0032550005707889795\n0.0032532596960663795\n0.0032497793436050415\n0.0032445623073726892\n0.003241955302655697\n0.003238481003791094\n0.00323500856757164\n0.0032324055209755898\n0.00322806928306818\n0.0029948963783681393\n0.0032220035791397095\n0.0032185399904847145\n0.0032150782644748688\n0.0032116184011101723\n0.003208160400390625\n0.003204704262316227\n0.003201249986886978\n0.0031977975741028786\n0.0025787353515625\n0.003190898336470127\n0.0031874515116214752\n0.0031840065494179726\n0.003181423991918564\n0.0031779822893440723\n0.00317454244941473\n0.003171104472130537\n0.0031676683574914932\n0.0031642341054975986\n0.0031608017161488533\n0.003131701610982418\n0.0031539425253868103\n0.0031513723079115152\n0.0025771858636289835\n0.003145379014313221\n0.0031419568695127964\n0.003138536587357521\n0.003135118167847395\n0.003131701610982418\n0.0031308478210121393\n0.003124874085187912\n0.0031223157420754433\n0.003118906170129776\n0.003116350155323744\n0.0031129438430070877\n0.003109539393335581\n0.003106136806309223\n0.003101886250078678\n0.0030984878540039062\n0.0030950913205742836\n0.00282940361648798\n0.003092545084655285\n0.0030900000128895044\n0.002605926711112261\n0.0030815238133072853\n0.0030755973421037197\n0.0030713677406311035\n0.0030688312835991383\n0.0030662959907203913\n0.0030629171524196863\n0.0030595401767641306\n0.003056165063753724\n0.003052791813388467\n0.0019426066428422928\n0.0030460509005934\n0.0030426832381635904\n0.00303931743837893\n0.003035953501239419\n0.003032591426745057\n0.003030911087989807\n0.003025872865691781\n0.003022516379132867\n0.003019161755219102\n0.003015808993950486\n0.0030124580953270197\n0.0030091090593487024\n0.0030057618860155344\n0.0030024165753275156\n0.0029999087564647198\n0.0029957315418869257\n0.0029923918191343546\n0.0027938038110733032\n0.0008259835303761065\n0.0029840506613254547\n0.0029815505258738995\n0.0029782187193632126\n0.002974888775497675\n0.002972392598167062\n0.0029690659139305353\n0.002965741092339158\n0.0029615876264870167\n0.002958266995847225\n0.002954948227852583\n0.0008242303156293929\n0.0029491449240595102\n0.002946659456938505\n0.002941691782325506\n0.0029392095748335123\n0.002935901517048478\n0.002932595321908593\n0.0029292909894138575\n0.0008898845408111811\n0.0029235128313302994\n0.002920213621109724\n0.0029177404940128326\n0.0029111504554748535\n0.0029111504554748535\n0.002907858230173588\n0.0029045678675174713\n0.002901279367506504\n0.002897992730140686\n0.0028947079554200172\n0.0027488209307193756\n0.0028889640234410763\n0.002885684370994568\n0.0028824065811932087\n0.0028791306540369987\n0.002875856589525938\n0.002871766686439514\n0.002868496812880039\n0.0028652288019657135\n0.002861962653696537\n0.0028603302780538797\n0.002857066923752427\n0.002852175384759903\n0.0028489166870713234\n0.002846473827958107\n0.0028432183898985386\n0.0028399648144841194\n0.0028367131017148495\n0.0028334632515907288\n0.00282940361648798\n0.002826969139277935\n0.0028245358262211084\n0.0028212929610162973\n0.0028172419406473637\n0.002814812818542123\n0.0028115755412727594\n0.002809148747473955\n0.0023433836176991463\n0.0028026825748384\n0.0028002597391605377\n0.002797030843794346\n0.0027938038110733032\n0.00279057864099741\n0.0027865497395396233\n0.0027833287604153156\n0.0027801096439361572\n0.002776892390102148\n0.0027728735003620386\n0.002769660437479615\n0.0027680546045303345\n0.002764844335615635\n0.0027608340606093407\n0.002757627982646227\n0.0027544237673282623\n0.002751221414655447\n0.002748020924627781\n0.002744822297245264\n0.0027416255325078964\n0.002738430630415678\n0.002734439680352807\n0.002732844091951847\n0.002728857100009918\n0.0027256696484982967\n0.0027232803404331207\n0.0027200961485505104\n0.002595810452476144\n0.0019426066428422928\n0.0021758603397756815\n0.002709760330617428\n0.0027073780074715614\n0.0027049966156482697\n0.0027018231339752674\n0.002699444303289056\n0.0026962740812450647\n0.002693105721846223\n0.0013305083848536015\n0.002686774590983987\n0.0026844022795557976\n0.0026812409050762653\n0.0026780813932418823\n0.0026749237440526485\n0.0026709793601185083\n0.0026678259018808603\n0.0026646743062883615\n0.0026607373729348183\n0.0026568034663796425\n0.002653658390045166\n0.0026497296057641506\n0.00264658872038126\n0.002516353502869606\n0.0023014694452285767\n0.0026426652912050486\n0.002637177240103483\n0.002251237165182829\n0.002630912233144045\n0.002626218367367983\n0.002622310072183609\n0.002619966398924589\n0.0026176238898187876\n0.0026152823120355606\n0.002612941898405552\n0.0026098228991031647\n0.0026067057624459267\n0.0026043690741062164\n0.0026012551970779896\n0.002597365528345108\n0.002594255842268467\n0.002591148018836975\n0.0025872658006846905\n0.0025841621682047844\n0.002580285305157304\n0.00257796049118042\n0.0025756368413567543\n0.0025709925685077906\n0.0025686719454824924\n0.0025655794888734818\n0.0025624888949096203\n0.002559400163590908\n0.002557084895670414\n0.002553999423980713\n0.0021829837933182716\n0.0025486042723059654\n0.0013511816505342722\n0.0025432149413973093\n0.0005518226535059512\n0.002538600005209446\n0.0025355257093906403\n0.0025332211516797543\n0.0025301501154899597\n0.0025270809419453144\n0.0025247803423553705\n0.0025224806740880013\n0.002519416157156229\n0.002516353502869606\n0.002514057792723179\n0.002510998398065567\n0.0025079408660531044\n0.0023176020476967096\n0.0025025946088135242\n0.002499542199075222\n0.002496491651982069\n0.002170169958844781\n0.0024903961457312107\n0.00248811230994761\n0.0024850687477737665\n0.0024820270482450724\n0.001985207200050354\n0.0024759492371231318\n0.0024729131255298853\n0.002469878876581788\n0.00246684649027884\n5.5220210924744606e-05\n0.0024607873056083918\n0.0024577605072408915\n0.0024547355715185404\n0.0024509569630026817\n0.0024479362182319164\n0.0024464265443384647\n0.0024434085935354233\n0.0024396388325840235\n0.002436625072732568\n0.0024343659169971943\n0.0024313554167747498\n0.0024290988221764565\n0.002426091581583023\n0.0015642642974853516\n0.002420082688331604\n0.0013551106676459312\n0.0024148309603333473\n0.002411832567304373\n0.0024110833182930946\n0.002407338470220566\n0.0018680132925510406\n0.0024028485640883446\n0.0024006052408367395\n0.002397615695372224\n0.0023953747004270554\n0.002393134869635105\n0.0023901499807834625\n0.002387166954576969\n0.002384185791015625\n0.00238120649009943\n0.0023782290518283844\n0.0023745098151266575\n0.0023715365678071976\n0.002368565183132887\n0.0023648536298424006\n0.0023641115985810757\n0.0023618864361196756\n0.0023581800051033497\n0.0023529960308223963\n0.0023507759906351566\n0.0023478176444768906\n0.0023448611609637737\n0.002341906540095806\n0.0023389537818729877\n0.0023360028862953186\n0.0023330538533627987\n0.002329370239749551\n0.0023264253977686167\n0.0023242179304361343\n0.0023212763480842113\n0.0023176020476967096\n0.0023146646562963724\n0.0023117291275411844\n0.0023087954614311457\n0.002305863657966256\n0.0023022014647722244\n0.0022992738522589207\n0.002296348102390766\n0.002293424215167761\n0.002291962970048189\n0.002287582028657198\n0.0018195295706391335\n0.0022824762854725122\n0.0022802897728979588\n0.0022781044244766235\n0.002275192178785801\n0.0022730091586709023\n0.002270827302709222\n0.0022679197136312723\n0.002265013987198472\n0.00226283585652709\n0.002259933389723301\n0.002257032785564661\n3.8852973375469446e-05\n0.0022519612684845924\n0.002249065786600113\n0.0022468953393399715\n0.002244003117084503\n0.002241835230961442\n0.0022389462683349848\n0.002236059168353677\n0.002233173931017518\n0.0022302905563265085\n0.0022274090442806482\n0.0021305459085851908\n0.002222370821982622\n0.0022202134132385254\n0.0022173384204506874\n0.0022144652903079987\n0.002211594022810459\n0.002208724617958069\n0.0005044937133789062\n0.002202991396188736\n0.0022001275792717934\n0.002197265625\n0.002194405533373356\n0.002191547304391861\n0.0021779960952699184\n0.002185836434364319\n0.00218369672074914\n0.0021808454766869545\n0.0021779960952699184\n0.0021751485764980316\n0.002172302920371294\n0.0021694591268897057\n0.0021659070625901222\n0.0021644870284944773\n0.0021630674600601196\n0.0021602297201752663\n0.002155976602807641\n0.0021531435195356607\n0.002151019871234894\n0.002148897387087345\n0.002146068960428238\n0.0021439488045871258\n0.0021411236375570297\n0.002138300333172083\n0.0021354788914322853\n0.002133364090695977\n0.002129841595888138\n0.002127025742083788\n0.0021242117509245872\n0.002121399622410536\n0.0021185893565416336\n0.002115079201757908\n0.0021122731268405914\n0.002111571840941906\n0.002108768094331026\n0.0021052660886198282\n0.0021017668768763542\n0.0020989696495234966\n0.0020968730095773935\n0.002094079041853547\n0.002092682756483555\n0.0020891940221190453\n0.002086405176669359\n0.0020836181938648224\n0.002080833073705435\n0.0020780498161911964\n0.0020752684213221073\n0.0020724888890981674\n0.0020697112195193768\n0.002067629247903824\n0.0020648548379540443\n0.0020613893866539\n0.0020586191676557064\n0.002055850811302662\n0.0020537758246064186\n0.0016147554852068424\n0.0020489380694925785\n0.0020461762323975563\n0.0020441061351448298\n0.0005051793996244669\n0.0020392797887325287\n0.002036524470895529\n0.002034459263086319\n0.0020323949865996838\n0.002029644325375557\n0.002027582610026002\n0.0020248352084308863\n0.00202208966948092\n0.0020200316794216633\n0.002017289400100708\n0.002014548983424902\n0.002011810429394245\n0.0020097577944397926\n0.002007022500038147\n0.0020042890682816505\n0.0020015574991703033\n0.0019988277927041054\n0.0019960999488830566\n0.00010234513320028782\n0.0019913306459784508\n0.0019886079244315624\n0.0019858870655298233\n0.0019831680692732334\n0.001981130102649331\n0.0019784143660217524\n0.001975700492039323\n0.0019729884807020426\n0.0019702783320099115\n0.0019682468846440315\n0.001964863622561097\n0.001962834969162941\n0.0019601318053901196\n0.0019581057131290436\n0.0019554058089852333\n0.0019527077674865723\n0.0019500115886330605\n0.0019479906186461449\n0.0018785800784826279\n0.0019419342279434204\n0.0010963762179017067\n0.001937901834025979\n0.001935215899720788\n0.001933202613145113\n0.0019318610429763794\n0.0019291792996227741\n0.001926499418914318\n0.0019244907889515162\n0.0019218141678720713\n1.4032761100679636e-05\n0.0019178027287125587\n0.001915130764245987\n0.0019131279550492764\n0.0019104592502117157\n0.0019077924080193043\n0.001905127428472042\n0.0019031299743801355\n0.0018865247257053852\n2.2375024855136871e-07\n0.0018964791670441628\n0.0018938221037387848\n0.0018918304704129696\n0.0018891766667366028\n0.0018871875945478678\n0.0018845370505005121\n0.0018818883690983057\n0.0018792415503412485\n0.0018765965942293406\n0.0018739535007625818\n0.0018713122699409723\n0.0018693325109779835\n0.0018666945397853851\n0.001864058431237936\n0.001606182660907507\n0.0014785677194595337\n0.0018574763089418411\n0.0018548467196524143\n0.0018528758082538843\n0.0018436918035149574\n0.0018482808955013752\n0.0018456578254699707\n0.0018436918035149574\n0.0008449482265859842\n0.00183845404535532\n0.0018364917486906052\n0.0007741973386146128\n0.0018319173250347376\n0.0018293058965355158\n0.0018273484893143177\n0.0018117274157702923\n0.0018234369345009327\n0.0018195295706391335\n0.0018175775185227394\n0.0018149763345718384\n0.0018123770132660866\n0.001809779554605484\n0.0018071839585900307\n0.0018045902252197266\n0.0018013506196439266\n0.0017987610772252083\n0.0017968202009797096\n0.0017948802560567856\n0.0017916494980454445\n0.001586673315614462\n0.0017884215340018272\n0.0017851965967565775\n0.001783262938261032\n0.001781330443918705\n0.0017793988808989525\n0.0017774684820324183\n0.0017748961690813303\n0.0017723257187753916\n0.001769757131114602\n0.0017671904060989618\n0.0007154906634241343\n0.0007049180567264557\n0.001760141458362341\n0.0017582215368747711\n0.0017556631937623024\n0.0017537456005811691\n0.000593813369050622\n0.0017505520954728127\n0.001746723661199212\n0.0017448109574615955\n0.0017422623932361603\n0.0017397156916558743\n0.0017378069460391998\n0.001735263504087925\n0.0017327219247817993\n0.001730182208120823\n0.0017276443541049957\n0.001724474597722292\n0.0017219409346580505\n0.0017206748016178608\n0.0017168791964650154\n0.0017162470612674952\n0.0017137194517999887\n0.0017111937049776316\n0.0017080390825867653\n0.0017055175267159939\n0.0017036276403814554\n0.0017011093441396952\n0.0016992217861115932\n0.0016967067494988441\n0.0016941935755312443\n0.0012967735528945923\n0.0016898000612854958\n0.0016872920095920563\n0.001684785820543766\n0.001682281494140625\n0.0016804044134914875\n0.0016779033467173576\n0.001675404142588377\n0.0016729068011045456\n0.0008699691388756037\n0.0016685409937053919\n0.001666671596467495\n0.0016641807742416859\n0.0016623139381408691\n0.0016598263755440712\n0.0016573406755924225\n0.0016554775647819042\n0.0016529951244592667\n0.0016505145467817783\n0.0016486553940922022\n0.001646178076043725\n0.001643702620640397\n0.0016412290278822184\n0.0016387572977691889\n0.0016362874303013086\n0.0016338194254785776\n0.0016313532833009958\n0.0016295048408210278\n0.0016270419582724571\n0.0016258112154901028\n0.0016221217811107635\n0.0016202786937355995\n0.00161843653768301\n0.001616595545783639\n0.0016147554852068424\n0.0016123037785291672\n0.0016098539344966412\n0.0016080178320407867\n0.0016055712476372719\n0.0016031265258789062\n0.0016006836667656898\n0.0015982426702976227\n0.0015958035364747047\n0.0015939753502607346\n0.0015915394760668278\n0.0015897138509899378\n0.0015872812364250422\n0.0015848504845052958\n0.0015824215952306986\n0.0015793880447745323\n0.0004891888820566237\n0.0015763575211167336\n0.0004237054381519556\n0.001571514643728733\n0.0015690959990024567\n0.0015672831796109676\n0.0015654715243726969\n0.0015630575362592936\n0.0015612482093274593\n0.0015588374808430672\n0.0015570307150483131\n0.0015546232461929321\n0.0015522176399827003\n0.0015504145994782448\n0.0015480122528970242\n0.0015456117689609528\n0.0015432131476700306\n0.0015408163890242577\n0.001289640087634325\n0.001163041451945901\n0.0015348326414823532\n0.0015330398455262184\n0.0015312479808926582\n0.0015288605354726315\n0.001526474952697754\n0.0015246870461851358\n0.0015223047230392694\n0.001520519144833088\n0.0015181400813162327\n0.0015157628804445267\n0.00151338754221797\n0.0015110140666365623\n0.001508642453700304\n0.001506272703409195\n0.001503904815763235\n0.0015021301805973053\n0.0014985839370638132\n0.0013952851295471191\n0.001496812328696251\n0.0014950418844819069\n0.0014920933172106743\n0.0014885589480400085\n0.0014867933932691813\n0.0014850287698209286\n0.0014832653105258942\n0.0014815027825534344\n0.0014791544526815414\n0.0014773944858461618\n0.00147504941560328\n0.0014727062080055475\n0.001196583965793252\n0.001468610018491745\n0.0014662719331681728\n0.00146393571048975\n0.0014616013504564762\n0.0008525052689947188\n0.0014575207605957985\n0.0014551915228366852\n0.00046650884905830026\n0.0014511197805404663\n0.0014487956650555134\n0.0014476343058049679\n0.0014458931982517242\n0.001442993525415659\n3.651157021522522e-05\n0.001439517829567194\n0.001437781611457467\n0.0014360463246703148\n0.0014337343163788319\n0.0014320015907287598\n0.001429692842066288\n0.0014279624447226524\n0.0014256569556891918\n0.0014239291194826365\n0.001421626890078187\n0.0014199013821780682\n0.0014170280192047358\n0.0014153053052723408\n0.001413010060787201\n0.0014107166789472103\n0.0014089979231357574\n0.001406707800924778\n0.0014044195413589478\n0.0004373188712634146\n0.0014004195109009743\n0.00043476984137669206\n0.00035253592068329453\n0.0013947151601314545\n0.0013935756869614124\n0.001391867408528924\n0.0013901600614190102\n0.0013878853060305119\n0.0013861805200576782\n0.0013844766654074192\n0.001382206566631794\n0.0013805052731186152\n1.1578435078263283e-05\n0.0013765394687652588\n0.0013748416677117348\n0.0013731447979807854\n0.0013708840124309063\n0.0013044776860624552\n0.0013674963265657425\n0.0013658041134476662\n0.0013641128316521645\n0.0013618594966828823\n0.0013601707760244608\n0.0013579207006841898\n0.001355672487989068\n0.0013534261379390955\n0.0013511816505342722\n0.0013489390257745981\n0.0013466982636600733\n0.0013444593641906977\n0.0013433406129479408\n0.0009933230467140675\n0.0013394285924732685\n0.0013366378843784332\n0.0013349647633731365\n0.001333292806521058\n0.0008898845408111811\n0.0013299519196152687\n0.0013277269899845123\n0.0013260594569146633\n0.0013243930879980326\n0.0013221728149801493\n0.001320508774369955\n0.001318291760981083\n0.00131607661023736\n0.0013138633221387863\n0.0013116518966853619\n0.0013094423338770866\n0.0013077864423394203\n0.0013061314821243286\n0.0013033756986260414\n0.0013011731207370758\n0.0012989724054932594\n0.0012973230332136154\n0.0012951255775988102\n0.0012929299846291542\n0.0012918328866362572\n0.001289640087634325\n0.0012874491512775421\n0.0012852600775659084\n0.0012836195528507233\n0.0012814337387681007\n0.0012797955423593521\n0.0012776129879057407\n0.0012759773526340723\n0.001273798057809472\n0.001271620625630021\n0.0012694450560957193\n0.0012678145430982113\n0.0012650995049625635\n0.0012640142813324928\n0.0012623872607946396\n0.0012596780434250832\n0.0012580538168549538\n0.0012564307544380426\n0.0012542682234197855\n0.0007472634897567332\n0.001250488217920065\n0.0012488700449466705\n0.0012467140331864357\n0.00124455988407135\n0.0012429454363882542\n0.0012407945469021797\n0.001239182660356164\n0.0012375717051327229\n0.0012359619140625\n0.0012338170781731606\n0.0012316741049289703\n0.001230068039149046\n0.0012279283255338669\n0.001225790474563837\n0.0012236544862389565\n0.001221520360559225\n0.001219388097524643\n0.00121725769713521\n0.0012156611774116755\n0.0012140655890107155\n0.001099409768357873\n0.0012103468179702759\n0.0010082873050123453\n0.0012071637902408838\n0.0012055737897753716\n0.00014771061250939965\n0.0012029262725263834\n0.001201339066028595\n0.0011992244981229305\n0.0007393592968583107\n0.0011960561387240887\n0.0011944735888391733\n0.0011928919702768326\n0.001190784852951765\n0.0011892057955265045\n0.0011871019378304482\n0.001184999942779541\n0.0011828998103737831\n0.0011808015406131744\n0.001178705133497715\n0.0011776576284319162\n0.0011760871857404709\n0.0011734722647815943\n0.0011713823769241571\n0.0011698161251842976\n0.0011682510375976562\n0.0011661658063530922\n0.0007339753792621195\n0.0011625210754573345\n0.00116096087731421\n0.00115940161049366\n0.0011573242954909801\n0.0011552488431334496\n0.0011531752534210682\n0.001151103526353836\n0.0011490336619317532\n0.0011490336619317532\n0.0011464490089565516\n0.0011433511972427368\n0.0011418038047850132\n0.0011402575764805079\n0.0011381974909454584\n0.0011371681466698647\n0.0011356249451637268\n0.0011330554261803627\n0.0009608902619220316\n0.001129975775256753\n0.0011289501562714577\n0.0011274125427007675\n0.0011258760932832956\n0.00032639503479003906\n0.0002559628919698298\n0.001121272798627615\n0.0011197405401617289\n0.0011182092130184174\n0.0011166790500283241\n7.32766930013895e-05\n0.0011136217508465052\n0.0011120946146547794\n0.001110060140490532\n0.0011085355654358864\n0.00110650435090065\n0.001104982104152441\n0.000724087527487427\n0.0011009280569851398\n0.001099409768357873\n0.0010973869357258081\n0.0010958709754049778\n0.0010938514024019241\n0.0007077567861415446\n0.0010898178443312645\n0.0010883072391152382\n0.0010862946510314941\n0.0010842839255928993\n0.0010822750627994537\n0.0010807695798575878\n0.0010792652610689402\n0.0010772610548883677\n0.0010757590644061565\n0.0010742582380771637\n0.0010722586885094643\n0.001070760190486908\n0.0010687639005482197\n0.0010672679636627436\n0.0010652749333530664\n0.0008020401000976562\n0.001061791554093361\n0.0010598036460578442\n0.0010593070182949305\n0.0010563293471932411\n0.0007874148432165384\n0.0010533558670431376\n0.0010518706403672695\n0.001049892045557499\n0.001048409380018711\n0.0010464340448379517\n0.0010449537076056004\n0.001042981632053852\n0.001041011419147253\n0.0010390430688858032\n0.0010380595922470093\n0.0010356029961258173\n0.0010341303423047066\n0.0010326588526368141\n0.0010311882942914963\n0.0010297189000993967\n0.001027761260047555\n0.001026294194161892\n0.0010243398137390614\n0.00102238729596138\n0.001020924188196659\n0.001018974930047989\n0.0010175141505897045\n0.0010160545352846384\n0.0010141099337488413\n0.0010126526467502117\n0.001011681742966175\n0.0010092565789818764\n0.0010073184967041016\n0.0010058660991489887\n0.001003931276500225\n0.0010024814400821924\n0.0010010325349867344\n0.0009991023689508438\n0.0009971740655601025\n0.0009957291185855865\n0.0009942851029336452\n0.000992361456155777\n0.0009909200016409159\n0.0009894794784486294\n0.0009875604882836342\n0.0009856433607637882\n0.0009842067956924438\n0.000982292927801609\n0.0009808586910367012\n0.0009679982904344797\n0.00097751640714705\n0.0009760857210494578\n0.0009746560826897621\n0.0006243135430850089\n0.0009713243343867362\n0.0009698981884866953\n0.0009684730903245509\n0.0009665745892561972\n0.0009646779508329928\n0.0009632566943764687\n0.0009538084268569946\n0.0009599445038475096\n0.0009585267398506403\n0.0009571100235916674\n0.0009552226983942091\n0.0009538084268569946\n0.0009519243612885475\n0.0009500421583652496\n0.0009486317285336554\n0.0009467527852393687\n0.0009448757045902312\n0.0009439378627575934\n0.0009420635760761797\n0.0009401911520399153\n0.0009387880563735962\n0.0009373860084451735\n0.0009359850082546473\n0.0009341186378151178\n0.00093272008234635\n0.0009308569715358317\n0.0009294608607888222\n0.0009276010096073151\n0.0009262073435820639\n0.000924350752029568\n0.0009229595307260752\n0.0009215693571604788\n0.0005867696017958224\n0.0009187921532429755\n0.0009169430122710764\n0.0009160191402770579\n0.0009146342054009438\n0.0009132503182627261\n0.0009118674788624048\n0.00091048568719998\n0.0009091049432754517\n0.0009072655811905861\n2.1446554455906153e-05\n0.0009045100305229425\n0.00090267532505095\n0.0009013005183078349\n0.0008994690724648535\n0.0008980967104434967\n0.0008967253961600363\n0.0008953551296144724\n0.000893529737368226\n0.0008921619155444205\n0.0008907951414585114\n0.0008889744058251381\n0.0008876100764609873\n0.0008857926004566252\n0.0008839769870974123\n0.0003829612978734076\n0.0008821632363833487\n0.0008803513483144343\n0.0008780891075730324\n0.0008767331601120532\n0.0008753782603889704\n0.000874024408403784\n0.0008726716041564941\n0.0008708694949746132\n0.0008695191354490817\n0.0008677202858962119\n0.000726964557543397\n0.000865025504026562\n0.0008632313110865653\n0.0008618868887424469\n0.0006904089823365211\n0.0008596485131420195\n0.000858306884765625\n0.0008569663041271269\n0.0008556267712265253\n0.0008542882860638201\n0.0008529508486390114\n0.000851169228553772\n0.0003655462060123682\n0.0008480558753944933\n0.0008467233274132013\n0.0008453918271698058\n0.000338918121997267\n0.0008436181233264506\n0.000328604131937027\n0.0008396340999752283\n0.0008387500420212746\n0.0008374248282052577\n0.0008361006621271372\n0.0008347775437869132\n0.0008330150158144534\n0.0008316943421959877\n0.0008299350738525391\n0.0008286168449558318\n0.0004688187036663294\n0.0008255450520664454\n0.0008242303156293929\n0.0008229166269302368\n0.0008211666718125343\n0.0008198554278351367\n0.0008181087323464453\n0.000816799933090806\n0.0007780231535434723\n0.0008150564972311258\n0.0008137501426972449\n0.0008115752134472132\n0.0005210787057876587\n0.0008085352019406855\n0.0008072340860962868\n0.0008063672576099634\n0.0008050678879953921\n0.0008037695661187172\n0.0008020401000976562\n0.0008007442229427397\n0.0007994493935257196\n0.0007977245841175318\n0.0007960016373544931\n0.0007947106496430933\n0.0007929909625090659\n0.0007917024195194244\n0.0007899859920144081\n0.000788699893746525\n0.0007869867258705199\n0.0007857030723243952\n0.0007852754206396639\n0.0007831389084458351\n0.0007814317941665649\n0.0007801526808179915\n0.0007788746152073145\n0.0007775975973345339\n0.0007758965366519988\n0.0007746219635009766\n0.0007733484380878508\n0.0007720759604126215\n0.0007708045304752886\n0.0007691109203733504\n0.0007682648138143122\n0.0007665739976800978\n0.0007653071079403162\n0.0006338814273476601\n0.0007623551064170897\n0.0007610917091369629\n0.0007598293595947325\n0.0007581478566862643\n0.0007568879518657923\n0.0007552097085863352\n0.0007543712854385376\n0.0007526958361268044\n0.0007514404715038836\n0.0007501861546188593\n0.0007485153619199991\n0.0007472634897567332\n0.000746429490391165\n0.0007447628886438906\n0.000743514159694314\n0.0007418508175760508\n0.0007406045333482325\n0.0006980477483011782\n0.0007377006113529205\n0.0007364578195847571\n0.0007352160755544901\n0.000733562046661973\n0.0007327357307076454\n0.0007310844957828522\n0.0007298472919501364\n0.0007286111358553171\n0.0007273760274983943\n0.0005089589976705611\n0.0007249089539982378\n0.000724087527487427\n0.0007228562608361244\n0.0007216260419227183\n3.415369428694248e-05\n0.0007191687473095953\n0.0007179416716098785\n0.0007167156436480582\n1.7220154404640198e-06\n0.0007142667309381068\n0.0007134513580240309\n0.0007122291717678308\n0.000710601219907403\n0.00070978794246912\n0.0007085688994266093\n0.000707350904121995\n0.0007061339565552771\n0.0007049180567264557\n0.0007032984867691994\n0.0007020850316621363\n0.0007008726242929697\n3.762636333703995e-05\n0.0006984509527683258\n0.0006812182837165892\n0.0006960334721952677\n0.0006948263035155833\n0.0006936201825737953\n0.0006924151093699038\n0.0006912110839039087\n0.0006900081061758101\n0.0006884057656861842\n0.0006872052326798439\n0.0006860057474114001\n0.0006848073098808527\n0.0006832110229879618\n0.0006824135780334473\n0.0006808200851082802\n0.0006800240371376276\n0.0006788308382965624\n0.0006776386871933937\n0.0006760507822036743\n0.000674861075822264\n0.00067367241717875\n0.0006720891688019037\n0.0006709029548801482\n0.0006697177886962891\n0.000629279762506485\n0.0006681391969323158\n0.0006661685765720904\n0.000664987601339817\n0.0006638076738454401\n0.0006626287940889597\n0.0006614509620703757\n0.0006598821491934359\n0.0006587067618966103\n0.0006571412086486816\n0.000656750111375004\n0.000655186886433512\n0.0006536255241371691\n0.0006524557247757912\n0.0006512869731523097\n0.0006501192692667246\n0.000648952613119036\n0.0006477870047092438\n0.0005626283236779273\n0.0006454589311033487\n0.0006442964659072459\n0.0006431350484490395\n0.0006415881216526031\n0.0006404291489161551\n0.0006392712239176035\n0.0006377289537340403\n0.0006369585171341896\n0.0006354190409183502\n0.0006342656561173499\n0.000633113319054246\n0.0006319620297290385\n0.0006308117881417274\n0.0006296625942923129\n0.0006285144481807947\n0.000627367349807173\n0.0006258395151235163\n0.000624694861471653\n0.0006235512555576861\n0.0006224086973816156\n0.0006212671869434416\n0.0006197468028403819\n0.0006186077371239662\n0.000617469719145447\n0.0006163327489048243\n0.0006151968264020979\n0.0006140619516372681\n0.0006129281246103346\n0.0006117953453212976\n0.0006102866027504206\n0.000609532929956913\n0.0006080269813537598\n0.000606898742262274\n0.0006061471649445593\n0.000605020672082901\n0.0006035203114151955\n0.00039925571763888\n0.00032446818659082055\n0.0006005251780152321\n0.000599403923843056\n0.0005982837174087763\n0.000597164558712393\n0.0005960464477539062\n0.0005949293845333159\n0.000593813369050622\n0.0005926984013058245\n0.0005915844812989235\n0.0005908424500375986\n0.000589730276260525\n0.0005886191502213478\n0.0005871392786502838\n0.000586030597332865\n0.0005849229637533426\n0.0005838163779117167\n0.0005823425599373877\n0.0005812384188175201\n0.000580135325435549\n0.0005790332797914743\n0.0005775655154138803\n0.000576465914491564\n0.0005753673613071442\n0.0005742698558606207\n0.0005731733981519938\n0.00039742846274748445\n0.0005148279014974833\n0.0005706190713681281\n0.0005691620172001421\n0.0005673433188349009\n0.0005666166543960571\n0.0005655275308527052\n0.0005644394550472498\n0.0005633524269796908\n0.0005622664466500282\n0.0005608201026916504\n0.0005597365670837462\n0.0005593756213784218\n0.0005575726390816271\n0.0005564922466874123\n0.0005554129020310938\n0.0005543346051126719\n0.0005532573559321463\n0.0005521811544895172\n0.0005511060007847846\n0.0005500318948179483\n0.0005489588365890086\n0.0005478868260979652\n0.0005468158633448184\n0.0005457459483295679\n0.0005446770810522139\n0.0005439650849439204\n0.0004754497203975916\n0.0005418318905867636\n0.00031817727722227573\n0.0005400574300438166\n0.0005393484607338905\n0.0005386399570852518\n0.0005375780747272074\n0.0005365172401070595\n0.000535457453224808\n0.0005343987140804529\n0.0005333410226739943\n0.0005322843790054321\n0.0005312287830747664\n0.0005298229516483843\n0.000528769800439477\n0.000527717696968466\n0.0005263165221549571\n0.0005256166332401335\n0.0005245676729828119\n0.0005231706891208887\n0.0005221241735853255\n0.0005210787057876587\n0.00033639464527368546\n0.0005193385877646506\n0.00021368358284235\n0.00015219493070617318\n0.0005165604525245726\n0.0005158670828677714\n0.0005151741788722575\n0.0005141356959939003\n1.3861805200576782e-05\n0.0005124072195030749\n0.0005113715305924416\n0.0005110265337862074\n0.0005096477107144892\n0.0004540691734291613\n0.0005079268012195826\n0.0005068956525065005\n0.0005058655515313148\n0.0005051793996244669\n0.0005041510448791087\n0.0005031237378716469\n0.0005020974786020815\n0.0005007307627238333\n0.0004997069481760263\n0.0004986841813661158\n0.0004973221221007407\n0.0004963018000125885\n0.0004949430003762245\n0.0004661793354898691\n0.0004929082933813334\n0.0004918925114907324\n0.000490877777338028\n0.0004898640909232199\n0.0004888514522463083\n0.0004878398613072932\n0.00048682931810617447\n0.0004858198226429522\n0.0004848113749176264\n0.000483803974930197\n0.00048279762268066406\n0.00048179231816902757\n0.0004807880613952875\n0.0004797848523594439\n0.00047878269106149673\n0.000477781577501446\n0.0004767815116792917\n0.000476115383207798\n0.0004747845232486725\n0.0004741197917610407\n0.0004731235676445067\n0.00047246000031009316\n0.0004714655224233866\n0.00031736126402392983\n0.00046981038758531213\n0.0004688187036663294\n0.00046815816313028336\n0.000467168225441128\n0.0004661793354898691\n0.00046519149327650666\n0.0003722869441844523\n0.0004632189520634711\n0.00046223425306379795\n0.00046125060180202127\n0.000460267998278141\n0.0004592864424921572\n0.00045863265404477715\n0.00045765284448862076\n0.00045634806156158447\n0.0004556963685899973\n0.0004547197022475302\n0.0004537440836429596\n0.0004527695127762854\n0.00045179598964750767\n0.00045082351425662637\n0.00044952851021662354\n0.0004485584795475006\n0.0004434026777744293\n0.00044662156142294407\n0.0004459768533706665\n0.00044501066440716386\n0.00044436712050810456\n0.0004430814296938479\n0.0004421183839440346\n0.00044147693552076817\n0.00044051563600078225\n0.0004392355331219733\n0.00037908926606178284\n0.00043763802386820316\n0.000436680915299803\n0.0004357248544692993\n0.00043476984137669206\n0.00043381587602198124\n0.00043318048119544983\n0.00043191108852624893\n0.00043127709068357944\n0.0004303269670344889\n0.00042937789112329483\n0.0004284298629499972\n0.0004277984262444079\n0.00042685214430093765\n0.00019900040933862329\n0.0004249627236276865\n0.0004240195848979056\n0.0004162008408457041\n0.00042245001532137394\n0.0004215096705593169\n0.00042088335612788796\n0.0004199447575956583\n0.0004190072068013251\n0.0004180707037448883\n0.00041713524842634797\n0.0004162008408457041\n0.000409998232498765\n0.0004146458231844008\n0.00041371420957148075\n0.00041278364369645715\n0.00041185412555933\n0.00041092565516009927\n0.000410616397857666\n0.00040907185757532716\n0.00040814653038978577\n0.000407530227676034\n0.000309795665089041\n0.00040568411350250244\n0.0004050696734338999\n0.00040414888644590974\n0.00040353561053052545\n0.0004026165697723627\n0.0004016985767520964\n0.00040108716348186135\n0.00040017091669142246\n0.00039925571763888\n0.000398341566324234\n0.00039742846274748445\n0.00039682030910626054\n0.0003959089517593384\n0.00039499864215031266\n0.0003940893802791834\n0.00039318116614595056\n0.00039227399975061417\n0.0003913678810931742\n0.0003904628101736307\n0.00038955878699198365\n0.00038865581154823303\n0.0003880544099956751\n0.00038715318078175187\n0.0003862529993057251\n0.00038535386556759477\n0.000384755025152117\n0.0003838576376438141\n0.0003829612978734076\n0.00038206600584089756\n0.0003814697265625\n0.00038057618075981736\n0.00037968368269503117\n0.0003787922323681414\n0.0003779018297791481\n0.0003297114744782448\n0.00037701247492805123\n0.0003755325451493263\n0.00037494138814508915\n0.0003740555257536471\n0.0003734655329026282\n0.0003725814167410135\n0.0003716983483172953\n0.00037081632763147354\n0.0003699353546835482\n0.0003690554294735193\n0.00036846939474344254\n0.00036788382567465305\n0.000366714084520936\n0.0003658380010165274\n0.0003652545274235308\n0.0003643801901489496\n0.00036350690061226487\n0.0003629252896644175\n0.00036205374635756016\n0.0003366745659150183\n0.0003606035024859011\n0.0003597347531467676\n0.00035886705154553056\n0.00035800039768218994\n0.00035713479155674577\n0.0003565583028830588\n0.0002812130842357874\n0.0003548316308297217\n0.00035425700480118394\n0.000353395938873291\n0.00035253592068329453\n0.000351963157299906\n0.00035110488533973694\n0.00011936202645301819\n0.000349676760379225\n0.00034910632530227304\n0.00034825154580175877\n0.00034739781403914094\n0.00034711346961557865\n0.00034597725607454777\n0.0003454098477959633\n0.00034455960849300027\n0.00034399336436763406\n0.00034314487129449844\n0.0003425797913223505\n0.0003417330444790423\n0.0003408873453736305\n0.0003400426940061152\n0.0003391990903764963\n0.0003383565344847739\n0.00033779541263356805\n0.0002835207269527018\n0.00033639464527368546\n0.00033527612686157227\n0.00033471756614744663\n0.0003338805981911719\n0.0003333232016302645\n0.0003324879799038172\n0.0003316538059152663\n0.00033109827199950814\n0.0002470100298523903\n0.0003297114744782448\n0.0003288807929493487\n0.0003283275873400271\n0.0003274986520409584\n0.0003269466105848551\n0.0003261194215156138\n0.00032556854421272874\n0.00032474310137331486\n0.0003239187062717974\n0.0003230953589081764\n0.00032254704274237156\n0.00032172544160857797\n0.0003209048882126808\n0.0003200853825546801\n0.00024294998729601502\n0.00031872186809778214\n0.00031817727722227573\n0.00031763315200805664\n0.0003168178373016417\n0.0003160035703331232\n0.00031546130776405334\n0.00031464878702536225\n0.00015351548790931702\n0.0003130268887616694\n0.00031275697983801365\n0.00031194795155897737\n0.0003111399710178375\n0.00031060189940035343\n0.000309795665089041\n0.00030925875762477517\n0.00030845426954329014\n0.00021861889399588108\n0.00030711578438058496\n0.00030658120522275567\n0.00030578020960092545\n0.00030498026171699166\n0.0003044475452043116\n0.00030364934355020523\n0.0003028521896339953\n0.00027157366275787354\n0.00030179094756022096\n0.00030073156813159585\n0.0002999382559210062\n0.00029940996319055557\n0.00029861839720979333\n0.00029809126863256097\n0.00029730144888162613\n0.00029677548445761204\n0.0002959874109365046\n0.0002954626106657088\n0.00029493827605620027\n0.0002941526472568512\n0.00029336806619539857\n0.0002928455942310393\n0.00029206275939941406\n0.00029154145158827305\n0.00029076036298647523\n0.00028998032212257385\n0.00028972054133191705\n0.0002889418974518776\n0.0002881643013097346\n0.000287387752905488\n0.00028687063604593277\n0.0001631139311939478\n0.00028557988116517663\n0.00028480682522058487\n0.00028429203666746616\n0.0002835207269527018\n0.0002827504649758339\n0.00028249394381418824\n0.00026037596398964524\n0.0002812130842357874\n0.0002801904920488596\n0.00027967989444732666\n0.00027916976250708103\n0.0002784054377116263\n0.000277642160654068\n0.0002771338913589716\n0.0002763723605312407\n0.00027561187744140625\n0.00020968751050531864\n0.00027485244208946824\n0.00023422972299158573\n0.00027358904480934143\n0.00027283240342512727\n0.0002723285579122603\n4.563480615615845e-08\n0.00027132226387038827\n0.00027056876569986343\n0.00027006701566278934\n0.00026956573128700256\n0.00022065412485972047\n0.0002683145576156676\n0.0002678149030543864\n0.0002670662943273783\n0.00026656780391931534\n0.00026582094142213464\n0.00026532361516728997\n0.0002645784988999367\n0.0002640823367983103\n0.0002633389667607844\n0.00026284396881237626\n0.00026210234500467777\n0.0002616085112094879\n0.00026086863363161683\n0.00026037596398964524\n0.00025963783264160156\n0.0002589007490314543\n0.00025840994203463197\n0.00024318788200616837\n0.00025767460465431213\n0.00025669578462839127\n0.00017562235007062554\n0.0002554748789407313\n0.0002549873315729201\n0.0002545002498663962\n0.0002540136338211596\n0.0002535274834372103\n0.0002527991309762001\n0.0002523141447454691\n0.0002518296241760254\n0.000251345569267869\n0.0002506203600205481\n0.00025013746926561\n0.0002494140062481165\n0.00024893227964639664\n0.0002484510187059641\n0.00024773000041022897\n0.0002472499036230147\n0.00024653063155710697\n0.00024605169892311096\n0.00024533417308703065\n0.0002448564046062529\n0.000244140625\n0.00024342589313164353\n0.00024294998729601502\n0.00024223700165748596\n0.00024176225997507572\n0.00024105102056637406\n0.0002405774430371821\n0.00023986794985830784\n0.00023915950441733003\n0.0002386877895332873\n0.00023798109032213688\n0.0002375105395913124\n0.0002368055866099894\n0.0002363362000323832\n0.0002356329932808876\n0.00023493083426728845\n0.00023446331033483148\n0.00023376289755105972\n0.00023329653777182102\n0.00023259787121787667\n0.00023213267559185624\n0.00023166794562712312\n0.00023097172379493713\n0.00023050815798342228\n0.0002298136823810637\n0.00022935128072276711\n0.0002236105501651764\n0.00022842787439003587\n0.0002279668697156012\n0.0002270462573505938\n0.00022658664966002107\n0.00022612750763073564\n0.0002254396677017212\n0.00022498168982565403\n0.00019728211918845773\n0.00022406713105738163\n0.0002236105501651764\n0.00022292655194178224\n0.00022247113520279527\n0.0002220161841250956\n0.00022133463062345982\n0.00022065412485972047\n0.00022020103642717004\n0.0002195222768932581\n0.00021907035261392593\n0.0002183933393098414\n0.0002179425791837275\n0.00021749228471890092\n0.00019323104061186314\n0.00014147261390462518\n9.837094694375992e-05\n0.00021569576347246766\n3.256741911172867e-05\n0.00021502398885786533\n0.00021480029681697488\n0.00021435326198115945\n0.00021390669280663133\n8.381903171539307e-09\n0.000213014951441437\n0.0002125697792507708\n0.00021212507272139192\n0.00021168083185330033\n0.00021123705664649606\n0.00021057226695120335\n0.00021012965589761734\n0.00020946661243215203\n0.0002090251655317843\n0.0002083638682961464\n0.00020770361879840493\n0.00020726403454318643\n0.00020660553127527237\n0.00020594807574525476\n0.00020529166795313358\n0.00020529166795313358\n0.00020485464483499527\n0.0002041999832727015\n0.00020332873100414872\n0.0002028938033618033\n0.00020245934138074517\n0.00020202534506097436\n0.00020159181440249085\n0.00020115874940529466\n0.00020072615006938577\n0.00020029401639476418\n0.0001998623483814299\n0.0001992157194763422\n0.00013324717292562127\n0.0001983551774173975\n0.00019771099323406816\n0.00019728211918845773\n0.0001968537108041346\n0.00019621197134256363\n5.0019531045109034e-05\n0.0001951447338797152\n0.00019471865380182862\n0.00017785374075174332\n0.00019429303938522935\n0.00019386789062991738\n0.00019323104061186314\n0.00019238353706896305\n0.00019196048378944397\n0.0001915378961712122\n0.00019111577421426773\n0.00019069411791861057\n0.00019006250659003854\n0.00018964201444759965\n0.00018922198796644807\n0.00013096723705530167\n0.0001881739590317011\n0.00018775556236505508\n0.00014475831994786859\n0.00018671160796657205\n0.00018629484111443162\n0.0001858785399235785\n0.00018525496125221252\n0.00018483982421457767\n0.00018442515283823013\n0.00015712861204519868\n0.00018359720706939697\n0.00018318393267691135\n0.00018277112394571304\n0.0001367931836284697\n0.00018194690346717834\n0.00018153549171984196\n0.00018112454563379288\n0.0001807140652090311\n0.00018030405044555664\n0.00017968990141525865\n0.00017928105080500245\n0.00017866864800453186\n0.00017826096154749393\n0.00017765030497685075\n0.0001772437826730311\n0.0001766348723322153\n0.00017622951418161392\n0.00017582462169229984\n0.00017562235007062554\n0.00017481442773714662\n0.00017441116506233811\n0.00017400836804881692\n0.00017360603669658303\n0.00010405021021142602\n0.00017040414968505502\n0.00017240183660760522\n2.302933717146516e-05\n0.00016233534552156925\n0.00017120182747021317\n0.00017080275574699044\n0.00017060339450836182\n0.00017020502127707005\n0.00016980711370706558\n0.00016940967179834843\n0.0001688143820501864\n0.0001684181042946875\n0.00016802229220047593\n0.00016742944717407227\n0.00016703479923307896\n0.00016664061695337296\n0.00016624690033495426\n0.00016565719852223992\n0.0001652646460570395\n0.00016487255925312638\n0.00016448093811050057\n0.00016408978262916207\n0.00016350392252206802\n0.0001631139311939478\n0.00016272440552711487\n0.00016214099014177918\n0.00016175262862816453\n0.00016136473277583718\n0.00016117095947265625\n0.0001605903380550444\n0.000160203839186579\n0.00015981780597940087\n0.00015943223843351007\n0.00015885476022958755\n0.00015847035683691502\n0.00015808641910552979\n0.00015751138562336564\n0.0001505054533481598\n0.00015712861204519868\n0.00015655532479286194\n0.00015598308527842164\n0.0001556021743454039\n0.00015541189350187778\n3.353232750669122e-05\n0.00015465193428099155\n0.0001542726531624794\n0.00015389383770525455\n0.00015351548790931702\n0.00015313760377466679\n0.00015276018530130386\n0.00015238323248922825\n2.690649125725031e-06\n0.00015163072384893894\n0.00015125516802072525\n0.00015069270739331841\n0.000150318315718323\n0.00014994438970461488\n0.00014957092935219407\n0.0001493843737989664\n0.00014882540563121438\n0.0001484533422626555\n0.00014808174455538392\n0.00014771061250939965\n0.0001473399461247027\n0.00014696974540129304\n0.0001466000103391707\n0.00014641531743109226\n0.00014586193719878793\n0.0001454935991205275\n0.0001451257267035544\n0.00014457479119300842\n0.00014420808292925358\n0.00014384184032678604\n0.00014329334953799844\n0.0001431107521057129\n0.0001167096197605133\n0.0001425636583007872\n0.00011457648361101747\n0.00014183582970872521\n0.00014110986376181245\n0.00014074757928028703\n0.00014056661166250706\n0.0001402050256729126\n0.00013984390534460545\n0.0001394832506775856\n0.00013912306167185307\n0.00013876333832740784\n0.00013840408064424992\n0.0001380452886223793\n3.762636333703995e-05\n0.00013750797370448709\n0.00013715034583583474\n0.0001367931836284697\n0.00013643648708239198\n0.00013608025619760156\n7.017538882791996e-05\n0.00013554678298532963\n0.00013519171625375748\n6.808200851082802e-06\n0.0001344829797744751\n0.00013430608669295907\n0.0001339526497758925\n3.550440305843949e-05\n3.282917896285653e-05\n0.00013289513299241662\n0.00013271928764879704\n0.00013236794620752335\n0.00013201707042753696\n0.0001316666603088379\n0.00013131671585142612\n0.00013096723705530167\n0.00013079267228022218\n4.756823182106018e-05\n0.0001300955773331225\n0.0001297477283515036\n0.00012940034503117204\n7.145944982767105e-05\n0.0001287069753743708\n0.00012836098903790116\n0.00012801546836271882\n0.00012767041334882379\n0.00012732582399621606\n0.00012698170030489564\n0.00012663804227486253\n0.0001264663878828287\n0.00012612342834472656\n0.00012560986215248704\n0.00012543890625238419\n0.00012509734369814396\n0.00012475624680519104\n0.00012441561557352543\n0.00012407545000314713\n0.00012373575009405613\n0.00012339651584625244\n0.00012305774725973606\n0.000122719444334507\n0.00012238160707056522\n0.00012204423546791077\n0.00012170732952654362\n0.00012137088924646378\n0.00012103491462767124\n0.00012069940567016602\n0.0001203643623739481\n0.00012002978473901749\n0.00011969567276537418\n0.00011936202645301819\n0.0001190288458019495\n0.00011869613081216812\n0.00011836388148367405\n0.00011803209781646729\n0.00011786638060584664\n0.00011736992746591568\n0.00011703954078257084\n0.00011687452206388116\n0.00011654483387246728\n0.00011621561134234071\n0.00011572265066206455\n0.00011539459228515625\n0.00011506699956953526\n0.00011473987251520157\n0.00011441321112215519\n0.00010203663259744644\n0.00011392409214749932\n0.00011359859490767121\n0.00011327356332913041\n0.00011311122216284275\n3.424292663112283e-05\n0.00011246302165091038\n0.00011213961988687515\n0.00011197809362784028\n0.00011165539035573602\n0.00011133315274491906\n1.805886859074235e-05\n0.00011069007450714707\n0.00011052959598600864\n0.00011020898818969727\n0.0001098888460546732\n0.00010956916958093643\n0.00010924995876848698\n0.0001087720156647265\n0.00010861293412744999\n0.00010813638800755143\n9.806849993765354e-05\n0.00010766088962554932\n0.00010734447278082371\n0.0001070285215973854\n0.00010671303607523441\n0.00010639801621437073\n0.00010608346201479435\n0.00010576937347650528\n0.00010561250383034348\n0.00010514259338378906\n0.00010498618939891458\n0.00010467373067513108\n0.0001043617376126349\n0.00010405021021142602\n0.00010373914847150445\n0.00010342855239287019\n0.0001032734289765358\n0.0001029635313898325\n0.0001026540994644165\n0.00010234513320028782\n0.00010096054757013917\n0.0001018825569190085\n0.00010142102837562561\n0.00010111392475664616\n0.00010096054757013917\n0.00010065414244309068\n0.00010034820297732949\n0.00010004272917285562\n9.973772102966905e-05\n9.943317854776978e-05\n9.912910172715783e-05\n9.89772379398346e-05\n9.86738596111536e-05\n4.507601261138916e-05\n9.806849993765354e-05\n9.79174510575831e-05\n9.761570254340768e-05\n9.731441969051957e-05\n9.701360249891877e-05\n9.686336852610111e-05\n9.656324982643127e-05\n9.626359678804874e-05\n9.596440941095352e-05\n9.566568769514561e-05\n9.5367431640625e-05\n9.50696412473917e-05\n9.477231651544571e-05\n9.447545744478703e-05\n9.432720253244042e-05\n9.40310419537127e-05\n9.373534703627229e-05\n6.235350156202912e-05\n9.314535418525338e-05\n9.285105625167489e-05\n9.270408190786839e-05\n9.241048246622086e-05\n9.211734868586063e-05\n9.197095641866326e-05\n9.1678521130234e-05\n9.124074131250381e-05\n9.10950475372374e-05\n9.080400923267007e-05\n9.051343658939004e-05\n9.036832489073277e-05\n8.99336882866919e-05\n8.978904224932194e-05\n8.950009942054749e-05\n8.921162225306034e-05\n8.906755829229951e-05\n3.2741809263825417e-05\n8.849246660247445e-05\n8.820561924949288e-05\n8.806237019598484e-05\n8.777622133493423e-05\n8.763332152739167e-05\n8.734787115827203e-05\n8.706288645043969e-05\n8.677836740389466e-05\n8.649431401863694e-05\n8.621072629466653e-05\n8.606910705566406e-05\n8.578621782362461e-05\n1.5438126865774393e-05\n8.522183634340763e-05\n8.508103201165795e-05\n8.479977259412408e-05\n8.465931750833988e-05\n8.437875658273697e-05\n8.409866131842136e-05\n8.381903171539307e-05\n8.353986777365208e-05\n4.4565240386873484e-05\n8.31219949759543e-05\n8.284399518743157e-05\n8.270516991615295e-05\n8.24278686195612e-05\n8.228939259424806e-05\n8.201278978958726e-05\n8.173665264621377e-05\n8.146098116412759e-05\n8.132332004606724e-05\n8.104834705591202e-05\n6.689131259918213e-05\n8.04997980594635e-05\n8.02262220531702e-05\n8.00896086730063e-05\n7.981673115864396e-05\n5.902542034164071e-05\n7.940828800201416e-05\n7.91365746408701e-05\n7.90008925832808e-05\n7.87298777140677e-05\n7.84593285061419e-05\n7.832422852516174e-05\n7.805437780916691e-05\n7.778499275445938e-05\n7.765047485008836e-05\n7.738178828731179e-05\n7.711356738582253e-05\n7.697963155806065e-05\n7.671210914850235e-05\n7.644505240023136e-05\n7.617846131324768e-05\n7.604534039273858e-05\n7.577944779768586e-05\n7.551402086392045e-05\n7.524905959144235e-05\n7.511675357818604e-05\n7.485249079763889e-05\n7.458869367837906e-05\n7.445696974173188e-05\n7.419387111440301e-05\n7.393123814836144e-05\n7.366907084360719e-05\n6.776774534955621e-05\n7.340736920014024e-05\n5.855743074789643e-05\n7.275515235960484e-05\n7.00476230122149e-05\n7.249508053064346e-05\n7.22354743629694e-05\n7.197633385658264e-05\n5.7395605836063623e-05\n7.158849621191621e-05\n7.145944982767105e-05\n7.120170630514622e-05\n7.107300916686654e-05\n6.808200851082802e-06\n7.068761624395847e-05\n7.043126970529556e-05\n7.030327105894685e-05\n7.00476230122149e-05\n6.979244062677026e-05\n6.966502405703068e-05\n6.9410540163517e-05\n6.915652193129063e-05\n6.890296936035156e-05\n3.317982191219926e-05\n6.86498824506998e-05\n6.839726120233536e-05\n6.814510561525822e-05\n6.789341568946838e-05\n6.776774534955621e-05\n6.764219142496586e-05\n6.739143282175064e-05\n6.726622814312577e-05\n6.701616803184152e-05\n6.676657358184457e-05\n6.664195097982883e-05\n6.639305502176285e-05\n6.614462472498417e-05\n6.602058419957757e-05\n6.577285239472985e-05\n6.552558625116944e-05\n6.540212780237198e-05\n6.527878576889634e-05\n6.503245094791055e-05\n6.478658178821206e-05\n6.454117828980088e-05\n6.441865116357803e-05\n6.417394615709782e-05\n6.405176827684045e-05\n6.380776176229119e-05\n6.36859331279993e-05\n6.344262510538101e-05\n6.319978274405003e-05\n6.307853618636727e-05\n6.283639231696725e-05\n6.271549500524998e-05\n6.247404962778091e-05\n6.223306991159916e-05\n6.199255585670471e-05\n6.187247345224023e-05\n6.163265788927674e-05\n6.139330798760056e-05\n6.127380765974522e-05\n6.11544237472117e-05\n6.091600516811013e-05\n6.067805225029588e-05\n6.044056499376893e-05\n6.03219959884882e-05\n6.008520722389221e-05\n5.996698746457696e-05\n5.9730897191911936e-05\n5.9613026678562164e-05\n5.93776348978281e-05\n5.926011363044381e-05\n5.902542034164071e-05\n5.879119271412492e-05\n5.855743074789643e-05\n5.59026375412941e-05\n5.820766091346741e-05\n5.8091303799301386e-05\n5.7858938816934824e-05\n5.7742930948734283e-05\n5.751126445829868e-05\n5.7395605836063623e-05\n5.7164637837558985e-05\n5.7049328461289406e-05\n1.9446248188614845e-05\n5.670409882441163e-05\n4.809588426724076e-05\n3.904342884197831e-05\n5.624542245641351e-05\n5.6131044402718544e-05\n5.59026375412941e-05\n5.5788608733564615e-05\n5.556090036407113e-05\n5.544722080230713e-05\n5.5220210924744606e-05\n5.5106880608946085e-05\n5.4880569223314524e-05\n5.465472349897027e-05\n5.4541975259780884e-05\n5.431682802736759e-05\n5.409214645624161e-05\n5.386793054640293e-05\n5.3755997214466333e-05\n5.353247979655862e-05\n5.34208957105875e-05\n5.330942803993821e-05\n5.308684194460511e-05\n5.29757235199213e-05\n2.636393764987588e-05\n5.2643066737800837e-05\n5.242187762632966e-05\n8.7177031673491e-06\n5.20909670740366e-05\n5.198089638724923e-05\n5.187094211578369e-05\n5.176110425963998e-05\n5.154177779331803e-05\n5.14322891831398e-05\n5.1322916988283396e-05\n5.1104521844536066e-05\n5.099549889564514e-05\n5.0777802243828773e-05\n5.066912854090333e-05\n5.045213038101792e-05\n4.620995605364442e-05\n5.0127506256103516e-05\n4.991167224943638e-05\n4.980392986908555e-05\n4.9588794354349375e-05\n4.948140121996403e-05\n4.9266964197158813e-05\n4.915992030873895e-05\n4.8946181777864695e-05\n4.883948713541031e-05\n4.862644709646702e-05\n4.852010169997811e-05\n4.8307760152965784e-05\n4.820176400244236e-05\n4.809588426724076e-05\n4.788447404280305e-05\n4.767352947965264e-05\n4.756823182106018e-05\n4.7463050577789545e-05\n4.7253037337213755e-05\n4.71482053399086e-05\n4.693889059126377e-05\n4.68344078399241e-05\n4.673004150390625e-05\n4.652165807783604e-05\n4.641764098778367e-05\n4.620995605364442e-05\n4.610628820955753e-05\n4.589930176734924e-05\n4.579598316922784e-05\n1.709931530058384e-05\n4.5486725866794586e-05\n4.528113640844822e-05\n4.5178516302257776e-05\n4.507601261138916e-05\n4.487135447561741e-05\n4.4769200030714273e-05\n4.4667162001132965e-05\n4.446343518793583e-05\n4.436174640432e-05\n4.415871808305383e-05\n4.405737854540348e-05\n2.1730142179876566e-05\n3.9234349969774485e-05\n4.365318454802036e-05\n4.355242708697915e-05\n4.335126141086221e-05\n4.3250853195786476e-05\n4.3050386011600494e-05\n4.2950327042490244e-05\n4.2750558350235224e-05\n4.2650848627090454e-05\n4.255125531926751e-05\n2.909504109993577e-05\n4.20550350099802e-05\n4.20550350099802e-05\n4.195614019408822e-05\n4.1857361793518066e-05\n9.832496289163828e-06\n4.1561725083738565e-05\n4.146341234445572e-05\n4.13652160204947e-05\n4.116917261853814e-05\n4.10713255405426e-05\n4.097359487786889e-05\n3.8757920265197754e-05\n4.068110138177872e-05\n4.058383638039231e-05\n2.1094689145684242e-05\n4.029273986816406e-05\n4.0195940528064966e-05\n4.00992576032877e-05\n4.0002691093832254e-05\n3.980990732088685e-05\n3.971369005739689e-05\n3.9521604776382446e-05\n3.9425736758857965e-05\n3.932998515665531e-05\n3.9138831198215485e-05\n3.230670699849725e-05\n3.8852973375469446e-05\n3.8757920265197754e-05\n3.866298357024789e-05\n3.856816329061985e-05\n3.8378871977329254e-05\n3.8284400943666697e-05\n3.8190046325325966e-05\n3.8001686334609985e-05\n3.7907680962234735e-05\n3.781379200518131e-05\n3.762636333703995e-05\n3.7532823625952005e-05\n3.743940033018589e-05\n3.725290298461914e-05\n3.7159828934818506e-05\n3.697403008118272e-05\n3.6881305277347565e-05\n2.2302905563265085e-05\n3.669620491564274e-05\n3.651157021522522e-05\n3.64194274879992e-05\n1.1839496437460184e-05\n3.6143697798252106e-05\n3.6052020732313395e-05\n3.596046008169651e-05\n3.577768802642822e-05\n3.577768802642822e-05\n3.559538163244724e-05\n3.541354089975357e-05\n3.5322795156389475e-05\n3.5232165828347206e-05\n3.505125641822815e-05\n3.084912896156311e-05\n3.4780765417963266e-05\n3.469083458185196e-05\n3.460102016106248e-05\n3.4511322155594826e-05\n3.4332275390625e-05\n2.8196722269058228e-05\n3.415369428694248e-05\n3.4064578358083963e-05\n3.397557884454727e-05\n3.379792906343937e-05\n3.370927879586816e-05\n3.3620744943618774e-05\n3.344402648508549e-05\n3.3355841878801584e-05\n2.6836118195205927e-05\n3.3091986551880836e-05\n3.300426760688424e-05\n3.291666507720947e-05\n3.2741809263825417e-05\n3.265455598011613e-05\n3.256741911172867e-05\n3.239349462091923e-05\n3.230670699849725e-05\n3.2220035791397095e-05\n3.213348099961877e-05\n1.4320015907287598e-05\n3.187451511621475e-05\n3.1788425985723734e-05\n3.170245327055454e-05\n3.161659697070718e-05\n3.153085708618164e-05\n3.1359726563096046e-05\n3.1359726563096046e-05\n3.118906170129776e-05\n3.110390389338136e-05\n3.101886250078678e-05\n3.084912896156311e-05\n3.0764436814934015e-05\n3.067986108362675e-05\n3.0595401767641306e-05\n3.0426832381635904e-05\n3.0342722311615944e-05\n3.0174851417541504e-05\n3.0091090593487024e-05\n3.000744618475437e-05\n2.9923918191343546e-05\n2.9757211450487375e-05\n2.967403270304203e-05\n2.9590970370918512e-05\n2.950802445411682e-05\n2.9425194952636957e-05\n2.925988519564271e-05\n2.9177404940128326e-05\n2.909504109993577e-05\n2.8930662665516138e-05\n2.8848648071289062e-05\n2.8766749892383814e-05\n2.7552247047424316e-05\n2.8603302780538797e-05\n2.844032132998109e-05\n2.8359005227684975e-05\n2.8277805540710688e-05\n2.8196722269058228e-05\n9.453448001295328e-06\n2.8034904971718788e-05\n2.7873553335666656e-05\n2.779305214062333e-05\n2.7712667360901833e-05\n8.381903171539307e-09\n2.7552247047424316e-05\n2.74722115136683e-05\n2.7392292395234108e-05\n2.7312489692121744e-05\n2.7153233531862497e-05\n2.7073780074715614e-05\n4.563480615615845e-08\n2.691522240638733e-05\n5.59026375412941e-07\n2.675713039934635e-05\n2.6678259018808603e-05\n2.6599504053592682e-05\n2.6520865503698587e-05\n2.644234336912632e-05\n2.636393764987588e-05\n2.6285648345947266e-05\n2.620747545734048e-05\n2.612941898405552e-05\n2.6051478926092386e-05\n2.597365528345108e-05\n2.58959480561316e-05\n2.5740882847458124e-05\n2.5663524866104126e-05\n2.5586283300071955e-05\n2.550915814936161e-05\n2.5432149413973093e-05\n2.5355257093906403e-05\n2.527848118916154e-05\n2.5201821699738503e-05\n2.5125278625637293e-05\n2.504885196685791e-05\n2.4972541723400354e-05\n2.4820270482450724e-05\n2.474430948495865e-05\n2.46684649027884e-05\n2.459273673593998e-05\n2.4517124984413385e-05\n2.4366250727325678e-05\n2.4290988221764565e-05\n2.4215842131525278e-05\n2.414081245660782e-05\n2.4065899197012186e-05\n2.399110235273838e-05\n2.384185791015625e-05\n2.3767410311847925e-05\n2.3693079128861427e-05\n2.3618864361196756e-05\n2.3470784071832895e-05\n2.3396918550133705e-05\n2.3323169443756342e-05\n2.3323169443756342e-05\n2.3176020476967096e-05\n2.3102620616555214e-05\n2.302933717146516e-05\n2.295617014169693e-05\n2.2810185328125954e-05\n2.2737367544323206e-05\n2.2664666175842285e-05\n2.259208122268319e-05\n2.2519612684845924e-05\n2.2447260562330484e-05\n1.52587890625e-05\n2.2302905563265085e-05\n2.2230902686715126e-05\n2.2159016225486994e-05\n2.208724617958069e-05\n2.201559254899621e-05\n2.194405533373356e-05\n2.1801330149173737e-05\n2.1730142179876566e-05\n2.1659070625901222e-05\n2.1588115487247705e-05\n2.1517276763916016e-05\n2.1375948563218117e-05\n2.1305459085851908e-05\n2.1235086023807526e-05\n2.116482937708497e-05\n2.1094689145684242e-05\n2.102466532960534e-05\n2.0954757928848267e-05\n2.08152923732996e-05\n2.08152923732996e-05\n2.0745734218508005e-05\n7.089751306921244e-06\n2.06069671548903e-05\n2.0537758246064186e-05\n2.04686657525599e-05\n2.039968967437744e-05\n2.033083001151681e-05\n2.0193459931761026e-05\n2.0124949514865875e-05\n2.005655551329255e-05\n1.9988277927041054e-05\n1.9920116756111383e-05\n9.97656024992466e-06\n1.9784143660217524e-05\n1.9716331735253334e-05\n1.964863622561097e-05\n5.704932846128941e-07\n1.9581057131290436e-05\n1.9513594452291727e-05\n1.937901834025979e-05\n1.9311904907226562e-05\n1.924490788951516e-05\n1.9178027287125587e-05\n1.911126310005784e-05\n1.904461532831192e-05\n1.8978083971887827e-05\n1.891166903078556e-05\n1.884537050500512e-05\n1.877918839454651e-05\n1.8713122699409723e-05\n1.8647173419594765e-05\n1.8581340555101633e-05\n1.851562410593033e-05\n1.1320284102112055e-05\n1.83845404535532e-05\n1.8319173250347376e-05\n1.825392246246338e-05\n1.818878808990121e-05\n1.8123770132660866e-05\n8.762814104557037e-06\n1.799408346414566e-05\n1.7929414752870798e-05\n1.7864862456917763e-05\n1.7800426576286554e-05\n1.7736107110977173e-05\n1.7671904060989618e-05\n4.794506821781397e-06\n1.754384720697999e-05\n1.7479993402957916e-05\n1.741625601425767e-05\n1.735263504087925e-05\n1.7289130482822657e-05\n1.722574234008789e-05\n1.716247061267495e-05\n1.709931530058384e-05\n1.5139812603592873e-05\n1.6973353922367096e-05\n1.6910547856241465e-05\n1.684785820543766e-05\n1.6785284969955683e-05\n1.6722828149795532e-05\n1.666048774495721e-05\n1.6598263755440712e-05\n1.6536156181246042e-05\n1.64741650223732e-05\n1.6412290278822184e-05\n1.6350531950592995e-05\n1.6288890037685633e-05\n1.6227364540100098e-05\n1.616595545783639e-05\n1.610466279089451e-05\n1.6043486539274454e-05\n1.5982426702976227e-05\n3.3248797990381718e-06\n1.5860656276345253e-05\n3.904628101736307e-06\n2.852175384759903e-09\n1.5739351511001587e-05\n1.5678873751312494e-05\n8.663628250360489e-07\n1.561851240694523e-05\n1.555826747789979e-05\n1.5498138964176178e-05\n1.5438126865774393e-05\n1.5378231182694435e-05\n1.8030405044555664e-06\n1.5318451914936304e-05\n1.52587890625e-05\n1.5199242625385523e-05\n1.5139812603592873e-05\n2.3516477085649967e-06\n9.406590834259987e-06\n1.4962221030145884e-05\n1.4903256669640541e-05\n1.4844408724457026e-05\n5.031237378716469e-06\n1.4727062080055475e-05\n1.4727062080055475e-05\n1.466856338083744e-05\n1.4610181096941233e-05\n1.4551915228366852e-05\n7.712282240390778e-06\n1.4493765775114298e-05\n1.4435732737183571e-05\n1.437781611457467e-05\n1.4320015907287598e-05\n1.4262332115322351e-05\n6.968388333916664e-06\n1.414731377735734e-05\n1.414731377735734e-05\n3.7553254514932632e-06\n1.4032761100679636e-05\n1.3975659385323524e-05\n1.391867408528924e-05\n1.3861805200576782e-05\n1.3805052731186152e-05\n1.3748416677117348e-05\n1.3691897038370371e-05\n1.3635493814945221e-05\n1.3635493814945221e-05\n1.3579207006841898e-05\n1.3466982636600733e-05\n1.3466982636600733e-05\n1.341104507446289e-05\n1.3355223927646875e-05\n1.3299519196152687e-05\n4.9970694817602634e-06\n1.3188458979129791e-05\n1.3133103493601084e-05\n1.3077864423394203e-05\n1.3077864423394203e-05\n1.302274176850915e-05\n7.043126970529556e-09\n1.2912845704704523e-05\n9.837094694375992e-09\n1.285807229578495e-05\n1.2803415302187204e-05\n1.2748874723911285e-05\n1.2694450560957193e-05\n1.2640142813324928e-05\n1.0662712156772614e-05\n1.258595148101449e-05\n1.2531876564025879e-05\n9.266717825084925e-06\n5.48601383343339e-06\n1.2370350304991007e-05\n1.2370350304991007e-05\n1.2316741049289703e-05\n1.2263248208910227e-05\n1.2209871783852577e-05\n1.2156611774116755e-05\n1.2103468179702759e-05\n1.205044100061059e-05\n1.205044100061059e-05\n1.1997530236840248e-05\n1.1944735888391733e-05\n1.1892057955265045e-05\n1.1839496437460184e-05\n1.178705133497715e-05\n1.178705133497715e-05\n1.1734722647815943e-05\n1.1682510375976562e-05\n1.163041451945901e-05\n1.1578435078263283e-05\n1.1526572052389383e-05\n1.147482544183731e-05\n1.147482544183731e-05\n1.1371681466698647e-05\n1.1371681466698647e-05\n1.1320284102112055e-05\n1.126900315284729e-05\n1.1217838618904352e-05\n1.1166790500283241e-05\n1.1115858796983957e-05\n1.10650435090065e-05\n3.814697265625e-06\n1.101434463635087e-05\n1.0963762179017067e-05\n1.091329613700509e-05\n1.0862946510314941e-05\n1.0862946510314941e-05\n1.0812713298946619e-05\n1.0762596502900124e-05\n1.0712596122175455e-05\n1.0662712156772614e-05\n1.0612944606691599e-05\n1.0563293471932411e-05\n1.051375875249505e-05\n1.051375875249505e-05\n1.0464340448379517e-05\n1.041503855958581e-05\n1.036585308611393e-05\n1.0316784027963877e-05\n7.089751306921244e-06\n1.0218995157629251e-05\n1.0218995157629251e-05\n8.53842357173562e-06\n1.0121671948581934e-05\n1.0121671948581934e-05\n1.0073184967041016e-05\n1.0024814400821924e-05\n9.97656024992466e-06\n9.928422514349222e-06\n9.928422514349222e-06\n9.880401194095612e-06\n9.832496289163828e-06\n9.784707799553871e-06\n9.737035725265741e-06\n9.689480066299438e-06\n9.642040822654963e-06\n9.642040822654963e-06\n9.594717994332314e-06\n9.547511581331491e-06\n9.500421583652496e-06\n9.453448001295328e-06\n9.406590834259987e-06\n9.359850082546473e-06\n9.313225746154785e-06\n9.313225746154785e-06\n9.266717825084925e-06\n9.220326319336891e-06\n9.174051228910685e-06\n9.127892553806305e-06\n9.081850294023752e-06\n9.081850294023752e-06\n9.035924449563026e-06\n8.990115020424128e-06\n8.944422006607056e-06\n8.944422006607056e-06\n8.89884540811181e-06\n8.853385224938393e-06\n8.853385224938393e-06\n8.808041457086802e-06\n8.762814104557037e-06\n8.7177031673491e-06\n8.67270864546299e-06\n8.67270864546299e-06\n8.627830538898706e-06\n8.58306884765625e-06\n8.53842357173562e-06\n8.493894711136818e-06\n8.493894711136818e-06\n8.405186235904694e-06\n8.405186235904694e-06\n8.361006621271372e-06\n8.316943421959877e-06\n8.272996637970209e-06\n8.229166269302368e-06\n7.669965270906687e-06\n1.7020502127707005e-06\n6.1104074120521545e-06\n8.098373655229807e-06\n8.098373655229807e-06\n8.055008947849274e-06\n8.011760655790567e-06\n7.968628779053688e-06\n7.968628779053688e-06\n7.925613317638636e-06\n7.88271427154541e-06\n7.839931640774012e-06\n7.79726542532444e-06\n7.79726542532444e-06\n7.754715625196695e-06\n7.712282240390778e-06\n6.186077371239662e-06\n7.669965270906687e-06\n7.627764716744423e-06\n7.627764716744423e-06\n7.585680577903986e-06\n7.543712854385376e-06\n3.4511322155594826e-07\n5.886191502213478e-06\n7.460126653313637e-06\n7.418508175760508e-06\n7.418508175760508e-06\n7.377006113529205e-06\n7.33562046661973e-06\n7.294351235032082e-06\n7.294351235032082e-06\n7.25319841876626e-06\n7.212162017822266e-06\n7.171242032200098e-06\n7.171242032200098e-06\n7.130438461899757e-06\n7.089751306921244e-06\n7.049180567264557e-06\n7.049180567264557e-06\n7.008726242929697e-06\n6.968388333916664e-06\n6.928166840225458e-06\n6.928166840225458e-06\n6.888061761856079e-06\n6.848073098808527e-06\n6.848073098808527e-06\n6.808200851082802e-06\n6.768445018678904e-06\n6.728805601596832e-06\n6.728805601596832e-06\n6.689282599836588e-06\n6.6498760133981705e-06\n6.6498760133981705e-06\n6.61058584228158e-06\n6.571412086486816e-06\n6.53235474601388e-06\n6.53235474601388e-06\n6.49341382086277e-06\n1.742097083479166e-06\n6.454589311033487e-06\n6.4158812165260315e-06\n6.377289537340403e-06\n6.377289537340403e-06\n6.338814273476601e-06\n6.300455424934626e-06\n6.2622129917144775e-06\n6.2622129917144775e-06\n6.224086973816156e-06\n5.923269782215357e-06\n6.186077371239662e-06\n6.148184183984995e-06\n4.243338480591774e-06\n6.1104074120521545e-06\n6.072747055441141e-06\n6.035203114151955e-06\n6.035203114151955e-06\n5.450332537293434e-06\n5.997775588184595e-06\n5.9604644775390625e-06\n5.923269782215357e-06\n5.886191502213478e-06\n2.8172507882118225e-08\n5.849229637533426e-06\n5.849229637533426e-06\n5.812384188175201e-06\n3.844557795673609e-06\n5.7756551541388035e-06\n5.7390425354242325e-06\n5.7390425354242325e-06\n5.702546332031488e-06\n5.666166543960571e-06\n5.666166543960571e-06\n5.629903171211481e-06\n5.629903171211481e-06\n2.1234736777842045e-06\n5.5577256716787815e-06\n5.5577256716787815e-06\n5.521811544895172e-06\n5.521811544895172e-06\n2.8172507882118225e-06\n5.450332537293434e-06\n5.450332537293434e-06\n5.4147676564753056e-06\n5.4147676564753056e-06\n5.379319190979004e-06\n2.5669578462839127e-06\n5.343987140804529e-06\n5.308771505951881e-06\n1.5465193428099155e-06\n5.2736722864210606e-06\n5.238689482212067e-06\n5.238689482212067e-06\n5.2038230933249e-06\n5.2038230933249e-06\n5.16907311975956e-06\n5.1344395615160465e-06\n5.1344395615160465e-06\n3.3248797990381718e-06\n5.065521690994501e-06\n5.065521690994501e-06\n5.031237378716469e-06\n4.9970694817602634e-06\n4.9970694817602634e-06\n4.963018000125885e-06\n4.861562047153711e-06\n4.9290829338133335e-06\n4.895264282822609e-06\n4.895264282822609e-06\n4.861562047153711e-06\n4.827976226806641e-06\n4.827976226806641e-06\n4.794506821781397e-06\n4.794506821781397e-06\n4.76115383207798e-06\n4.72791725769639e-06\n4.72791725769639e-06\n4.694797098636627e-06\n4.661793354898691e-06\n4.661793354898691e-06\n4.628906026482582e-06\n4.5961351133883e-06\n4.5961351133883e-06\n4.563480615615845e-06\n4.563480615615845e-06\n4.5309425331652164e-06\n4.498520866036415e-06\n4.498520866036415e-06\n4.466215614229441e-06\n4.434026777744293e-06\n4.434026777744293e-06\n4.401954356580973e-06\n4.401954356580973e-06\n4.369998350739479e-06\n4.338158760219812e-06\n4.338158760219812e-06\n4.306435585021973e-06\n4.27482882514596e-06\n4.27482882514596e-06\n4.243338480591774e-06\n4.243338480591774e-06\n4.211964551359415e-06\n3.904628101736307e-06\n4.180707037448883e-06\n4.149565938860178e-06\n4.149565938860178e-06\n4.1185412555933e-06\n4.087632987648249e-06\n4.087632987648249e-06\n4.056841135025024e-06\n4.056841135025024e-06\n4.026165697723627e-06\n3.995606675744057e-06\n3.995606675744057e-06\n3.965164069086313e-06\n3.965164069086313e-06\n3.934837877750397e-06\n3.904628101736307e-06\n3.904628101736307e-06\n3.8745347410440445e-06\n3.8745347410440445e-06\n3.844557795673609e-06\n3.844557795673609e-06\n3.814697265625e-06\n3.784953150898218e-06\n3.784953150898218e-06\n3.7553254514932632e-06\n3.5800039768218994e-06\n1.4530960470438004e-06\n3.7258141674101353e-06\n3.6964192986488342e-06\n3.6964192986488342e-06\n3.66714084520936e-06\n3.637978807091713e-06\n3.637978807091713e-06\n3.6089331842958927e-06\n3.5800039768218994e-06\n3.5800039768218994e-06\n3.5800039768218994e-06\n3.551191184669733e-06\n3.5224948078393936e-06\n3.5224948078393936e-06\n3.493914846330881e-06\n2.2819731384515762e-06\n3.4654513001441956e-06\n3.4654513001441956e-06\n3.437104169279337e-06\n3.437104169279337e-06\n3.4088734537363052e-06\n3.3807591535151005e-06\n3.3807591535151005e-06\n3.3527612686157227e-06\n3.3527612686157227e-06\n3.3248797990381718e-06\n3.3248797990381718e-06\n3.297114744782448e-06\n3.269466105848551e-06\n3.269466105848551e-06\n3.2419338822364807e-06\n3.2419338822364807e-06\n3.2145180739462376e-06\n3.2145180739462376e-06\n3.1872186809778214e-06\n3.1872186809778214e-06\n3.101886250078678e-07\n3.160035703331232e-06\n3.1329691410064697e-06\n3.1060189940035343e-06\n3.1060189940035343e-06\n3.1060189940035343e-06\n3.079185262322426e-06\n3.079185262322426e-06\n3.0524679459631443e-06\n3.0258670449256897e-06\n1.5276018530130386e-06\n2.999382559210062e-06\n2.999382559210062e-06\n2.9730144888162613e-06\n2.9730144888162613e-06\n2.9467628337442875e-06\n5.93776348978281e-07\n2.9206275939941406e-06\n2.9206275939941406e-06\n2.8946087695658207e-06\n2.8946087695658207e-06\n2.5669578462839127e-06\n2.8687063604593277e-06\n2.8429203666746616e-06\n2.8429203666746616e-06\n2.8172507882118225e-06\n2.8172507882118225e-06\n2.7916976250708103e-06\n2.7916976250708103e-06\n2.766260877251625e-06\n2.766260877251625e-06\n2.7409405447542667e-06\n2.7409405447542667e-06\n2.7157366275787354e-06\n2.7157366275787354e-06\n2.690649125725031e-06\n2.690649125725031e-06\n4.405737854540348e-07\n2.6656780391931534e-06\n2.640823367983103e-06\n2.640823367983103e-06\n2.616085112094879e-06\n2.616085112094879e-06\n2.5914632715284824e-06\n2.5914632715284824e-06\n2.5669578462839127e-06\n2.5669578462839127e-06\n2.54256883636117e-06\n2.54256883636117e-06\n1.9069411791861057e-06\n2.518296241760254e-06\n2.518296241760254e-06\n2.494140062481165e-06\n2.494140062481165e-06\n2.470100298523903e-06\n2.470100298523903e-06\n2.446176949888468e-06\n2.446176949888468e-06\n2.4223700165748596e-06\n2.4223700165748596e-06\n2.3986794985830784e-06\n2.3986794985830784e-06\n2.375105395913124e-06\n2.375105395913124e-06\n1.4347606338560581e-06\n2.3516477085649967e-06\n2.3283064365386963e-06\n2.3283064365386963e-06\n2.305081579834223e-06\n2.305081579834223e-06\n2.305081579834223e-06\n2.2819731384515762e-06\n2.2819731384515762e-06\n2.2589811123907566e-06\n2.236105501651764e-06\n2.236105501651764e-06\n2.236105501651764e-06\n2.213346306234598e-06\n2.213346306234598e-06\n2.1907035261392593e-06\n2.1907035261392593e-06\n4.20550350099802e-07\n2.1681771613657475e-06\n2.1681771613657475e-06\n8.381903171539307e-09\n2.1457672119140625e-06\n2.1234736777842045e-06\n2.1234736777842045e-06\n2.1234736777842045e-06\n2.1012965589761734e-06\n2.5669578462839127e-08\n2.0792358554899693e-06\n2.0792358554899693e-06\n2.057291567325592e-06\n2.057291567325592e-06\n2.0354636944830418e-06\n9.388313628733158e-07\n2.0354636944830418e-06\n2.0137522369623184e-06\n8.950009942054749e-07\n1.992157194763422e-06\n1.992157194763422e-06\n1.992157194763422e-06\n1.9706785678863525e-06\n1.9706785678863525e-06\n1.9706785678863525e-06\n1.94931635633111e-06\n1.94931635633111e-06\n1.9280705600976944e-06\n1.9280705600976944e-06\n1.9069411791861057e-06\n1.9069411791861057e-06\n1.380452886223793e-06\n1.885928213596344e-06\n1.885928213596344e-06\n1.8650316633284092e-06\n1.8650316633284092e-06\n1.8650316633284092e-06\n1.8442515283823013e-06\n1.8442515283823013e-06\n1.8235878087580204e-06\n1.8235878087580204e-06\n1.8235878087580204e-06\n1.8030405044555664e-06\n1.8030405044555664e-06\n1.29226827993989e-06\n1.7826096154749393e-06\n1.7826096154749393e-06\n1.7622951418161392e-06\n1.7622951418161392e-06\n1.742097083479166e-06\n1.742097083479166e-06\n1.742097083479166e-06\n1.7220154404640198e-06\n1.7220154404640198e-06\n1.7020502127707005e-06\n1.7020502127707005e-06\n1.7020502127707005e-06\n1.682201400399208e-06\n1.682201400399208e-06\n1.6624690033495426e-06\n1.6624690033495426e-06\n1.642853021621704e-06\n1.642853021621704e-06\n1.6233534552156925e-06\n1.6233534552156925e-06\n1.6233534552156925e-06\n1.6039703041315079e-06\n1.6039703041315079e-06\n1.6039703041315079e-06\n9.241048246622086e-07\n1.5847035683691502e-06\n1.5655532479286194e-06\n1.5655532479286194e-06\n1.5655532479286194e-06\n1.5465193428099155e-06\n8.806237019598484e-07\n1.5276018530130386e-06\n1.5276018530130386e-06\n1.5276018530130386e-06\n1.5088007785379887e-06\n1.5088007785379887e-06\n1.4901161193847656e-06\n1.4901161193847656e-06\n1.4715478755533695e-06\n1.4715478755533695e-06\n1.4715478755533695e-06\n1.4530960470438004e-06\n1.4530960470438004e-06\n1.4347606338560581e-06\n1.4347606338560581e-06\n1.4347606338560581e-06\n1.4165416359901428e-06\n1.4165416359901428e-06\n1.4165416359901428e-06\n1.3984390534460545e-06\n1.380452886223793e-06\n1.380452886223793e-06\n8.381903171539307e-07\n1.3625831343233585e-06\n1.3625831343233585e-06\n1.3625831343233585e-06\n1.344829797744751e-06\n1.344829797744751e-06\n1.344829797744751e-06\n1.3271928764879704e-06\n1.3271928764879704e-06\n7.543712854385376e-08\n1.3096723705530167e-06\n1.3096723705530167e-06\n1.29226827993989e-06\n1.29226827993989e-06\n1.29226827993989e-06\n1.27498060464859e-06\n1.27498060464859e-06\n1.27498060464859e-06\n1.2578093446791172e-06\n1.2578093446791172e-06\n1.2407545000314713e-06\n1.2407545000314713e-06\n1.2407545000314713e-06\n1.2238160707056522e-06\n1.2238160707056522e-06\n8.381903171539307e-07\n1.2069940567016602e-06\n1.2069940567016602e-06\n1.2069940567016602e-06\n1.190288458019495e-06\n1.190288458019495e-06\n1.190288458019495e-06\n1.1736992746591568e-06\n1.1736992746591568e-06\n1.1572265066206455e-06\n1.1572265066206455e-06\n1.1572265066206455e-06\n1.1408701539039612e-06\n9.241048246622086e-07\n1.1408701539039612e-06\n1.1408701539039612e-06\n1.1246302165091038e-06\n1.1085066944360733e-06\n1.1085066944360733e-06\n1.1085066944360733e-06\n1.1085066944360733e-06\n1.0924995876848698e-06\n1.0924995876848698e-06\n1.0766088962554932e-06\n1.0766088962554932e-06\n1.0766088962554932e-06\n1.0608346201479435e-06\n1.0608346201479435e-06\n1.0608346201479435e-06\n1.0451767593622208e-06\n1.0451767593622208e-06\n1.0451767593622208e-06\n1.029635313898325e-06\n1.029635313898325e-06\n1.029635313898325e-06\n1.014210283756256e-06\n1.014210283756256e-06\n9.989016689360142e-07\n9.989016689360142e-07\n9.989016689360142e-07\n9.837094694375992e-07\n9.837094694375992e-07\n9.837094694375992e-07\n9.686336852610111e-07\n9.686336852610111e-07\n9.686336852610111e-07\n9.5367431640625e-07\n9.5367431640625e-07\n9.5367431640625e-07\n9.388313628733158e-07\n9.388313628733158e-07\n9.388313628733158e-07\n9.241048246622086e-07\n9.241048246622086e-07\n9.241048246622086e-07\n7.564667612314224e-07\n9.094947017729282e-07\n9.094947017729282e-07\n8.950009942054749e-07\n8.950009942054749e-07\n8.950009942054749e-07\n8.806237019598484e-07\n8.806237019598484e-07\n8.806237019598484e-07\n8.663628250360489e-07\n8.663628250360489e-07\n8.663628250360489e-07\n7.564667612314224e-07\n8.522183634340763e-07\n8.522183634340763e-07\n8.381903171539307e-07\n8.381903171539307e-07\n8.381903171539307e-07\n8.381903171539307e-07\n8.24278686195612e-07\n8.24278686195612e-07\n8.24278686195612e-07\n8.104834705591202e-07\n8.104834705591202e-07\n8.104834705591202e-07\n7.968046702444553e-07\n7.968046702444553e-07\n7.968046702444553e-07\n7.832422852516174e-07\n7.832422852516174e-07\n7.832422852516174e-07\n7.697963155806065e-07\n7.697963155806065e-07\n7.697963155806065e-07\n7.564667612314224e-07\n7.564667612314224e-07\n7.564667612314224e-07\n7.564667612314224e-07\n7.432536222040653e-07\n7.432536222040653e-07\n7.301568984985352e-07\n7.301568984985352e-07\n7.301568984985352e-07\n7.301568984985352e-07\n7.171765901148319e-07\n7.171765901148319e-07\n7.171765901148319e-07\n7.043126970529556e-07\n7.043126970529556e-07\n7.043126970529556e-07\n6.915652193129063e-07\n6.915652193129063e-07\n6.915652193129063e-07\n5.476758815348148e-07\n6.789341568946838e-07\n6.789341568946838e-07\n5.93776348978281e-07\n6.664195097982883e-07\n6.664195097982883e-07\n6.664195097982883e-07\n6.664195097982883e-07\n6.540212780237198e-07\n6.540212780237198e-07\n6.540212780237198e-07\n6.540212780237198e-07\n6.417394615709782e-07\n6.417394615709782e-07\n6.417394615709782e-07\n6.295740604400635e-07\n6.295740604400635e-07\n6.295740604400635e-07\n6.175250746309757e-07\n6.175250746309757e-07\n6.175250746309757e-07\n6.175250746309757e-07\n6.055925041437149e-07\n6.055925041437149e-07\n6.055925041437149e-07\n5.93776348978281e-07\n5.93776348978281e-07\n5.93776348978281e-07\n5.93776348978281e-07\n5.820766091346741e-07\n5.820766091346741e-07\n5.820766091346741e-07\n5.820766091346741e-07\n5.704932846128941e-07\n5.704932846128941e-07\n5.704932846128941e-07\n5.59026375412941e-07\n5.59026375412941e-07\n5.59026375412941e-07\n5.59026375412941e-07\n5.476758815348148e-07\n5.476758815348148e-07\n5.476758815348148e-07\n5.476758815348148e-07\n5.364418029785156e-07\n5.364418029785156e-07\n5.364418029785156e-07\n5.253241397440434e-07\n5.253241397440434e-07\n5.253241397440434e-07\n5.253241397440434e-07\n5.14322891831398e-07\n5.14322891831398e-07\n5.14322891831398e-07\n2.1659070625901222e-07\n5.034380592405796e-07\n5.034380592405796e-07\n5.034380592405796e-07\n6.3388142734766e-08\n4.926696419715881e-07\n4.926696419715881e-07\n4.926696419715881e-07\n4.926696419715881e-07\n4.820176400244236e-07\n4.820176400244236e-07\n4.820176400244236e-07\n4.820176400244236e-07\n4.71482053399086e-07\n4.71482053399086e-07\n4.71482053399086e-07\n4.71482053399086e-07\n4.6106288209557533e-07\n4.6106288209557533e-07\n4.6106288209557533e-07\n4.6106288209557533e-07\n4.507601261138916e-07\n4.507601261138916e-07\n4.507601261138916e-07\n4.507601261138916e-07\n4.405737854540348e-07\n4.405737854540348e-07\n4.405737854540348e-07\n4.405737854540348e-07\n2.384185791015625e-07\n4.3050386011600494e-07\n4.3050386011600494e-07\n4.3050386011600494e-07\n4.20550350099802e-07\n4.20550350099802e-07\n4.20550350099802e-07\n2.0954757928848267e-09\n4.10713255405426e-07\n4.10713255405426e-07\n4.10713255405426e-07\n4.10713255405426e-07\n4.10713255405426e-07\n4.0099257603287697e-07\n4.0099257603287697e-07\n4.0099257603287697e-07\n4.0099257603287697e-07\n3.9138831198215485e-07\n3.9138831198215485e-07\n3.9138831198215485e-07\n3.9138831198215485e-07\n3.9138831198215485e-07\n3.8190046325325966e-07\n3.8190046325325966e-07\n2.53552570939064e-07\n3.8190046325325966e-07\n3.725290298461914e-07\n7.130438461899757e-08\n3.725290298461914e-07\n2.5669578462839127e-08\n3.725290298461914e-07\n3.632740117609501e-07\n3.632740117609501e-07\n3.632740117609501e-07\n3.632740117609501e-07\n3.541354089975357e-07\n3.541354089975357e-07\n3.541354089975357e-07\n3.541354089975357e-07\n3.541354089975357e-07\n3.4511322155594826e-07\n3.4511322155594826e-07\n3.4511322155594826e-07\n3.4511322155594826e-07\n3.4511322155594826e-07\n3.3620744943618774e-07\n3.3620744943618774e-07\n3.3620744943618774e-07\n3.3620744943618774e-07\n3.2741809263825417e-07\n7.968628779053688e-08\n3.2741809263825417e-07\n9.313225746154785e-08\n3.2741809263825417e-07\n3.2741809263825417e-07\n3.187451511621475e-07\n3.187451511621475e-07\n3.187451511621475e-07\n3.187451511621475e-07\n3.101886250078678e-07\n3.101886250078678e-07\n3.101886250078678e-07\n3.101886250078678e-07\n3.101886250078678e-07\n1.4551915228366852e-07\n3.0174851417541504e-07\n3.0174851417541504e-07\n3.0174851417541504e-07\n3.0174851417541504e-07\n2.934248186647892e-07\n2.934248186647892e-07\n2.934248186647892e-07\n2.934248186647892e-07\n2.852175384759903e-07\n2.852175384759903e-07\n2.852175384759903e-07\n2.852175384759903e-07\n2.771266736090183e-07\n2.771266736090183e-07\n2.771266736090183e-07\n2.771266736090183e-07\n2.771266736090183e-07\n2.771266736090183e-07\n2.691522240638733e-07\n2.691522240638733e-07\n2.691522240638733e-07\n2.691522240638733e-07\n2.691522240638733e-07\n1.178705133497715e-07\n2.612941898405552e-07\n2.612941898405552e-07\n2.612941898405552e-07\n2.612941898405552e-07\n2.612941898405552e-07\n2.53552570939064e-07\n2.53552570939064e-07\n2.53552570939064e-07\n2.53552570939064e-07\n2.53552570939064e-07\n2.459273673593998e-07\n2.459273673593998e-07\n1.0762596502900124e-07\n2.459273673593998e-07\n2.459273673593998e-07\n2.384185791015625e-07\n2.384185791015625e-07\n2.384185791015625e-07\n2.384185791015625e-07\n2.384185791015625e-07\n2.3102620616555214e-07\n1.341104507446289e-07\n2.3102620616555214e-07\n2.3102620616555214e-07\n2.0954757928848267e-09\n2.3102620616555214e-07\n2.2375024855136871e-07\n2.2375024855136871e-07\n2.2375024855136871e-07\n2.2375024855136871e-07\n6.3388142734766e-08\n2.2375024855136871e-07\n3.079185262322426e-08\n2.1659070625901222e-07\n2.1659070625901222e-07\n2.1659070625901222e-07\n2.1659070625901222e-07\n2.1659070625901222e-07\n1.4901161193847656e-08\n2.0954757928848267e-07\n2.0954757928848267e-07\n2.0954757928848267e-07\n2.0954757928848267e-07\n2.0954757928848267e-07\n2.0262086763978004e-07\n2.0262086763978004e-07\n2.0262086763978004e-07\n2.0262086763978004e-07\n2.0262086763978004e-07\n2.0262086763978004e-07\n6.3388142734766e-08\n1.9581057131290436e-07\n1.9581057131290436e-07\n1.9581057131290436e-07\n1.9581057131290436e-07\n1.9581057131290436e-07\n1.891166903078556e-07\n1.891166903078556e-07\n1.891166903078556e-07\n1.891166903078556e-07\n1.6350531950592995e-07\n1.891166903078556e-07\n1.825392246246338e-07\n1.825392246246338e-07\n1.825392246246338e-07\n1.825392246246338e-07\n1.825392246246338e-07\n1.825392246246338e-07\n1.760781742632389e-07\n1.760781742632389e-07\n1.760781742632389e-07\n1.760781742632389e-07\n1.760781742632389e-07\n1.760781742632389e-07\n1.6973353922367096e-07\n1.6973353922367096e-07\n1.6973353922367096e-07\n4.563480615615845e-08\n1.6973353922367096e-07\n1.6973353922367096e-07\n1.6350531950592995e-07\n1.6350531950592995e-07\n1.6350531950592995e-07\n1.6350531950592995e-07\n1.178705133497715e-07\n1.6350531950592995e-07\n1.5739351511001587e-07\n1.5739351511001587e-07\n1.5739351511001587e-07\n1.5739351511001587e-07\n1.5739351511001587e-07\n1.5739351511001587e-07\n1.5739351511001587e-07\n1.5139812603592873e-07\n1.5139812603592873e-07\n1.5139812603592873e-07\n1.5139812603592873e-07\n1.5139812603592873e-07\n1.5139812603592873e-07\n1.4551915228366852e-07\n1.4551915228366852e-07\n1.4551915228366852e-07\n1.126900315284729e-07\n1.4551915228366852e-07\n1.4551915228366852e-07\n1.4551915228366852e-07\n1.3975659385323524e-07\n1.3975659385323524e-07\n2.3283064365386963e-08\n1.3975659385323524e-07\n1.3975659385323524e-07\n1.3975659385323524e-07\n1.341104507446289e-07\n1.341104507446289e-07\n1.341104507446289e-07\n1.341104507446289e-07\n1.341104507446289e-07\n1.341104507446289e-07\n1.341104507446289e-07\n1.285807229578495e-07\n1.285807229578495e-07\n1.285807229578495e-07\n1.285807229578495e-07\n1.285807229578495e-07\n1.285807229578495e-07\n1.285807229578495e-07\n1.2316741049289703e-07\n1.2316741049289703e-07\n1.2316741049289703e-07\n1.2316741049289703e-07\n1.2316741049289703e-07\n1.2316741049289703e-07\n9.784707799553871e-08\n1.178705133497715e-07\n1.178705133497715e-07\n1.178705133497715e-07\n1.178705133497715e-07\n1.178705133497715e-07\n1.178705133497715e-07\n1.0762596502900124e-07\n1.126900315284729e-07\n1.126900315284729e-07\n1.126900315284729e-07\n1.126900315284729e-07\n1.126900315284729e-07\n1.126900315284729e-07\n1.126900315284729e-07\n1.0762596502900124e-07\n1.0762596502900124e-07\n1.0762596502900124e-07\n1.0762596502900124e-07\n2.5669578462839127e-08\n1.0762596502900124e-07\n5.820766091346741e-11\n1.026783138513565e-07\n1.026783138513565e-07\n1.026783138513565e-07\n1.026783138513565e-07\n1.026783138513565e-07\n1.026783138513565e-07\n1.026783138513565e-07\n3.079185262322426e-08\n1.026783138513565e-07\n9.784707799553871e-08\n9.784707799553871e-08\n7.543712854385376e-08\n9.784707799553871e-08\n9.784707799553871e-08\n9.784707799553871e-08\n9.784707799553871e-08\n9.784707799553871e-08\n8.853385224938393e-08\n9.313225746154785e-08\n9.313225746154785e-08\n9.313225746154785e-08\n9.313225746154785e-08\n9.313225746154785e-08\n9.313225746154785e-08\n9.313225746154785e-08\n8.853385224938393e-08\n8.853385224938393e-08\n8.853385224938393e-08\n8.853385224938393e-08\n8.853385224938393e-08\n8.853385224938393e-08\n8.853385224938393e-08\n8.853385224938393e-08\n8.405186235904694e-08\n8.405186235904694e-08\n8.405186235904694e-08\n8.405186235904694e-08\n8.405186235904694e-08\n8.405186235904694e-08\n8.405186235904694e-08\n8.405186235904694e-08\n7.968628779053688e-08\n7.968628779053688e-08\n7.968628779053688e-08\n7.968628779053688e-08\n7.968628779053688e-08\n7.968628779053688e-08\n7.968628779053688e-08\n7.968628779053688e-08\n7.968628779053688e-08\n7.543712854385376e-08\n3.934837877750397e-08\n7.543712854385376e-08\n7.543712854385376e-08\n7.543712854385376e-08\n5.2386894822120667e-08\n7.543712854385376e-08\n7.543712854385376e-08\n7.130438461899757e-08\n4.563480615615845e-08\n7.130438461899757e-08\n7.130438461899757e-08\n7.130438461899757e-08\n7.130438461899757e-08\n7.130438461899757e-08\n7.130438461899757e-08\n7.130438461899757e-08\n6.728805601596832e-08\n6.728805601596832e-08\n2.8172507882118225e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.3388142734766e-08\n6.3388142734766e-08\n6.3388142734766e-08\n6.3388142734766e-08\n6.3388142734766e-08\n6.3388142734766e-08\n6.3388142734766e-08\n6.3388142734766e-08\n6.3388142734766e-08\n5.960464477539063e-08\n5.960464477539063e-08\n5.593756213784218e-08\n5.960464477539063e-08\n5.960464477539063e-08\n5.960464477539063e-08\n5.960464477539063e-08\n5.960464477539063e-08\n4.71482053399086e-09\n5.960464477539063e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.238689482212067e-10\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n4.895264282822609e-08\n4.895264282822609e-08\n5.238689482212067e-10\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n5.820766091346741e-09\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.71482053399086e-09\n4.243338480591774e-08\n4.243338480591774e-08\n4.243338480591774e-08\n3.3527612686157227e-08\n4.243338480591774e-08\n4.243338480591774e-08\n4.243338480591774e-08\n5.238689482212067e-10\n1.682201400399208e-08\n4.243338480591774e-08\n4.243338480591774e-08\n4.243338480591774e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.079185262322426e-08\n3.079185262322426e-08\n9.313225746154785e-10\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n1.885928213596344e-08\n8.381903171539307e-09\n3.079185262322426e-08\n3.079185262322426e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.0954757928848267e-09\n2.3283064365386963e-08\n9.313225746154785e-10\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n5.820766091346741e-11\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n1.885928213596344e-08\n1.885928213596344e-08\n9.837094694375992e-09\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n9.837094694375992e-09\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4551915228366852e-09\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n2.0954757928848267e-09\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n2.3283064365386963e-10\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n4.71482053399086e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n8.381903171539307e-09\n9.837094694375992e-09\n7.043126970529556e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.313225746154785e-10\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n9.313225746154785e-10\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n2.0954757928848267e-09\n5.820766091346741e-09\n5.820766091346741e-09\n2.0954757928848267e-09\n5.820766091346741e-09\n9.313225746154785e-10\n5.820766091346741e-09\n5.238689482212067e-10\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n2.852175384759903e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n2.852175384759903e-09\n5.820766091346741e-09\n4.243338480591774e-08\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n6.3388142734766e-08\n5.820766091346741e-09\n5.820766091346741e-09\n4.71482053399086e-09\n5.820766091346741e-09\n5.820766091346741e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n5.820766091346741e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n5.820766091346741e-11\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n1.4551915228366852e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n5.238689482212067e-10\n3.725290298461914e-09\n1.682201400399208e-08\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.3283064365386963e-10\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.0954757928848267e-09\n2.852175384759903e-09\n2.852175384759903e-09\n5.960464477539063e-08\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n9.313225746154785e-10\n9.313225746154785e-10\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n1.3096723705530167e-08\n2.0954757928848267e-09\n1.4551915228366852e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n3.637978807091713e-08\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n8.381903171539307e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n8.381903171539307e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n5.820766091346741e-11\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n3.637978807091713e-08\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n5.593756213784218e-08\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n1.2316741049289703e-07\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n5.238689482212067e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n5.238689482212067e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n2.771266736090183e-07\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n1.4551915228366852e-09\n7.301568984985352e-07\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n3.4511322155594826e-07\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n2.0954757928848267e-09\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n2.5669578462839127e-08\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n3.9138831198215485e-07\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n1.4551915228366852e-09\n1.2578093446791172e-06\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n7.832422852516174e-07\n9.313225746154785e-10\n9.313225746154785e-10\n1.4551915228366852e-09\n1.4551915228366852e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n2.1659070625901222e-07\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n3.934837877750397e-08\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n7.968628779053688e-08\n6.3388142734766e-08\n3.934837877750397e-06\n9.313225746154785e-10\n1.4551915228366852e-09\n2.0954757928848267e-09\n7.089751306921244e-06\n4.71482053399086e-09\n5.820766091346741e-09\n7.043126970529556e-09\n7.043126970529556e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n3.866298357024789e-05\n2.1907035261392593e-06\n9.837094694375992e-09\n8.381903171539307e-09\n9.837094694375992e-09\n9.837094694375992e-09\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n1.891166903078556e-07\n5.820766091346741e-09\n5.820766091346741e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.794506821781397e-06\n4.71482053399086e-09\n5.820766091346741e-09\n7.043126970529556e-09\n9.837094694375992e-09\n1.1408701539039612e-08\n1.3096723705530167e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.1408701539039612e-08\n1.4089979231357574e-05\n9.837094694375992e-09\n8.381903171539307e-09\n8.381903171539307e-09\n7.043126970529556e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n1.94931635633111e-06\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n8.229166269302368e-06\n7.043126970529556e-09\n8.381903171539307e-09\n1.1408701539039612e-08\n1.3096723705530167e-08\n1.1997530236840248e-05\n1.885928213596344e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n9.837094694375992e-09\n9.837094694375992e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n5.820766091346741e-09\n5.820766091346741e-09\n1.2578093446791172e-06\n5.820766091346741e-09\n7.043126970529556e-09\n8.381903171539307e-09\n8.381903171539307e-09\n9.837094694375992e-09\n1.2209871783852577e-05\n9.837094694375992e-09\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.3096723705530167e-08\n3.1329691410064697e-06\n1.4901161193847656e-08\n1.682201400399208e-08\n1.885928213596344e-08\n2.1012965589761734e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.3283064365386963e-08\n6.211275467649102e-05\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.190288458019495e-06\n4.861562047153711e-06\n1.682201400399208e-08\n1.682201400399208e-08\n1.885928213596344e-08\n1.885928213596344e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n2.1012965589761734e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.682201400399208e-08\n1.6624690033495426e-06\n1.682201400399208e-08\n1.885928213596344e-08\n1.885928213596344e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n6.035203114151955e-06\n4.369998350739479e-06\n2.1012965589761734e-08\n2.1012965589761734e-08\n1.885928213596344e-08\n1.885928213596344e-08\n2.771266736090183e-07\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n1.3579207006841898e-05\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.3283064365386963e-08\n2.5669578462839127e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n1.10650435090065e-05\n3.079185262322426e-08\n3.637978807091713e-08\n4.243338480591774e-08\n4.895264282822609e-08\n5.593756213784218e-08\n6.3388142734766e-08\n6.3388142734766e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.3388142734766e-08\n5.960464477539063e-08\n5.593756213784218e-08\n5.2386894822120667e-08\n5.996698746457696e-05\n4.563480615615845e-08\n4.243338480591774e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n5.0235597882419825e-05\n2.3283064365386963e-08\n1.6722828149795532e-05\n7.472053403034806e-05\n8.405186235904694e-08\n1.0762596502900124e-07\n1.285807229578495e-07\n1.4551915228366852e-07\n1.5739351511001587e-07\n1.6973353922367096e-07\n1.760781742632389e-07\n1.825392246246338e-07\n1.825392246246338e-07\n1.760781742632389e-07\n1.6973353922367096e-07\n1.6350531950592995e-07\n1.5739351511001587e-07\n1.4551915228366852e-07\n1.341104507446289e-07\n1.178705133497715e-07\n1.0762596502900124e-07\n9.313225746154785e-08\n8.405186235904694e-08\n7.130438461899757e-08\n5.960464477539063e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.243338480591774e-08\n0.00025767460465431213\n4.243338480591774e-08\n9.313225746154785e-08\n1.6973353922367096e-07\n2.612941898405552e-07\n3.4511322155594826e-07\n4.3050386011600494e-07\n5.14322891831398e-07\n5.704932846128941e-07\n0.0034073484130203724\n6.664195097982883e-07\n7.043126970529556e-07\n7.171765901148319e-07\n7.171765901148319e-07\n7.171765901148319e-07\n6.915652193129063e-07\n6.664195097982883e-07\n6.295740604400635e-07\n5.820766091346741e-07\n5.364418029785156e-07\n4.820176400244236e-07\n4.3050386011600494e-07\n3.8190046325325966e-07\n3.2741809263825417e-07\n3.187451511621475e-07\n3.2741809263825417e-07\n3.3620744943618774e-07\n3.4511322155594826e-07\n0.0019084590021520853\n3.541354089975357e-07\n3.632740117609501e-07\n3.632740117609501e-07\n4.507601261138916e-07\n5.93776348978281e-07\n0.0005706190713681281\n8.806237019598484e-07\n9.989016689360142e-07\n1.1246302165091038e-06\n1.2238160707056522e-06\n1.3096723705530167e-06\n1.380452886223793e-06\n1.4347606338560581e-06\n1.4715478755533695e-06\n1.4901161193847656e-06\n1.5088007785379887e-06\n1.5088007785379887e-06\n1.4901161193847656e-06\n1.4715478755533695e-06\n1.4530960470438004e-06\n1.4165416359901428e-06\n1.380452886223793e-06\n1.3271928764879704e-06\n1.27498060464859e-06\n1.2238160707056522e-06\n1.1736992746591568e-06\n1.1246302165091038e-06\n1.0608346201479435e-06\n9.989016689360142e-07\n9.388313628733158e-07\n8.950009942054749e-07\n0.005522021092474461\n3.559538163244724e-05\n0.004053524695336819\n4.180707037448883e-06\n0.028909754008054733\n8.990115020424128e-06\n1.1423195246607065e-05\n1.3635493814945221e-05\n1.555826747789979e-05\n1.709931530058384e-05\n1.825392246246338e-05\n1.8978083971887827e-05\n1.924490788951516e-05\n1.937901834025979e-05\n2.1235086023807526e-05\n2.288311952725053e-05\n2.4290988221764565e-05\n2.5355257093906403e-05\n2.612941898405552e-05\n2.6678259018808603e-05\n2.6994443032890558e-05\n2.7073780074715614e-05\n2.6994443032890558e-05\n2.6678259018808603e-05\n2.620747545734048e-05\n0.004519903101027012\n2.5125278625637293e-05\n3.000744618475437e-05\n3.496097633615136e-05\n3.904342884197831e-05\n4.2253173887729645e-05\n4.446343518793583e-05\n4.569278098642826e-05\n4.6002736780792475e-05\n4.538387292996049e-05\n4.405737854540348e-05\n4.215404624119401e-05\n3.971369005739689e-05\n3.6881305277347565e-05\n3.379792906343937e-05\n3.051105886697769e-05\n2.7232803404331207e-05\n2.399110235273838e-05\n2.08152923732996e-05\n1.985207200050354e-05\n1.964863622561097e-05\n1.9311904907226562e-05\n1.884537050500512e-05\n1.8319173250347376e-05\n1.7671904060989618e-05\n1.7036276403814554e-05\n1.6350531950592995e-05\n1.561851240694523e-05\n1.4844408724457026e-05\n0.05610007792711258\n1.3355223927646875e-05\n1.2694450560957193e-05\n1.1997530236840248e-05\n1.126900315284729e-05\n1.0612944606691599e-05\n9.928422514349222e-06\n9.266717825084925e-06\n8.627830538898706e-06\n8.011760655790567e-06\n7.418508175760508e-06\n7.377006113529205e-06\n7.712282240390778e-06\n8.011760655790567e-06\n8.229166269302368e-06\n8.449482265859842e-06\n8.67270864546299e-06\n8.808041457086802e-06\n8.944422006607056e-06\n9.035924449563026e-06\n9.081850294023752e-06\n9.127892553806305e-06\n9.174051228910685e-06\n9.174051228910685e-06\n9.174051228910685e-06\n9.174051228910685e-06\n9.127892553806305e-06\n0.013086248189210892\n9.035924449563026e-06\n8.990115020424128e-06\n8.944422006607056e-06\n0.0004589594900608063\n1.2156611774116755e-05\n1.3748416677117348e-05\n1.5199242625385523e-05\n0.00023352965945377946\n0.008146097883582115\n0.00015163072384893894\n0.00012137088924646378\n0.00016175262862816453\n3.4064578358083963e-05\n0.00013984390534460545\n0.00010080728679895401\n0.0001688143820501864\n0.00016097730258479714\n0.00011720467591658235\n4.948140121996403e-05\n1.7020502127707005e-06\n3.8284400943666697e-05\n0.00017562235007062554\n0.0003355555818416178\n0.0003910660743713379\n0.00028506439412012696\n0.0001035837922245264\n0.0002923235879279673\n0.0013981363736093044\n0.00952631514519453\n1.5438126865774393e-05\n0.0007158988737501204\n0.1274288296699524\n1.735263504087925e-05\n3.388669574633241e-05\n6.478658178821206e-05\n9.973772102966905e-05\n0.00013572449097409844\n0.00017040414968505502\n0.0002018085215240717\n0.00022888934472575784\n0.0002506203600205481\n0.0002668169909156859\n0.0002773879677988589\n0.00028249394381418824\n0.0002822375390678644\n0.0002773879677988589\n0.0002688146778382361\n0.0002564513706602156\n0.00024176225997507572\n0.0002247528755106032\n0.9303640723228455\n0.00019090488785877824\n0.0001742097083479166\n0.00017240183660760522\n0.0002368055866099894\n0.00030179094756022096\n0.00036496296525001526\n0.0004243338480591774\n2.903494119644165\n0.0005301742348819971\n0.0005753673613071442\n0.0006136838928796351\n0.0006454589311033487\n0.0006701127276755869\n0.0006880054716020823\n0.0007000649347901344\n0.0007065394893288612\n0.000707350904121995\n0.0007037032046355307\n0.0012890922371298075\n0.005709543824195862\n0.0041081104427576065\n0.0022403905168175697\n0.000576465914491564\n8.49403440952301e-05\n0.001960807479918003\n0.003222869709134102\n1.3805052731186152e-05\n0.013504981994628906\n0.002825346775352955\n0.002059311605989933\n0.0053744809702038765\n0.004355242475867271\n0.004446343518793583\n0.01279464177787304\n0.0032463008537888527\n0.0010973869357258081\n0.007898733019828796\n0.0015552248805761337\n584.6871948242188\n0.005245502572506666\n0.005542449653148651\n0.015221375972032547\n0.025549927726387978\n0.026584582403302193\n0.014904886484146118\n0.014210854656994343\n0.012197681702673435\n0.009699857793748379\n0.007408875972032547\n0.005671558901667595\n0.004557939246296883\n0.004003164824098349\n0.003920568153262138\n56.634376525878906\n0.004340150859206915\n27.55152702331543\n0.004598204046487808\n15.520733833312988\n0.004807472229003906\n0.0048956857062876225\n0.004972857888787985\n26.056385040283203\n0.005102819297462702\n0.005155273247510195\n0.005200290121138096\n0.005749969277530909\n0.006298162043094635\n0.0068056960590183735\n0.007270310074090958\n0.007691270671784878\n0.0080718994140625\n0.008411265909671783\n0.008711984381079674\n0.008976012468338013\n0.009205877780914307\n0.009403103962540627\n23.48276138305664\n0.009722412563860416\n0.009847691282629967\n16.20376968383789\n0.010039406828582287\n0.010108323767781258\n7.868412017822266\n0.010203663259744644\n0.010231425985693932\n0.010243777185678482\n0.010240688920021057\n0.01022525317966938\n0.010199040174484253\n0.010160551406443119\n6.241038799285889\n0.01054714247584343\n0.010940950363874435\n0.011290035210549831\n0.011596900410950184\n0.011862964369356632\n0.012090065516531467\n0.012283779680728912\n0.012444965541362762\n0.012576382607221603\n0.012680981308221817\n0.012760145589709282\n0.012818817049264908\n0.012855122797191143\n0.012874159961938858\n0.012875891290605068\n0.012863773852586746\n0.012836098670959473\n0.012798094190657139\n0.012748083099722862\n0.012689574621617794\n0.01262262649834156\n0.012547308579087257\n0.012465400621294975\n0.012376969680190086\n0.012283779680728912\n0.01218588836491108\n0.012085032649338245\n0.011979585513472557\n0.0118712754920125\n0.01176180224865675\n0.011647894978523254\n0.01153454277664423\n0.011418482288718224\n0.01130138710141182\n0.011183282360434532\n0.011065796948969364\n0.010945739224553108\n0.010826336219906807\n0.010707588866353035\n0.010589495301246643\n0.01047049555927515\n0.010350615717470646\n0.010232970118522644\n0.010114461183547974\n0.009998168796300888\n0.0098810326308012\n0.009766093455255032\n0.009701360017061234\n0.009687839075922966\n0.009663823992013931\n0.009629353880882263\n0.009584486484527588\n0.00953376293182373\n0.009474260732531548\n0.009407543577253819\n0.009336638264358044\n1.9191820621490479\n0.00919270608574152\n0.009118244983255863\n0.009041184559464455\n0.008958673104643822\n0.00887366570532322\n0.008846376091241837\n0.008816263638436794\n0.008777622133493423\n0.008733361028134823\n0.008682101964950562\n0.008626740425825119\n0.008567319251596928\n0.00850388128310442\n0.0084364740177989\n0.008366543799638748\n0.008294124156236649\n0.008251100778579712\n0.008216486312448978\n0.008173665031790733\n0.008124077692627907\n0.008069157600402832\n0.008010326884686947\n0.007944908924400806\n0.007877049967646599\n0.007839176803827286\n0.00780139397829771\n0.007758325897157192\n0.0077100167982280254\n0.0076578520238399506\n0.007601873017847538\n0.007543448358774185\n0.007481289096176624\n0.007441747467964888\n0.0074049364775419235\n0.007361669093370438\n0.007312003523111343\n0.00725860521197319\n0.007253406103700399\n0.007250807248055935\n0.0072443122044205666\n0.007233926095068455\n4.8946181777864695e-05\n0.007209288887679577\n0.007195044308900833\n0.00717693567276001\n0.00715755857527256\n0.007136918604373932\n0.007113734260201454\n0.007088018115609884\n0.007062348537147045\n0.007034165784716606\n0.007006039377301931\n0.006975420285016298\n0.0069448682479560375\n0.006914383266121149\n0.006901700980961323\n0.006889030337333679\n0.0068687815219163895\n0.006842250004410744\n0.0068094730377197266\n0.00677300663664937\n0.005329828709363937\n0.006731629371643066\n0.006722868885844946\n0.006710363551974297\n0.006689131259918213\n0.006671670824289322\n0.0066567230969667435\n0.006639305502176285\n0.006619427353143692\n0.006598339416086674\n0.006574810482561588\n0.0065513234585523605\n0.006534044165164232\n0.006513092666864395\n0.006486029364168644\n0.006463928148150444\n0.006443089805543423\n0.006421062164008617\n0.0063978517428040504\n0.006373465061187744\n0.006347909104079008\n0.006341831758618355\n0.006328472401946783\n0.006310277618467808\n0.006284848786890507\n0.006257057189941406\n0.0062377601861953735\n0.0062124780379235744\n0.006195651832967997\n0.006181247532367706\n0.004462637938559055\n0.006148898974061012\n0.006130964495241642\n0.0037187738344073296\n0.006095173768699169\n0.006076128222048283\n0.006055925041437149\n0.006034569814801216\n0.0060108862817287445\n0.005987249314785004\n0.005977807566523552\n0.005961302667856216\n0.005951881408691406\n0.005935411900281906\n0.005914270877838135\n0.0058873118832707405\n0.005868594162166119\n0.005846405401825905\n0.005832413211464882\n0.005819601938128471\n0.005804479122161865\n0.005788215436041355\n0.005769656039774418\n0.005748812109231949\n0.005728006362915039\n0.005706085357815027\n0.005681905895471573\n0.005668112076818943\n0.005656629800796509\n0.005641720723360777\n0.005621109623461962\n0.005608532577753067\n0.005589122883975506\n0.004745253827422857\n0.004899957217276096\n0.005527691915631294\n0.005512953735888004\n0.005495972465723753\n0.00547788804396987\n0.00545757869258523\n0.005440683104097843\n0.0054215663112699986\n0.005404726602137089\n0.005394635256379843\n0.005378956440836191\n0.005357714369893074\n0.005338744260370731\n0.005324260331690311\n0.005307572428137064\n0.005290910601615906\n0.005273167043924332\n0.005258772522211075\n0.005242187529802322\n0.0052245259284973145\n0.005205793306231499\n0.005193689838051796\n0.0051783062517642975\n0.005157464649528265\n0.005139946471899748\n0.005123550072312355\n0.0051082707941532135\n0.005093013867735863\n0.005077780224382877\n0.005061483476310968\n0.005044129211455584\n0.0050257230177521706\n0.005011670291423798\n0.004997637122869492\n0.004982546903192997\n0.004963178187608719\n0.004947066772729158\n0.0044656964018940926\n0.004922413267195225\n0.00490957498550415\n0.0048956857062876225\n0.004880750086158514\n0.0048637087456882\n0.004845635034143925\n0.004826534539461136\n0.004808530211448669\n0.004307040944695473\n0.004781058989465237\n0.00476313941180706\n0.0047494592145085335\n0.004732648842036724\n0.004499409347772598\n0.004698071628808975\n0.004682396538555622\n0.004668832756578922\n0.004655288532376289\n0.004638645797967911\n0.004626183304935694\n0.004612701013684273\n0.00459716934710741\n0.0045837294310331345\n0.004567215219140053\n0.004553819540888071\n0.004539415240287781\n0.004524007439613342\n0.004508625715970993\n0.004494293127208948\n0.0044779409654438496\n0.004462637938559055\n0.004446343518793583\n0.004437190946191549\n0.004422972444444895\n0.004408776760101318\n0.004393592476844788\n0.00437641516327858\n0.004364310298115015\n0.004350209143012762\n0.004334121476858854\n0.0043210722506046295\n0.004102244507521391\n0.003858711803331971\n0.004283040761947632\n0.004269071854650974\n0.004257116466760635\n0.004242195747792721\n0.004222342278808355\n0.004210452549159527\n0.004198579583317041\n0.004185736179351807\n0.00417586974799633\n0.003961758688092232\n0.004149289336055517\n0.004133577924221754\n0.004119854886084795\n0.004105176776647568\n0.004091501235961914\n0.004074925556778908\n0.004062272608280182\n0.004048668779432774\n0.0040370263159275055\n0.0040254006162285805\n0.00400992576032877\n0.003996409475803375\n0.003984842449426651\n0.003971369005739689\n0.003955998457968235\n0.00394640676677227\n0.0039339554496109486\n0.0039177024737000465\n0.0039033894427120686\n0.003891006112098694\n0.002784939017146826\n0.0038700944278389215\n0.0038577639497816563\n0.0038435610476881266\n0.001370319165289402\n0.003822777420282364\n0.0038114646449685097\n0.003798287594690919\n0.0037851333618164062\n0.0037720019463449717\n0.003757957834750414\n0.003744873683899641\n0.0037318123504519463\n0.0037178434431552887\n0.0036052020732313395\n0.003699258901178837\n0.003689057193696499\n0.0036760936491191387\n0.003658536821603775\n0.0036502350121736526\n0.0036382602993398905\n0.003621712327003479\n0.0036125353071838617\n0.0015438126865774393\n0.0035914722830057144\n0.003580507356673479\n0.003568647662177682\n0.003554987721145153\n0.0012542682234197855\n0.0035377228632569313\n0.0010508811101317406\n0.0035150698386132717\n0.003500610124319792\n0.0034897848963737488\n0.0034798765555024147\n0.003469083458185196\n0.003457409795373678\n0.0034448602236807346\n0.0034305457957088947\n0.003420721972361207\n0.0026121619157493114\n0.0034037865698337555\n0.0033913347870111465\n0.0033762454986572266\n0.0033691562712192535\n0.002387912478297949\n0.0033496993128210306\n0.0033355841878801584\n0.003321498865261674\n0.0033127106726169586\n0.0033021802082657814\n0.0032907910645008087\n0.0032785478979349136\n0.003265455598011613\n0.003251519287005067\n0.003245431464165449\n0.0032358765602111816\n0.0032254690304398537\n0.003214213065803051\n0.003201249986886978\n0.0031874515116214752\n0.0031762621365487576\n0.0031676683574914932\n0.003157371189445257\n0.0031462348997592926\n0.003138536587357521\n0.0031265802681446075\n0.00311379530467093\n0.003103586146607995\n0.00309169664978981\n0.0030798299703747034\n0.0030696766916662455\n0.0030586961656808853\n0.0029690659139305353\n0.0030384762212634087\n0.003027551807463169\n0.0030166469514369965\n0.0030065984465181828\n0.0029957315418869257\n0.00298571796156466\n0.0029757211450487375\n0.0029640793800354004\n0.0029557778034359217\n0.0029441751539707184\n0.0029350747354328632\n0.002925163134932518\n0.002915268298238516\n0.0029045678675174713\n0.0028938869945704937\n0.0028848648071289062\n0.0028742202557623386\n0.0028652288019657135\n0.0028554359450936317\n0.002846473827958107\n0.0028359005227684975\n0.002825346775352955\n0.0028172419406473637\n0.002805914729833603\n0.00279783783480525\n0.002788160927593708\n0.0027776965871453285\n0.002768857404589653\n0.00275923078879714\n0.0027488209307193756\n0.002737632254138589\n0.002729654312133789\n0.0027193003334105015\n0.002712938468903303\n0.002703409641981125\n0.0026907306164503098\n0.002682821359485388\n0.0026749237440526485\n0.0026662498712539673\n0.0011233175173401833\n0.0026481589302420616\n0.0026395285967737436\n0.002630912233144045\n0.002622310072183609\n0.0026137218810617924\n0.002603590488433838\n0.0025934786535799503\n0.00246684649027884\n0.0025771858636289835\n0.002567898714914918\n0.0021481900475919247\n0.0025439844466745853\n0.002536294050514698\n0.0025293827056884766\n0.0025217144284397364\n0.0025132927112281322\n0.0025041215121746063\n0.0024949670769274235\n0.0024850687477737665\n0.002476708497852087\n0.0024676043540239334\n0.00234929658472538\n0.0024494463577866554\n0.0024419003166258335\n0.002432107925415039\n0.001992692705243826\n0.0024170810356736183\n0.002409585053101182\n0.002401352860033512\n0.002393134869635105\n0.0023864214308559895\n0.002378973178565502\n0.0023700506426393986\n0.002361144870519638\n0.0023507759906351566\n0.0023411682341247797\n0.0023315800353884697\n0.0019258297979831696\n0.002315398771315813\n0.002306596376001835\n0.0023000056389719248\n0.002291232580319047\n0.0022824762854725122\n0.002274464350193739\n0.001850905828177929\n0.002257757820188999\n0.002249065786600113\n0.002241112757474184\n0.002232452854514122\n0.002225968986749649\n0.0022173384204506874\n0.002211594022810459\n0.0022044240031391382\n0.0021965503692626953\n0.0021886909380555153\n0.002179420553147793\n0.002170169958844781\n0.001630120910704136\n0.0021602297201752663\n0.0021531435195356607\n0.0021439488045871258\n0.0018039420247077942\n0.0021249151322990656\n0.0020454861223697662\n0.002108768094331026\n0.0021017668768763542\n0.002094777300953865\n0.002087102271616459\n0.002079441212117672\n0.0020710998214781284\n0.0020634683314710855\n0.002056542783975601\n0.0020510107278823853\n0.0020434162579476833\n0.002033771015703678\n0.0020268955267965794\n0.001709300559014082\n0.002014548983424902\n0.002007022500038147\n0.001999509986490011\n0.001992692705243826\n0.0019858870655298233\n0.0019777356646955013\n0.0019696010276675224\n0.001963511109352112\n0.0016741552390158176\n0.001925160177052021\n0.0019419342279434204\n0.0017684735357761383\n0.0019298496190458536\n0.0019218141678720713\n0.0019131279550492764\n0.0019077924080193043\n0.0019011334516108036\n0.0018944861367344856\n0.0018865247257053852\n0.0018785800784826279\n0.0018706521950662136\n0.001864058431237936\n0.0018587918020784855\n0.0018515624105930328\n0.0018430366180837154\n0.0018364917486906052\n0.0018299585208296776\n0.0018234369345009327\n0.0018162766937166452\n0.001809779554605484\n0.0018026460893452168\n0.0017981140408664942\n0.0017910036258399487\n0.001783262938261032\n0.0017768251709640026\n0.001769757131114602\n0.0017639845609664917\n0.0017575817182660103\n0.0017492754850536585\n0.0017435364425182343\n0.0017365349922329187\n0.001730182208120823\n0.0017238410655409098\n0.0017181439325213432\n0.0017111937049776316\n0.0017055175267159939\n0.001699850894510746\n0.0016935656312853098\n0.00168666522949934\n0.0016810300294309855\n0.0016747796908020973\n0.001666671596467495\n0.0016610699240118265\n0.001654856838285923\n0.0016486553940922022\n0.0016430839896202087\n0.0016362874303013086\n0.0016295048408210278\n0.0013618594966828823\n0.0016208929009735584\n0.0016147554852068424\n0.0016067943070083857\n0.0015994629357010126\n0.0015939753502607346\n0.0014337343163788319\n0.0005120618734508753\n0.0015830285847187042\n0.001577569404616952\n0.0015703050885349512\n0.0015618512406945229\n0.0015570307150483131\n0.001551616471260786\n0.0015456117689609528\n0.001539020100608468\n0.0015318451914936304\n0.0015246870461851358\n0.001520519144833088\n0.0015151689294725657\n0.001510421046987176\n0.0009995847940444946\n0.0014997655525803566\n0.0014938621316105127\n0.0014867933932691813\n0.0014797414187341928\n0.00147504941560328\n0.0014703648630529642\n0.0014656877610832453\n0.001459851861000061\n0.0014540276024490595\n0.0014493765775114298\n0.0014441530220210552\n0.001436624675989151\n0.0014320015907287598\n0.0014268094673752785\n0.0014210515655577183\n0.0014170280192047358\n0.001411863137036562\n0.0014049913734197617\n0.001399848610162735\n0.0013947151601314545\n0.0013895912561565638\n0.0007368719670921564\n0.001378238433972001\n0.00046981038758531213\n0.001369189703837037\n0.0013652401976287365\n0.0013607335276901722\n0.001355672487989068\n0.00135062076151371\n0.001345018856227398\n0.0013394285924732685\n0.0013344073668122292\n0.001329395454376936\n0.001323837786912918\n0.0013177378568798304\n0.0013116518966853619\n0.0013088902924209833\n0.0013044776860624552\n0.001300072530284524\n0.0012945765629410744\n0.00128854438662529\n0.0012825264129787683\n0.0012770676985383034\n0.0012732534669339657\n0.0012683579698204994\n0.0012634717859327793\n0.0012580538168549538\n0.0012531876564025879\n0.0012477918062359095\n0.0012429454363882542\n0.0012386455200612545\n0.0012343530543148518\n0.0012284631375223398\n0.0012241883669048548\n0.0012188553810119629\n0.0012140655890107155\n0.0011310018599033356\n0.0012055737897753716\n0.001200810307636857\n0.0011960561387240887\n0.0011918381787836552\n0.0011871019378304482\n0.0011823750101029873\n0.0011771339923143387\n0.0011729495599865913\n0.0011677294969558716\n0.0008780891075730324\n0.001161480788141489\n0.0011573242954909801\n0.0011521391570568085\n0.0001981403329409659\n0.001144899521023035\n0.0011412883177399635\n0.0011366535909473896\n0.0011320284102112055\n0.0011274125427007675\n0.0011228062212467194\n0.0011187195777893066\n0.0011141309514641762\n0.0011090436019003391\n0.0011039678938686848\n0.001098398119211197\n0.0010943561792373657\n0.001091329613700509\n0.0010873007122427225\n0.0010827770456671715\n0.0010782629251480103\n0.0010732582304626703\n0.00106926285661757\n0.001065772958099842\n0.001061791554093361\n0.0010573214385658503\n0.00093365233624354\n0.0008880647365003824\n0.0010449537076056004\n0.0010405192151665688\n0.0010360940359532833\n0.0010326588526368141\n0.0010292292572557926\n0.0010238515678793192\n0.0010199493262916803\n0.0010160545352846384\n0.001011681742966175\n0.0010092565789818764\n0.0007947106496430933\n0.0010019983164966106\n0.0009971740655601025\n0.0009914003312587738\n0.0009875604882836342\n0.0009842067956924438\n0.0009808586910367012\n0.0009760857210494578\n0.0009722756803967059\n0.0009679982904344797\n0.0009642040822654963\n0.0009594717994332314\n0.0009571100235916674\n0.0009533372358419001\n0.000948161818087101\n0.0009448757045902312\n0.0009420635760761797\n0.0009383205906488001\n0.0009345850558020175\n0.0009313225746154785\n0.0009271363378502429\n0.0009234231547452509\n0.0009192547295242548\n0.0009155573789030313\n0.0009118674788624048\n0.0009072655811905861\n0.0009035924449563026\n0.0009013005183078349\n0.0008980967104434967\n0.000893529737368226\n0.0008889744058251381\n0.0008853385224938393\n0.0008826164994388819\n0.0008785413228906691\n0.0008758297772146761\n0.0008722209022380412\n0.0008686194778420031\n0.0008645767811685801\n0.000860543514136225\n0.000795571191702038\n0.0008551804930903018\n0.0008516144589520991\n0.0008471673936583102\n0.0008431749884039164\n0.0008400763035751879\n0.000836983323097229\n0.0008338960469700396\n0.0008303747163154185\n0.0008272996637970209\n0.000823354406747967\n0.0008198554278351367\n0.0008163638995029032\n0.0008128798217512667\n0.0008089691400527954\n0.0008059340179897845\n0.0008020401000976562\n0.0007990180165506899\n0.0007960016373544931\n0.0007925613317638636\n0.0007891284767538309\n0.0007857030723243952\n0.0007831389084458351\n0.000779726542532444\n0.0007767468341626227\n0.0007737728301435709\n0.0006816165987402201\n0.0007678419351577759\n0.0007648850441910326\n0.00076151272514835\n0.0007581478566862643\n0.0007556290947832167\n0.0007526958361268044\n0.0007489328854717314\n0.0007451793644577265\n0.0007422664784826338\n0.0007389444508589804\n0.0007360437884926796\n0.000733562046661973\n0.0007294351235032082\n0.0007261419668793678\n0.0007228562608361244\n0.0007208064780570567\n0.0007179416716098785\n0.0007150825695134699\n0.0007114149630069733\n0.0007085688994266093\n0.0007057285401970148\n0.0007028938853181899\n0.0007000649347901344\n0.00041154451901093125\n0.00033779541263356805\n0.0006920136511325836\n0.0006892067031003535\n0.0006872052326798439\n0.0006840089336037636\n0.0006816165987402201\n0.0006788308382965624\n0.0006760507822036743\n0.0006728805601596832\n0.0006697177886962891\n0.0006661685765720904\n0.0006642008665949106\n0.00047878269106149673\n0.0006598821491934359\n0.0006575324223376811\n0.0006540156900882721\n0.0006520660244859755\n0.000649341382086277\n0.0006458466523326933\n0.0006415881216526031\n0.0006396570825017989\n0.0006369585171341896\n0.0006342656561173499\n0.00033779541263356805\n0.0006288970471359789\n0.000627367349807173\n0.000624694861471653\n0.0006212671869434416\n0.0006189873092807829\n0.0006159539916552603\n0.0006129281246103346\n0.0005436092615127563\n0.0006080269813537598\n0.000605020672082901\n0.0006031455122865736\n0.0006005251780152321\n0.0005982837174087763\n0.0005964190349914134\n0.000593813369050622\n0.0005912134074606001\n0.0005886191502213478\n0.000585661269724369\n0.0005823425599373877\n0.000580135325435549\n0.0005779322818852961\n0.0005753673613071442\n0.0005724430084228516\n0.0005702546332031488\n0.0005301742348819971\n0.0005655275308527052\n0.0005629903171211481\n0.0005608201026916504\n0.0005579330027103424\n0.0005557725671678782\n0.0005532573559321463\n0.0004678280674852431\n0.0005503898137249053\n0.0005482440465129912\n0.0005450332537293434\n0.0005414767656475306\n0.0005397028871811926\n0.0005375780747272074\n0.00053581059910357\n0.0005336934700608253\n0.0005315805319696665\n0.0005294717848300934\n0.0005273672286421061\n0.0005249172099865973\n0.0005221241735853255\n0.0005196863785386086\n0.000516907311975956\n0.0005144817405380309\n0.0005120618734508753\n0.000509992241859436\n0.0004084548563696444\n0.00023422972299158573\n0.0005051793996244669\n0.0005031237378716469\n0.0005003893747925758\n0.0004973221221007407\n0.0004952825256623328\n0.0004932471201755106\n0.0004912159056402743\n0.0004885141388513148\n0.0004864927032031119\n0.00048447545850649476\n0.0004824624047614634\n0.00048011913895606995\n0.000478448870126158\n0.00047644838923588395\n0.00044823536882176995\n0.00047212839126586914\n0.00046981038758531213\n0.00046749808825552464\n0.0004655206575989723\n0.00040660664672032\n0.00046256236964836717\n0.0004605954163707793\n0.0004579793312586844\n0.0004556963685899973\n0.0004537440836429596\n0.0004521203809417784\n0.0004501757794059813\n0.00044823536882176995\n0.0004462991491891444\n0.00044436712050810456\n0.0004421183839440346\n0.0004398753517307341\n0.00043795729288831353\n0.00043636211194097996\n0.00043476984137669206\n0.0004325455520302057\n0.0004300104919821024\n0.0004284298629499972\n0.00042685214430093765\n0.0004249627236276865\n0.0004230774939060211\n0.00042088335612788796\n0.00030658120522275567\n0.00041775876889005303\n0.0004155784845352173\n0.00041340390453115106\n0.00041154451901093125\n0.0004096893244422972\n0.00040783832082524896\n0.00040599150815978646\n0.00040414888644590974\n0.0004023104556836188\n0.0004004762158729136\n0.0003986461670137942\n0.0003971243277192116\n0.00039560539880767465\n0.00039348378777503967\n0.00039197184378281236\n0.00039016135269775987\n0.00038865581154823303\n0.0003868530038744211\n0.00014530960470438004\n0.00038415665039792657\n0.00038266275078058243\n0.00038087391294538975\n0.00037908926606178284\n0.0003773088101297617\n0.0003752369084395468\n0.0003734655329026282\n0.0003719925880432129\n0.00037022889591753483\n0.00036846939474344254\n0.0003664219402708113\n0.00036496296525001526\n0.0003632160369306803\n0.0003617634647525847\n0.0003600242198444903\n0.00035886705154553056\n0.00035684648901224136\n0.000355694442987442\n0.00018964201444759965\n0.00035310914972797036\n0.0003516769502311945\n0.0003502476611174643\n0.00034853635588660836\n0.00034654513001441956\n4.652165807783604e-05\n0.00034455960849300027\n0.00034342758590355515\n0.00034145102836191654\n0.00033948017517104745\n0.0003375150263309479\n0.00033639464527368546\n0.00033499678829684854\n0.0003333232016302645\n0.0003316538059152663\n0.0003305432037450373\n0.000328604131937027\n0.0003274986520409584\n0.0003261194215156138\n0.00032446818659082055\n0.0003228211426176131\n0.0003211782895959914\n0.0003200853825546801\n0.00031872186809778214\n0.00031708949245512486\n0.00031519035110250115\n0.0003138373140245676\n0.00031248718732967973\n0.0003114091814495623\n0.00031006429344415665\n0.00030872231582179666\n4.9266964197158813e-05\n0.0003060470917262137\n0.0003047138452529907\n0.0003033835091628134\n0.00030232133576646447\n0.00030073156813159585\n0.00029888213612139225\n0.0002975646057166159\n0.0002959874109365046\n0.00029441440710797906\n0.00029336806619539857\n0.00029180204728618264\n0.00029050023294985294\n0.00028946087695658207\n0.0002881643013097346\n0.00028687063604593277\n0.00028557988116517663\n0.00028429203666746616\n0.0002827504649758339\n0.0002812130842357874\n0.0002801904920488596\n0.00027891487115994096\n0.000277642160654068\n0.0002763723605312407\n0.00022269878536462784\n0.00027384149143472314\n0.00027258042246103287\n0.00027132226387038827\n0.00027056876569986343\n0.0002693152637220919\n0.0002675652503967285\n0.0002660697791725397\n0.0002650751266628504\n0.0002640823367983103\n0.00026284396881237626\n0.0002616085112094879\n0.00026037596398964524\n0.00025914632715284824\n0.0002579196006990969\n0.00025669578462839127\n0.00025523104704916477\n0.00025377050042152405\n0.00025304179871454835\n0.00025207182625308633\n0.00025110371643677354\n0.00024989619851112366\n0.0002484510187059641\n0.0002467702724970877\n0.0002458124072290957\n0.0002448564046062529\n0.00014457479119300842\n0.00024294998729601502\n0.00024176225997507572\n0.0002405774430371821\n0.00023939553648233414\n0.00012002978473901749\n0.0002368055866099894\n0.0002031112089753151\n0.00023539882386103272\n0.00023446331033483148\n0.0002330635325051844\n0.0002314357552677393\n0.0002302765497006476\n0.00022935128072276711\n7.818924495950341e-05\n0.00022750633070245385\n0.00022658664966002107\n0.00022589811123907566\n0.0002247528755106032\n0.0002238387824036181\n0.00022269878536462784\n0.00022156169870868325\n0.00022042752243578434\n0.0002192962565459311\n0.00021816790103912354\n0.00021704245591536164\n0.00021591992117464542\n0.00021457672119140625\n0.00021390669280663133\n0.000213014951441437\n0.00021168083185330033\n0.0002107937471009791\n0.00020968751050531864\n0.00020880461670458317\n0.00020792358554899693\n0.00020682491594925523\n0.00020594807574525476\n0.00020485464483499527\n0.00020376412430778146\n0.0002028938033618033\n0.00020202534506097436\n0.00020115874940529466\n0.0001998623483814299\n0.00019900040933862329\n0.0001979256048798561\n0.0001968537108041346\n0.0001772437826730311\n0.0001951447338797152\n0.00019429303938522935\n0.00019323104061186314\n0.0001921719522215426\n0.0001451257267035544\n0.00019090488785877824\n0.00019006250659003854\n0.00018922198796644807\n8.159875869750977e-05\n0.00018754653865471482\n0.00018671160796657205\n0.00018567056395113468\n4.799012094736099e-05\n0.00018359720706939697\n0.00018277112394571304\n0.00018194690346717834\n0.00018091924721375108\n0.00018009921768680215\n0.0001790768001228571\n0.00017866864800453186\n0.00017785374075174332\n0.00017683772603049874\n0.00010909052798524499\n0.00017562235007062554\n2.04686657525599e-05\n0.00017400836804881692\n0.00017320417100563645\n0.0001726022455841303\n0.00011736992746591568\n0.00017140153795480728\n0.00017060339450836182\n0.00016980711370706558\n8.361006621271372e-06\n0.0001682201400399208\n0.00016742944717407227\n0.0001664437004365027\n0.00016585364937782288\n0.00016506854444742203\n0.00016408978262916207\n0.0001631139311939478\n0.00016233534552156925\n0.00016194675117731094\n0.00016136473277583718\n0.0001605903380550444\n0.00015943223843351007\n0.00015885476022958755\n0.0001582783297635615\n0.00015751138562336564\n0.00015674630412831903\n0.00015579257160425186\n0.00015541189350187778\n0.00015484174946323037\n0.00015389383770525455\n0.00015276018530130386\n0.00015219493070617318\n0.00015144288772717118\n0.00015069270739331841\n0.00014994438970461488\n0.00014919793466106057\n0.0001484533422626555\n0.00014752522110939026\n4.589930176734924e-05\n0.00014641531743109226\n0.0001456777099519968\n0.00014494196511805058\n0.00014420808292925358\n0.0001434760633856058\n0.0001425636583007872\n0.00014201761223375797\n0.0001412911806255579\n0.00014056661166250706\n0.00013984390534460545\n0.00013930309796705842\n0.00013840408064424992\n0.00013786606723442674\n0.0001373291015625\n0.00013661477714776993\n0.0001359023153781891\n0.00013519171625375748\n0.00013430608669295907\n0.0001339526497758925\n0.00013324717292562127\n0.00013254355872049928\n0.0001314916298724711\n0.00013096723705530167\n0.00013044389197602868\n0.0001297477283515036\n0.000129226827993989\n0.0001287069753743708\n8.02262220531702e-05\n6.008520722389221e-05\n0.00012698170030489564\n0.0001264663878828287\n0.00012578093446791172\n0.00012509734369814396\n0.00012441561557352543\n0.00012356607476249337\n0.000122719444334507\n0.00012238160707056522\n0.00012204423546791077\n0.00012137088924646378\n0.0001203643623739481\n0.00012002978473901749\n0.00011952879140153527\n8.806237019598484e-05\n0.00011803209781646729\n0.00011753529543057084\n0.00011703954078257084\n0.00011638016439974308\n0.00011572265066206455\n5.045213038101792e-05\n0.00011457648361101747\n0.00011392409214749932\n0.00011327356332913041\n0.00011278688907623291\n0.00010924995876848698\n0.00011165539035573602\n0.00011117220856249332\n0.00011052959598600864\n0.0001098888460546732\n0.00010940950596705079\n0.0001087720156647265\n5.855743074789643e-05\n0.00010781927267089486\n0.00010734447278082371\n0.00010671303607523441\n0.00010624068090692163\n0.00010576937347650528\n0.00010514259338378906\n0.00010451767593622208\n0.00010389462113380432\n0.00010342855239287019\n0.0001029635313898325\n0.0001026540994644165\n0.00010203663259744644\n9.746500290930271e-05\n0.00010065414244309068\n0.00010034820297732949\n9.989016689360142e-05\n9.943317854776978e-05\n9.89772379398346e-05\n9.837094694375992e-05\n9.79174510575831e-05\n9.731441969051957e-05\n9.671325096860528e-05\n9.626359678804874e-05\n9.566568769514561e-05\n9.521847823634744e-05\n9.477231651544571e-05\n9.432720253244042e-05\n9.373534703627229e-05\n9.314535418525338e-05\n9.270408190786839e-05\n9.226385736837983e-05\n9.182468056678772e-05\n9.124074131250381e-05\n9.080400923267007e-05\n9.022332960739732e-05\n8.99336882866919e-05\n8.9355802629143e-05\n8.89236107468605e-05\n8.849246660247445e-05\n8.791923755779862e-05\n8.749053813517094e-05\n8.692056871950626e-05\n8.663628250360489e-05\n8.606910705566406e-05\n8.564494783058763e-05\n8.522183634340763e-05\n8.479977259412408e-05\n8.437875658273697e-05\n8.39587883092463e-05\n8.340046042576432e-05\n8.298293687403202e-05\n8.256646106019616e-05\n8.215103298425674e-05\n8.173665264621377e-05\n8.132332004606724e-05\n8.091103518381715e-05\n8.036295184865594e-05\n7.995311170816422e-05\n7.954431930556893e-05\n7.90008925832808e-05\n7.87298777140677e-05\n7.832422852516174e-05\n7.791962707415223e-05\n7.738178828731179e-05\n7.697963155806065e-05\n7.671210914850235e-05\n7.631169864907861e-05\n7.577944779768586e-05\n7.551402086392045e-05\n7.498456398025155e-05\n7.458869367837906e-05\n7.419387111440301e-05\n7.393123814836144e-05\n7.35381618142128e-05\n7.301568984985352e-05\n7.262505823746324e-05\n7.22354743629694e-05\n7.1846938226372e-05\n7.158849621191621e-05\n7.120170630514622e-05\n7.068761624395847e-05\n7.043126970529556e-05\n6.991997361183167e-05\n6.953772390261292e-05\n6.915652193129063e-05\n6.877636769786477e-05\n6.852351361885667e-05\n6.801920244470239e-05\n6.764219142496586e-05\n6.726622814312577e-05\n6.701616803184152e-05\n6.664195097982883e-05\n6.62687816657126e-05\n6.58966600894928e-05\n6.552558625116944e-05\n6.515556015074253e-05\n6.490945816040039e-05\n6.454117828980088e-05\n6.417394615709782e-05\n6.380776176229119e-05\n6.356422090902925e-05\n6.307853618636727e-05\n4.652165807783604e-05\n6.235350156202912e-05\n6.211275467649102e-05\n6.175250746309757e-05\n6.139330798760056e-05\n6.103515625e-05\n6.067805225029588e-05\n6.03219959884882e-05\n6.008520722389221e-05\n5.9730897191911936e-05\n5.93776348978281e-05\n4.631374031305313e-05\n5.879119271412492e-05\n5.844072438776493e-05\n5.820766091346741e-05\n5.7858938816934824e-05\n5.751126445829868e-05\n5.728006362915039e-05\n5.6934135500341654e-05\n5.658925510942936e-05\n5.624542245641351e-05\n5.601678276434541e-05\n5.567469634115696e-05\n5.5333657655864954e-05\n5.5106880608946085e-05\n5.4767588153481483e-05\n5.4541975259780884e-05\n5.4204429034143686e-05\n9.406590834259987e-06\n5.3755997214466333e-05\n5.34208957105875e-05\n5.319807678461075e-05\n5.286472151055932e-05\n3.70668713003397e-05\n5.231145769357681e-05\n5.198089638724923e-05\n5.165138281881809e-05\n5.1322916988283396e-05\n5.1104521844536066e-05\n4.097359487786889e-05\n5.066912854090333e-05\n5.034380592405796e-05\n4.991167224943638e-05\n4.969630390405655e-05\n4.948140121996403e-05\n4.915992030873895e-05\n4.883948713541031e-05\n4.862644709646702e-05\n4.8307760152965784e-05\n4.809588426724076e-05\n4.788447404280305e-05\n4.756823182106018e-05\n4.7253037337213755e-05\n4.704348975792527e-05\n4.673004150390625e-05\n4.652165807783604e-05\n3.0595401767641306e-05\n4.610628820955753e-05\n4.579598316922784e-05\n4.558969521895051e-05\n4.528113640844822e-05\n4.507601261138916e-05\n4.4769200030714273e-05\n4.446343518793583e-05\n4.4260174036026e-05\n4.395615542307496e-05\n4.37540584243834e-05\n4.355242708697915e-05\n4.3250853195786476e-05\n4.3050386011600494e-05\n3.025872865691781e-05\n2.6678259018808603e-05\n4.235241794958711e-05\n4.215404624119401e-05\n4.195614019408822e-05\n4.175869980826974e-05\n4.146341234445572e-05\n4.116917261853814e-05\n4.097359487786889e-05\n4.068110138177872e-05\n4.0486687794327736e-05\n4.029273986816406e-05\n4.00992576032877e-05\n2.967403270304203e-05\n3.9617589209228754e-05\n3.9425736758857965e-05\n3.239349462091923e-05\n3.8948142901062965e-05\n3.8757920265197754e-05\n3.856816329061985e-05\n3.8284400943666697e-05\n3.809580812230706e-05\n3.7907680962234735e-05\n3.7720019463449717e-05\n3.743940033018589e-05\n3.2480398658663034e-05\n3.70668713003397e-05\n3.6881305277347565e-05\n3.669620491564274e-05\n3.651157021522522e-05\n3.632740117609501e-05\n3.568647662177682e-05\n3.586901584640145e-05\n3.568647662177682e-05\n3.541354089975357e-05\n3.5232165828347206e-05\n3.496097633615136e-05\n3.4780765417963266e-05\n3.460102016106248e-05\n3.4421740565449e-05\n3.424292663112283e-05\n2.5432149413973093e-05\n3.379792906343937e-05\n3.3620744943618774e-05\n3.3355841878801584e-05\n3.326777368783951e-05\n3.3091986551880836e-05\n3.282917896285653e-05\n3.265455598011613e-05\n3.2480398658663034e-05\n2.7712667360901833e-05\n3.213348099961877e-05\n3.19607206620276e-05\n3.1788425985723734e-05\n3.161659697070718e-05\n3.1359726563096046e-05\n3.118906170129776e-05\n3.101886250078678e-05\n3.084912896156311e-05\n3.067986108362675e-05\n3.051105886697769e-05\n3.0342722311615944e-05\n3.0174851417541504e-05\n2.9923918191343546e-05\n2.9757211450487375e-05\n2.9590970370918512e-05\n2.2302905563265085e-05\n2.934248186647892e-05\n2.9177404940128326e-05\n2.901279367506504e-05\n2.8848648071289062e-05\n2.8684968128800392e-05\n2.844032132998109e-05\n2.8196722269058228e-05\n2.8115755412727594e-05\n2.795417094603181e-05\n2.779305214062333e-05\n2.763239899650216e-05\n1.3635493814945221e-05\n2.7312489692121744e-05\n2.7153233531862497e-05\n2.6994443032890558e-05\n2.6836118195205927e-05\n2.6678259018808603e-05\n2.6520865503698587e-05\n2.636393764987588e-05\n2.620747545734048e-05\n2.6051478926092386e-05\n2.58959480561316e-05\n2.5740882847458124e-05\n2.5586283300071955e-05\n2.5432149413973093e-05\n2.527848118916154e-05\n2.5125278625637293e-05\n2.4972541723400354e-05\n2.4820270482450724e-05\n2.46684649027884e-05\n2.1235086023807526e-05\n2.4441629648208618e-05\n2.4290988221764565e-05\n2.414081245660782e-05\n2.399110235273838e-05\n2.384185791015625e-05\n2.3693079128861427e-05\n2.3544766008853912e-05\n2.3396918550133705e-05\n2.3249536752700806e-05\n2.3176020476967096e-05\n2.302933717146516e-05\n2.288311952725053e-05\n2.2737367544323206e-05\n2.1588115487247705e-05\n2.2447260562330484e-05\n2.2302905563265085e-05\n2.2159016225486994e-05\n2.039968967437744e-05\n2.194405533373356e-05\n2.1801330149173737e-05\n2.1659070625901222e-05\n2.1517276763916016e-05\n2.1375948563218117e-05\n2.1305459085851908e-05\n2.116482937708497e-05\n2.102466532960534e-05\n1.9446248188614845e-05\n2.08152923732996e-05\n2.067629247903824e-05\n2.0537758246064186e-05\n2.04686657525599e-05\n1.760781742632389e-05\n2.0193459931761026e-05\n2.005655551329255e-05\n1.9920116756111383e-05\n1.9784143660217524e-05\n1.9716331735253334e-05\n1.964863622561097e-05\n1.9446248188614845e-05\n1.937901834025979e-05\n1.9311904907226562e-05\n1.9178027287125587e-05\n1.904461532831192e-05\n1.891166903078556e-05\n1.877918839454651e-05\n1.8713122699409723e-05\n1.8581340555101633e-05\n1.845002407208085e-05\n1.8319173250347376e-05\n1.818878808990121e-05\n1.8123770132660866e-05\n1.805886859074235e-05\n1.7929414752870798e-05\n1.7736107110977173e-05\n1.7671904060989618e-05\n7.460126653313637e-06\n1.7479993402957916e-05\n1.741625601425767e-05\n1.7289130482822657e-05\n1.716247061267495e-05\n1.709931530058384e-05\n1.6973353922367096e-05\n1.684785820543766e-05\n1.6722828149795532e-05\n1.666048774495721e-05\n7.543712854385376e-06\n1.64741650223732e-05\n1.6350531950592995e-05\n1.6227364540100098e-05\n1.616595545783639e-05\n1.6043486539274454e-05\n1.4204764738678932e-05\n1.5860656276345253e-05\n1.5799945686012506e-05\n1.5678873751312494e-05\n1.555826747789979e-05\n1.5498138964176178e-05\n1.5378231182694435e-05\n1.5318451914936304e-05\n1.5199242625385523e-05\n1.5139812603592873e-05\n1.5021301805973053e-05\n1.4962221030145884e-05\n1.4844408724457026e-05\n1.4727062080055475e-05\n1.466856338083744e-05\n1.4610181096941233e-05\n1.4493765775114298e-05\n1.437781611457467e-05\n1.4320015907287598e-05\n1.4204764738678932e-05\n7.88271427154541e-06\n1.4089979231357574e-05\n1.3975659385323524e-05\n5.2736722864210606e-06\n1.3861805200576782e-05\n1.3805052731186152e-05\n1.3691897038370371e-05\n1.3579207006841898e-05\n1.3523036614060402e-05\n5.593756213784218e-08\n1.3355223927646875e-05\n1.3299519196152687e-05\n1.3188458979129791e-05\n1.3077864423394203e-05\n1.302274176850915e-05\n1.2912845704704523e-05\n1.285807229578495e-05\n1.2803415302187204e-05\n1.2694450560957193e-05\n1.2640142813324928e-05\n1.258595148101449e-05\n1.2477918062359095e-05\n1.2370350304991007e-05\n1.2316741049289703e-05\n1.2263248208910227e-05\n1.2156611774116755e-05\n1.2103468179702759e-05\n1.205044100061059e-05\n1.1944735888391733e-05\n8.53842357173562e-06\n1.178705133497715e-05\n1.1734722647815943e-05\n1.1682510375976562e-05\n1.163041451945901e-05\n1.1526572052389383e-05\n2.612941898405552e-07\n1.1423195246607065e-05\n1.1371681466698647e-05\n1.1320284102112055e-05\n1.1217838618904352e-05\n1.1166790500283241e-05\n1.10650435090065e-05\n1.101434463635087e-05\n1.091329613700509e-05\n1.0862946510314941e-05\n1.0812713298946619e-05\n1.0712596122175455e-05\n1.0662712156772614e-05\n1.0612944606691599e-05\n1.051375875249505e-05\n1.0464340448379517e-05\n1.036585308611393e-05\n1.036585308611393e-05\n1.026783138513565e-05\n1.0218995157629251e-05\n1.0121671948581934e-05\n1.0073184967041016e-05\n1.0024814400821924e-05\n9.928422514349222e-06\n9.880401194095612e-06\n9.832496289163828e-06\n9.737035725265741e-06\n9.689480066299438e-06\n9.642040822654963e-06\n9.594717994332314e-06\n9.500421583652496e-06\n9.453448001295328e-06\n9.406590834259987e-06\n9.313225746154785e-06\n9.266717825084925e-06\n9.220326319336891e-06\n9.174051228910685e-06\n9.127892553806305e-06\n9.035924449563026e-06\n8.990115020424128e-06\n8.944422006607056e-06\n8.853385224938393e-06\n4.434026777744293e-06\n8.762814104557037e-06\n8.7177031673491e-06\n8.627830538898706e-06\n8.58306884765625e-06\n8.53842357173562e-06\n8.449482265859842e-06\n6.53235474601388e-06\n8.361006621271372e-06\n8.316943421959877e-06\n8.272996637970209e-06\n8.229166269302368e-06\n8.185452315956354e-06\n8.141854777932167e-06\n8.055008947849274e-06\n8.011760655790567e-06\n7.968628779053688e-06\n7.925613317638636e-06\n7.839931640774012e-06\n7.79726542532444e-06\n7.754715625196695e-06\n7.712282240390778e-06\n7.669965270906687e-06\n7.627764716744423e-06\n7.543712854385376e-06\n7.501861546188593e-06\n7.460126653313637e-06\n7.418508175760508e-06\n7.33562046661973e-06\n7.294351235032082e-06\n7.25319841876626e-06\n7.212162017822266e-06\n7.171242032200098e-06\n7.089751306921244e-06\n7.049180567264557e-06\n7.008726242929697e-06\n6.968388333916664e-06\n6.928166840225458e-06\n6.888061761856079e-06\n6.848073098808527e-06\n6.808200851082802e-06\n6.768445018678904e-06\n6.689282599836588e-06\n4.9970694817602634e-06\n6.61058584228158e-06\n6.571412086486816e-06\n6.53235474601388e-06\n6.49341382086277e-06\n6.454589311033487e-06\n6.4158812165260315e-06\n6.377289537340403e-06\n6.338814273476601e-06\n6.300455424934626e-06\n6.224086973816156e-06\n6.186077371239662e-06\n6.148184183984995e-06\n6.1104074120521545e-06\n6.072747055441141e-06\n6.035203114151955e-06\n3.66714084520936e-06\n5.9604644775390625e-06\n5.923269782215357e-06\n5.886191502213478e-06\n5.849229637533426e-06\n5.812384188175201e-06\n5.7756551541388035e-06\n5.7390425354242325e-06\n5.666166543960571e-06\n5.629903171211481e-06\n5.593756213784218e-06\n5.5577256716787815e-06\n3.5224948078393936e-06\n5.521811544895172e-06\n5.48601383343339e-06\n5.4147676564753056e-06\n5.4147676564753056e-06\n5.379319190979004e-06\n5.343987140804529e-06\n5.308771505951881e-06\n5.2736722864210606e-06\n5.2038230933249e-06\n5.16907311975956e-06\n5.1344395615160465e-06\n5.09992241859436e-06\n5.065521690994501e-06\n5.031237378716469e-06\n4.9970694817602634e-06\n4.963018000125885e-06\n4.9290829338133335e-06\n4.9290829338133335e-06\n4.895264282822609e-06\n4.861562047153711e-06\n4.827976226806641e-06\n4.76115383207798e-06\n4.72791725769639e-06\n4.694797098636627e-06\n4.661793354898691e-06\n4.628906026482582e-06\n3.3527612686157227e-06\n4.5961351133883e-06\n4.563480615615845e-06\n4.5309425331652164e-06\n4.498520866036415e-06\n1.3271928764879704e-06\n4.434026777744293e-06\n4.401954356580973e-06\n4.369998350739479e-06\n4.338158760219812e-06\n4.306435585021973e-06\n4.27482882514596e-06\n4.27482882514596e-06\n4.243338480591774e-06\n4.211964551359415e-06\n4.180707037448883e-06\n4.149565938860178e-06\n4.1185412555933e-06\n4.087632987648249e-06\n4.056841135025024e-06\n4.026165697723627e-06\n4.026165697723627e-06\n3.995606675744057e-06\n3.965164069086313e-06\n3.934837877750397e-06\n3.904628101736307e-06\n3.8745347410440445e-06\n3.844557795673609e-06\n3.814697265625e-06\n3.784953150898218e-06\n3.7553254514932632e-06\n3.7553254514932632e-06\n3.7258141674101353e-06\n3.6964192986488342e-06\n2.1012965589761734e-06\n3.637978807091713e-06\n3.6089331842958927e-06\n3.6089331842958927e-06\n3.5800039768218994e-06\n3.551191184669733e-06\n3.5224948078393936e-06\n2.8172507882118225e-06\n3.4654513001441956e-06\n3.437104169279337e-06\n3.437104169279337e-06\n3.4088734537363052e-06\n3.3807591535151005e-06\n3.3527612686157227e-06\n3.3248797990381718e-06\n3.297114744782448e-06\n3.297114744782448e-06\n3.269466105848551e-06\n3.2419338822364807e-06\n2.1457672119140625e-06\n3.2145180739462376e-06\n1.4165416359901428e-06\n3.160035703331232e-06\n3.1329691410064697e-06\n3.1060189940035343e-06\n3.1060189940035343e-06\n3.079185262322426e-06\n3.0524679459631443e-06\n3.0524679459631443e-06\n3.0258670449256897e-06\n2.999382559210062e-06\n2.9730144888162613e-06\n2.9467628337442875e-06\n2.9467628337442875e-06\n2.8946087695658207e-06\n2.8946087695658207e-06\n2.8687063604593277e-06\n2.4223700165748596e-06\n2.8172507882118225e-06\n2.8172507882118225e-06\n2.7916976250708103e-06\n2.766260877251625e-06\n2.7409405447542667e-06\n1.9706785678863525e-06\n2.7157366275787354e-06\n1.94931635633111e-06\n2.6656780391931534e-06\n2.640823367983103e-06\n2.640823367983103e-06\n2.616085112094879e-06\n2.5914632715284824e-06\n2.5669578462839127e-06\n2.5669578462839127e-06\n2.54256883636117e-06\n2.518296241760254e-06\n2.518296241760254e-06\n2.494140062481165e-06\n2.470100298523903e-06\n2.446176949888468e-06\n2.446176949888468e-06\n2.4223700165748596e-06\n2.3986794985830784e-06\n2.375105395913124e-06\n2.375105395913124e-06\n2.3516477085649967e-06\n2.3283064365386963e-06\n2.3283064365386963e-06\n2.305081579834223e-06\n2.2819731384515762e-06\n2.2589811123907566e-06\n2.2589811123907566e-06\n2.236105501651764e-06\n2.213346306234598e-06\n2.213346306234598e-06\n2.1907035261392593e-06\n2.1681771613657475e-06\n2.1457672119140625e-06\n2.1457672119140625e-06\n2.1234736777842045e-06\n2.1234736777842045e-06\n2.1012965589761734e-06\n2.0792358554899693e-06\n9.989016689360142e-07\n2.057291567325592e-06\n2.0354636944830418e-06\n2.0137522369623184e-06\n2.0137522369623184e-06\n1.4715478755533695e-06\n1.992157194763422e-06\n1.9706785678863525e-06\n1.94931635633111e-06\n1.94931635633111e-06\n1.9280705600976944e-06\n1.9069411791861057e-06\n1.9069411791861057e-06\n1.885928213596344e-06\n1.8650316633284092e-06\n1.8650316633284092e-06\n1.8442515283823013e-06\n1.8235878087580204e-06\n1.8235878087580204e-06\n1.8030405044555664e-06\n1.8030405044555664e-06\n1.7826096154749393e-06\n1.7622951418161392e-06\n1.742097083479166e-06\n1.742097083479166e-06\n5.034380592405796e-07\n1.7220154404640198e-06\n1.7020502127707005e-06\n1.7020502127707005e-06\n1.682201400399208e-06\n1.6624690033495426e-06\n9.313225746154785e-10\n1.6624690033495426e-06\n1.642853021621704e-06\n1.6233534552156925e-06\n1.6039703041315079e-06\n1.6039703041315079e-06\n2.852175384759903e-07\n1.5847035683691502e-06\n1.5655532479286194e-06\n1.5655532479286194e-06\n1.5655532479286194e-06\n1.5465193428099155e-06\n1.5276018530130386e-06\n1.5276018530130386e-06\n1.5088007785379887e-06\n1.5088007785379887e-06\n1.4901161193847656e-06\n1.4715478755533695e-06\n1.4715478755533695e-06\n1.4530960470438004e-06\n1.4347606338560581e-06\n1.4347606338560581e-06\n1.4165416359901428e-06\n1.4165416359901428e-06\n1.3984390534460545e-06\n1.3984390534460545e-06\n1.380452886223793e-06\n1.3625831343233585e-06\n1.3625831343233585e-06\n1.344829797744751e-06\n1.344829797744751e-06\n1.3271928764879704e-06\n1.3271928764879704e-06\n1.3096723705530167e-06\n1.3096723705530167e-06\n1.29226827993989e-06\n1.27498060464859e-06\n1.27498060464859e-06\n1.2578093446791172e-06\n1.2578093446791172e-06\n1.2407545000314713e-06\n1.2407545000314713e-06\n7.301568984985352e-07\n1.2238160707056522e-06\n1.2069940567016602e-06\n5.59026375412941e-07\n1.190288458019495e-06\n1.190288458019495e-06\n1.1736992746591568e-06\n1.1572265066206455e-06\n1.0924995876848698e-06\n1.1408701539039612e-06\n1.1408701539039612e-06\n1.1408701539039612e-06\n1.1246302165091038e-06\n1.1085066944360733e-06\n1.1085066944360733e-06\n1.0924995876848698e-06\n9.784707799553871e-08\n1.0924995876848698e-06\n1.0766088962554932e-06\n1.0766088962554932e-06\n1.0608346201479435e-06\n1.0608346201479435e-06\n1.0451767593622208e-06\n1.0451767593622208e-06\n1.029635313898325e-06\n1.014210283756256e-06\n1.014210283756256e-06\n1.014210283756256e-06\n9.989016689360142e-07\n9.989016689360142e-07\n9.837094694375992e-07\n9.837094694375992e-07\n9.686336852610111e-07\n9.686336852610111e-07\n9.5367431640625e-07\n4.3050386011600494e-07\n9.388313628733158e-07\n9.388313628733158e-07\n9.241048246622086e-07\n9.241048246622086e-07\n9.094947017729282e-07\n9.094947017729282e-07\n8.950009942054749e-07\n8.950009942054749e-07\n6.055925041437149e-07\n8.806237019598484e-07\n8.806237019598484e-07\n8.663628250360489e-07\n8.663628250360489e-07\n8.522183634340763e-07\n8.522183634340763e-07\n8.381903171539307e-07\n8.381903171539307e-07\n8.24278686195612e-07\n8.24278686195612e-07\n8.104834705591202e-07\n8.104834705591202e-07\n7.968046702444553e-07\n7.968046702444553e-07\n7.832422852516174e-07\n7.832422852516174e-07\n7.832422852516174e-07\n7.697963155806065e-07\n7.697963155806065e-07\n7.564667612314224e-07\n7.564667612314224e-07\n7.432536222040653e-07\n7.432536222040653e-07\n7.432536222040653e-07\n7.301568984985352e-07\n7.301568984985352e-07\n7.171765901148319e-07\n7.171765901148319e-07\n7.043126970529556e-07\n7.043126970529556e-07\n7.043126970529556e-07\n6.915652193129063e-07\n6.915652193129063e-07\n6.789341568946838e-07\n6.789341568946838e-07\n6.664195097982883e-07\n4.0099257603287697e-07\n6.664195097982883e-07\n6.540212780237198e-07\n6.540212780237198e-07\n6.540212780237198e-07\n6.417394615709782e-07\n6.417394615709782e-07\n6.295740604400635e-07\n6.295740604400635e-07\n6.295740604400635e-07\n6.175250746309757e-07\n6.175250746309757e-07\n6.055925041437149e-07\n6.055925041437149e-07\n5.93776348978281e-07\n5.93776348978281e-07\n5.93776348978281e-07\n5.820766091346741e-07\n5.820766091346741e-07\n5.704932846128941e-07\n5.704932846128941e-07\n5.704932846128941e-07\n5.59026375412941e-07\n5.59026375412941e-07\n5.476758815348148e-07\n5.476758815348148e-07\n5.476758815348148e-07\n5.364418029785156e-07\n5.364418029785156e-07\n4.3050386011600494e-07\n5.253241397440434e-07\n5.253241397440434e-07\n5.14322891831398e-07\n5.14322891831398e-07\n5.14322891831398e-07\n5.034380592405796e-07\n5.034380592405796e-07\n4.926696419715881e-07\n4.926696419715881e-07\n4.926696419715881e-07\n4.820176400244236e-07\n4.820176400244236e-07\n4.820176400244236e-07\n4.71482053399086e-07\n4.71482053399086e-07\n4.71482053399086e-07\n4.6106288209557533e-07\n4.6106288209557533e-07\n4.405737854540348e-07\n4.507601261138916e-07\n4.507601261138916e-07\n4.405737854540348e-07\n4.405737854540348e-07\n4.405737854540348e-07\n4.3050386011600494e-07\n4.3050386011600494e-07\n4.3050386011600494e-07\n4.20550350099802e-07\n4.20550350099802e-07\n4.20550350099802e-07\n4.10713255405426e-07\n4.10713255405426e-07\n4.10713255405426e-07\n4.0099257603287697e-07\n3.8190046325325966e-07\n4.0099257603287697e-07\n3.9138831198215485e-07\n3.9138831198215485e-07\n3.9138831198215485e-07\n3.8190046325325966e-07\n3.8190046325325966e-07\n3.8190046325325966e-07\n3.725290298461914e-07\n3.101886250078678e-07\n3.725290298461914e-07\n3.632740117609501e-07\n3.632740117609501e-07\n3.632740117609501e-07\n3.541354089975357e-07\n3.541354089975357e-07\n3.541354089975357e-07\n3.4511322155594826e-07\n3.4511322155594826e-07\n3.4511322155594826e-07\n3.3620744943618774e-07\n3.3620744943618774e-07\n3.3620744943618774e-07\n3.3620744943618774e-07\n3.2741809263825417e-07\n3.2741809263825417e-07\n3.101886250078678e-07\n3.187451511621475e-07\n6.728805601596832e-08\n3.187451511621475e-07\n3.187451511621475e-07\n3.101886250078678e-07\n3.101886250078678e-07\n3.101886250078678e-07\n2.852175384759903e-07\n2.852175384759903e-09\n2.691522240638733e-07\n3.0174851417541504e-07\n2.934248186647892e-07\n2.934248186647892e-07\n2.934248186647892e-07\n2.852175384759903e-07\n2.852175384759903e-07\n2.852175384759903e-07\n2.852175384759903e-07\n2.771266736090183e-07\n2.771266736090183e-07\n6.728805601596832e-08\n2.771266736090183e-07\n2.691522240638733e-07\n2.691522240638733e-07\n2.691522240638733e-07\n2.691522240638733e-07\n2.0954757928848267e-07\n2.612941898405552e-07\n2.612941898405552e-07\n2.612941898405552e-07\n2.53552570939064e-07\n5.593756213784218e-08\n2.53552570939064e-07\n2.459273673593998e-07\n2.459273673593998e-07\n2.459273673593998e-07\n2.459273673593998e-07\n2.384185791015625e-07\n2.384185791015625e-07\n2.384185791015625e-07\n2.384185791015625e-07\n2.3102620616555214e-07\n2.3102620616555214e-07\n2.3102620616555214e-07\n2.3102620616555214e-07\n2.2375024855136871e-07\n2.2375024855136871e-07\n1.6350531950592995e-07\n2.2375024855136871e-07\n2.1659070625901222e-07\n2.1659070625901222e-07\n2.1659070625901222e-07\n2.1659070625901222e-07\n2.0954757928848267e-07\n2.0954757928848267e-07\n1.126900315284729e-07\n2.0954757928848267e-07\n2.0954757928848267e-07\n2.0262086763978004e-07\n2.0262086763978004e-07\n2.0262086763978004e-07\n2.0262086763978004e-07\n1.9581057131290436e-07\n1.9581057131290436e-07\n1.9581057131290436e-07\n1.891166903078556e-07\n1.891166903078556e-07\n1.891166903078556e-07\n1.2316741049289703e-07\n1.825392246246338e-07\n1.825392246246338e-07\n1.825392246246338e-07\n1.825392246246338e-07\n1.825392246246338e-07\n1.760781742632389e-07\n1.760781742632389e-07\n1.760781742632389e-07\n1.760781742632389e-07\n1.6973353922367096e-07\n1.6973353922367096e-07\n1.6973353922367096e-07\n1.6973353922367096e-07\n1.6350531950592995e-07\n1.6350531950592995e-07\n1.6350531950592995e-07\n1.6350531950592995e-07\n1.6350531950592995e-07\n1.5739351511001587e-07\n1.5739351511001587e-07\n1.5739351511001587e-07\n1.5739351511001587e-07\n1.5139812603592873e-07\n1.5139812603592873e-07\n1.5139812603592873e-07\n1.5139812603592873e-07\n1.5139812603592873e-07\n1.4551915228366852e-07\n1.4551915228366852e-07\n1.4551915228366852e-07\n1.4551915228366852e-07\n1.3975659385323524e-07\n1.3975659385323524e-07\n1.3975659385323524e-07\n1.3975659385323524e-07\n1.3975659385323524e-07\n1.341104507446289e-07\n1.341104507446289e-07\n9.313225746154785e-08\n1.341104507446289e-07\n1.341104507446289e-07\n1.285807229578495e-07\n9.313225746154785e-10\n1.285807229578495e-07\n1.285807229578495e-07\n1.285807229578495e-07\n4.71482053399086e-09\n1.2316741049289703e-07\n1.2316741049289703e-07\n1.2316741049289703e-07\n1.2316741049289703e-07\n1.2316741049289703e-07\n1.178705133497715e-07\n1.178705133497715e-07\n1.178705133497715e-07\n1.178705133497715e-07\n1.178705133497715e-07\n1.126900315284729e-07\n1.126900315284729e-07\n1.126900315284729e-07\n1.126900315284729e-07\n1.126900315284729e-07\n1.126900315284729e-07\n1.0762596502900124e-07\n1.0762596502900124e-07\n1.0762596502900124e-07\n1.0762596502900124e-07\n1.0762596502900124e-07\n1.026783138513565e-07\n1.026783138513565e-07\n8.853385224938393e-08\n1.026783138513565e-07\n1.026783138513565e-07\n1.026783138513565e-07\n9.784707799553871e-08\n9.784707799553871e-08\n6.728805601596832e-08\n9.784707799553871e-08\n9.784707799553871e-08\n8.853385224938393e-08\n9.313225746154785e-08\n9.313225746154785e-08\n9.313225746154785e-08\n9.313225746154785e-08\n9.313225746154785e-08\n8.853385224938393e-08\n8.853385224938393e-08\n8.853385224938393e-08\n6.3388142734766e-08\n8.853385224938393e-08\n2.8172507882118225e-08\n8.405186235904694e-08\n8.405186235904694e-08\n8.405186235904694e-08\n8.405186235904694e-08\n8.405186235904694e-08\n8.405186235904694e-08\n7.968628779053688e-08\n7.968628779053688e-08\n7.968628779053688e-08\n7.968628779053688e-08\n7.968628779053688e-08\n7.968628779053688e-08\n7.543712854385376e-08\n7.543712854385376e-08\n7.543712854385376e-08\n7.543712854385376e-08\n7.543712854385376e-08\n7.543712854385376e-08\n3.637978807091713e-08\n7.130438461899757e-08\n7.130438461899757e-08\n7.130438461899757e-08\n7.130438461899757e-08\n7.130438461899757e-08\n7.130438461899757e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.728805601596832e-08\n6.3388142734766e-08\n6.3388142734766e-08\n6.3388142734766e-08\n6.3388142734766e-08\n1.4901161193847656e-08\n6.3388142734766e-08\n6.3388142734766e-08\n5.960464477539063e-08\n5.960464477539063e-08\n5.960464477539063e-08\n5.960464477539063e-08\n3.934837877750397e-08\n5.960464477539063e-08\n5.960464477539063e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.593756213784218e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n5.2386894822120667e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.895264282822609e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n3.934837877750397e-08\n1.885928213596344e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.563480615615845e-08\n4.243338480591774e-08\n4.243338480591774e-08\n4.243338480591774e-08\n4.243338480591774e-08\n4.243338480591774e-08\n4.243338480591774e-08\n4.243338480591774e-08\n5.820766091346741e-11\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.934837877750397e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n2.1012965589761734e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.637978807091713e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.3527612686157227e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n3.079185262322426e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.8172507882118225e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n1.4901161193847656e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.5669578462839127e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.1012965589761734e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.3283064365386963e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n2.1012965589761734e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.1408701539039612e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.885928213596344e-08\n1.1408701539039612e-08\n1.885928213596344e-08\n3.725290298461914e-09\n2.3283064365386963e-10\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n0.0\n1.682201400399208e-08\n9.313225746154785e-10\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.4901161193847656e-08\n1.682201400399208e-08\n1.682201400399208e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4551915228366852e-09\n1.4901161193847656e-08\n5.820766091346741e-11\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.4901161193847656e-08\n3.725290298461914e-09\n1.4901161193847656e-08\n1.4901161193847656e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.3096723705530167e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n9.837094694375992e-09\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n1.1408701539039612e-08\n8.381903171539307e-09\n1.1408701539039612e-08\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n4.71482053399086e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n9.837094694375992e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n8.381903171539307e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n4.71482053399086e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n7.043126970529556e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n2.0954757928848267e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n5.820766091346741e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n2.852175384759903e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n4.71482053399086e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n2.852175384759903e-09\n3.725290298461914e-09\n3.725290298461914e-09\n3.725290298461914e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.3283064365386963e-10\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.852175384759903e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n2.0954757928848267e-09\n5.820766091346741e-11\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n9.313225746154785e-10\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n5.238689482212067e-10\n5.820766091346741e-11\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n1.4551915228366852e-09\n5.238689482212067e-10\n1.4551915228366852e-09\n1.4551915228366852e-09\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n5.238689482212067e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n2.3283064365386963e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n2.3283064365386963e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n9.313225746154785e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n0.0\n5.238689482212067e-10\n5.238689482212067e-10\n5.820766091346741e-11\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.820766091346741e-11\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n0.0\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.820766091346741e-11\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n5.820766091346741e-11\n5.238689482212067e-10\n5.238689482212067e-10\n5.238689482212067e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n0.0\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n5.820766091346741e-11\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n5.820766091346741e-11\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n2.3283064365386963e-10\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n2.3283064365386963e-10\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0954757928848267e-09\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n9.313225746154785e-10\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.3283064365386963e-10\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n5.820766091346741e-11\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[58], line 44\n     42 print(loss.item())\n     43 optimizer.zero_grad()\n---&gt; 44 loss.backward()\n     45 optimizer.step()\n     47 observation = next_observation\n\nFile ~/mambaforge/lib/python3.10/site-packages/torch/_tensor.py:487, in Tensor.backward(self, gradient, retain_graph, create_graph, inputs)\n    477 if has_torch_function_unary(self):\n    478     return handle_torch_function(\n    479         Tensor.backward,\n    480         (self,),\n   (...)\n    485         inputs=inputs,\n    486     )\n--&gt; 487 torch.autograd.backward(\n    488     self, gradient, retain_graph, create_graph, inputs=inputs\n    489 )\n\nFile ~/mambaforge/lib/python3.10/site-packages/torch/autograd/__init__.py:200, in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\n    195     retain_graph = create_graph\n    197 # The reason we repeat same the comment below is that\n    198 # some Python versions print out the first line of a multi-line function\n    199 # calls in the traceback and some print out the last line\n--&gt; 200 Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n    201     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n    202     allow_unreachable=True, accumulate_grad=True)\n\nKeyboardInterrupt: \n\n\n\n\npd.Series(rewards).plot()\n\n\n\n\n\n\n\n\n\nimport flappy_bird_gymnasium\nenv = gym.make(\"FlappyBird-v0\", render_mode=\"human\", use_lidar=False)\n\n\nobs, _ = env.reset()\n\n\nn_bins = 16\nobs_low = env.observation_space.low\nobs_high = env.observation_space.high\n\n# Discretize the observation space\ndef discretize_observation(observation):\n    bins = np.linspace(obs_low, obs_high, n_bins)\n    return tuple(np.digitize(observation, bins))\n\n# Define the variables for the MultiIndex\nvariables = [\n    \"last_pipe_h_pos\",\n    \"last_top_pipe_v_pos\",\n    \"last_bottom_pipe_v_pos\",\n    \"next_pipe_h_pos\",\n    \"next_top_pipe_v_pos\",\n    \"next_bottom_pipe_v_pos\",\n    \"next_next_pipe_h_pos\",\n    \"next_next_top_pipe_v_pos\",\n    \"next_next_bottom_pipe_v_pos\",\n    \"player_v_pos\",\n    \"player_v_vel\",\n    \"player_rotation\",\n]\n\n# leave out the first three and last three variables\nvar_consider = variables[3:-3]\n\n\nindices = [range(n_bins+1) for _ in var_consider] + [range(env.action_space.n)]\n\n# Create a Q-table with a MultiIndex\nq_table_np = np.random.randn(*[len(idx) for idx in indices]) * 0.2\n\nq_table_np.shape\n\n(17, 17, 17, 17, 17, 17, 2)\n\n\n\nobs_high, obs_low\n\n(array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n array([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]))\n\n\n\nbins = np.linspace(obs_low[0], obs_high[0], n_bins)\nbins\n\narray([-1.        , -0.86666667, -0.73333333, -0.6       , -0.46666667,\n       -0.33333333, -0.2       , -0.06666667,  0.06666667,  0.2       ,\n        0.33333333,  0.46666667,  0.6       ,  0.73333333,  0.86666667,\n        1.        ])\n\n\n\ndef discretize_observation(observation):\n    bins = np.linspace(obs_low[0], obs_high[0], n_bins)\n    return tuple(np.digitize(observation, bins))\n\nobs, _ = env.reset()\n\nprint(obs[3:-3], discretize_observation(obs[3:-3]))\n\n[1. 0. 1. 1. 0. 1.] (16, 8, 16, 16, 8, 16)\n\n\n\n# Hyperparameters\nalpha = 0.05 # learning rate\n\ngamma = 0.99 # discount factor\n\n# Exploration settings\nepsilon = 1.0 # exploration rate\n\neps_mult = 0.99\n\n# Number of episodes\nn_episodes = 8000\n\n# Number of steps per episode\nn_steps = 500\n\n\n# q learning loop\nenv = gym.make(\"FlappyBird-v0\", render_mode=None, use_lidar=False)\nrewards = np.zeros(n_episodes)\n\nfor episode in range(n_episodes):\n    epsilon = epsilon * eps_mult\n    if episode % 100 == 0:\n        print(f\"Episode {episode}\")\n    observation, _ = env.reset(seed=episode)\n    observation = observation[3:-3]\n    cumulative_reward = 0\n    for step in range(n_steps):\n        # discretize the observation\n        obs = discretize_observation(observation)\n        \n        # select the action\n        if np.random.rand() &lt; epsilon:\n            action = env.action_space.sample()\n        else:\n            action = np.argmax(q_table_np[obs])\n        \n        # take the action\n        next_observation, reward, terminated, truncated, info = env.step(action)\n        next_observation = next_observation[3:-3]\n        cumulative_reward += reward\n        # discretize the next observation\n        next_obs = discretize_observation(next_observation)\n        \n        # update the q-table\n        q_table_np[obs + (action,)] += alpha * (reward + gamma * np.max(q_table_np[next_obs]) - q_table_np[obs + (action,)])\n        \n        observation = next_observation\n        # rewards update\n        rewards[episode] = cumulative_reward\n        \n        if terminated:\n            print(f\"Episode {episode} terminated after {step} steps\")\n            break\n\nEpisode 0\nEpisode 0 terminated after 49 steps\nEpisode 1 terminated after 49 steps\nEpisode 2 terminated after 49 steps\nEpisode 3 terminated after 49 steps\nEpisode 4 terminated after 49 steps\nEpisode 5 terminated after 49 steps\nEpisode 6 terminated after 49 steps\nEpisode 7 terminated after 49 steps\nEpisode 8 terminated after 49 steps\nEpisode 9 terminated after 49 steps\nEpisode 10 terminated after 49 steps\nEpisode 11 terminated after 49 steps\nEpisode 12 terminated after 49 steps\nEpisode 13 terminated after 49 steps\nEpisode 14 terminated after 49 steps\nEpisode 15 terminated after 49 steps\nEpisode 16 terminated after 49 steps\nEpisode 17 terminated after 49 steps\nEpisode 18 terminated after 49 steps\nEpisode 19 terminated after 49 steps\nEpisode 20 terminated after 49 steps\nEpisode 21 terminated after 49 steps\nEpisode 22 terminated after 49 steps\nEpisode 23 terminated after 49 steps\nEpisode 24 terminated after 49 steps\nEpisode 25 terminated after 49 steps\nEpisode 26 terminated after 49 steps\nEpisode 27 terminated after 49 steps\nEpisode 28 terminated after 49 steps\nEpisode 29 terminated after 49 steps\nEpisode 30 terminated after 49 steps\nEpisode 31 terminated after 49 steps\nEpisode 32 terminated after 49 steps\nEpisode 33 terminated after 49 steps\nEpisode 34 terminated after 49 steps\nEpisode 35 terminated after 49 steps\nEpisode 36 terminated after 49 steps\nEpisode 37 terminated after 49 steps\nEpisode 38 terminated after 49 steps\nEpisode 39 terminated after 49 steps\nEpisode 40 terminated after 49 steps\nEpisode 41 terminated after 49 steps\nEpisode 42 terminated after 49 steps\nEpisode 43 terminated after 49 steps\nEpisode 44 terminated after 49 steps\nEpisode 45 terminated after 49 steps\nEpisode 46 terminated after 49 steps\nEpisode 47 terminated after 49 steps\nEpisode 48 terminated after 49 steps\nEpisode 49 terminated after 49 steps\nEpisode 50 terminated after 49 steps\nEpisode 51 terminated after 49 steps\nEpisode 52 terminated after 49 steps\nEpisode 53 terminated after 49 steps\nEpisode 54 terminated after 49 steps\nEpisode 55 terminated after 49 steps\nEpisode 56 terminated after 49 steps\nEpisode 57 terminated after 49 steps\nEpisode 58 terminated after 49 steps\nEpisode 59 terminated after 49 steps\nEpisode 60 terminated after 49 steps\nEpisode 61 terminated after 49 steps\nEpisode 62 terminated after 49 steps\nEpisode 63 terminated after 49 steps\nEpisode 64 terminated after 49 steps\nEpisode 65 terminated after 49 steps\nEpisode 66 terminated after 49 steps\nEpisode 67 terminated after 49 steps\nEpisode 68 terminated after 49 steps\nEpisode 69 terminated after 49 steps\nEpisode 70 terminated after 49 steps\nEpisode 71 terminated after 49 steps\nEpisode 72 terminated after 49 steps\nEpisode 73 terminated after 49 steps\nEpisode 74 terminated after 49 steps\nEpisode 75 terminated after 49 steps\nEpisode 76 terminated after 49 steps\nEpisode 77 terminated after 49 steps\nEpisode 78 terminated after 49 steps\nEpisode 79 terminated after 49 steps\nEpisode 80 terminated after 49 steps\nEpisode 81 terminated after 49 steps\nEpisode 82 terminated after 49 steps\nEpisode 83 terminated after 49 steps\nEpisode 84 terminated after 49 steps\nEpisode 85 terminated after 49 steps\nEpisode 86 terminated after 49 steps\nEpisode 87 terminated after 49 steps\nEpisode 88 terminated after 49 steps\nEpisode 89 terminated after 49 steps\nEpisode 90 terminated after 49 steps\nEpisode 91 terminated after 49 steps\nEpisode 92 terminated after 49 steps\nEpisode 93 terminated after 49 steps\nEpisode 94 terminated after 49 steps\nEpisode 95 terminated after 49 steps\nEpisode 96 terminated after 49 steps\nEpisode 97 terminated after 49 steps\nEpisode 98 terminated after 49 steps\nEpisode 99 terminated after 49 steps\nEpisode 100\nEpisode 100 terminated after 49 steps\nEpisode 101 terminated after 49 steps\nEpisode 102 terminated after 49 steps\nEpisode 103 terminated after 49 steps\nEpisode 104 terminated after 49 steps\nEpisode 105 terminated after 49 steps\nEpisode 106 terminated after 49 steps\nEpisode 107 terminated after 49 steps\nEpisode 108 terminated after 49 steps\nEpisode 109 terminated after 49 steps\nEpisode 110 terminated after 49 steps\nEpisode 111 terminated after 49 steps\nEpisode 112 terminated after 49 steps\nEpisode 113 terminated after 49 steps\nEpisode 114 terminated after 49 steps\nEpisode 115 terminated after 49 steps\nEpisode 116 terminated after 49 steps\nEpisode 117 terminated after 30 steps\nEpisode 118 terminated after 49 steps\nEpisode 119 terminated after 49 steps\nEpisode 120 terminated after 49 steps\nEpisode 121 terminated after 49 steps\nEpisode 122 terminated after 49 steps\nEpisode 123 terminated after 49 steps\nEpisode 124 terminated after 49 steps\nEpisode 125 terminated after 50 steps\nEpisode 126 terminated after 49 steps\nEpisode 127 terminated after 49 steps\nEpisode 128 terminated after 63 steps\nEpisode 129 terminated after 49 steps\nEpisode 130 terminated after 49 steps\nEpisode 131 terminated after 49 steps\nEpisode 132 terminated after 49 steps\nEpisode 133 terminated after 49 steps\nEpisode 134 terminated after 49 steps\nEpisode 135 terminated after 49 steps\nEpisode 136 terminated after 49 steps\nEpisode 137 terminated after 49 steps\nEpisode 138 terminated after 49 steps\nEpisode 139 terminated after 49 steps\nEpisode 140 terminated after 49 steps\nEpisode 141 terminated after 49 steps\nEpisode 142 terminated after 49 steps\nEpisode 143 terminated after 51 steps\nEpisode 144 terminated after 49 steps\nEpisode 145 terminated after 49 steps\nEpisode 146 terminated after 49 steps\nEpisode 147 terminated after 61 steps\nEpisode 148 terminated after 49 steps\nEpisode 149 terminated after 49 steps\nEpisode 150 terminated after 49 steps\nEpisode 151 terminated after 49 steps\nEpisode 152 terminated after 49 steps\nEpisode 153 terminated after 49 steps\nEpisode 154 terminated after 49 steps\nEpisode 155 terminated after 49 steps\nEpisode 156 terminated after 49 steps\nEpisode 157 terminated after 49 steps\nEpisode 158 terminated after 49 steps\nEpisode 159 terminated after 49 steps\nEpisode 160 terminated after 49 steps\nEpisode 161 terminated after 49 steps\nEpisode 162 terminated after 49 steps\nEpisode 163 terminated after 49 steps\nEpisode 164 terminated after 57 steps\nEpisode 165 terminated after 49 steps\nEpisode 166 terminated after 49 steps\nEpisode 167 terminated after 49 steps\nEpisode 168 terminated after 49 steps\nEpisode 169 terminated after 49 steps\nEpisode 170 terminated after 49 steps\nEpisode 171 terminated after 49 steps\nEpisode 172 terminated after 49 steps\nEpisode 173 terminated after 49 steps\nEpisode 174 terminated after 49 steps\nEpisode 175 terminated after 49 steps\nEpisode 176 terminated after 49 steps\nEpisode 177 terminated after 49 steps\nEpisode 178 terminated after 32 steps\nEpisode 179 terminated after 49 steps\nEpisode 180 terminated after 49 steps\nEpisode 181 terminated after 49 steps\nEpisode 182 terminated after 49 steps\nEpisode 183 terminated after 56 steps\nEpisode 184 terminated after 49 steps\nEpisode 185 terminated after 49 steps\nEpisode 186 terminated after 49 steps\nEpisode 187 terminated after 52 steps\nEpisode 188 terminated after 49 steps\nEpisode 189 terminated after 49 steps\nEpisode 190 terminated after 55 steps\nEpisode 191 terminated after 56 steps\nEpisode 192 terminated after 49 steps\nEpisode 193 terminated after 49 steps\nEpisode 194 terminated after 49 steps\nEpisode 195 terminated after 30 steps\nEpisode 196 terminated after 49 steps\nEpisode 197 terminated after 49 steps\nEpisode 198 terminated after 49 steps\nEpisode 199 terminated after 49 steps\nEpisode 200\nEpisode 200 terminated after 32 steps\nEpisode 201 terminated after 30 steps\nEpisode 202 terminated after 49 steps\nEpisode 203 terminated after 32 steps\nEpisode 204 terminated after 49 steps\nEpisode 205 terminated after 53 steps\nEpisode 206 terminated after 49 steps\nEpisode 207 terminated after 30 steps\nEpisode 208 terminated after 49 steps\nEpisode 209 terminated after 49 steps\nEpisode 210 terminated after 30 steps\nEpisode 211 terminated after 49 steps\nEpisode 212 terminated after 49 steps\nEpisode 213 terminated after 49 steps\nEpisode 214 terminated after 52 steps\nEpisode 215 terminated after 49 steps\nEpisode 216 terminated after 30 steps\nEpisode 217 terminated after 49 steps\nEpisode 218 terminated after 49 steps\nEpisode 219 terminated after 49 steps\nEpisode 220 terminated after 49 steps\nEpisode 221 terminated after 49 steps\nEpisode 222 terminated after 49 steps\nEpisode 223 terminated after 47 steps\nEpisode 224 terminated after 49 steps\nEpisode 225 terminated after 49 steps\nEpisode 226 terminated after 49 steps\nEpisode 227 terminated after 58 steps\nEpisode 228 terminated after 49 steps\nEpisode 229 terminated after 49 steps\nEpisode 230 terminated after 49 steps\nEpisode 231 terminated after 49 steps\nEpisode 232 terminated after 49 steps\nEpisode 233 terminated after 49 steps\nEpisode 234 terminated after 49 steps\nEpisode 235 terminated after 37 steps\nEpisode 236 terminated after 52 steps\nEpisode 237 terminated after 49 steps\nEpisode 238 terminated after 56 steps\nEpisode 239 terminated after 34 steps\nEpisode 240 terminated after 30 steps\nEpisode 241 terminated after 30 steps\nEpisode 242 terminated after 49 steps\nEpisode 243 terminated after 59 steps\nEpisode 244 terminated after 49 steps\nEpisode 245 terminated after 49 steps\nEpisode 246 terminated after 30 steps\nEpisode 247 terminated after 30 steps\nEpisode 248 terminated after 49 steps\nEpisode 249 terminated after 49 steps\nEpisode 250 terminated after 36 steps\nEpisode 251 terminated after 30 steps\nEpisode 252 terminated after 53 steps\nEpisode 253 terminated after 57 steps\nEpisode 254 terminated after 49 steps\nEpisode 255 terminated after 49 steps\nEpisode 256 terminated after 54 steps\nEpisode 257 terminated after 30 steps\nEpisode 258 terminated after 52 steps\nEpisode 259 terminated after 30 steps\nEpisode 260 terminated after 49 steps\nEpisode 261 terminated after 30 steps\nEpisode 262 terminated after 56 steps\nEpisode 263 terminated after 30 steps\nEpisode 264 terminated after 51 steps\nEpisode 265 terminated after 49 steps\nEpisode 266 terminated after 56 steps\nEpisode 267 terminated after 30 steps\nEpisode 268 terminated after 51 steps\nEpisode 269 terminated after 49 steps\nEpisode 270 terminated after 30 steps\nEpisode 271 terminated after 49 steps\nEpisode 272 terminated after 49 steps\nEpisode 273 terminated after 30 steps\nEpisode 274 terminated after 30 steps\nEpisode 275 terminated after 49 steps\nEpisode 276 terminated after 49 steps\nEpisode 277 terminated after 49 steps\nEpisode 278 terminated after 53 steps\nEpisode 279 terminated after 49 steps\nEpisode 280 terminated after 30 steps\nEpisode 281 terminated after 49 steps\nEpisode 282 terminated after 49 steps\nEpisode 283 terminated after 30 steps\nEpisode 284 terminated after 59 steps\nEpisode 285 terminated after 49 steps\nEpisode 286 terminated after 30 steps\nEpisode 287 terminated after 30 steps\nEpisode 288 terminated after 49 steps\nEpisode 289 terminated after 49 steps\nEpisode 290 terminated after 49 steps\nEpisode 291 terminated after 58 steps\nEpisode 292 terminated after 51 steps\nEpisode 293 terminated after 49 steps\nEpisode 294 terminated after 36 steps\nEpisode 295 terminated after 58 steps\nEpisode 296 terminated after 49 steps\nEpisode 297 terminated after 54 steps\nEpisode 298 terminated after 56 steps\nEpisode 299 terminated after 49 steps\nEpisode 300\nEpisode 300 terminated after 49 steps\nEpisode 301 terminated after 49 steps\nEpisode 302 terminated after 49 steps\nEpisode 303 terminated after 49 steps\nEpisode 304 terminated after 30 steps\nEpisode 305 terminated after 52 steps\nEpisode 306 terminated after 30 steps\nEpisode 307 terminated after 30 steps\nEpisode 308 terminated after 49 steps\nEpisode 309 terminated after 57 steps\nEpisode 310 terminated after 30 steps\nEpisode 311 terminated after 58 steps\nEpisode 312 terminated after 53 steps\nEpisode 313 terminated after 30 steps\nEpisode 314 terminated after 52 steps\nEpisode 315 terminated after 49 steps\nEpisode 316 terminated after 30 steps\nEpisode 317 terminated after 49 steps\nEpisode 318 terminated after 30 steps\nEpisode 319 terminated after 56 steps\nEpisode 320 terminated after 30 steps\nEpisode 321 terminated after 49 steps\nEpisode 322 terminated after 49 steps\nEpisode 323 terminated after 53 steps\nEpisode 324 terminated after 30 steps\nEpisode 325 terminated after 30 steps\nEpisode 326 terminated after 49 steps\nEpisode 327 terminated after 50 steps\nEpisode 328 terminated after 30 steps\nEpisode 329 terminated after 30 steps\nEpisode 330 terminated after 49 steps\nEpisode 331 terminated after 30 steps\nEpisode 332 terminated after 30 steps\nEpisode 333 terminated after 57 steps\nEpisode 334 terminated after 49 steps\nEpisode 335 terminated after 30 steps\nEpisode 336 terminated after 30 steps\nEpisode 337 terminated after 30 steps\nEpisode 338 terminated after 30 steps\nEpisode 339 terminated after 30 steps\nEpisode 340 terminated after 30 steps\nEpisode 341 terminated after 49 steps\nEpisode 342 terminated after 30 steps\nEpisode 343 terminated after 30 steps\nEpisode 344 terminated after 63 steps\nEpisode 345 terminated after 30 steps\nEpisode 346 terminated after 30 steps\nEpisode 347 terminated after 53 steps\nEpisode 348 terminated after 30 steps\nEpisode 349 terminated after 30 steps\nEpisode 350 terminated after 30 steps\nEpisode 351 terminated after 30 steps\nEpisode 352 terminated after 30 steps\nEpisode 353 terminated after 30 steps\nEpisode 354 terminated after 49 steps\nEpisode 355 terminated after 54 steps\nEpisode 356 terminated after 49 steps\nEpisode 357 terminated after 30 steps\nEpisode 358 terminated after 30 steps\nEpisode 359 terminated after 30 steps\nEpisode 360 terminated after 49 steps\nEpisode 361 terminated after 49 steps\nEpisode 362 terminated after 30 steps\nEpisode 363 terminated after 30 steps\nEpisode 364 terminated after 30 steps\nEpisode 365 terminated after 30 steps\nEpisode 366 terminated after 49 steps\nEpisode 367 terminated after 30 steps\nEpisode 368 terminated after 49 steps\nEpisode 369 terminated after 49 steps\nEpisode 370 terminated after 57 steps\nEpisode 371 terminated after 30 steps\nEpisode 372 terminated after 30 steps\nEpisode 373 terminated after 30 steps\nEpisode 374 terminated after 49 steps\nEpisode 375 terminated after 30 steps\nEpisode 376 terminated after 49 steps\nEpisode 377 terminated after 30 steps\nEpisode 378 terminated after 53 steps\nEpisode 379 terminated after 30 steps\nEpisode 380 terminated after 30 steps\nEpisode 381 terminated after 30 steps\nEpisode 382 terminated after 30 steps\nEpisode 383 terminated after 30 steps\nEpisode 384 terminated after 30 steps\nEpisode 385 terminated after 30 steps\nEpisode 386 terminated after 49 steps\nEpisode 387 terminated after 30 steps\nEpisode 388 terminated after 30 steps\nEpisode 389 terminated after 30 steps\nEpisode 390 terminated after 63 steps\nEpisode 391 terminated after 30 steps\nEpisode 392 terminated after 49 steps\nEpisode 393 terminated after 30 steps\nEpisode 394 terminated after 30 steps\nEpisode 395 terminated after 30 steps\nEpisode 396 terminated after 30 steps\nEpisode 397 terminated after 30 steps\nEpisode 398 terminated after 30 steps\nEpisode 399 terminated after 85 steps\nEpisode 400\nEpisode 400 terminated after 49 steps\nEpisode 401 terminated after 30 steps\nEpisode 402 terminated after 51 steps\nEpisode 403 terminated after 49 steps\nEpisode 404 terminated after 30 steps\nEpisode 405 terminated after 30 steps\nEpisode 406 terminated after 49 steps\nEpisode 407 terminated after 30 steps\nEpisode 408 terminated after 85 steps\nEpisode 409 terminated after 30 steps\nEpisode 410 terminated after 55 steps\nEpisode 411 terminated after 30 steps\nEpisode 412 terminated after 30 steps\nEpisode 413 terminated after 30 steps\nEpisode 414 terminated after 50 steps\nEpisode 415 terminated after 30 steps\nEpisode 416 terminated after 30 steps\nEpisode 417 terminated after 30 steps\nEpisode 418 terminated after 51 steps\nEpisode 419 terminated after 30 steps\nEpisode 420 terminated after 30 steps\nEpisode 421 terminated after 30 steps\nEpisode 422 terminated after 30 steps\nEpisode 423 terminated after 30 steps\nEpisode 424 terminated after 30 steps\nEpisode 425 terminated after 30 steps\nEpisode 426 terminated after 49 steps\nEpisode 427 terminated after 30 steps\nEpisode 428 terminated after 30 steps\nEpisode 429 terminated after 30 steps\nEpisode 430 terminated after 30 steps\nEpisode 431 terminated after 30 steps\nEpisode 432 terminated after 49 steps\nEpisode 433 terminated after 30 steps\nEpisode 434 terminated after 30 steps\nEpisode 435 terminated after 30 steps\nEpisode 436 terminated after 30 steps\nEpisode 437 terminated after 30 steps\nEpisode 438 terminated after 30 steps\nEpisode 439 terminated after 30 steps\nEpisode 440 terminated after 30 steps\nEpisode 441 terminated after 30 steps\nEpisode 442 terminated after 30 steps\nEpisode 443 terminated after 30 steps\nEpisode 444 terminated after 30 steps\nEpisode 445 terminated after 30 steps\nEpisode 446 terminated after 30 steps\nEpisode 447 terminated after 30 steps\nEpisode 448 terminated after 30 steps\nEpisode 449 terminated after 30 steps\nEpisode 450 terminated after 30 steps\nEpisode 451 terminated after 50 steps\nEpisode 452 terminated after 49 steps\nEpisode 453 terminated after 30 steps\nEpisode 454 terminated after 30 steps\nEpisode 455 terminated after 30 steps\nEpisode 456 terminated after 30 steps\nEpisode 457 terminated after 30 steps\nEpisode 458 terminated after 54 steps\nEpisode 459 terminated after 30 steps\nEpisode 460 terminated after 30 steps\nEpisode 461 terminated after 30 steps\nEpisode 462 terminated after 30 steps\nEpisode 463 terminated after 30 steps\nEpisode 464 terminated after 30 steps\nEpisode 465 terminated after 30 steps\nEpisode 466 terminated after 30 steps\nEpisode 467 terminated after 63 steps\nEpisode 468 terminated after 30 steps\nEpisode 469 terminated after 30 steps\nEpisode 470 terminated after 30 steps\nEpisode 471 terminated after 30 steps\nEpisode 472 terminated after 30 steps\nEpisode 473 terminated after 30 steps\nEpisode 474 terminated after 30 steps\nEpisode 475 terminated after 30 steps\nEpisode 476 terminated after 30 steps\nEpisode 477 terminated after 30 steps\nEpisode 478 terminated after 30 steps\nEpisode 479 terminated after 30 steps\nEpisode 480 terminated after 30 steps\nEpisode 481 terminated after 30 steps\nEpisode 482 terminated after 30 steps\nEpisode 483 terminated after 30 steps\nEpisode 484 terminated after 30 steps\nEpisode 485 terminated after 30 steps\nEpisode 486 terminated after 30 steps\nEpisode 487 terminated after 30 steps\nEpisode 488 terminated after 30 steps\nEpisode 489 terminated after 30 steps\nEpisode 490 terminated after 30 steps\nEpisode 491 terminated after 30 steps\nEpisode 492 terminated after 49 steps\nEpisode 493 terminated after 30 steps\nEpisode 494 terminated after 30 steps\nEpisode 495 terminated after 30 steps\nEpisode 496 terminated after 30 steps\nEpisode 497 terminated after 30 steps\nEpisode 498 terminated after 30 steps\nEpisode 499 terminated after 30 steps\nEpisode 500\nEpisode 500 terminated after 30 steps\nEpisode 501 terminated after 30 steps\nEpisode 502 terminated after 30 steps\nEpisode 503 terminated after 30 steps\nEpisode 504 terminated after 30 steps\nEpisode 505 terminated after 30 steps\nEpisode 506 terminated after 30 steps\nEpisode 507 terminated after 30 steps\nEpisode 508 terminated after 30 steps\nEpisode 509 terminated after 30 steps\nEpisode 510 terminated after 30 steps\nEpisode 511 terminated after 30 steps\nEpisode 512 terminated after 30 steps\nEpisode 513 terminated after 30 steps\nEpisode 514 terminated after 30 steps\nEpisode 515 terminated after 30 steps\nEpisode 516 terminated after 30 steps\nEpisode 517 terminated after 30 steps\nEpisode 518 terminated after 30 steps\nEpisode 519 terminated after 30 steps\nEpisode 520 terminated after 30 steps\nEpisode 521 terminated after 30 steps\nEpisode 522 terminated after 30 steps\nEpisode 523 terminated after 30 steps\nEpisode 524 terminated after 30 steps\nEpisode 525 terminated after 30 steps\nEpisode 526 terminated after 30 steps\nEpisode 527 terminated after 30 steps\nEpisode 528 terminated after 30 steps\nEpisode 529 terminated after 30 steps\nEpisode 530 terminated after 30 steps\nEpisode 531 terminated after 30 steps\nEpisode 532 terminated after 30 steps\nEpisode 533 terminated after 30 steps\nEpisode 534 terminated after 30 steps\nEpisode 535 terminated after 30 steps\nEpisode 536 terminated after 30 steps\nEpisode 537 terminated after 30 steps\nEpisode 538 terminated after 30 steps\nEpisode 539 terminated after 30 steps\nEpisode 540 terminated after 30 steps\nEpisode 541 terminated after 30 steps\nEpisode 542 terminated after 30 steps\nEpisode 543 terminated after 30 steps\nEpisode 544 terminated after 30 steps\nEpisode 545 terminated after 30 steps\nEpisode 546 terminated after 85 steps\nEpisode 547 terminated after 30 steps\nEpisode 548 terminated after 30 steps\nEpisode 549 terminated after 30 steps\nEpisode 550 terminated after 49 steps\nEpisode 551 terminated after 30 steps\nEpisode 552 terminated after 30 steps\nEpisode 553 terminated after 30 steps\nEpisode 554 terminated after 30 steps\nEpisode 555 terminated after 49 steps\nEpisode 556 terminated after 30 steps\nEpisode 557 terminated after 30 steps\nEpisode 558 terminated after 30 steps\nEpisode 559 terminated after 30 steps\nEpisode 560 terminated after 30 steps\nEpisode 561 terminated after 30 steps\nEpisode 562 terminated after 30 steps\nEpisode 563 terminated after 30 steps\nEpisode 564 terminated after 49 steps\nEpisode 565 terminated after 30 steps\nEpisode 566 terminated after 30 steps\nEpisode 567 terminated after 30 steps\nEpisode 568 terminated after 30 steps\nEpisode 569 terminated after 30 steps\nEpisode 570 terminated after 30 steps\nEpisode 571 terminated after 30 steps\nEpisode 572 terminated after 30 steps\nEpisode 573 terminated after 30 steps\nEpisode 574 terminated after 30 steps\nEpisode 575 terminated after 30 steps\nEpisode 576 terminated after 30 steps\nEpisode 577 terminated after 30 steps\nEpisode 578 terminated after 50 steps\nEpisode 579 terminated after 30 steps\nEpisode 580 terminated after 30 steps\nEpisode 581 terminated after 30 steps\nEpisode 582 terminated after 30 steps\nEpisode 583 terminated after 30 steps\nEpisode 584 terminated after 30 steps\nEpisode 585 terminated after 30 steps\nEpisode 586 terminated after 30 steps\nEpisode 587 terminated after 30 steps\nEpisode 588 terminated after 30 steps\nEpisode 589 terminated after 30 steps\nEpisode 590 terminated after 30 steps\nEpisode 591 terminated after 30 steps\nEpisode 592 terminated after 30 steps\nEpisode 593 terminated after 30 steps\nEpisode 594 terminated after 30 steps\nEpisode 595 terminated after 30 steps\nEpisode 596 terminated after 30 steps\nEpisode 597 terminated after 30 steps\nEpisode 598 terminated after 30 steps\nEpisode 599 terminated after 30 steps\nEpisode 600\nEpisode 600 terminated after 30 steps\nEpisode 601 terminated after 30 steps\nEpisode 602 terminated after 30 steps\nEpisode 603 terminated after 30 steps\nEpisode 604 terminated after 58 steps\nEpisode 605 terminated after 30 steps\nEpisode 606 terminated after 30 steps\nEpisode 607 terminated after 30 steps\nEpisode 608 terminated after 30 steps\nEpisode 609 terminated after 30 steps\nEpisode 610 terminated after 30 steps\nEpisode 611 terminated after 30 steps\nEpisode 612 terminated after 30 steps\nEpisode 613 terminated after 85 steps\nEpisode 614 terminated after 30 steps\nEpisode 615 terminated after 30 steps\nEpisode 616 terminated after 30 steps\nEpisode 617 terminated after 30 steps\nEpisode 618 terminated after 30 steps\nEpisode 619 terminated after 30 steps\nEpisode 620 terminated after 30 steps\nEpisode 621 terminated after 30 steps\nEpisode 622 terminated after 30 steps\nEpisode 623 terminated after 30 steps\nEpisode 624 terminated after 30 steps\nEpisode 625 terminated after 30 steps\nEpisode 626 terminated after 30 steps\nEpisode 627 terminated after 30 steps\nEpisode 628 terminated after 30 steps\nEpisode 629 terminated after 30 steps\nEpisode 630 terminated after 30 steps\nEpisode 631 terminated after 30 steps\nEpisode 632 terminated after 30 steps\nEpisode 633 terminated after 30 steps\nEpisode 634 terminated after 30 steps\nEpisode 635 terminated after 30 steps\nEpisode 636 terminated after 30 steps\nEpisode 637 terminated after 30 steps\nEpisode 638 terminated after 30 steps\nEpisode 639 terminated after 30 steps\nEpisode 640 terminated after 30 steps\nEpisode 641 terminated after 30 steps\nEpisode 642 terminated after 30 steps\nEpisode 643 terminated after 30 steps\nEpisode 644 terminated after 30 steps\nEpisode 645 terminated after 30 steps\nEpisode 646 terminated after 30 steps\nEpisode 647 terminated after 30 steps\nEpisode 648 terminated after 30 steps\nEpisode 649 terminated after 30 steps\nEpisode 650 terminated after 30 steps\nEpisode 651 terminated after 30 steps\nEpisode 652 terminated after 30 steps\nEpisode 653 terminated after 30 steps\nEpisode 654 terminated after 30 steps\nEpisode 655 terminated after 30 steps\nEpisode 656 terminated after 30 steps\nEpisode 657 terminated after 30 steps\nEpisode 658 terminated after 30 steps\nEpisode 659 terminated after 30 steps\nEpisode 660 terminated after 30 steps\nEpisode 661 terminated after 30 steps\nEpisode 662 terminated after 30 steps\nEpisode 663 terminated after 30 steps\nEpisode 664 terminated after 30 steps\nEpisode 665 terminated after 52 steps\nEpisode 666 terminated after 30 steps\nEpisode 667 terminated after 30 steps\nEpisode 668 terminated after 57 steps\nEpisode 669 terminated after 30 steps\nEpisode 670 terminated after 30 steps\nEpisode 671 terminated after 30 steps\nEpisode 672 terminated after 30 steps\nEpisode 673 terminated after 30 steps\nEpisode 674 terminated after 30 steps\nEpisode 675 terminated after 30 steps\nEpisode 676 terminated after 30 steps\nEpisode 677 terminated after 30 steps\nEpisode 678 terminated after 30 steps\nEpisode 679 terminated after 30 steps\nEpisode 680 terminated after 30 steps\nEpisode 681 terminated after 30 steps\nEpisode 682 terminated after 30 steps\nEpisode 683 terminated after 30 steps\nEpisode 684 terminated after 30 steps\nEpisode 685 terminated after 30 steps\nEpisode 686 terminated after 30 steps\nEpisode 687 terminated after 30 steps\nEpisode 688 terminated after 30 steps\nEpisode 689 terminated after 30 steps\nEpisode 690 terminated after 30 steps\nEpisode 691 terminated after 30 steps\nEpisode 692 terminated after 30 steps\nEpisode 693 terminated after 30 steps\nEpisode 694 terminated after 30 steps\nEpisode 695 terminated after 30 steps\nEpisode 696 terminated after 30 steps\nEpisode 697 terminated after 30 steps\nEpisode 698 terminated after 30 steps\nEpisode 699 terminated after 30 steps\nEpisode 700\nEpisode 700 terminated after 30 steps\nEpisode 701 terminated after 30 steps\nEpisode 702 terminated after 30 steps\nEpisode 703 terminated after 30 steps\nEpisode 704 terminated after 30 steps\nEpisode 705 terminated after 30 steps\nEpisode 706 terminated after 30 steps\nEpisode 707 terminated after 30 steps\nEpisode 708 terminated after 30 steps\nEpisode 709 terminated after 30 steps\nEpisode 710 terminated after 30 steps\nEpisode 711 terminated after 30 steps\nEpisode 712 terminated after 30 steps\nEpisode 713 terminated after 30 steps\nEpisode 714 terminated after 30 steps\nEpisode 715 terminated after 30 steps\nEpisode 716 terminated after 30 steps\nEpisode 717 terminated after 30 steps\nEpisode 718 terminated after 30 steps\nEpisode 719 terminated after 30 steps\nEpisode 720 terminated after 30 steps\nEpisode 721 terminated after 30 steps\nEpisode 722 terminated after 30 steps\nEpisode 723 terminated after 30 steps\nEpisode 724 terminated after 30 steps\nEpisode 725 terminated after 30 steps\nEpisode 726 terminated after 30 steps\nEpisode 727 terminated after 30 steps\nEpisode 728 terminated after 30 steps\nEpisode 729 terminated after 30 steps\nEpisode 730 terminated after 30 steps\nEpisode 731 terminated after 30 steps\nEpisode 732 terminated after 30 steps\nEpisode 733 terminated after 30 steps\nEpisode 734 terminated after 30 steps\nEpisode 735 terminated after 30 steps\nEpisode 736 terminated after 30 steps\nEpisode 737 terminated after 30 steps\nEpisode 738 terminated after 30 steps\nEpisode 739 terminated after 30 steps\nEpisode 740 terminated after 30 steps\nEpisode 741 terminated after 30 steps\nEpisode 742 terminated after 30 steps\nEpisode 743 terminated after 30 steps\nEpisode 744 terminated after 30 steps\nEpisode 745 terminated after 30 steps\nEpisode 746 terminated after 30 steps\nEpisode 747 terminated after 30 steps\nEpisode 748 terminated after 30 steps\nEpisode 749 terminated after 30 steps\nEpisode 750 terminated after 30 steps\nEpisode 751 terminated after 30 steps\nEpisode 752 terminated after 30 steps\nEpisode 753 terminated after 30 steps\nEpisode 754 terminated after 30 steps\nEpisode 755 terminated after 30 steps\nEpisode 756 terminated after 30 steps\nEpisode 757 terminated after 30 steps\nEpisode 758 terminated after 30 steps\nEpisode 759 terminated after 30 steps\nEpisode 760 terminated after 30 steps\nEpisode 761 terminated after 30 steps\nEpisode 762 terminated after 30 steps\nEpisode 763 terminated after 30 steps\nEpisode 764 terminated after 30 steps\nEpisode 765 terminated after 30 steps\nEpisode 766 terminated after 30 steps\nEpisode 767 terminated after 30 steps\nEpisode 768 terminated after 30 steps\nEpisode 769 terminated after 30 steps\nEpisode 770 terminated after 30 steps\nEpisode 771 terminated after 30 steps\nEpisode 772 terminated after 30 steps\nEpisode 773 terminated after 30 steps\nEpisode 774 terminated after 30 steps\nEpisode 775 terminated after 30 steps\nEpisode 776 terminated after 30 steps\nEpisode 777 terminated after 30 steps\nEpisode 778 terminated after 30 steps\nEpisode 779 terminated after 30 steps\nEpisode 780 terminated after 30 steps\nEpisode 781 terminated after 30 steps\nEpisode 782 terminated after 30 steps\nEpisode 783 terminated after 30 steps\nEpisode 784 terminated after 30 steps\nEpisode 785 terminated after 30 steps\nEpisode 786 terminated after 30 steps\nEpisode 787 terminated after 30 steps\nEpisode 788 terminated after 30 steps\nEpisode 789 terminated after 30 steps\nEpisode 790 terminated after 30 steps\nEpisode 791 terminated after 30 steps\nEpisode 792 terminated after 30 steps\nEpisode 793 terminated after 30 steps\nEpisode 794 terminated after 30 steps\nEpisode 795 terminated after 30 steps\nEpisode 796 terminated after 30 steps\nEpisode 797 terminated after 30 steps\nEpisode 798 terminated after 30 steps\nEpisode 799 terminated after 30 steps\nEpisode 800\nEpisode 800 terminated after 30 steps\nEpisode 801 terminated after 30 steps\nEpisode 802 terminated after 30 steps\nEpisode 803 terminated after 30 steps\nEpisode 804 terminated after 30 steps\nEpisode 805 terminated after 30 steps\nEpisode 806 terminated after 30 steps\nEpisode 807 terminated after 30 steps\nEpisode 808 terminated after 30 steps\nEpisode 809 terminated after 30 steps\nEpisode 810 terminated after 30 steps\nEpisode 811 terminated after 30 steps\nEpisode 812 terminated after 30 steps\nEpisode 813 terminated after 30 steps\nEpisode 814 terminated after 30 steps\nEpisode 815 terminated after 49 steps\nEpisode 816 terminated after 30 steps\nEpisode 817 terminated after 30 steps\nEpisode 818 terminated after 30 steps\nEpisode 819 terminated after 30 steps\nEpisode 820 terminated after 30 steps\nEpisode 821 terminated after 30 steps\nEpisode 822 terminated after 30 steps\nEpisode 823 terminated after 30 steps\nEpisode 824 terminated after 30 steps\nEpisode 825 terminated after 30 steps\nEpisode 826 terminated after 30 steps\nEpisode 827 terminated after 30 steps\nEpisode 828 terminated after 30 steps\nEpisode 829 terminated after 30 steps\nEpisode 830 terminated after 30 steps\nEpisode 831 terminated after 30 steps\nEpisode 832 terminated after 30 steps\nEpisode 833 terminated after 30 steps\nEpisode 834 terminated after 30 steps\nEpisode 835 terminated after 30 steps\nEpisode 836 terminated after 30 steps\nEpisode 837 terminated after 30 steps\nEpisode 838 terminated after 30 steps\nEpisode 839 terminated after 30 steps\nEpisode 840 terminated after 30 steps\nEpisode 841 terminated after 30 steps\nEpisode 842 terminated after 30 steps\nEpisode 843 terminated after 30 steps\nEpisode 844 terminated after 30 steps\nEpisode 845 terminated after 30 steps\nEpisode 846 terminated after 30 steps\nEpisode 847 terminated after 30 steps\nEpisode 848 terminated after 30 steps\nEpisode 849 terminated after 30 steps\nEpisode 850 terminated after 30 steps\nEpisode 851 terminated after 30 steps\nEpisode 852 terminated after 30 steps\nEpisode 853 terminated after 30 steps\nEpisode 854 terminated after 30 steps\nEpisode 855 terminated after 30 steps\nEpisode 856 terminated after 30 steps\nEpisode 857 terminated after 30 steps\nEpisode 858 terminated after 30 steps\nEpisode 859 terminated after 30 steps\nEpisode 860 terminated after 30 steps\nEpisode 861 terminated after 30 steps\nEpisode 862 terminated after 30 steps\nEpisode 863 terminated after 30 steps\nEpisode 864 terminated after 30 steps\nEpisode 865 terminated after 30 steps\nEpisode 866 terminated after 30 steps\nEpisode 867 terminated after 30 steps\nEpisode 868 terminated after 30 steps\nEpisode 869 terminated after 30 steps\nEpisode 870 terminated after 30 steps\nEpisode 871 terminated after 30 steps\nEpisode 872 terminated after 30 steps\nEpisode 873 terminated after 30 steps\nEpisode 874 terminated after 30 steps\nEpisode 875 terminated after 30 steps\nEpisode 876 terminated after 30 steps\nEpisode 877 terminated after 30 steps\nEpisode 878 terminated after 30 steps\nEpisode 879 terminated after 30 steps\nEpisode 880 terminated after 30 steps\nEpisode 881 terminated after 30 steps\nEpisode 882 terminated after 30 steps\nEpisode 883 terminated after 30 steps\nEpisode 884 terminated after 30 steps\nEpisode 885 terminated after 30 steps\nEpisode 886 terminated after 30 steps\nEpisode 887 terminated after 30 steps\nEpisode 888 terminated after 30 steps\nEpisode 889 terminated after 30 steps\nEpisode 890 terminated after 30 steps\nEpisode 891 terminated after 30 steps\nEpisode 892 terminated after 30 steps\nEpisode 893 terminated after 30 steps\nEpisode 894 terminated after 30 steps\nEpisode 895 terminated after 30 steps\nEpisode 896 terminated after 30 steps\nEpisode 897 terminated after 30 steps\nEpisode 898 terminated after 30 steps\nEpisode 899 terminated after 30 steps\nEpisode 900\nEpisode 900 terminated after 30 steps\nEpisode 901 terminated after 30 steps\nEpisode 902 terminated after 30 steps\nEpisode 903 terminated after 30 steps\nEpisode 904 terminated after 30 steps\nEpisode 905 terminated after 30 steps\nEpisode 906 terminated after 30 steps\nEpisode 907 terminated after 30 steps\nEpisode 908 terminated after 30 steps\nEpisode 909 terminated after 30 steps\nEpisode 910 terminated after 30 steps\nEpisode 911 terminated after 30 steps\nEpisode 912 terminated after 30 steps\nEpisode 913 terminated after 30 steps\nEpisode 914 terminated after 30 steps\nEpisode 915 terminated after 30 steps\nEpisode 916 terminated after 30 steps\nEpisode 917 terminated after 30 steps\nEpisode 918 terminated after 30 steps\nEpisode 919 terminated after 30 steps\nEpisode 920 terminated after 30 steps\nEpisode 921 terminated after 30 steps\nEpisode 922 terminated after 30 steps\nEpisode 923 terminated after 30 steps\nEpisode 924 terminated after 30 steps\nEpisode 925 terminated after 30 steps\nEpisode 926 terminated after 30 steps\nEpisode 927 terminated after 30 steps\nEpisode 928 terminated after 30 steps\nEpisode 929 terminated after 30 steps\nEpisode 930 terminated after 30 steps\nEpisode 931 terminated after 30 steps\nEpisode 932 terminated after 30 steps\nEpisode 933 terminated after 30 steps\nEpisode 934 terminated after 30 steps\nEpisode 935 terminated after 30 steps\nEpisode 936 terminated after 30 steps\nEpisode 937 terminated after 30 steps\nEpisode 938 terminated after 30 steps\nEpisode 939 terminated after 30 steps\nEpisode 940 terminated after 30 steps\nEpisode 941 terminated after 30 steps\nEpisode 942 terminated after 30 steps\nEpisode 943 terminated after 30 steps\nEpisode 944 terminated after 30 steps\nEpisode 945 terminated after 30 steps\nEpisode 946 terminated after 30 steps\nEpisode 947 terminated after 30 steps\nEpisode 948 terminated after 30 steps\nEpisode 949 terminated after 30 steps\nEpisode 950 terminated after 30 steps\nEpisode 951 terminated after 30 steps\nEpisode 952 terminated after 30 steps\nEpisode 953 terminated after 30 steps\nEpisode 954 terminated after 30 steps\nEpisode 955 terminated after 30 steps\nEpisode 956 terminated after 30 steps\nEpisode 957 terminated after 30 steps\nEpisode 958 terminated after 30 steps\nEpisode 959 terminated after 30 steps\nEpisode 960 terminated after 30 steps\nEpisode 961 terminated after 30 steps\nEpisode 962 terminated after 30 steps\nEpisode 963 terminated after 30 steps\nEpisode 964 terminated after 30 steps\nEpisode 965 terminated after 30 steps\nEpisode 966 terminated after 30 steps\nEpisode 967 terminated after 30 steps\nEpisode 968 terminated after 30 steps\nEpisode 969 terminated after 30 steps\nEpisode 970 terminated after 30 steps\nEpisode 971 terminated after 30 steps\nEpisode 972 terminated after 30 steps\nEpisode 973 terminated after 30 steps\nEpisode 974 terminated after 30 steps\nEpisode 975 terminated after 30 steps\nEpisode 976 terminated after 30 steps\nEpisode 977 terminated after 30 steps\nEpisode 978 terminated after 30 steps\nEpisode 979 terminated after 30 steps\nEpisode 980 terminated after 30 steps\nEpisode 981 terminated after 30 steps\nEpisode 982 terminated after 30 steps\nEpisode 983 terminated after 30 steps\nEpisode 984 terminated after 30 steps\nEpisode 985 terminated after 30 steps\nEpisode 986 terminated after 30 steps\nEpisode 987 terminated after 30 steps\nEpisode 988 terminated after 30 steps\nEpisode 989 terminated after 30 steps\nEpisode 990 terminated after 30 steps\nEpisode 991 terminated after 30 steps\nEpisode 992 terminated after 30 steps\nEpisode 993 terminated after 30 steps\nEpisode 994 terminated after 30 steps\nEpisode 995 terminated after 30 steps\nEpisode 996 terminated after 30 steps\nEpisode 997 terminated after 30 steps\nEpisode 998 terminated after 30 steps\nEpisode 999 terminated after 30 steps\nEpisode 1000\nEpisode 1000 terminated after 30 steps\nEpisode 1001 terminated after 30 steps\nEpisode 1002 terminated after 30 steps\nEpisode 1003 terminated after 30 steps\nEpisode 1004 terminated after 30 steps\nEpisode 1005 terminated after 30 steps\nEpisode 1006 terminated after 30 steps\nEpisode 1007 terminated after 30 steps\nEpisode 1008 terminated after 30 steps\nEpisode 1009 terminated after 30 steps\nEpisode 1010 terminated after 30 steps\nEpisode 1011 terminated after 30 steps\nEpisode 1012 terminated after 30 steps\nEpisode 1013 terminated after 30 steps\nEpisode 1014 terminated after 30 steps\nEpisode 1015 terminated after 30 steps\nEpisode 1016 terminated after 30 steps\nEpisode 1017 terminated after 30 steps\nEpisode 1018 terminated after 30 steps\nEpisode 1019 terminated after 30 steps\nEpisode 1020 terminated after 30 steps\nEpisode 1021 terminated after 30 steps\nEpisode 1022 terminated after 30 steps\nEpisode 1023 terminated after 30 steps\nEpisode 1024 terminated after 30 steps\nEpisode 1025 terminated after 30 steps\nEpisode 1026 terminated after 30 steps\nEpisode 1027 terminated after 30 steps\nEpisode 1028 terminated after 30 steps\nEpisode 1029 terminated after 30 steps\nEpisode 1030 terminated after 30 steps\nEpisode 1031 terminated after 30 steps\nEpisode 1032 terminated after 30 steps\nEpisode 1033 terminated after 30 steps\nEpisode 1034 terminated after 30 steps\nEpisode 1035 terminated after 30 steps\nEpisode 1036 terminated after 30 steps\nEpisode 1037 terminated after 30 steps\nEpisode 1038 terminated after 30 steps\nEpisode 1039 terminated after 30 steps\nEpisode 1040 terminated after 30 steps\nEpisode 1041 terminated after 30 steps\nEpisode 1042 terminated after 30 steps\nEpisode 1043 terminated after 30 steps\nEpisode 1044 terminated after 30 steps\nEpisode 1045 terminated after 30 steps\nEpisode 1046 terminated after 30 steps\nEpisode 1047 terminated after 30 steps\nEpisode 1048 terminated after 30 steps\nEpisode 1049 terminated after 30 steps\nEpisode 1050 terminated after 30 steps\nEpisode 1051 terminated after 30 steps\nEpisode 1052 terminated after 30 steps\nEpisode 1053 terminated after 30 steps\nEpisode 1054 terminated after 30 steps\nEpisode 1055 terminated after 30 steps\nEpisode 1056 terminated after 30 steps\nEpisode 1057 terminated after 30 steps\nEpisode 1058 terminated after 30 steps\nEpisode 1059 terminated after 30 steps\nEpisode 1060 terminated after 30 steps\nEpisode 1061 terminated after 30 steps\nEpisode 1062 terminated after 30 steps\nEpisode 1063 terminated after 30 steps\nEpisode 1064 terminated after 30 steps\nEpisode 1065 terminated after 30 steps\nEpisode 1066 terminated after 30 steps\nEpisode 1067 terminated after 30 steps\nEpisode 1068 terminated after 30 steps\nEpisode 1069 terminated after 30 steps\nEpisode 1070 terminated after 30 steps\nEpisode 1071 terminated after 30 steps\nEpisode 1072 terminated after 30 steps\nEpisode 1073 terminated after 30 steps\nEpisode 1074 terminated after 30 steps\nEpisode 1075 terminated after 30 steps\nEpisode 1076 terminated after 30 steps\nEpisode 1077 terminated after 30 steps\nEpisode 1078 terminated after 30 steps\nEpisode 1079 terminated after 30 steps\nEpisode 1080 terminated after 30 steps\nEpisode 1081 terminated after 30 steps\nEpisode 1082 terminated after 30 steps\nEpisode 1083 terminated after 30 steps\nEpisode 1084 terminated after 30 steps\nEpisode 1085 terminated after 30 steps\nEpisode 1086 terminated after 30 steps\nEpisode 1087 terminated after 30 steps\nEpisode 1088 terminated after 30 steps\nEpisode 1089 terminated after 30 steps\nEpisode 1090 terminated after 30 steps\nEpisode 1091 terminated after 30 steps\nEpisode 1092 terminated after 30 steps\nEpisode 1093 terminated after 30 steps\nEpisode 1094 terminated after 30 steps\nEpisode 1095 terminated after 30 steps\nEpisode 1096 terminated after 30 steps\nEpisode 1097 terminated after 30 steps\nEpisode 1098 terminated after 30 steps\nEpisode 1099 terminated after 30 steps\nEpisode 1100\nEpisode 1100 terminated after 30 steps\nEpisode 1101 terminated after 30 steps\nEpisode 1102 terminated after 30 steps\nEpisode 1103 terminated after 30 steps\nEpisode 1104 terminated after 30 steps\nEpisode 1105 terminated after 30 steps\nEpisode 1106 terminated after 30 steps\nEpisode 1107 terminated after 30 steps\nEpisode 1108 terminated after 30 steps\nEpisode 1109 terminated after 30 steps\nEpisode 1110 terminated after 30 steps\nEpisode 1111 terminated after 30 steps\nEpisode 1112 terminated after 30 steps\nEpisode 1113 terminated after 30 steps\nEpisode 1114 terminated after 30 steps\nEpisode 1115 terminated after 30 steps\nEpisode 1116 terminated after 30 steps\nEpisode 1117 terminated after 30 steps\nEpisode 1118 terminated after 30 steps\nEpisode 1119 terminated after 30 steps\nEpisode 1120 terminated after 30 steps\nEpisode 1121 terminated after 30 steps\nEpisode 1122 terminated after 30 steps\nEpisode 1123 terminated after 30 steps\nEpisode 1124 terminated after 30 steps\nEpisode 1125 terminated after 30 steps\nEpisode 1126 terminated after 30 steps\nEpisode 1127 terminated after 30 steps\nEpisode 1128 terminated after 30 steps\nEpisode 1129 terminated after 30 steps\nEpisode 1130 terminated after 30 steps\nEpisode 1131 terminated after 30 steps\nEpisode 1132 terminated after 30 steps\nEpisode 1133 terminated after 30 steps\nEpisode 1134 terminated after 30 steps\nEpisode 1135 terminated after 30 steps\nEpisode 1136 terminated after 30 steps\nEpisode 1137 terminated after 30 steps\nEpisode 1138 terminated after 30 steps\nEpisode 1139 terminated after 30 steps\nEpisode 1140 terminated after 30 steps\nEpisode 1141 terminated after 30 steps\nEpisode 1142 terminated after 30 steps\nEpisode 1143 terminated after 30 steps\nEpisode 1144 terminated after 30 steps\nEpisode 1145 terminated after 30 steps\nEpisode 1146 terminated after 30 steps\nEpisode 1147 terminated after 30 steps\nEpisode 1148 terminated after 30 steps\nEpisode 1149 terminated after 30 steps\nEpisode 1150 terminated after 30 steps\nEpisode 1151 terminated after 30 steps\nEpisode 1152 terminated after 30 steps\nEpisode 1153 terminated after 30 steps\nEpisode 1154 terminated after 30 steps\nEpisode 1155 terminated after 30 steps\nEpisode 1156 terminated after 30 steps\nEpisode 1157 terminated after 30 steps\nEpisode 1158 terminated after 30 steps\nEpisode 1159 terminated after 30 steps\nEpisode 1160 terminated after 30 steps\nEpisode 1161 terminated after 30 steps\nEpisode 1162 terminated after 30 steps\nEpisode 1163 terminated after 30 steps\nEpisode 1164 terminated after 30 steps\nEpisode 1165 terminated after 30 steps\nEpisode 1166 terminated after 30 steps\nEpisode 1167 terminated after 30 steps\nEpisode 1168 terminated after 30 steps\nEpisode 1169 terminated after 30 steps\nEpisode 1170 terminated after 30 steps\nEpisode 1171 terminated after 30 steps\nEpisode 1172 terminated after 30 steps\nEpisode 1173 terminated after 30 steps\nEpisode 1174 terminated after 30 steps\nEpisode 1175 terminated after 30 steps\nEpisode 1176 terminated after 30 steps\nEpisode 1177 terminated after 30 steps\nEpisode 1178 terminated after 30 steps\nEpisode 1179 terminated after 30 steps\nEpisode 1180 terminated after 30 steps\nEpisode 1181 terminated after 30 steps\nEpisode 1182 terminated after 30 steps\nEpisode 1183 terminated after 30 steps\nEpisode 1184 terminated after 30 steps\nEpisode 1185 terminated after 30 steps\nEpisode 1186 terminated after 30 steps\nEpisode 1187 terminated after 30 steps\nEpisode 1188 terminated after 30 steps\nEpisode 1189 terminated after 30 steps\nEpisode 1190 terminated after 30 steps\nEpisode 1191 terminated after 30 steps\nEpisode 1192 terminated after 30 steps\nEpisode 1193 terminated after 30 steps\nEpisode 1194 terminated after 30 steps\nEpisode 1195 terminated after 30 steps\nEpisode 1196 terminated after 30 steps\nEpisode 1197 terminated after 30 steps\nEpisode 1198 terminated after 30 steps\nEpisode 1199 terminated after 30 steps\nEpisode 1200\nEpisode 1200 terminated after 30 steps\nEpisode 1201 terminated after 30 steps\nEpisode 1202 terminated after 30 steps\nEpisode 1203 terminated after 30 steps\nEpisode 1204 terminated after 30 steps\nEpisode 1205 terminated after 30 steps\nEpisode 1206 terminated after 30 steps\nEpisode 1207 terminated after 30 steps\nEpisode 1208 terminated after 30 steps\nEpisode 1209 terminated after 30 steps\nEpisode 1210 terminated after 30 steps\nEpisode 1211 terminated after 30 steps\nEpisode 1212 terminated after 30 steps\nEpisode 1213 terminated after 30 steps\nEpisode 1214 terminated after 30 steps\nEpisode 1215 terminated after 30 steps\nEpisode 1216 terminated after 30 steps\nEpisode 1217 terminated after 30 steps\nEpisode 1218 terminated after 30 steps\nEpisode 1219 terminated after 30 steps\nEpisode 1220 terminated after 30 steps\nEpisode 1221 terminated after 30 steps\nEpisode 1222 terminated after 30 steps\nEpisode 1223 terminated after 30 steps\nEpisode 1224 terminated after 30 steps\nEpisode 1225 terminated after 30 steps\nEpisode 1226 terminated after 30 steps\nEpisode 1227 terminated after 30 steps\nEpisode 1228 terminated after 30 steps\nEpisode 1229 terminated after 30 steps\nEpisode 1230 terminated after 30 steps\nEpisode 1231 terminated after 30 steps\nEpisode 1232 terminated after 30 steps\nEpisode 1233 terminated after 30 steps\nEpisode 1234 terminated after 30 steps\nEpisode 1235 terminated after 30 steps\nEpisode 1236 terminated after 30 steps\nEpisode 1237 terminated after 30 steps\nEpisode 1238 terminated after 30 steps\nEpisode 1239 terminated after 30 steps\nEpisode 1240 terminated after 30 steps\nEpisode 1241 terminated after 30 steps\nEpisode 1242 terminated after 30 steps\nEpisode 1243 terminated after 30 steps\nEpisode 1244 terminated after 30 steps\nEpisode 1245 terminated after 30 steps\nEpisode 1246 terminated after 30 steps\nEpisode 1247 terminated after 30 steps\nEpisode 1248 terminated after 30 steps\nEpisode 1249 terminated after 30 steps\nEpisode 1250 terminated after 30 steps\nEpisode 1251 terminated after 30 steps\nEpisode 1252 terminated after 30 steps\nEpisode 1253 terminated after 30 steps\nEpisode 1254 terminated after 30 steps\nEpisode 1255 terminated after 30 steps\nEpisode 1256 terminated after 30 steps\nEpisode 1257 terminated after 30 steps\nEpisode 1258 terminated after 30 steps\nEpisode 1259 terminated after 30 steps\nEpisode 1260 terminated after 30 steps\nEpisode 1261 terminated after 30 steps\nEpisode 1262 terminated after 30 steps\nEpisode 1263 terminated after 30 steps\nEpisode 1264 terminated after 30 steps\nEpisode 1265 terminated after 30 steps\nEpisode 1266 terminated after 30 steps\nEpisode 1267 terminated after 30 steps\nEpisode 1268 terminated after 30 steps\nEpisode 1269 terminated after 30 steps\nEpisode 1270 terminated after 30 steps\nEpisode 1271 terminated after 30 steps\nEpisode 1272 terminated after 30 steps\nEpisode 1273 terminated after 30 steps\nEpisode 1274 terminated after 30 steps\nEpisode 1275 terminated after 30 steps\nEpisode 1276 terminated after 30 steps\nEpisode 1277 terminated after 30 steps\nEpisode 1278 terminated after 30 steps\nEpisode 1279 terminated after 30 steps\nEpisode 1280 terminated after 30 steps\nEpisode 1281 terminated after 30 steps\nEpisode 1282 terminated after 30 steps\nEpisode 1283 terminated after 30 steps\nEpisode 1284 terminated after 30 steps\nEpisode 1285 terminated after 30 steps\nEpisode 1286 terminated after 30 steps\nEpisode 1287 terminated after 30 steps\nEpisode 1288 terminated after 30 steps\nEpisode 1289 terminated after 30 steps\nEpisode 1290 terminated after 30 steps\nEpisode 1291 terminated after 30 steps\nEpisode 1292 terminated after 30 steps\nEpisode 1293 terminated after 30 steps\nEpisode 1294 terminated after 30 steps\nEpisode 1295 terminated after 30 steps\nEpisode 1296 terminated after 30 steps\nEpisode 1297 terminated after 30 steps\nEpisode 1298 terminated after 30 steps\nEpisode 1299 terminated after 30 steps\nEpisode 1300\nEpisode 1300 terminated after 30 steps\nEpisode 1301 terminated after 30 steps\nEpisode 1302 terminated after 30 steps\nEpisode 1303 terminated after 30 steps\nEpisode 1304 terminated after 30 steps\nEpisode 1305 terminated after 30 steps\nEpisode 1306 terminated after 30 steps\nEpisode 1307 terminated after 30 steps\nEpisode 1308 terminated after 30 steps\nEpisode 1309 terminated after 30 steps\nEpisode 1310 terminated after 30 steps\nEpisode 1311 terminated after 30 steps\nEpisode 1312 terminated after 30 steps\nEpisode 1313 terminated after 30 steps\nEpisode 1314 terminated after 30 steps\nEpisode 1315 terminated after 30 steps\nEpisode 1316 terminated after 30 steps\nEpisode 1317 terminated after 30 steps\nEpisode 1318 terminated after 30 steps\nEpisode 1319 terminated after 30 steps\nEpisode 1320 terminated after 30 steps\nEpisode 1321 terminated after 30 steps\nEpisode 1322 terminated after 30 steps\nEpisode 1323 terminated after 30 steps\nEpisode 1324 terminated after 30 steps\nEpisode 1325 terminated after 30 steps\nEpisode 1326 terminated after 30 steps\nEpisode 1327 terminated after 30 steps\nEpisode 1328 terminated after 30 steps\nEpisode 1329 terminated after 30 steps\nEpisode 1330 terminated after 30 steps\nEpisode 1331 terminated after 30 steps\nEpisode 1332 terminated after 30 steps\nEpisode 1333 terminated after 30 steps\nEpisode 1334 terminated after 30 steps\nEpisode 1335 terminated after 30 steps\nEpisode 1336 terminated after 30 steps\nEpisode 1337 terminated after 30 steps\nEpisode 1338 terminated after 30 steps\nEpisode 1339 terminated after 30 steps\nEpisode 1340 terminated after 30 steps\nEpisode 1341 terminated after 30 steps\nEpisode 1342 terminated after 30 steps\nEpisode 1343 terminated after 30 steps\nEpisode 1344 terminated after 30 steps\nEpisode 1345 terminated after 30 steps\nEpisode 1346 terminated after 30 steps\nEpisode 1347 terminated after 30 steps\nEpisode 1348 terminated after 30 steps\nEpisode 1349 terminated after 30 steps\nEpisode 1350 terminated after 30 steps\nEpisode 1351 terminated after 30 steps\nEpisode 1352 terminated after 30 steps\nEpisode 1353 terminated after 30 steps\nEpisode 1354 terminated after 30 steps\nEpisode 1355 terminated after 30 steps\nEpisode 1356 terminated after 30 steps\nEpisode 1357 terminated after 30 steps\nEpisode 1358 terminated after 30 steps\nEpisode 1359 terminated after 30 steps\nEpisode 1360 terminated after 30 steps\nEpisode 1361 terminated after 30 steps\nEpisode 1362 terminated after 30 steps\nEpisode 1363 terminated after 30 steps\nEpisode 1364 terminated after 30 steps\nEpisode 1365 terminated after 30 steps\nEpisode 1366 terminated after 30 steps\nEpisode 1367 terminated after 30 steps\nEpisode 1368 terminated after 30 steps\nEpisode 1369 terminated after 30 steps\nEpisode 1370 terminated after 30 steps\nEpisode 1371 terminated after 30 steps\nEpisode 1372 terminated after 30 steps\nEpisode 1373 terminated after 30 steps\nEpisode 1374 terminated after 30 steps\nEpisode 1375 terminated after 30 steps\nEpisode 1376 terminated after 30 steps\nEpisode 1377 terminated after 30 steps\nEpisode 1378 terminated after 30 steps\nEpisode 1379 terminated after 30 steps\nEpisode 1380 terminated after 30 steps\nEpisode 1381 terminated after 30 steps\nEpisode 1382 terminated after 30 steps\nEpisode 1383 terminated after 30 steps\nEpisode 1384 terminated after 30 steps\nEpisode 1385 terminated after 30 steps\nEpisode 1386 terminated after 30 steps\nEpisode 1387 terminated after 30 steps\nEpisode 1388 terminated after 30 steps\nEpisode 1389 terminated after 30 steps\nEpisode 1390 terminated after 30 steps\nEpisode 1391 terminated after 30 steps\nEpisode 1392 terminated after 30 steps\nEpisode 1393 terminated after 30 steps\nEpisode 1394 terminated after 30 steps\nEpisode 1395 terminated after 30 steps\nEpisode 1396 terminated after 30 steps\nEpisode 1397 terminated after 30 steps\nEpisode 1398 terminated after 30 steps\nEpisode 1399 terminated after 30 steps\nEpisode 1400\nEpisode 1400 terminated after 30 steps\nEpisode 1401 terminated after 30 steps\nEpisode 1402 terminated after 30 steps\nEpisode 1403 terminated after 30 steps\nEpisode 1404 terminated after 30 steps\nEpisode 1405 terminated after 30 steps\nEpisode 1406 terminated after 30 steps\nEpisode 1407 terminated after 30 steps\nEpisode 1408 terminated after 30 steps\nEpisode 1409 terminated after 30 steps\nEpisode 1410 terminated after 30 steps\nEpisode 1411 terminated after 30 steps\nEpisode 1412 terminated after 30 steps\nEpisode 1413 terminated after 30 steps\nEpisode 1414 terminated after 30 steps\nEpisode 1415 terminated after 30 steps\nEpisode 1416 terminated after 30 steps\nEpisode 1417 terminated after 30 steps\nEpisode 1418 terminated after 30 steps\nEpisode 1419 terminated after 30 steps\nEpisode 1420 terminated after 30 steps\nEpisode 1421 terminated after 30 steps\nEpisode 1422 terminated after 30 steps\nEpisode 1423 terminated after 30 steps\nEpisode 1424 terminated after 30 steps\nEpisode 1425 terminated after 30 steps\nEpisode 1426 terminated after 30 steps\nEpisode 1427 terminated after 30 steps\nEpisode 1428 terminated after 30 steps\nEpisode 1429 terminated after 30 steps\nEpisode 1430 terminated after 30 steps\nEpisode 1431 terminated after 30 steps\nEpisode 1432 terminated after 30 steps\nEpisode 1433 terminated after 30 steps\nEpisode 1434 terminated after 30 steps\nEpisode 1435 terminated after 30 steps\nEpisode 1436 terminated after 30 steps\nEpisode 1437 terminated after 30 steps\nEpisode 1438 terminated after 30 steps\nEpisode 1439 terminated after 30 steps\nEpisode 1440 terminated after 30 steps\nEpisode 1441 terminated after 30 steps\nEpisode 1442 terminated after 30 steps\nEpisode 1443 terminated after 30 steps\nEpisode 1444 terminated after 30 steps\nEpisode 1445 terminated after 30 steps\nEpisode 1446 terminated after 30 steps\nEpisode 1447 terminated after 30 steps\nEpisode 1448 terminated after 30 steps\nEpisode 1449 terminated after 30 steps\nEpisode 1450 terminated after 30 steps\nEpisode 1451 terminated after 30 steps\nEpisode 1452 terminated after 30 steps\nEpisode 1453 terminated after 30 steps\nEpisode 1454 terminated after 30 steps\nEpisode 1455 terminated after 30 steps\nEpisode 1456 terminated after 30 steps\nEpisode 1457 terminated after 30 steps\nEpisode 1458 terminated after 30 steps\nEpisode 1459 terminated after 30 steps\nEpisode 1460 terminated after 30 steps\nEpisode 1461 terminated after 30 steps\nEpisode 1462 terminated after 30 steps\nEpisode 1463 terminated after 30 steps\nEpisode 1464 terminated after 30 steps\nEpisode 1465 terminated after 30 steps\nEpisode 1466 terminated after 30 steps\nEpisode 1467 terminated after 30 steps\nEpisode 1468 terminated after 30 steps\nEpisode 1469 terminated after 30 steps\nEpisode 1470 terminated after 30 steps\nEpisode 1471 terminated after 30 steps\nEpisode 1472 terminated after 30 steps\nEpisode 1473 terminated after 30 steps\nEpisode 1474 terminated after 30 steps\nEpisode 1475 terminated after 30 steps\nEpisode 1476 terminated after 30 steps\nEpisode 1477 terminated after 30 steps\nEpisode 1478 terminated after 30 steps\nEpisode 1479 terminated after 30 steps\nEpisode 1480 terminated after 30 steps\nEpisode 1481 terminated after 30 steps\nEpisode 1482 terminated after 30 steps\nEpisode 1483 terminated after 30 steps\nEpisode 1484 terminated after 30 steps\nEpisode 1485 terminated after 30 steps\nEpisode 1486 terminated after 30 steps\nEpisode 1487 terminated after 30 steps\nEpisode 1488 terminated after 30 steps\nEpisode 1489 terminated after 30 steps\nEpisode 1490 terminated after 30 steps\nEpisode 1491 terminated after 30 steps\nEpisode 1492 terminated after 30 steps\nEpisode 1493 terminated after 30 steps\nEpisode 1494 terminated after 30 steps\nEpisode 1495 terminated after 30 steps\nEpisode 1496 terminated after 30 steps\nEpisode 1497 terminated after 30 steps\nEpisode 1498 terminated after 30 steps\nEpisode 1499 terminated after 30 steps\nEpisode 1500\nEpisode 1500 terminated after 30 steps\nEpisode 1501 terminated after 30 steps\nEpisode 1502 terminated after 30 steps\nEpisode 1503 terminated after 30 steps\nEpisode 1504 terminated after 30 steps\nEpisode 1505 terminated after 30 steps\nEpisode 1506 terminated after 30 steps\nEpisode 1507 terminated after 30 steps\nEpisode 1508 terminated after 30 steps\nEpisode 1509 terminated after 30 steps\nEpisode 1510 terminated after 30 steps\nEpisode 1511 terminated after 30 steps\nEpisode 1512 terminated after 30 steps\nEpisode 1513 terminated after 30 steps\nEpisode 1514 terminated after 30 steps\nEpisode 1515 terminated after 30 steps\nEpisode 1516 terminated after 30 steps\nEpisode 1517 terminated after 30 steps\nEpisode 1518 terminated after 30 steps\nEpisode 1519 terminated after 30 steps\nEpisode 1520 terminated after 30 steps\nEpisode 1521 terminated after 30 steps\nEpisode 1522 terminated after 30 steps\nEpisode 1523 terminated after 30 steps\nEpisode 1524 terminated after 30 steps\nEpisode 1525 terminated after 30 steps\nEpisode 1526 terminated after 30 steps\nEpisode 1527 terminated after 30 steps\nEpisode 1528 terminated after 30 steps\nEpisode 1529 terminated after 30 steps\nEpisode 1530 terminated after 30 steps\nEpisode 1531 terminated after 30 steps\nEpisode 1532 terminated after 30 steps\nEpisode 1533 terminated after 30 steps\nEpisode 1534 terminated after 30 steps\nEpisode 1535 terminated after 30 steps\nEpisode 1536 terminated after 30 steps\nEpisode 1537 terminated after 30 steps\nEpisode 1538 terminated after 30 steps\nEpisode 1539 terminated after 30 steps\nEpisode 1540 terminated after 30 steps\nEpisode 1541 terminated after 30 steps\nEpisode 1542 terminated after 30 steps\nEpisode 1543 terminated after 30 steps\nEpisode 1544 terminated after 30 steps\nEpisode 1545 terminated after 30 steps\nEpisode 1546 terminated after 30 steps\nEpisode 1547 terminated after 30 steps\nEpisode 1548 terminated after 30 steps\nEpisode 1549 terminated after 30 steps\nEpisode 1550 terminated after 30 steps\nEpisode 1551 terminated after 30 steps\nEpisode 1552 terminated after 30 steps\nEpisode 1553 terminated after 30 steps\nEpisode 1554 terminated after 30 steps\nEpisode 1555 terminated after 30 steps\nEpisode 1556 terminated after 30 steps\nEpisode 1557 terminated after 30 steps\nEpisode 1558 terminated after 30 steps\nEpisode 1559 terminated after 30 steps\nEpisode 1560 terminated after 30 steps\nEpisode 1561 terminated after 30 steps\nEpisode 1562 terminated after 30 steps\nEpisode 1563 terminated after 30 steps\nEpisode 1564 terminated after 30 steps\nEpisode 1565 terminated after 30 steps\nEpisode 1566 terminated after 30 steps\nEpisode 1567 terminated after 30 steps\nEpisode 1568 terminated after 30 steps\nEpisode 1569 terminated after 30 steps\nEpisode 1570 terminated after 30 steps\nEpisode 1571 terminated after 30 steps\nEpisode 1572 terminated after 30 steps\nEpisode 1573 terminated after 30 steps\nEpisode 1574 terminated after 30 steps\nEpisode 1575 terminated after 30 steps\nEpisode 1576 terminated after 30 steps\nEpisode 1577 terminated after 30 steps\nEpisode 1578 terminated after 30 steps\nEpisode 1579 terminated after 30 steps\nEpisode 1580 terminated after 30 steps\nEpisode 1581 terminated after 30 steps\nEpisode 1582 terminated after 30 steps\nEpisode 1583 terminated after 30 steps\nEpisode 1584 terminated after 30 steps\nEpisode 1585 terminated after 30 steps\nEpisode 1586 terminated after 30 steps\nEpisode 1587 terminated after 30 steps\nEpisode 1588 terminated after 30 steps\nEpisode 1589 terminated after 30 steps\nEpisode 1590 terminated after 30 steps\nEpisode 1591 terminated after 30 steps\nEpisode 1592 terminated after 30 steps\nEpisode 1593 terminated after 30 steps\nEpisode 1594 terminated after 30 steps\nEpisode 1595 terminated after 30 steps\nEpisode 1596 terminated after 30 steps\nEpisode 1597 terminated after 30 steps\nEpisode 1598 terminated after 30 steps\nEpisode 1599 terminated after 30 steps\nEpisode 1600\nEpisode 1600 terminated after 30 steps\nEpisode 1601 terminated after 30 steps\nEpisode 1602 terminated after 30 steps\nEpisode 1603 terminated after 30 steps\nEpisode 1604 terminated after 30 steps\nEpisode 1605 terminated after 30 steps\nEpisode 1606 terminated after 30 steps\nEpisode 1607 terminated after 30 steps\nEpisode 1608 terminated after 30 steps\nEpisode 1609 terminated after 30 steps\nEpisode 1610 terminated after 30 steps\nEpisode 1611 terminated after 30 steps\nEpisode 1612 terminated after 30 steps\nEpisode 1613 terminated after 30 steps\nEpisode 1614 terminated after 30 steps\nEpisode 1615 terminated after 30 steps\nEpisode 1616 terminated after 30 steps\nEpisode 1617 terminated after 30 steps\nEpisode 1618 terminated after 30 steps\nEpisode 1619 terminated after 30 steps\nEpisode 1620 terminated after 30 steps\nEpisode 1621 terminated after 30 steps\nEpisode 1622 terminated after 30 steps\nEpisode 1623 terminated after 30 steps\nEpisode 1624 terminated after 30 steps\nEpisode 1625 terminated after 30 steps\nEpisode 1626 terminated after 30 steps\nEpisode 1627 terminated after 30 steps\nEpisode 1628 terminated after 30 steps\nEpisode 1629 terminated after 30 steps\nEpisode 1630 terminated after 30 steps\nEpisode 1631 terminated after 30 steps\nEpisode 1632 terminated after 30 steps\nEpisode 1633 terminated after 30 steps\nEpisode 1634 terminated after 30 steps\nEpisode 1635 terminated after 30 steps\nEpisode 1636 terminated after 30 steps\nEpisode 1637 terminated after 30 steps\nEpisode 1638 terminated after 30 steps\nEpisode 1639 terminated after 30 steps\nEpisode 1640 terminated after 30 steps\nEpisode 1641 terminated after 30 steps\nEpisode 1642 terminated after 30 steps\nEpisode 1643 terminated after 30 steps\nEpisode 1644 terminated after 30 steps\nEpisode 1645 terminated after 30 steps\nEpisode 1646 terminated after 30 steps\nEpisode 1647 terminated after 30 steps\nEpisode 1648 terminated after 30 steps\nEpisode 1649 terminated after 30 steps\nEpisode 1650 terminated after 30 steps\nEpisode 1651 terminated after 30 steps\nEpisode 1652 terminated after 30 steps\nEpisode 1653 terminated after 30 steps\nEpisode 1654 terminated after 30 steps\nEpisode 1655 terminated after 30 steps\nEpisode 1656 terminated after 30 steps\nEpisode 1657 terminated after 30 steps\nEpisode 1658 terminated after 30 steps\nEpisode 1659 terminated after 30 steps\nEpisode 1660 terminated after 30 steps\nEpisode 1661 terminated after 30 steps\nEpisode 1662 terminated after 30 steps\nEpisode 1663 terminated after 30 steps\nEpisode 1664 terminated after 30 steps\nEpisode 1665 terminated after 30 steps\nEpisode 1666 terminated after 30 steps\nEpisode 1667 terminated after 30 steps\nEpisode 1668 terminated after 30 steps\nEpisode 1669 terminated after 30 steps\nEpisode 1670 terminated after 30 steps\nEpisode 1671 terminated after 30 steps\nEpisode 1672 terminated after 30 steps\nEpisode 1673 terminated after 30 steps\nEpisode 1674 terminated after 30 steps\nEpisode 1675 terminated after 30 steps\nEpisode 1676 terminated after 30 steps\nEpisode 1677 terminated after 30 steps\nEpisode 1678 terminated after 30 steps\nEpisode 1679 terminated after 30 steps\nEpisode 1680 terminated after 30 steps\nEpisode 1681 terminated after 30 steps\nEpisode 1682 terminated after 30 steps\nEpisode 1683 terminated after 30 steps\nEpisode 1684 terminated after 30 steps\nEpisode 1685 terminated after 30 steps\nEpisode 1686 terminated after 30 steps\nEpisode 1687 terminated after 30 steps\nEpisode 1688 terminated after 30 steps\nEpisode 1689 terminated after 30 steps\nEpisode 1690 terminated after 30 steps\nEpisode 1691 terminated after 30 steps\nEpisode 1692 terminated after 30 steps\nEpisode 1693 terminated after 30 steps\nEpisode 1694 terminated after 30 steps\nEpisode 1695 terminated after 30 steps\nEpisode 1696 terminated after 30 steps\nEpisode 1697 terminated after 30 steps\nEpisode 1698 terminated after 30 steps\nEpisode 1699 terminated after 30 steps\nEpisode 1700\nEpisode 1700 terminated after 30 steps\nEpisode 1701 terminated after 30 steps\nEpisode 1702 terminated after 30 steps\nEpisode 1703 terminated after 30 steps\nEpisode 1704 terminated after 30 steps\nEpisode 1705 terminated after 30 steps\nEpisode 1706 terminated after 30 steps\nEpisode 1707 terminated after 30 steps\nEpisode 1708 terminated after 30 steps\nEpisode 1709 terminated after 30 steps\nEpisode 1710 terminated after 30 steps\nEpisode 1711 terminated after 30 steps\nEpisode 1712 terminated after 30 steps\nEpisode 1713 terminated after 30 steps\nEpisode 1714 terminated after 30 steps\nEpisode 1715 terminated after 30 steps\nEpisode 1716 terminated after 30 steps\nEpisode 1717 terminated after 30 steps\nEpisode 1718 terminated after 30 steps\nEpisode 1719 terminated after 30 steps\nEpisode 1720 terminated after 30 steps\nEpisode 1721 terminated after 30 steps\nEpisode 1722 terminated after 30 steps\nEpisode 1723 terminated after 30 steps\nEpisode 1724 terminated after 30 steps\nEpisode 1725 terminated after 30 steps\nEpisode 1726 terminated after 30 steps\nEpisode 1727 terminated after 30 steps\nEpisode 1728 terminated after 30 steps\nEpisode 1729 terminated after 30 steps\nEpisode 1730 terminated after 30 steps\nEpisode 1731 terminated after 30 steps\nEpisode 1732 terminated after 30 steps\nEpisode 1733 terminated after 30 steps\nEpisode 1734 terminated after 30 steps\nEpisode 1735 terminated after 30 steps\nEpisode 1736 terminated after 30 steps\nEpisode 1737 terminated after 30 steps\nEpisode 1738 terminated after 30 steps\nEpisode 1739 terminated after 30 steps\nEpisode 1740 terminated after 30 steps\nEpisode 1741 terminated after 30 steps\nEpisode 1742 terminated after 30 steps\nEpisode 1743 terminated after 30 steps\nEpisode 1744 terminated after 30 steps\nEpisode 1745 terminated after 30 steps\nEpisode 1746 terminated after 30 steps\nEpisode 1747 terminated after 30 steps\nEpisode 1748 terminated after 30 steps\nEpisode 1749 terminated after 30 steps\nEpisode 1750 terminated after 30 steps\nEpisode 1751 terminated after 30 steps\nEpisode 1752 terminated after 30 steps\nEpisode 1753 terminated after 30 steps\nEpisode 1754 terminated after 30 steps\nEpisode 1755 terminated after 30 steps\nEpisode 1756 terminated after 30 steps\nEpisode 1757 terminated after 30 steps\nEpisode 1758 terminated after 30 steps\nEpisode 1759 terminated after 30 steps\nEpisode 1760 terminated after 30 steps\nEpisode 1761 terminated after 30 steps\nEpisode 1762 terminated after 30 steps\nEpisode 1763 terminated after 30 steps\nEpisode 1764 terminated after 30 steps\nEpisode 1765 terminated after 30 steps\nEpisode 1766 terminated after 30 steps\nEpisode 1767 terminated after 30 steps\nEpisode 1768 terminated after 30 steps\nEpisode 1769 terminated after 30 steps\nEpisode 1770 terminated after 30 steps\nEpisode 1771 terminated after 30 steps\nEpisode 1772 terminated after 30 steps\nEpisode 1773 terminated after 30 steps\nEpisode 1774 terminated after 30 steps\nEpisode 1775 terminated after 30 steps\nEpisode 1776 terminated after 30 steps\nEpisode 1777 terminated after 30 steps\nEpisode 1778 terminated after 30 steps\nEpisode 1779 terminated after 30 steps\nEpisode 1780 terminated after 30 steps\nEpisode 1781 terminated after 30 steps\nEpisode 1782 terminated after 30 steps\nEpisode 1783 terminated after 30 steps\nEpisode 1784 terminated after 30 steps\nEpisode 1785 terminated after 30 steps\nEpisode 1786 terminated after 30 steps\nEpisode 1787 terminated after 30 steps\nEpisode 1788 terminated after 30 steps\nEpisode 1789 terminated after 30 steps\nEpisode 1790 terminated after 30 steps\nEpisode 1791 terminated after 30 steps\nEpisode 1792 terminated after 30 steps\nEpisode 1793 terminated after 30 steps\nEpisode 1794 terminated after 30 steps\nEpisode 1795 terminated after 30 steps\nEpisode 1796 terminated after 30 steps\nEpisode 1797 terminated after 30 steps\nEpisode 1798 terminated after 30 steps\nEpisode 1799 terminated after 30 steps\nEpisode 1800\nEpisode 1800 terminated after 30 steps\nEpisode 1801 terminated after 30 steps\nEpisode 1802 terminated after 30 steps\nEpisode 1803 terminated after 30 steps\nEpisode 1804 terminated after 30 steps\nEpisode 1805 terminated after 30 steps\nEpisode 1806 terminated after 30 steps\nEpisode 1807 terminated after 30 steps\nEpisode 1808 terminated after 30 steps\nEpisode 1809 terminated after 30 steps\nEpisode 1810 terminated after 30 steps\nEpisode 1811 terminated after 30 steps\nEpisode 1812 terminated after 30 steps\nEpisode 1813 terminated after 30 steps\nEpisode 1814 terminated after 30 steps\nEpisode 1815 terminated after 30 steps\nEpisode 1816 terminated after 30 steps\nEpisode 1817 terminated after 30 steps\nEpisode 1818 terminated after 30 steps\nEpisode 1819 terminated after 30 steps\nEpisode 1820 terminated after 30 steps\nEpisode 1821 terminated after 30 steps\nEpisode 1822 terminated after 30 steps\nEpisode 1823 terminated after 30 steps\nEpisode 1824 terminated after 30 steps\nEpisode 1825 terminated after 30 steps\nEpisode 1826 terminated after 30 steps\nEpisode 1827 terminated after 30 steps\nEpisode 1828 terminated after 30 steps\nEpisode 1829 terminated after 30 steps\nEpisode 1830 terminated after 30 steps\nEpisode 1831 terminated after 30 steps\nEpisode 1832 terminated after 30 steps\nEpisode 1833 terminated after 30 steps\nEpisode 1834 terminated after 30 steps\nEpisode 1835 terminated after 30 steps\nEpisode 1836 terminated after 30 steps\nEpisode 1837 terminated after 30 steps\nEpisode 1838 terminated after 30 steps\nEpisode 1839 terminated after 30 steps\nEpisode 1840 terminated after 30 steps\nEpisode 1841 terminated after 30 steps\nEpisode 1842 terminated after 30 steps\nEpisode 1843 terminated after 30 steps\nEpisode 1844 terminated after 30 steps\nEpisode 1845 terminated after 30 steps\nEpisode 1846 terminated after 30 steps\nEpisode 1847 terminated after 30 steps\nEpisode 1848 terminated after 30 steps\nEpisode 1849 terminated after 30 steps\nEpisode 1850 terminated after 30 steps\nEpisode 1851 terminated after 30 steps\nEpisode 1852 terminated after 30 steps\nEpisode 1853 terminated after 30 steps\nEpisode 1854 terminated after 30 steps\nEpisode 1855 terminated after 30 steps\nEpisode 1856 terminated after 30 steps\nEpisode 1857 terminated after 30 steps\nEpisode 1858 terminated after 30 steps\nEpisode 1859 terminated after 30 steps\nEpisode 1860 terminated after 30 steps\nEpisode 1861 terminated after 30 steps\nEpisode 1862 terminated after 30 steps\nEpisode 1863 terminated after 30 steps\nEpisode 1864 terminated after 30 steps\nEpisode 1865 terminated after 30 steps\nEpisode 1866 terminated after 30 steps\nEpisode 1867 terminated after 30 steps\nEpisode 1868 terminated after 30 steps\nEpisode 1869 terminated after 30 steps\nEpisode 1870 terminated after 30 steps\nEpisode 1871 terminated after 30 steps\nEpisode 1872 terminated after 30 steps\nEpisode 1873 terminated after 30 steps\nEpisode 1874 terminated after 30 steps\nEpisode 1875 terminated after 30 steps\nEpisode 1876 terminated after 30 steps\nEpisode 1877 terminated after 30 steps\nEpisode 1878 terminated after 30 steps\nEpisode 1879 terminated after 30 steps\nEpisode 1880 terminated after 30 steps\nEpisode 1881 terminated after 30 steps\nEpisode 1882 terminated after 30 steps\nEpisode 1883 terminated after 30 steps\nEpisode 1884 terminated after 30 steps\nEpisode 1885 terminated after 30 steps\nEpisode 1886 terminated after 30 steps\nEpisode 1887 terminated after 30 steps\nEpisode 1888 terminated after 30 steps\nEpisode 1889 terminated after 30 steps\nEpisode 1890 terminated after 30 steps\nEpisode 1891 terminated after 30 steps\nEpisode 1892 terminated after 30 steps\nEpisode 1893 terminated after 30 steps\nEpisode 1894 terminated after 30 steps\nEpisode 1895 terminated after 30 steps\nEpisode 1896 terminated after 30 steps\nEpisode 1897 terminated after 30 steps\nEpisode 1898 terminated after 30 steps\nEpisode 1899 terminated after 30 steps\nEpisode 1900\nEpisode 1900 terminated after 30 steps\nEpisode 1901 terminated after 30 steps\nEpisode 1902 terminated after 30 steps\nEpisode 1903 terminated after 30 steps\nEpisode 1904 terminated after 30 steps\nEpisode 1905 terminated after 30 steps\nEpisode 1906 terminated after 30 steps\nEpisode 1907 terminated after 30 steps\nEpisode 1908 terminated after 30 steps\nEpisode 1909 terminated after 30 steps\nEpisode 1910 terminated after 30 steps\nEpisode 1911 terminated after 30 steps\nEpisode 1912 terminated after 30 steps\nEpisode 1913 terminated after 30 steps\nEpisode 1914 terminated after 30 steps\nEpisode 1915 terminated after 30 steps\nEpisode 1916 terminated after 30 steps\nEpisode 1917 terminated after 30 steps\nEpisode 1918 terminated after 30 steps\nEpisode 1919 terminated after 30 steps\nEpisode 1920 terminated after 30 steps\nEpisode 1921 terminated after 30 steps\nEpisode 1922 terminated after 30 steps\nEpisode 1923 terminated after 30 steps\nEpisode 1924 terminated after 30 steps\nEpisode 1925 terminated after 30 steps\nEpisode 1926 terminated after 30 steps\nEpisode 1927 terminated after 30 steps\nEpisode 1928 terminated after 30 steps\nEpisode 1929 terminated after 30 steps\nEpisode 1930 terminated after 30 steps\nEpisode 1931 terminated after 30 steps\nEpisode 1932 terminated after 30 steps\nEpisode 1933 terminated after 30 steps\nEpisode 1934 terminated after 30 steps\nEpisode 1935 terminated after 30 steps\nEpisode 1936 terminated after 30 steps\nEpisode 1937 terminated after 30 steps\nEpisode 1938 terminated after 30 steps\nEpisode 1939 terminated after 30 steps\nEpisode 1940 terminated after 30 steps\nEpisode 1941 terminated after 30 steps\nEpisode 1942 terminated after 30 steps\nEpisode 1943 terminated after 30 steps\nEpisode 1944 terminated after 30 steps\nEpisode 1945 terminated after 30 steps\nEpisode 1946 terminated after 30 steps\nEpisode 1947 terminated after 30 steps\nEpisode 1948 terminated after 30 steps\nEpisode 1949 terminated after 30 steps\nEpisode 1950 terminated after 30 steps\nEpisode 1951 terminated after 30 steps\nEpisode 1952 terminated after 30 steps\nEpisode 1953 terminated after 30 steps\nEpisode 1954 terminated after 30 steps\nEpisode 1955 terminated after 30 steps\nEpisode 1956 terminated after 30 steps\nEpisode 1957 terminated after 30 steps\nEpisode 1958 terminated after 30 steps\nEpisode 1959 terminated after 30 steps\nEpisode 1960 terminated after 30 steps\nEpisode 1961 terminated after 30 steps\nEpisode 1962 terminated after 30 steps\nEpisode 1963 terminated after 30 steps\nEpisode 1964 terminated after 30 steps\nEpisode 1965 terminated after 30 steps\nEpisode 1966 terminated after 30 steps\nEpisode 1967 terminated after 30 steps\nEpisode 1968 terminated after 30 steps\nEpisode 1969 terminated after 30 steps\nEpisode 1970 terminated after 30 steps\nEpisode 1971 terminated after 30 steps\nEpisode 1972 terminated after 30 steps\nEpisode 1973 terminated after 30 steps\nEpisode 1974 terminated after 30 steps\nEpisode 1975 terminated after 30 steps\nEpisode 1976 terminated after 30 steps\nEpisode 1977 terminated after 30 steps\nEpisode 1978 terminated after 30 steps\nEpisode 1979 terminated after 30 steps\nEpisode 1980 terminated after 30 steps\nEpisode 1981 terminated after 30 steps\nEpisode 1982 terminated after 30 steps\nEpisode 1983 terminated after 30 steps\nEpisode 1984 terminated after 30 steps\nEpisode 1985 terminated after 30 steps\nEpisode 1986 terminated after 30 steps\nEpisode 1987 terminated after 30 steps\nEpisode 1988 terminated after 30 steps\nEpisode 1989 terminated after 30 steps\nEpisode 1990 terminated after 30 steps\nEpisode 1991 terminated after 30 steps\nEpisode 1992 terminated after 30 steps\nEpisode 1993 terminated after 30 steps\nEpisode 1994 terminated after 30 steps\nEpisode 1995 terminated after 30 steps\nEpisode 1996 terminated after 30 steps\nEpisode 1997 terminated after 30 steps\nEpisode 1998 terminated after 30 steps\nEpisode 1999 terminated after 30 steps\nEpisode 2000\nEpisode 2000 terminated after 30 steps\nEpisode 2001 terminated after 30 steps\nEpisode 2002 terminated after 30 steps\nEpisode 2003 terminated after 30 steps\nEpisode 2004 terminated after 30 steps\nEpisode 2005 terminated after 30 steps\nEpisode 2006 terminated after 30 steps\nEpisode 2007 terminated after 30 steps\nEpisode 2008 terminated after 30 steps\nEpisode 2009 terminated after 30 steps\nEpisode 2010 terminated after 30 steps\nEpisode 2011 terminated after 30 steps\nEpisode 2012 terminated after 30 steps\nEpisode 2013 terminated after 30 steps\nEpisode 2014 terminated after 30 steps\nEpisode 2015 terminated after 30 steps\nEpisode 2016 terminated after 30 steps\nEpisode 2017 terminated after 30 steps\nEpisode 2018 terminated after 30 steps\nEpisode 2019 terminated after 30 steps\nEpisode 2020 terminated after 30 steps\nEpisode 2021 terminated after 30 steps\nEpisode 2022 terminated after 30 steps\nEpisode 2023 terminated after 30 steps\nEpisode 2024 terminated after 30 steps\nEpisode 2025 terminated after 30 steps\nEpisode 2026 terminated after 30 steps\nEpisode 2027 terminated after 30 steps\nEpisode 2028 terminated after 30 steps\nEpisode 2029 terminated after 30 steps\nEpisode 2030 terminated after 30 steps\nEpisode 2031 terminated after 30 steps\nEpisode 2032 terminated after 30 steps\nEpisode 2033 terminated after 30 steps\nEpisode 2034 terminated after 30 steps\nEpisode 2035 terminated after 30 steps\nEpisode 2036 terminated after 30 steps\nEpisode 2037 terminated after 30 steps\nEpisode 2038 terminated after 30 steps\nEpisode 2039 terminated after 30 steps\nEpisode 2040 terminated after 30 steps\nEpisode 2041 terminated after 30 steps\nEpisode 2042 terminated after 30 steps\nEpisode 2043 terminated after 30 steps\nEpisode 2044 terminated after 30 steps\nEpisode 2045 terminated after 30 steps\nEpisode 2046 terminated after 30 steps\nEpisode 2047 terminated after 30 steps\nEpisode 2048 terminated after 30 steps\nEpisode 2049 terminated after 30 steps\nEpisode 2050 terminated after 30 steps\nEpisode 2051 terminated after 30 steps\nEpisode 2052 terminated after 30 steps\nEpisode 2053 terminated after 30 steps\nEpisode 2054 terminated after 30 steps\nEpisode 2055 terminated after 30 steps\nEpisode 2056 terminated after 30 steps\nEpisode 2057 terminated after 30 steps\nEpisode 2058 terminated after 30 steps\nEpisode 2059 terminated after 30 steps\nEpisode 2060 terminated after 30 steps\nEpisode 2061 terminated after 30 steps\nEpisode 2062 terminated after 30 steps\nEpisode 2063 terminated after 30 steps\nEpisode 2064 terminated after 30 steps\nEpisode 2065 terminated after 30 steps\nEpisode 2066 terminated after 30 steps\nEpisode 2067 terminated after 30 steps\nEpisode 2068 terminated after 30 steps\nEpisode 2069 terminated after 30 steps\nEpisode 2070 terminated after 30 steps\nEpisode 2071 terminated after 30 steps\nEpisode 2072 terminated after 30 steps\nEpisode 2073 terminated after 30 steps\nEpisode 2074 terminated after 30 steps\nEpisode 2075 terminated after 30 steps\nEpisode 2076 terminated after 30 steps\nEpisode 2077 terminated after 30 steps\nEpisode 2078 terminated after 30 steps\nEpisode 2079 terminated after 30 steps\nEpisode 2080 terminated after 30 steps\nEpisode 2081 terminated after 30 steps\nEpisode 2082 terminated after 30 steps\nEpisode 2083 terminated after 30 steps\nEpisode 2084 terminated after 30 steps\nEpisode 2085 terminated after 30 steps\nEpisode 2086 terminated after 30 steps\nEpisode 2087 terminated after 30 steps\nEpisode 2088 terminated after 30 steps\nEpisode 2089 terminated after 30 steps\nEpisode 2090 terminated after 30 steps\nEpisode 2091 terminated after 30 steps\nEpisode 2092 terminated after 30 steps\nEpisode 2093 terminated after 30 steps\nEpisode 2094 terminated after 30 steps\nEpisode 2095 terminated after 30 steps\nEpisode 2096 terminated after 30 steps\nEpisode 2097 terminated after 30 steps\nEpisode 2098 terminated after 30 steps\nEpisode 2099 terminated after 30 steps\nEpisode 2100\nEpisode 2100 terminated after 30 steps\nEpisode 2101 terminated after 30 steps\nEpisode 2102 terminated after 30 steps\nEpisode 2103 terminated after 30 steps\nEpisode 2104 terminated after 30 steps\nEpisode 2105 terminated after 30 steps\nEpisode 2106 terminated after 30 steps\nEpisode 2107 terminated after 30 steps\nEpisode 2108 terminated after 30 steps\nEpisode 2109 terminated after 30 steps\nEpisode 2110 terminated after 30 steps\nEpisode 2111 terminated after 30 steps\nEpisode 2112 terminated after 30 steps\nEpisode 2113 terminated after 30 steps\nEpisode 2114 terminated after 30 steps\nEpisode 2115 terminated after 30 steps\nEpisode 2116 terminated after 30 steps\nEpisode 2117 terminated after 30 steps\nEpisode 2118 terminated after 30 steps\nEpisode 2119 terminated after 30 steps\nEpisode 2120 terminated after 30 steps\nEpisode 2121 terminated after 30 steps\nEpisode 2122 terminated after 30 steps\nEpisode 2123 terminated after 30 steps\nEpisode 2124 terminated after 30 steps\nEpisode 2125 terminated after 30 steps\nEpisode 2126 terminated after 30 steps\nEpisode 2127 terminated after 30 steps\nEpisode 2128 terminated after 30 steps\nEpisode 2129 terminated after 30 steps\nEpisode 2130 terminated after 30 steps\nEpisode 2131 terminated after 30 steps\nEpisode 2132 terminated after 30 steps\nEpisode 2133 terminated after 30 steps\nEpisode 2134 terminated after 30 steps\nEpisode 2135 terminated after 30 steps\nEpisode 2136 terminated after 30 steps\nEpisode 2137 terminated after 30 steps\nEpisode 2138 terminated after 30 steps\nEpisode 2139 terminated after 30 steps\nEpisode 2140 terminated after 30 steps\nEpisode 2141 terminated after 30 steps\nEpisode 2142 terminated after 30 steps\nEpisode 2143 terminated after 30 steps\nEpisode 2144 terminated after 30 steps\nEpisode 2145 terminated after 30 steps\nEpisode 2146 terminated after 30 steps\nEpisode 2147 terminated after 30 steps\nEpisode 2148 terminated after 30 steps\nEpisode 2149 terminated after 30 steps\nEpisode 2150 terminated after 30 steps\nEpisode 2151 terminated after 30 steps\nEpisode 2152 terminated after 30 steps\nEpisode 2153 terminated after 30 steps\nEpisode 2154 terminated after 30 steps\nEpisode 2155 terminated after 30 steps\nEpisode 2156 terminated after 30 steps\nEpisode 2157 terminated after 30 steps\nEpisode 2158 terminated after 30 steps\nEpisode 2159 terminated after 30 steps\nEpisode 2160 terminated after 30 steps\nEpisode 2161 terminated after 30 steps\nEpisode 2162 terminated after 30 steps\nEpisode 2163 terminated after 30 steps\nEpisode 2164 terminated after 30 steps\nEpisode 2165 terminated after 30 steps\nEpisode 2166 terminated after 30 steps\nEpisode 2167 terminated after 30 steps\nEpisode 2168 terminated after 30 steps\nEpisode 2169 terminated after 30 steps\nEpisode 2170 terminated after 30 steps\nEpisode 2171 terminated after 30 steps\nEpisode 2172 terminated after 30 steps\nEpisode 2173 terminated after 30 steps\nEpisode 2174 terminated after 30 steps\nEpisode 2175 terminated after 30 steps\nEpisode 2176 terminated after 30 steps\nEpisode 2177 terminated after 30 steps\nEpisode 2178 terminated after 30 steps\nEpisode 2179 terminated after 30 steps\nEpisode 2180 terminated after 30 steps\nEpisode 2181 terminated after 30 steps\nEpisode 2182 terminated after 30 steps\nEpisode 2183 terminated after 30 steps\nEpisode 2184 terminated after 30 steps\nEpisode 2185 terminated after 30 steps\nEpisode 2186 terminated after 30 steps\nEpisode 2187 terminated after 30 steps\nEpisode 2188 terminated after 30 steps\nEpisode 2189 terminated after 30 steps\nEpisode 2190 terminated after 30 steps\nEpisode 2191 terminated after 30 steps\nEpisode 2192 terminated after 30 steps\nEpisode 2193 terminated after 30 steps\nEpisode 2194 terminated after 30 steps\nEpisode 2195 terminated after 30 steps\nEpisode 2196 terminated after 30 steps\nEpisode 2197 terminated after 30 steps\nEpisode 2198 terminated after 30 steps\nEpisode 2199 terminated after 30 steps\nEpisode 2200\nEpisode 2200 terminated after 30 steps\nEpisode 2201 terminated after 30 steps\nEpisode 2202 terminated after 30 steps\nEpisode 2203 terminated after 30 steps\nEpisode 2204 terminated after 30 steps\nEpisode 2205 terminated after 30 steps\nEpisode 2206 terminated after 30 steps\nEpisode 2207 terminated after 30 steps\nEpisode 2208 terminated after 30 steps\nEpisode 2209 terminated after 30 steps\nEpisode 2210 terminated after 30 steps\nEpisode 2211 terminated after 30 steps\nEpisode 2212 terminated after 30 steps\nEpisode 2213 terminated after 30 steps\nEpisode 2214 terminated after 30 steps\nEpisode 2215 terminated after 30 steps\nEpisode 2216 terminated after 30 steps\nEpisode 2217 terminated after 30 steps\nEpisode 2218 terminated after 30 steps\nEpisode 2219 terminated after 30 steps\nEpisode 2220 terminated after 30 steps\nEpisode 2221 terminated after 30 steps\nEpisode 2222 terminated after 30 steps\nEpisode 2223 terminated after 30 steps\nEpisode 2224 terminated after 30 steps\nEpisode 2225 terminated after 30 steps\nEpisode 2226 terminated after 30 steps\nEpisode 2227 terminated after 30 steps\nEpisode 2228 terminated after 30 steps\nEpisode 2229 terminated after 30 steps\nEpisode 2230 terminated after 30 steps\nEpisode 2231 terminated after 30 steps\nEpisode 2232 terminated after 30 steps\nEpisode 2233 terminated after 30 steps\nEpisode 2234 terminated after 30 steps\nEpisode 2235 terminated after 30 steps\nEpisode 2236 terminated after 30 steps\nEpisode 2237 terminated after 30 steps\nEpisode 2238 terminated after 30 steps\nEpisode 2239 terminated after 30 steps\nEpisode 2240 terminated after 30 steps\nEpisode 2241 terminated after 30 steps\nEpisode 2242 terminated after 30 steps\nEpisode 2243 terminated after 30 steps\nEpisode 2244 terminated after 30 steps\nEpisode 2245 terminated after 30 steps\nEpisode 2246 terminated after 30 steps\nEpisode 2247 terminated after 30 steps\nEpisode 2248 terminated after 30 steps\nEpisode 2249 terminated after 30 steps\nEpisode 2250 terminated after 30 steps\nEpisode 2251 terminated after 30 steps\nEpisode 2252 terminated after 30 steps\nEpisode 2253 terminated after 30 steps\nEpisode 2254 terminated after 30 steps\nEpisode 2255 terminated after 30 steps\nEpisode 2256 terminated after 30 steps\nEpisode 2257 terminated after 30 steps\nEpisode 2258 terminated after 30 steps\nEpisode 2259 terminated after 30 steps\nEpisode 2260 terminated after 30 steps\nEpisode 2261 terminated after 30 steps\nEpisode 2262 terminated after 30 steps\nEpisode 2263 terminated after 30 steps\nEpisode 2264 terminated after 30 steps\nEpisode 2265 terminated after 30 steps\nEpisode 2266 terminated after 30 steps\nEpisode 2267 terminated after 30 steps\nEpisode 2268 terminated after 30 steps\nEpisode 2269 terminated after 30 steps\nEpisode 2270 terminated after 30 steps\nEpisode 2271 terminated after 30 steps\nEpisode 2272 terminated after 30 steps\nEpisode 2273 terminated after 30 steps\nEpisode 2274 terminated after 30 steps\nEpisode 2275 terminated after 30 steps\nEpisode 2276 terminated after 30 steps\nEpisode 2277 terminated after 30 steps\nEpisode 2278 terminated after 30 steps\nEpisode 2279 terminated after 30 steps\nEpisode 2280 terminated after 30 steps\nEpisode 2281 terminated after 30 steps\nEpisode 2282 terminated after 30 steps\nEpisode 2283 terminated after 30 steps\nEpisode 2284 terminated after 30 steps\nEpisode 2285 terminated after 30 steps\nEpisode 2286 terminated after 30 steps\nEpisode 2287 terminated after 30 steps\nEpisode 2288 terminated after 30 steps\nEpisode 2289 terminated after 30 steps\nEpisode 2290 terminated after 30 steps\nEpisode 2291 terminated after 30 steps\nEpisode 2292 terminated after 30 steps\nEpisode 2293 terminated after 30 steps\nEpisode 2294 terminated after 30 steps\nEpisode 2295 terminated after 30 steps\nEpisode 2296 terminated after 30 steps\nEpisode 2297 terminated after 30 steps\nEpisode 2298 terminated after 30 steps\nEpisode 2299 terminated after 30 steps\nEpisode 2300\nEpisode 2300 terminated after 30 steps\nEpisode 2301 terminated after 30 steps\nEpisode 2302 terminated after 30 steps\nEpisode 2303 terminated after 30 steps\nEpisode 2304 terminated after 30 steps\nEpisode 2305 terminated after 30 steps\nEpisode 2306 terminated after 30 steps\nEpisode 2307 terminated after 30 steps\nEpisode 2308 terminated after 30 steps\nEpisode 2309 terminated after 30 steps\nEpisode 2310 terminated after 30 steps\nEpisode 2311 terminated after 30 steps\nEpisode 2312 terminated after 30 steps\nEpisode 2313 terminated after 30 steps\nEpisode 2314 terminated after 30 steps\nEpisode 2315 terminated after 30 steps\nEpisode 2316 terminated after 30 steps\nEpisode 2317 terminated after 30 steps\nEpisode 2318 terminated after 30 steps\nEpisode 2319 terminated after 30 steps\nEpisode 2320 terminated after 30 steps\nEpisode 2321 terminated after 30 steps\nEpisode 2322 terminated after 30 steps\nEpisode 2323 terminated after 30 steps\nEpisode 2324 terminated after 30 steps\nEpisode 2325 terminated after 30 steps\nEpisode 2326 terminated after 30 steps\nEpisode 2327 terminated after 30 steps\nEpisode 2328 terminated after 30 steps\nEpisode 2329 terminated after 30 steps\nEpisode 2330 terminated after 30 steps\nEpisode 2331 terminated after 30 steps\nEpisode 2332 terminated after 30 steps\nEpisode 2333 terminated after 30 steps\nEpisode 2334 terminated after 30 steps\nEpisode 2335 terminated after 30 steps\nEpisode 2336 terminated after 30 steps\nEpisode 2337 terminated after 30 steps\nEpisode 2338 terminated after 30 steps\nEpisode 2339 terminated after 30 steps\nEpisode 2340 terminated after 30 steps\nEpisode 2341 terminated after 30 steps\nEpisode 2342 terminated after 30 steps\nEpisode 2343 terminated after 30 steps\nEpisode 2344 terminated after 30 steps\nEpisode 2345 terminated after 30 steps\nEpisode 2346 terminated after 30 steps\nEpisode 2347 terminated after 30 steps\nEpisode 2348 terminated after 30 steps\nEpisode 2349 terminated after 30 steps\nEpisode 2350 terminated after 30 steps\nEpisode 2351 terminated after 30 steps\nEpisode 2352 terminated after 30 steps\nEpisode 2353 terminated after 30 steps\nEpisode 2354 terminated after 30 steps\nEpisode 2355 terminated after 30 steps\nEpisode 2356 terminated after 30 steps\nEpisode 2357 terminated after 30 steps\nEpisode 2358 terminated after 30 steps\nEpisode 2359 terminated after 30 steps\nEpisode 2360 terminated after 30 steps\nEpisode 2361 terminated after 30 steps\nEpisode 2362 terminated after 30 steps\nEpisode 2363 terminated after 30 steps\nEpisode 2364 terminated after 30 steps\nEpisode 2365 terminated after 30 steps\nEpisode 2366 terminated after 30 steps\nEpisode 2367 terminated after 30 steps\nEpisode 2368 terminated after 30 steps\nEpisode 2369 terminated after 30 steps\nEpisode 2370 terminated after 30 steps\nEpisode 2371 terminated after 30 steps\nEpisode 2372 terminated after 30 steps\nEpisode 2373 terminated after 30 steps\nEpisode 2374 terminated after 30 steps\nEpisode 2375 terminated after 30 steps\nEpisode 2376 terminated after 30 steps\nEpisode 2377 terminated after 30 steps\nEpisode 2378 terminated after 30 steps\nEpisode 2379 terminated after 30 steps\nEpisode 2380 terminated after 30 steps\nEpisode 2381 terminated after 30 steps\nEpisode 2382 terminated after 30 steps\nEpisode 2383 terminated after 30 steps\nEpisode 2384 terminated after 30 steps\nEpisode 2385 terminated after 30 steps\nEpisode 2386 terminated after 30 steps\nEpisode 2387 terminated after 30 steps\nEpisode 2388 terminated after 30 steps\nEpisode 2389 terminated after 30 steps\nEpisode 2390 terminated after 30 steps\nEpisode 2391 terminated after 30 steps\nEpisode 2392 terminated after 30 steps\nEpisode 2393 terminated after 30 steps\nEpisode 2394 terminated after 30 steps\nEpisode 2395 terminated after 30 steps\nEpisode 2396 terminated after 30 steps\nEpisode 2397 terminated after 30 steps\nEpisode 2398 terminated after 30 steps\nEpisode 2399 terminated after 30 steps\nEpisode 2400\nEpisode 2400 terminated after 30 steps\nEpisode 2401 terminated after 30 steps\nEpisode 2402 terminated after 30 steps\nEpisode 2403 terminated after 30 steps\nEpisode 2404 terminated after 30 steps\nEpisode 2405 terminated after 30 steps\nEpisode 2406 terminated after 30 steps\nEpisode 2407 terminated after 30 steps\nEpisode 2408 terminated after 30 steps\nEpisode 2409 terminated after 30 steps\nEpisode 2410 terminated after 30 steps\nEpisode 2411 terminated after 30 steps\nEpisode 2412 terminated after 30 steps\nEpisode 2413 terminated after 30 steps\nEpisode 2414 terminated after 30 steps\nEpisode 2415 terminated after 30 steps\nEpisode 2416 terminated after 30 steps\nEpisode 2417 terminated after 30 steps\nEpisode 2418 terminated after 30 steps\nEpisode 2419 terminated after 30 steps\nEpisode 2420 terminated after 30 steps\nEpisode 2421 terminated after 30 steps\nEpisode 2422 terminated after 30 steps\nEpisode 2423 terminated after 30 steps\nEpisode 2424 terminated after 30 steps\nEpisode 2425 terminated after 30 steps\nEpisode 2426 terminated after 30 steps\nEpisode 2427 terminated after 30 steps\nEpisode 2428 terminated after 30 steps\nEpisode 2429 terminated after 30 steps\nEpisode 2430 terminated after 30 steps\nEpisode 2431 terminated after 30 steps\nEpisode 2432 terminated after 30 steps\nEpisode 2433 terminated after 30 steps\nEpisode 2434 terminated after 30 steps\nEpisode 2435 terminated after 30 steps\nEpisode 2436 terminated after 30 steps\nEpisode 2437 terminated after 30 steps\nEpisode 2438 terminated after 30 steps\nEpisode 2439 terminated after 30 steps\nEpisode 2440 terminated after 30 steps\nEpisode 2441 terminated after 30 steps\nEpisode 2442 terminated after 30 steps\nEpisode 2443 terminated after 30 steps\nEpisode 2444 terminated after 30 steps\nEpisode 2445 terminated after 30 steps\nEpisode 2446 terminated after 30 steps\nEpisode 2447 terminated after 30 steps\nEpisode 2448 terminated after 30 steps\nEpisode 2449 terminated after 30 steps\nEpisode 2450 terminated after 30 steps\nEpisode 2451 terminated after 30 steps\nEpisode 2452 terminated after 30 steps\nEpisode 2453 terminated after 30 steps\nEpisode 2454 terminated after 30 steps\nEpisode 2455 terminated after 30 steps\nEpisode 2456 terminated after 30 steps\nEpisode 2457 terminated after 30 steps\nEpisode 2458 terminated after 30 steps\nEpisode 2459 terminated after 30 steps\nEpisode 2460 terminated after 30 steps\nEpisode 2461 terminated after 30 steps\nEpisode 2462 terminated after 30 steps\nEpisode 2463 terminated after 30 steps\nEpisode 2464 terminated after 30 steps\nEpisode 2465 terminated after 30 steps\nEpisode 2466 terminated after 30 steps\nEpisode 2467 terminated after 30 steps\nEpisode 2468 terminated after 30 steps\nEpisode 2469 terminated after 30 steps\nEpisode 2470 terminated after 30 steps\nEpisode 2471 terminated after 30 steps\nEpisode 2472 terminated after 30 steps\nEpisode 2473 terminated after 30 steps\nEpisode 2474 terminated after 30 steps\nEpisode 2475 terminated after 30 steps\nEpisode 2476 terminated after 30 steps\nEpisode 2477 terminated after 30 steps\nEpisode 2478 terminated after 30 steps\nEpisode 2479 terminated after 30 steps\nEpisode 2480 terminated after 30 steps\nEpisode 2481 terminated after 30 steps\nEpisode 2482 terminated after 30 steps\nEpisode 2483 terminated after 30 steps\nEpisode 2484 terminated after 30 steps\nEpisode 2485 terminated after 30 steps\nEpisode 2486 terminated after 30 steps\nEpisode 2487 terminated after 30 steps\nEpisode 2488 terminated after 30 steps\nEpisode 2489 terminated after 30 steps\nEpisode 2490 terminated after 30 steps\nEpisode 2491 terminated after 30 steps\nEpisode 2492 terminated after 30 steps\nEpisode 2493 terminated after 30 steps\nEpisode 2494 terminated after 30 steps\nEpisode 2495 terminated after 30 steps\nEpisode 2496 terminated after 30 steps\nEpisode 2497 terminated after 30 steps\nEpisode 2498 terminated after 30 steps\nEpisode 2499 terminated after 30 steps\nEpisode 2500\nEpisode 2500 terminated after 30 steps\nEpisode 2501 terminated after 30 steps\nEpisode 2502 terminated after 30 steps\nEpisode 2503 terminated after 30 steps\nEpisode 2504 terminated after 30 steps\nEpisode 2505 terminated after 30 steps\nEpisode 2506 terminated after 30 steps\nEpisode 2507 terminated after 30 steps\nEpisode 2508 terminated after 30 steps\nEpisode 2509 terminated after 30 steps\nEpisode 2510 terminated after 30 steps\nEpisode 2511 terminated after 30 steps\nEpisode 2512 terminated after 30 steps\nEpisode 2513 terminated after 30 steps\nEpisode 2514 terminated after 30 steps\nEpisode 2515 terminated after 30 steps\nEpisode 2516 terminated after 30 steps\nEpisode 2517 terminated after 30 steps\nEpisode 2518 terminated after 30 steps\nEpisode 2519 terminated after 30 steps\nEpisode 2520 terminated after 30 steps\nEpisode 2521 terminated after 30 steps\nEpisode 2522 terminated after 30 steps\nEpisode 2523 terminated after 30 steps\nEpisode 2524 terminated after 30 steps\nEpisode 2525 terminated after 30 steps\nEpisode 2526 terminated after 30 steps\nEpisode 2527 terminated after 30 steps\nEpisode 2528 terminated after 30 steps\nEpisode 2529 terminated after 30 steps\nEpisode 2530 terminated after 30 steps\nEpisode 2531 terminated after 30 steps\nEpisode 2532 terminated after 30 steps\nEpisode 2533 terminated after 30 steps\nEpisode 2534 terminated after 30 steps\nEpisode 2535 terminated after 30 steps\nEpisode 2536 terminated after 30 steps\nEpisode 2537 terminated after 30 steps\nEpisode 2538 terminated after 30 steps\nEpisode 2539 terminated after 30 steps\nEpisode 2540 terminated after 30 steps\nEpisode 2541 terminated after 30 steps\nEpisode 2542 terminated after 30 steps\nEpisode 2543 terminated after 30 steps\nEpisode 2544 terminated after 30 steps\nEpisode 2545 terminated after 30 steps\nEpisode 2546 terminated after 30 steps\nEpisode 2547 terminated after 30 steps\nEpisode 2548 terminated after 30 steps\nEpisode 2549 terminated after 30 steps\nEpisode 2550 terminated after 30 steps\nEpisode 2551 terminated after 30 steps\nEpisode 2552 terminated after 30 steps\nEpisode 2553 terminated after 30 steps\nEpisode 2554 terminated after 30 steps\nEpisode 2555 terminated after 30 steps\nEpisode 2556 terminated after 30 steps\nEpisode 2557 terminated after 30 steps\nEpisode 2558 terminated after 30 steps\nEpisode 2559 terminated after 30 steps\nEpisode 2560 terminated after 30 steps\nEpisode 2561 terminated after 30 steps\nEpisode 2562 terminated after 30 steps\nEpisode 2563 terminated after 30 steps\nEpisode 2564 terminated after 30 steps\nEpisode 2565 terminated after 30 steps\nEpisode 2566 terminated after 30 steps\nEpisode 2567 terminated after 30 steps\nEpisode 2568 terminated after 30 steps\nEpisode 2569 terminated after 30 steps\nEpisode 2570 terminated after 30 steps\nEpisode 2571 terminated after 30 steps\nEpisode 2572 terminated after 30 steps\nEpisode 2573 terminated after 30 steps\nEpisode 2574 terminated after 30 steps\nEpisode 2575 terminated after 30 steps\nEpisode 2576 terminated after 30 steps\nEpisode 2577 terminated after 30 steps\nEpisode 2578 terminated after 30 steps\nEpisode 2579 terminated after 30 steps\nEpisode 2580 terminated after 30 steps\nEpisode 2581 terminated after 30 steps\nEpisode 2582 terminated after 30 steps\nEpisode 2583 terminated after 30 steps\nEpisode 2584 terminated after 30 steps\nEpisode 2585 terminated after 30 steps\nEpisode 2586 terminated after 30 steps\nEpisode 2587 terminated after 30 steps\nEpisode 2588 terminated after 30 steps\nEpisode 2589 terminated after 30 steps\nEpisode 2590 terminated after 30 steps\nEpisode 2591 terminated after 30 steps\nEpisode 2592 terminated after 30 steps\nEpisode 2593 terminated after 30 steps\nEpisode 2594 terminated after 30 steps\nEpisode 2595 terminated after 30 steps\nEpisode 2596 terminated after 30 steps\nEpisode 2597 terminated after 30 steps\nEpisode 2598 terminated after 30 steps\nEpisode 2599 terminated after 30 steps\nEpisode 2600\nEpisode 2600 terminated after 30 steps\nEpisode 2601 terminated after 30 steps\nEpisode 2602 terminated after 30 steps\nEpisode 2603 terminated after 30 steps\nEpisode 2604 terminated after 30 steps\nEpisode 2605 terminated after 30 steps\nEpisode 2606 terminated after 30 steps\nEpisode 2607 terminated after 30 steps\nEpisode 2608 terminated after 30 steps\nEpisode 2609 terminated after 30 steps\nEpisode 2610 terminated after 30 steps\nEpisode 2611 terminated after 30 steps\nEpisode 2612 terminated after 30 steps\nEpisode 2613 terminated after 30 steps\nEpisode 2614 terminated after 30 steps\nEpisode 2615 terminated after 30 steps\nEpisode 2616 terminated after 30 steps\nEpisode 2617 terminated after 30 steps\nEpisode 2618 terminated after 30 steps\nEpisode 2619 terminated after 30 steps\nEpisode 2620 terminated after 30 steps\nEpisode 2621 terminated after 30 steps\nEpisode 2622 terminated after 30 steps\nEpisode 2623 terminated after 30 steps\nEpisode 2624 terminated after 30 steps\nEpisode 2625 terminated after 30 steps\nEpisode 2626 terminated after 30 steps\nEpisode 2627 terminated after 30 steps\nEpisode 2628 terminated after 30 steps\nEpisode 2629 terminated after 30 steps\nEpisode 2630 terminated after 30 steps\nEpisode 2631 terminated after 30 steps\nEpisode 2632 terminated after 30 steps\nEpisode 2633 terminated after 30 steps\nEpisode 2634 terminated after 30 steps\nEpisode 2635 terminated after 30 steps\nEpisode 2636 terminated after 30 steps\nEpisode 2637 terminated after 30 steps\nEpisode 2638 terminated after 30 steps\nEpisode 2639 terminated after 30 steps\nEpisode 2640 terminated after 30 steps\nEpisode 2641 terminated after 30 steps\nEpisode 2642 terminated after 30 steps\nEpisode 2643 terminated after 30 steps\nEpisode 2644 terminated after 30 steps\nEpisode 2645 terminated after 30 steps\nEpisode 2646 terminated after 30 steps\nEpisode 2647 terminated after 30 steps\nEpisode 2648 terminated after 30 steps\nEpisode 2649 terminated after 30 steps\nEpisode 2650 terminated after 30 steps\nEpisode 2651 terminated after 30 steps\nEpisode 2652 terminated after 30 steps\nEpisode 2653 terminated after 30 steps\nEpisode 2654 terminated after 30 steps\nEpisode 2655 terminated after 30 steps\nEpisode 2656 terminated after 30 steps\nEpisode 2657 terminated after 30 steps\nEpisode 2658 terminated after 30 steps\nEpisode 2659 terminated after 30 steps\nEpisode 2660 terminated after 30 steps\nEpisode 2661 terminated after 30 steps\nEpisode 2662 terminated after 30 steps\nEpisode 2663 terminated after 30 steps\nEpisode 2664 terminated after 30 steps\nEpisode 2665 terminated after 30 steps\nEpisode 2666 terminated after 30 steps\nEpisode 2667 terminated after 30 steps\nEpisode 2668 terminated after 30 steps\nEpisode 2669 terminated after 30 steps\nEpisode 2670 terminated after 30 steps\nEpisode 2671 terminated after 30 steps\nEpisode 2672 terminated after 30 steps\nEpisode 2673 terminated after 30 steps\nEpisode 2674 terminated after 30 steps\nEpisode 2675 terminated after 30 steps\nEpisode 2676 terminated after 30 steps\nEpisode 2677 terminated after 30 steps\nEpisode 2678 terminated after 30 steps\nEpisode 2679 terminated after 30 steps\nEpisode 2680 terminated after 30 steps\nEpisode 2681 terminated after 30 steps\nEpisode 2682 terminated after 30 steps\nEpisode 2683 terminated after 30 steps\nEpisode 2684 terminated after 30 steps\nEpisode 2685 terminated after 30 steps\nEpisode 2686 terminated after 30 steps\nEpisode 2687 terminated after 30 steps\nEpisode 2688 terminated after 30 steps\nEpisode 2689 terminated after 30 steps\nEpisode 2690 terminated after 30 steps\nEpisode 2691 terminated after 30 steps\nEpisode 2692 terminated after 30 steps\nEpisode 2693 terminated after 30 steps\nEpisode 2694 terminated after 30 steps\nEpisode 2695 terminated after 30 steps\nEpisode 2696 terminated after 30 steps\nEpisode 2697 terminated after 30 steps\nEpisode 2698 terminated after 30 steps\nEpisode 2699 terminated after 30 steps\nEpisode 2700\nEpisode 2700 terminated after 30 steps\nEpisode 2701 terminated after 30 steps\nEpisode 2702 terminated after 30 steps\nEpisode 2703 terminated after 30 steps\nEpisode 2704 terminated after 30 steps\nEpisode 2705 terminated after 30 steps\nEpisode 2706 terminated after 30 steps\nEpisode 2707 terminated after 30 steps\nEpisode 2708 terminated after 30 steps\nEpisode 2709 terminated after 30 steps\nEpisode 2710 terminated after 30 steps\nEpisode 2711 terminated after 30 steps\nEpisode 2712 terminated after 30 steps\nEpisode 2713 terminated after 30 steps\nEpisode 2714 terminated after 30 steps\nEpisode 2715 terminated after 30 steps\nEpisode 2716 terminated after 30 steps\nEpisode 2717 terminated after 30 steps\nEpisode 2718 terminated after 30 steps\nEpisode 2719 terminated after 30 steps\nEpisode 2720 terminated after 30 steps\nEpisode 2721 terminated after 30 steps\nEpisode 2722 terminated after 30 steps\nEpisode 2723 terminated after 30 steps\nEpisode 2724 terminated after 30 steps\nEpisode 2725 terminated after 30 steps\nEpisode 2726 terminated after 30 steps\nEpisode 2727 terminated after 30 steps\nEpisode 2728 terminated after 30 steps\nEpisode 2729 terminated after 30 steps\nEpisode 2730 terminated after 30 steps\nEpisode 2731 terminated after 30 steps\nEpisode 2732 terminated after 30 steps\nEpisode 2733 terminated after 30 steps\nEpisode 2734 terminated after 30 steps\nEpisode 2735 terminated after 30 steps\nEpisode 2736 terminated after 30 steps\nEpisode 2737 terminated after 30 steps\nEpisode 2738 terminated after 30 steps\nEpisode 2739 terminated after 30 steps\nEpisode 2740 terminated after 30 steps\nEpisode 2741 terminated after 30 steps\nEpisode 2742 terminated after 30 steps\nEpisode 2743 terminated after 30 steps\nEpisode 2744 terminated after 30 steps\nEpisode 2745 terminated after 30 steps\nEpisode 2746 terminated after 30 steps\nEpisode 2747 terminated after 30 steps\nEpisode 2748 terminated after 30 steps\nEpisode 2749 terminated after 30 steps\nEpisode 2750 terminated after 30 steps\nEpisode 2751 terminated after 30 steps\nEpisode 2752 terminated after 30 steps\nEpisode 2753 terminated after 30 steps\nEpisode 2754 terminated after 30 steps\nEpisode 2755 terminated after 30 steps\nEpisode 2756 terminated after 30 steps\nEpisode 2757 terminated after 30 steps\nEpisode 2758 terminated after 30 steps\nEpisode 2759 terminated after 30 steps\nEpisode 2760 terminated after 30 steps\nEpisode 2761 terminated after 30 steps\nEpisode 2762 terminated after 30 steps\nEpisode 2763 terminated after 30 steps\nEpisode 2764 terminated after 30 steps\nEpisode 2765 terminated after 30 steps\nEpisode 2766 terminated after 30 steps\nEpisode 2767 terminated after 30 steps\nEpisode 2768 terminated after 30 steps\nEpisode 2769 terminated after 30 steps\nEpisode 2770 terminated after 30 steps\nEpisode 2771 terminated after 30 steps\nEpisode 2772 terminated after 30 steps\nEpisode 2773 terminated after 30 steps\nEpisode 2774 terminated after 30 steps\nEpisode 2775 terminated after 30 steps\nEpisode 2776 terminated after 30 steps\nEpisode 2777 terminated after 30 steps\nEpisode 2778 terminated after 30 steps\nEpisode 2779 terminated after 30 steps\nEpisode 2780 terminated after 30 steps\nEpisode 2781 terminated after 30 steps\nEpisode 2782 terminated after 30 steps\nEpisode 2783 terminated after 30 steps\nEpisode 2784 terminated after 30 steps\nEpisode 2785 terminated after 30 steps\nEpisode 2786 terminated after 30 steps\nEpisode 2787 terminated after 30 steps\nEpisode 2788 terminated after 30 steps\nEpisode 2789 terminated after 30 steps\nEpisode 2790 terminated after 30 steps\nEpisode 2791 terminated after 30 steps\nEpisode 2792 terminated after 30 steps\nEpisode 2793 terminated after 30 steps\nEpisode 2794 terminated after 30 steps\nEpisode 2795 terminated after 30 steps\nEpisode 2796 terminated after 30 steps\nEpisode 2797 terminated after 30 steps\nEpisode 2798 terminated after 30 steps\nEpisode 2799 terminated after 30 steps\nEpisode 2800\nEpisode 2800 terminated after 30 steps\nEpisode 2801 terminated after 30 steps\nEpisode 2802 terminated after 30 steps\nEpisode 2803 terminated after 30 steps\nEpisode 2804 terminated after 30 steps\nEpisode 2805 terminated after 30 steps\nEpisode 2806 terminated after 30 steps\nEpisode 2807 terminated after 30 steps\nEpisode 2808 terminated after 30 steps\nEpisode 2809 terminated after 30 steps\nEpisode 2810 terminated after 30 steps\nEpisode 2811 terminated after 30 steps\nEpisode 2812 terminated after 30 steps\nEpisode 2813 terminated after 30 steps\nEpisode 2814 terminated after 30 steps\nEpisode 2815 terminated after 30 steps\nEpisode 2816 terminated after 30 steps\nEpisode 2817 terminated after 30 steps\nEpisode 2818 terminated after 30 steps\nEpisode 2819 terminated after 30 steps\nEpisode 2820 terminated after 30 steps\nEpisode 2821 terminated after 30 steps\nEpisode 2822 terminated after 30 steps\nEpisode 2823 terminated after 30 steps\nEpisode 2824 terminated after 30 steps\nEpisode 2825 terminated after 30 steps\nEpisode 2826 terminated after 30 steps\nEpisode 2827 terminated after 30 steps\nEpisode 2828 terminated after 30 steps\nEpisode 2829 terminated after 30 steps\nEpisode 2830 terminated after 30 steps\nEpisode 2831 terminated after 30 steps\nEpisode 2832 terminated after 30 steps\nEpisode 2833 terminated after 30 steps\nEpisode 2834 terminated after 30 steps\nEpisode 2835 terminated after 30 steps\nEpisode 2836 terminated after 30 steps\nEpisode 2837 terminated after 30 steps\nEpisode 2838 terminated after 30 steps\nEpisode 2839 terminated after 30 steps\nEpisode 2840 terminated after 30 steps\nEpisode 2841 terminated after 30 steps\nEpisode 2842 terminated after 30 steps\nEpisode 2843 terminated after 30 steps\nEpisode 2844 terminated after 30 steps\nEpisode 2845 terminated after 30 steps\nEpisode 2846 terminated after 30 steps\nEpisode 2847 terminated after 30 steps\nEpisode 2848 terminated after 30 steps\nEpisode 2849 terminated after 30 steps\nEpisode 2850 terminated after 30 steps\nEpisode 2851 terminated after 30 steps\nEpisode 2852 terminated after 30 steps\nEpisode 2853 terminated after 30 steps\nEpisode 2854 terminated after 30 steps\nEpisode 2855 terminated after 30 steps\nEpisode 2856 terminated after 30 steps\nEpisode 2857 terminated after 30 steps\nEpisode 2858 terminated after 30 steps\nEpisode 2859 terminated after 30 steps\nEpisode 2860 terminated after 30 steps\nEpisode 2861 terminated after 30 steps\nEpisode 2862 terminated after 30 steps\nEpisode 2863 terminated after 30 steps\nEpisode 2864 terminated after 30 steps\nEpisode 2865 terminated after 30 steps\nEpisode 2866 terminated after 30 steps\nEpisode 2867 terminated after 30 steps\nEpisode 2868 terminated after 30 steps\nEpisode 2869 terminated after 30 steps\nEpisode 2870 terminated after 30 steps\nEpisode 2871 terminated after 30 steps\nEpisode 2872 terminated after 30 steps\nEpisode 2873 terminated after 30 steps\nEpisode 2874 terminated after 30 steps\nEpisode 2875 terminated after 30 steps\nEpisode 2876 terminated after 30 steps\nEpisode 2877 terminated after 30 steps\nEpisode 2878 terminated after 30 steps\nEpisode 2879 terminated after 30 steps\nEpisode 2880 terminated after 30 steps\nEpisode 2881 terminated after 30 steps\nEpisode 2882 terminated after 30 steps\nEpisode 2883 terminated after 30 steps\nEpisode 2884 terminated after 30 steps\nEpisode 2885 terminated after 30 steps\nEpisode 2886 terminated after 30 steps\nEpisode 2887 terminated after 30 steps\nEpisode 2888 terminated after 30 steps\nEpisode 2889 terminated after 30 steps\nEpisode 2890 terminated after 30 steps\nEpisode 2891 terminated after 30 steps\nEpisode 2892 terminated after 30 steps\nEpisode 2893 terminated after 30 steps\nEpisode 2894 terminated after 30 steps\nEpisode 2895 terminated after 30 steps\nEpisode 2896 terminated after 30 steps\nEpisode 2897 terminated after 30 steps\nEpisode 2898 terminated after 30 steps\nEpisode 2899 terminated after 30 steps\nEpisode 2900\nEpisode 2900 terminated after 30 steps\nEpisode 2901 terminated after 30 steps\nEpisode 2902 terminated after 30 steps\nEpisode 2903 terminated after 30 steps\nEpisode 2904 terminated after 30 steps\nEpisode 2905 terminated after 30 steps\nEpisode 2906 terminated after 30 steps\nEpisode 2907 terminated after 30 steps\nEpisode 2908 terminated after 30 steps\nEpisode 2909 terminated after 30 steps\nEpisode 2910 terminated after 30 steps\nEpisode 2911 terminated after 30 steps\nEpisode 2912 terminated after 30 steps\nEpisode 2913 terminated after 30 steps\nEpisode 2914 terminated after 30 steps\nEpisode 2915 terminated after 30 steps\nEpisode 2916 terminated after 30 steps\nEpisode 2917 terminated after 30 steps\nEpisode 2918 terminated after 30 steps\nEpisode 2919 terminated after 30 steps\nEpisode 2920 terminated after 30 steps\nEpisode 2921 terminated after 30 steps\nEpisode 2922 terminated after 30 steps\nEpisode 2923 terminated after 30 steps\nEpisode 2924 terminated after 30 steps\nEpisode 2925 terminated after 30 steps\nEpisode 2926 terminated after 30 steps\nEpisode 2927 terminated after 30 steps\nEpisode 2928 terminated after 30 steps\nEpisode 2929 terminated after 30 steps\nEpisode 2930 terminated after 30 steps\nEpisode 2931 terminated after 30 steps\nEpisode 2932 terminated after 30 steps\nEpisode 2933 terminated after 30 steps\nEpisode 2934 terminated after 30 steps\nEpisode 2935 terminated after 30 steps\nEpisode 2936 terminated after 30 steps\nEpisode 2937 terminated after 30 steps\nEpisode 2938 terminated after 30 steps\nEpisode 2939 terminated after 30 steps\nEpisode 2940 terminated after 30 steps\nEpisode 2941 terminated after 30 steps\nEpisode 2942 terminated after 30 steps\nEpisode 2943 terminated after 30 steps\nEpisode 2944 terminated after 30 steps\nEpisode 2945 terminated after 30 steps\nEpisode 2946 terminated after 30 steps\nEpisode 2947 terminated after 30 steps\nEpisode 2948 terminated after 30 steps\nEpisode 2949 terminated after 30 steps\nEpisode 2950 terminated after 30 steps\nEpisode 2951 terminated after 30 steps\nEpisode 2952 terminated after 30 steps\nEpisode 2953 terminated after 30 steps\nEpisode 2954 terminated after 30 steps\nEpisode 2955 terminated after 30 steps\nEpisode 2956 terminated after 30 steps\nEpisode 2957 terminated after 30 steps\nEpisode 2958 terminated after 30 steps\nEpisode 2959 terminated after 30 steps\nEpisode 2960 terminated after 30 steps\nEpisode 2961 terminated after 30 steps\nEpisode 2962 terminated after 30 steps\nEpisode 2963 terminated after 30 steps\nEpisode 2964 terminated after 30 steps\nEpisode 2965 terminated after 30 steps\nEpisode 2966 terminated after 30 steps\nEpisode 2967 terminated after 30 steps\nEpisode 2968 terminated after 30 steps\nEpisode 2969 terminated after 30 steps\nEpisode 2970 terminated after 30 steps\nEpisode 2971 terminated after 30 steps\nEpisode 2972 terminated after 30 steps\nEpisode 2973 terminated after 30 steps\nEpisode 2974 terminated after 30 steps\nEpisode 2975 terminated after 30 steps\nEpisode 2976 terminated after 30 steps\nEpisode 2977 terminated after 30 steps\nEpisode 2978 terminated after 30 steps\nEpisode 2979 terminated after 30 steps\nEpisode 2980 terminated after 30 steps\nEpisode 2981 terminated after 30 steps\nEpisode 2982 terminated after 30 steps\nEpisode 2983 terminated after 30 steps\nEpisode 2984 terminated after 30 steps\nEpisode 2985 terminated after 30 steps\nEpisode 2986 terminated after 30 steps\nEpisode 2987 terminated after 30 steps\nEpisode 2988 terminated after 30 steps\nEpisode 2989 terminated after 30 steps\nEpisode 2990 terminated after 30 steps\nEpisode 2991 terminated after 30 steps\nEpisode 2992 terminated after 30 steps\nEpisode 2993 terminated after 30 steps\nEpisode 2994 terminated after 30 steps\nEpisode 2995 terminated after 30 steps\nEpisode 2996 terminated after 30 steps\nEpisode 2997 terminated after 30 steps\nEpisode 2998 terminated after 30 steps\nEpisode 2999 terminated after 30 steps\nEpisode 3000\nEpisode 3000 terminated after 30 steps\nEpisode 3001 terminated after 30 steps\nEpisode 3002 terminated after 30 steps\nEpisode 3003 terminated after 30 steps\nEpisode 3004 terminated after 30 steps\nEpisode 3005 terminated after 30 steps\nEpisode 3006 terminated after 30 steps\nEpisode 3007 terminated after 30 steps\nEpisode 3008 terminated after 30 steps\nEpisode 3009 terminated after 30 steps\nEpisode 3010 terminated after 30 steps\nEpisode 3011 terminated after 30 steps\nEpisode 3012 terminated after 30 steps\nEpisode 3013 terminated after 30 steps\nEpisode 3014 terminated after 30 steps\nEpisode 3015 terminated after 30 steps\nEpisode 3016 terminated after 30 steps\nEpisode 3017 terminated after 30 steps\nEpisode 3018 terminated after 30 steps\nEpisode 3019 terminated after 30 steps\nEpisode 3020 terminated after 30 steps\nEpisode 3021 terminated after 30 steps\nEpisode 3022 terminated after 30 steps\nEpisode 3023 terminated after 30 steps\nEpisode 3024 terminated after 30 steps\nEpisode 3025 terminated after 30 steps\nEpisode 3026 terminated after 30 steps\nEpisode 3027 terminated after 30 steps\nEpisode 3028 terminated after 30 steps\nEpisode 3029 terminated after 30 steps\nEpisode 3030 terminated after 30 steps\nEpisode 3031 terminated after 30 steps\nEpisode 3032 terminated after 30 steps\nEpisode 3033 terminated after 30 steps\nEpisode 3034 terminated after 30 steps\nEpisode 3035 terminated after 30 steps\nEpisode 3036 terminated after 30 steps\nEpisode 3037 terminated after 30 steps\nEpisode 3038 terminated after 30 steps\nEpisode 3039 terminated after 30 steps\nEpisode 3040 terminated after 30 steps\nEpisode 3041 terminated after 30 steps\nEpisode 3042 terminated after 30 steps\nEpisode 3043 terminated after 30 steps\nEpisode 3044 terminated after 30 steps\nEpisode 3045 terminated after 30 steps\nEpisode 3046 terminated after 30 steps\nEpisode 3047 terminated after 30 steps\nEpisode 3048 terminated after 30 steps\nEpisode 3049 terminated after 30 steps\nEpisode 3050 terminated after 30 steps\nEpisode 3051 terminated after 30 steps\nEpisode 3052 terminated after 30 steps\nEpisode 3053 terminated after 30 steps\nEpisode 3054 terminated after 30 steps\nEpisode 3055 terminated after 30 steps\nEpisode 3056 terminated after 30 steps\nEpisode 3057 terminated after 30 steps\nEpisode 3058 terminated after 30 steps\nEpisode 3059 terminated after 30 steps\nEpisode 3060 terminated after 30 steps\nEpisode 3061 terminated after 30 steps\nEpisode 3062 terminated after 30 steps\nEpisode 3063 terminated after 30 steps\nEpisode 3064 terminated after 30 steps\nEpisode 3065 terminated after 30 steps\nEpisode 3066 terminated after 30 steps\nEpisode 3067 terminated after 30 steps\nEpisode 3068 terminated after 30 steps\nEpisode 3069 terminated after 30 steps\nEpisode 3070 terminated after 30 steps\nEpisode 3071 terminated after 30 steps\nEpisode 3072 terminated after 30 steps\nEpisode 3073 terminated after 30 steps\nEpisode 3074 terminated after 30 steps\nEpisode 3075 terminated after 30 steps\nEpisode 3076 terminated after 30 steps\nEpisode 3077 terminated after 30 steps\nEpisode 3078 terminated after 30 steps\nEpisode 3079 terminated after 30 steps\nEpisode 3080 terminated after 30 steps\nEpisode 3081 terminated after 30 steps\nEpisode 3082 terminated after 30 steps\nEpisode 3083 terminated after 30 steps\nEpisode 3084 terminated after 30 steps\nEpisode 3085 terminated after 30 steps\nEpisode 3086 terminated after 30 steps\nEpisode 3087 terminated after 30 steps\nEpisode 3088 terminated after 30 steps\nEpisode 3089 terminated after 30 steps\nEpisode 3090 terminated after 30 steps\nEpisode 3091 terminated after 30 steps\nEpisode 3092 terminated after 30 steps\nEpisode 3093 terminated after 30 steps\nEpisode 3094 terminated after 30 steps\nEpisode 3095 terminated after 30 steps\nEpisode 3096 terminated after 30 steps\nEpisode 3097 terminated after 30 steps\nEpisode 3098 terminated after 30 steps\nEpisode 3099 terminated after 30 steps\nEpisode 3100\nEpisode 3100 terminated after 30 steps\nEpisode 3101 terminated after 30 steps\nEpisode 3102 terminated after 30 steps\nEpisode 3103 terminated after 30 steps\nEpisode 3104 terminated after 30 steps\nEpisode 3105 terminated after 30 steps\nEpisode 3106 terminated after 30 steps\nEpisode 3107 terminated after 30 steps\nEpisode 3108 terminated after 30 steps\nEpisode 3109 terminated after 30 steps\nEpisode 3110 terminated after 30 steps\nEpisode 3111 terminated after 30 steps\nEpisode 3112 terminated after 30 steps\nEpisode 3113 terminated after 30 steps\nEpisode 3114 terminated after 30 steps\nEpisode 3115 terminated after 30 steps\nEpisode 3116 terminated after 30 steps\nEpisode 3117 terminated after 30 steps\nEpisode 3118 terminated after 30 steps\nEpisode 3119 terminated after 30 steps\nEpisode 3120 terminated after 30 steps\nEpisode 3121 terminated after 30 steps\nEpisode 3122 terminated after 30 steps\nEpisode 3123 terminated after 30 steps\nEpisode 3124 terminated after 30 steps\nEpisode 3125 terminated after 30 steps\nEpisode 3126 terminated after 30 steps\nEpisode 3127 terminated after 30 steps\nEpisode 3128 terminated after 30 steps\nEpisode 3129 terminated after 30 steps\nEpisode 3130 terminated after 30 steps\nEpisode 3131 terminated after 30 steps\nEpisode 3132 terminated after 30 steps\nEpisode 3133 terminated after 30 steps\nEpisode 3134 terminated after 30 steps\nEpisode 3135 terminated after 30 steps\nEpisode 3136 terminated after 30 steps\nEpisode 3137 terminated after 30 steps\nEpisode 3138 terminated after 30 steps\nEpisode 3139 terminated after 30 steps\nEpisode 3140 terminated after 30 steps\nEpisode 3141 terminated after 30 steps\nEpisode 3142 terminated after 30 steps\nEpisode 3143 terminated after 30 steps\nEpisode 3144 terminated after 30 steps\nEpisode 3145 terminated after 30 steps\nEpisode 3146 terminated after 30 steps\nEpisode 3147 terminated after 30 steps\nEpisode 3148 terminated after 30 steps\nEpisode 3149 terminated after 30 steps\nEpisode 3150 terminated after 30 steps\nEpisode 3151 terminated after 30 steps\nEpisode 3152 terminated after 30 steps\nEpisode 3153 terminated after 30 steps\nEpisode 3154 terminated after 30 steps\nEpisode 3155 terminated after 30 steps\nEpisode 3156 terminated after 30 steps\nEpisode 3157 terminated after 30 steps\nEpisode 3158 terminated after 30 steps\nEpisode 3159 terminated after 30 steps\nEpisode 3160 terminated after 30 steps\nEpisode 3161 terminated after 30 steps\nEpisode 3162 terminated after 30 steps\nEpisode 3163 terminated after 30 steps\nEpisode 3164 terminated after 30 steps\nEpisode 3165 terminated after 30 steps\nEpisode 3166 terminated after 30 steps\nEpisode 3167 terminated after 30 steps\nEpisode 3168 terminated after 30 steps\nEpisode 3169 terminated after 30 steps\nEpisode 3170 terminated after 30 steps\nEpisode 3171 terminated after 30 steps\nEpisode 3172 terminated after 30 steps\nEpisode 3173 terminated after 30 steps\nEpisode 3174 terminated after 30 steps\nEpisode 3175 terminated after 30 steps\nEpisode 3176 terminated after 30 steps\nEpisode 3177 terminated after 30 steps\nEpisode 3178 terminated after 30 steps\nEpisode 3179 terminated after 30 steps\nEpisode 3180 terminated after 30 steps\nEpisode 3181 terminated after 30 steps\nEpisode 3182 terminated after 30 steps\nEpisode 3183 terminated after 30 steps\nEpisode 3184 terminated after 30 steps\nEpisode 3185 terminated after 30 steps\nEpisode 3186 terminated after 30 steps\nEpisode 3187 terminated after 30 steps\nEpisode 3188 terminated after 30 steps\nEpisode 3189 terminated after 30 steps\nEpisode 3190 terminated after 30 steps\nEpisode 3191 terminated after 30 steps\nEpisode 3192 terminated after 30 steps\nEpisode 3193 terminated after 30 steps\nEpisode 3194 terminated after 30 steps\nEpisode 3195 terminated after 30 steps\nEpisode 3196 terminated after 30 steps\nEpisode 3197 terminated after 30 steps\nEpisode 3198 terminated after 30 steps\nEpisode 3199 terminated after 30 steps\nEpisode 3200\nEpisode 3200 terminated after 30 steps\nEpisode 3201 terminated after 30 steps\nEpisode 3202 terminated after 30 steps\nEpisode 3203 terminated after 30 steps\nEpisode 3204 terminated after 30 steps\nEpisode 3205 terminated after 30 steps\nEpisode 3206 terminated after 30 steps\nEpisode 3207 terminated after 30 steps\nEpisode 3208 terminated after 30 steps\nEpisode 3209 terminated after 30 steps\nEpisode 3210 terminated after 30 steps\nEpisode 3211 terminated after 30 steps\nEpisode 3212 terminated after 30 steps\nEpisode 3213 terminated after 30 steps\nEpisode 3214 terminated after 30 steps\nEpisode 3215 terminated after 30 steps\nEpisode 3216 terminated after 30 steps\nEpisode 3217 terminated after 30 steps\nEpisode 3218 terminated after 30 steps\nEpisode 3219 terminated after 30 steps\nEpisode 3220 terminated after 30 steps\nEpisode 3221 terminated after 30 steps\nEpisode 3222 terminated after 30 steps\nEpisode 3223 terminated after 30 steps\nEpisode 3224 terminated after 30 steps\nEpisode 3225 terminated after 30 steps\nEpisode 3226 terminated after 30 steps\nEpisode 3227 terminated after 30 steps\nEpisode 3228 terminated after 30 steps\nEpisode 3229 terminated after 30 steps\nEpisode 3230 terminated after 30 steps\nEpisode 3231 terminated after 30 steps\nEpisode 3232 terminated after 30 steps\nEpisode 3233 terminated after 30 steps\nEpisode 3234 terminated after 30 steps\nEpisode 3235 terminated after 30 steps\nEpisode 3236 terminated after 30 steps\nEpisode 3237 terminated after 30 steps\nEpisode 3238 terminated after 30 steps\nEpisode 3239 terminated after 30 steps\nEpisode 3240 terminated after 30 steps\nEpisode 3241 terminated after 30 steps\nEpisode 3242 terminated after 30 steps\nEpisode 3243 terminated after 30 steps\nEpisode 3244 terminated after 30 steps\nEpisode 3245 terminated after 30 steps\nEpisode 3246 terminated after 30 steps\nEpisode 3247 terminated after 30 steps\nEpisode 3248 terminated after 30 steps\nEpisode 3249 terminated after 30 steps\nEpisode 3250 terminated after 30 steps\nEpisode 3251 terminated after 30 steps\nEpisode 3252 terminated after 30 steps\nEpisode 3253 terminated after 30 steps\nEpisode 3254 terminated after 30 steps\nEpisode 3255 terminated after 30 steps\nEpisode 3256 terminated after 30 steps\nEpisode 3257 terminated after 30 steps\nEpisode 3258 terminated after 30 steps\nEpisode 3259 terminated after 30 steps\nEpisode 3260 terminated after 30 steps\nEpisode 3261 terminated after 30 steps\nEpisode 3262 terminated after 30 steps\nEpisode 3263 terminated after 30 steps\nEpisode 3264 terminated after 30 steps\nEpisode 3265 terminated after 30 steps\nEpisode 3266 terminated after 30 steps\nEpisode 3267 terminated after 30 steps\nEpisode 3268 terminated after 30 steps\nEpisode 3269 terminated after 30 steps\nEpisode 3270 terminated after 30 steps\nEpisode 3271 terminated after 30 steps\nEpisode 3272 terminated after 30 steps\nEpisode 3273 terminated after 30 steps\nEpisode 3274 terminated after 30 steps\nEpisode 3275 terminated after 30 steps\nEpisode 3276 terminated after 30 steps\nEpisode 3277 terminated after 30 steps\nEpisode 3278 terminated after 30 steps\nEpisode 3279 terminated after 30 steps\nEpisode 3280 terminated after 30 steps\nEpisode 3281 terminated after 30 steps\nEpisode 3282 terminated after 30 steps\nEpisode 3283 terminated after 30 steps\nEpisode 3284 terminated after 30 steps\nEpisode 3285 terminated after 30 steps\nEpisode 3286 terminated after 30 steps\nEpisode 3287 terminated after 30 steps\nEpisode 3288 terminated after 30 steps\nEpisode 3289 terminated after 30 steps\nEpisode 3290 terminated after 30 steps\nEpisode 3291 terminated after 30 steps\nEpisode 3292 terminated after 30 steps\nEpisode 3293 terminated after 30 steps\nEpisode 3294 terminated after 30 steps\nEpisode 3295 terminated after 30 steps\nEpisode 3296 terminated after 30 steps\nEpisode 3297 terminated after 30 steps\nEpisode 3298 terminated after 30 steps\nEpisode 3299 terminated after 30 steps\nEpisode 3300\nEpisode 3300 terminated after 30 steps\nEpisode 3301 terminated after 30 steps\nEpisode 3302 terminated after 30 steps\nEpisode 3303 terminated after 30 steps\nEpisode 3304 terminated after 30 steps\nEpisode 3305 terminated after 30 steps\nEpisode 3306 terminated after 30 steps\nEpisode 3307 terminated after 30 steps\nEpisode 3308 terminated after 30 steps\nEpisode 3309 terminated after 30 steps\nEpisode 3310 terminated after 30 steps\nEpisode 3311 terminated after 30 steps\nEpisode 3312 terminated after 30 steps\nEpisode 3313 terminated after 30 steps\nEpisode 3314 terminated after 30 steps\nEpisode 3315 terminated after 30 steps\nEpisode 3316 terminated after 30 steps\nEpisode 3317 terminated after 30 steps\nEpisode 3318 terminated after 30 steps\nEpisode 3319 terminated after 30 steps\nEpisode 3320 terminated after 30 steps\nEpisode 3321 terminated after 30 steps\nEpisode 3322 terminated after 30 steps\nEpisode 3323 terminated after 30 steps\nEpisode 3324 terminated after 30 steps\nEpisode 3325 terminated after 30 steps\nEpisode 3326 terminated after 30 steps\nEpisode 3327 terminated after 30 steps\nEpisode 3328 terminated after 30 steps\nEpisode 3329 terminated after 30 steps\nEpisode 3330 terminated after 30 steps\nEpisode 3331 terminated after 30 steps\nEpisode 3332 terminated after 30 steps\nEpisode 3333 terminated after 30 steps\nEpisode 3334 terminated after 30 steps\nEpisode 3335 terminated after 30 steps\nEpisode 3336 terminated after 30 steps\nEpisode 3337 terminated after 30 steps\nEpisode 3338 terminated after 30 steps\nEpisode 3339 terminated after 30 steps\nEpisode 3340 terminated after 30 steps\nEpisode 3341 terminated after 30 steps\nEpisode 3342 terminated after 30 steps\nEpisode 3343 terminated after 30 steps\nEpisode 3344 terminated after 30 steps\nEpisode 3345 terminated after 30 steps\nEpisode 3346 terminated after 30 steps\nEpisode 3347 terminated after 30 steps\nEpisode 3348 terminated after 30 steps\nEpisode 3349 terminated after 30 steps\nEpisode 3350 terminated after 30 steps\nEpisode 3351 terminated after 30 steps\nEpisode 3352 terminated after 30 steps\nEpisode 3353 terminated after 30 steps\nEpisode 3354 terminated after 30 steps\nEpisode 3355 terminated after 30 steps\nEpisode 3356 terminated after 30 steps\nEpisode 3357 terminated after 30 steps\nEpisode 3358 terminated after 30 steps\nEpisode 3359 terminated after 30 steps\nEpisode 3360 terminated after 30 steps\nEpisode 3361 terminated after 30 steps\nEpisode 3362 terminated after 30 steps\nEpisode 3363 terminated after 30 steps\nEpisode 3364 terminated after 30 steps\nEpisode 3365 terminated after 30 steps\nEpisode 3366 terminated after 30 steps\nEpisode 3367 terminated after 30 steps\nEpisode 3368 terminated after 30 steps\nEpisode 3369 terminated after 30 steps\nEpisode 3370 terminated after 30 steps\nEpisode 3371 terminated after 30 steps\nEpisode 3372 terminated after 30 steps\nEpisode 3373 terminated after 30 steps\nEpisode 3374 terminated after 30 steps\nEpisode 3375 terminated after 30 steps\nEpisode 3376 terminated after 30 steps\nEpisode 3377 terminated after 30 steps\nEpisode 3378 terminated after 30 steps\nEpisode 3379 terminated after 30 steps\nEpisode 3380 terminated after 30 steps\nEpisode 3381 terminated after 30 steps\nEpisode 3382 terminated after 30 steps\nEpisode 3383 terminated after 30 steps\nEpisode 3384 terminated after 30 steps\nEpisode 3385 terminated after 30 steps\nEpisode 3386 terminated after 30 steps\nEpisode 3387 terminated after 30 steps\nEpisode 3388 terminated after 30 steps\nEpisode 3389 terminated after 30 steps\nEpisode 3390 terminated after 30 steps\nEpisode 3391 terminated after 30 steps\nEpisode 3392 terminated after 30 steps\nEpisode 3393 terminated after 30 steps\nEpisode 3394 terminated after 30 steps\nEpisode 3395 terminated after 30 steps\nEpisode 3396 terminated after 30 steps\nEpisode 3397 terminated after 30 steps\nEpisode 3398 terminated after 30 steps\nEpisode 3399 terminated after 30 steps\nEpisode 3400\nEpisode 3400 terminated after 30 steps\nEpisode 3401 terminated after 30 steps\nEpisode 3402 terminated after 30 steps\nEpisode 3403 terminated after 30 steps\nEpisode 3404 terminated after 30 steps\nEpisode 3405 terminated after 30 steps\nEpisode 3406 terminated after 30 steps\nEpisode 3407 terminated after 30 steps\nEpisode 3408 terminated after 30 steps\nEpisode 3409 terminated after 30 steps\nEpisode 3410 terminated after 30 steps\nEpisode 3411 terminated after 30 steps\nEpisode 3412 terminated after 30 steps\nEpisode 3413 terminated after 30 steps\nEpisode 3414 terminated after 30 steps\nEpisode 3415 terminated after 30 steps\nEpisode 3416 terminated after 30 steps\nEpisode 3417 terminated after 30 steps\nEpisode 3418 terminated after 30 steps\nEpisode 3419 terminated after 30 steps\nEpisode 3420 terminated after 30 steps\nEpisode 3421 terminated after 30 steps\nEpisode 3422 terminated after 30 steps\nEpisode 3423 terminated after 30 steps\nEpisode 3424 terminated after 30 steps\nEpisode 3425 terminated after 30 steps\nEpisode 3426 terminated after 30 steps\nEpisode 3427 terminated after 30 steps\nEpisode 3428 terminated after 30 steps\nEpisode 3429 terminated after 30 steps\nEpisode 3430 terminated after 30 steps\nEpisode 3431 terminated after 30 steps\nEpisode 3432 terminated after 30 steps\nEpisode 3433 terminated after 30 steps\nEpisode 3434 terminated after 30 steps\nEpisode 3435 terminated after 30 steps\nEpisode 3436 terminated after 30 steps\nEpisode 3437 terminated after 30 steps\nEpisode 3438 terminated after 30 steps\nEpisode 3439 terminated after 30 steps\nEpisode 3440 terminated after 30 steps\nEpisode 3441 terminated after 30 steps\nEpisode 3442 terminated after 30 steps\nEpisode 3443 terminated after 30 steps\nEpisode 3444 terminated after 30 steps\nEpisode 3445 terminated after 30 steps\nEpisode 3446 terminated after 30 steps\nEpisode 3447 terminated after 30 steps\nEpisode 3448 terminated after 30 steps\nEpisode 3449 terminated after 30 steps\nEpisode 3450 terminated after 30 steps\nEpisode 3451 terminated after 30 steps\nEpisode 3452 terminated after 30 steps\nEpisode 3453 terminated after 30 steps\nEpisode 3454 terminated after 30 steps\nEpisode 3455 terminated after 30 steps\nEpisode 3456 terminated after 30 steps\nEpisode 3457 terminated after 30 steps\nEpisode 3458 terminated after 30 steps\nEpisode 3459 terminated after 30 steps\nEpisode 3460 terminated after 30 steps\nEpisode 3461 terminated after 30 steps\nEpisode 3462 terminated after 30 steps\nEpisode 3463 terminated after 30 steps\nEpisode 3464 terminated after 30 steps\nEpisode 3465 terminated after 30 steps\nEpisode 3466 terminated after 30 steps\nEpisode 3467 terminated after 30 steps\nEpisode 3468 terminated after 30 steps\nEpisode 3469 terminated after 30 steps\nEpisode 3470 terminated after 30 steps\nEpisode 3471 terminated after 30 steps\nEpisode 3472 terminated after 30 steps\nEpisode 3473 terminated after 30 steps\nEpisode 3474 terminated after 30 steps\nEpisode 3475 terminated after 30 steps\nEpisode 3476 terminated after 30 steps\nEpisode 3477 terminated after 30 steps\nEpisode 3478 terminated after 30 steps\nEpisode 3479 terminated after 30 steps\nEpisode 3480 terminated after 30 steps\nEpisode 3481 terminated after 30 steps\nEpisode 3482 terminated after 30 steps\nEpisode 3483 terminated after 30 steps\nEpisode 3484 terminated after 30 steps\nEpisode 3485 terminated after 30 steps\nEpisode 3486 terminated after 30 steps\nEpisode 3487 terminated after 30 steps\nEpisode 3488 terminated after 30 steps\nEpisode 3489 terminated after 30 steps\nEpisode 3490 terminated after 30 steps\nEpisode 3491 terminated after 30 steps\nEpisode 3492 terminated after 30 steps\nEpisode 3493 terminated after 30 steps\nEpisode 3494 terminated after 30 steps\nEpisode 3495 terminated after 30 steps\nEpisode 3496 terminated after 30 steps\nEpisode 3497 terminated after 30 steps\nEpisode 3498 terminated after 30 steps\nEpisode 3499 terminated after 30 steps\nEpisode 3500\nEpisode 3500 terminated after 30 steps\nEpisode 3501 terminated after 30 steps\nEpisode 3502 terminated after 30 steps\nEpisode 3503 terminated after 30 steps\nEpisode 3504 terminated after 30 steps\nEpisode 3505 terminated after 30 steps\nEpisode 3506 terminated after 30 steps\nEpisode 3507 terminated after 30 steps\nEpisode 3508 terminated after 30 steps\nEpisode 3509 terminated after 30 steps\nEpisode 3510 terminated after 30 steps\nEpisode 3511 terminated after 30 steps\nEpisode 3512 terminated after 30 steps\nEpisode 3513 terminated after 30 steps\nEpisode 3514 terminated after 30 steps\nEpisode 3515 terminated after 30 steps\nEpisode 3516 terminated after 30 steps\nEpisode 3517 terminated after 30 steps\nEpisode 3518 terminated after 30 steps\nEpisode 3519 terminated after 30 steps\nEpisode 3520 terminated after 30 steps\nEpisode 3521 terminated after 30 steps\nEpisode 3522 terminated after 30 steps\nEpisode 3523 terminated after 30 steps\nEpisode 3524 terminated after 30 steps\nEpisode 3525 terminated after 30 steps\nEpisode 3526 terminated after 30 steps\nEpisode 3527 terminated after 30 steps\nEpisode 3528 terminated after 30 steps\nEpisode 3529 terminated after 30 steps\nEpisode 3530 terminated after 30 steps\nEpisode 3531 terminated after 30 steps\nEpisode 3532 terminated after 30 steps\nEpisode 3533 terminated after 30 steps\nEpisode 3534 terminated after 30 steps\nEpisode 3535 terminated after 30 steps\nEpisode 3536 terminated after 30 steps\nEpisode 3537 terminated after 30 steps\nEpisode 3538 terminated after 30 steps\nEpisode 3539 terminated after 30 steps\nEpisode 3540 terminated after 30 steps\nEpisode 3541 terminated after 30 steps\nEpisode 3542 terminated after 30 steps\nEpisode 3543 terminated after 30 steps\nEpisode 3544 terminated after 30 steps\nEpisode 3545 terminated after 30 steps\nEpisode 3546 terminated after 30 steps\nEpisode 3547 terminated after 30 steps\nEpisode 3548 terminated after 30 steps\nEpisode 3549 terminated after 30 steps\nEpisode 3550 terminated after 30 steps\nEpisode 3551 terminated after 30 steps\nEpisode 3552 terminated after 30 steps\nEpisode 3553 terminated after 30 steps\nEpisode 3554 terminated after 30 steps\nEpisode 3555 terminated after 30 steps\nEpisode 3556 terminated after 30 steps\nEpisode 3557 terminated after 30 steps\nEpisode 3558 terminated after 30 steps\nEpisode 3559 terminated after 30 steps\nEpisode 3560 terminated after 30 steps\nEpisode 3561 terminated after 30 steps\nEpisode 3562 terminated after 30 steps\nEpisode 3563 terminated after 30 steps\nEpisode 3564 terminated after 30 steps\nEpisode 3565 terminated after 30 steps\nEpisode 3566 terminated after 30 steps\nEpisode 3567 terminated after 30 steps\nEpisode 3568 terminated after 30 steps\nEpisode 3569 terminated after 30 steps\nEpisode 3570 terminated after 30 steps\nEpisode 3571 terminated after 30 steps\nEpisode 3572 terminated after 30 steps\nEpisode 3573 terminated after 30 steps\nEpisode 3574 terminated after 30 steps\nEpisode 3575 terminated after 30 steps\nEpisode 3576 terminated after 30 steps\nEpisode 3577 terminated after 30 steps\nEpisode 3578 terminated after 30 steps\nEpisode 3579 terminated after 30 steps\nEpisode 3580 terminated after 30 steps\nEpisode 3581 terminated after 30 steps\nEpisode 3582 terminated after 30 steps\nEpisode 3583 terminated after 30 steps\nEpisode 3584 terminated after 30 steps\nEpisode 3585 terminated after 30 steps\nEpisode 3586 terminated after 30 steps\nEpisode 3587 terminated after 30 steps\nEpisode 3588 terminated after 30 steps\nEpisode 3589 terminated after 30 steps\nEpisode 3590 terminated after 30 steps\nEpisode 3591 terminated after 30 steps\nEpisode 3592 terminated after 30 steps\nEpisode 3593 terminated after 30 steps\nEpisode 3594 terminated after 30 steps\nEpisode 3595 terminated after 30 steps\nEpisode 3596 terminated after 30 steps\nEpisode 3597 terminated after 30 steps\nEpisode 3598 terminated after 30 steps\nEpisode 3599 terminated after 30 steps\nEpisode 3600\nEpisode 3600 terminated after 30 steps\nEpisode 3601 terminated after 30 steps\nEpisode 3602 terminated after 30 steps\nEpisode 3603 terminated after 30 steps\nEpisode 3604 terminated after 30 steps\nEpisode 3605 terminated after 30 steps\nEpisode 3606 terminated after 30 steps\nEpisode 3607 terminated after 30 steps\nEpisode 3608 terminated after 30 steps\nEpisode 3609 terminated after 30 steps\nEpisode 3610 terminated after 30 steps\nEpisode 3611 terminated after 30 steps\nEpisode 3612 terminated after 30 steps\nEpisode 3613 terminated after 30 steps\nEpisode 3614 terminated after 30 steps\nEpisode 3615 terminated after 30 steps\nEpisode 3616 terminated after 30 steps\nEpisode 3617 terminated after 30 steps\nEpisode 3618 terminated after 30 steps\nEpisode 3619 terminated after 30 steps\nEpisode 3620 terminated after 30 steps\nEpisode 3621 terminated after 30 steps\nEpisode 3622 terminated after 30 steps\nEpisode 3623 terminated after 30 steps\nEpisode 3624 terminated after 30 steps\nEpisode 3625 terminated after 30 steps\nEpisode 3626 terminated after 30 steps\nEpisode 3627 terminated after 30 steps\nEpisode 3628 terminated after 30 steps\nEpisode 3629 terminated after 30 steps\nEpisode 3630 terminated after 30 steps\nEpisode 3631 terminated after 30 steps\nEpisode 3632 terminated after 30 steps\nEpisode 3633 terminated after 30 steps\nEpisode 3634 terminated after 30 steps\nEpisode 3635 terminated after 30 steps\nEpisode 3636 terminated after 30 steps\nEpisode 3637 terminated after 30 steps\nEpisode 3638 terminated after 30 steps\nEpisode 3639 terminated after 30 steps\nEpisode 3640 terminated after 30 steps\nEpisode 3641 terminated after 30 steps\nEpisode 3642 terminated after 30 steps\nEpisode 3643 terminated after 30 steps\nEpisode 3644 terminated after 30 steps\nEpisode 3645 terminated after 30 steps\nEpisode 3646 terminated after 30 steps\nEpisode 3647 terminated after 30 steps\nEpisode 3648 terminated after 30 steps\nEpisode 3649 terminated after 30 steps\nEpisode 3650 terminated after 30 steps\nEpisode 3651 terminated after 30 steps\nEpisode 3652 terminated after 30 steps\nEpisode 3653 terminated after 30 steps\nEpisode 3654 terminated after 30 steps\nEpisode 3655 terminated after 30 steps\nEpisode 3656 terminated after 30 steps\nEpisode 3657 terminated after 30 steps\nEpisode 3658 terminated after 30 steps\nEpisode 3659 terminated after 30 steps\nEpisode 3660 terminated after 30 steps\nEpisode 3661 terminated after 30 steps\nEpisode 3662 terminated after 30 steps\nEpisode 3663 terminated after 30 steps\nEpisode 3664 terminated after 30 steps\nEpisode 3665 terminated after 30 steps\nEpisode 3666 terminated after 30 steps\nEpisode 3667 terminated after 30 steps\nEpisode 3668 terminated after 30 steps\nEpisode 3669 terminated after 30 steps\nEpisode 3670 terminated after 30 steps\nEpisode 3671 terminated after 30 steps\nEpisode 3672 terminated after 30 steps\nEpisode 3673 terminated after 30 steps\nEpisode 3674 terminated after 30 steps\nEpisode 3675 terminated after 30 steps\nEpisode 3676 terminated after 30 steps\nEpisode 3677 terminated after 30 steps\nEpisode 3678 terminated after 30 steps\nEpisode 3679 terminated after 30 steps\nEpisode 3680 terminated after 30 steps\nEpisode 3681 terminated after 30 steps\nEpisode 3682 terminated after 30 steps\nEpisode 3683 terminated after 30 steps\nEpisode 3684 terminated after 30 steps\nEpisode 3685 terminated after 30 steps\nEpisode 3686 terminated after 30 steps\nEpisode 3687 terminated after 30 steps\nEpisode 3688 terminated after 30 steps\nEpisode 3689 terminated after 30 steps\nEpisode 3690 terminated after 30 steps\nEpisode 3691 terminated after 30 steps\nEpisode 3692 terminated after 30 steps\nEpisode 3693 terminated after 30 steps\nEpisode 3694 terminated after 30 steps\nEpisode 3695 terminated after 30 steps\nEpisode 3696 terminated after 30 steps\nEpisode 3697 terminated after 30 steps\nEpisode 3698 terminated after 30 steps\nEpisode 3699 terminated after 30 steps\nEpisode 3700\nEpisode 3700 terminated after 30 steps\nEpisode 3701 terminated after 30 steps\nEpisode 3702 terminated after 30 steps\nEpisode 3703 terminated after 30 steps\nEpisode 3704 terminated after 30 steps\nEpisode 3705 terminated after 30 steps\nEpisode 3706 terminated after 30 steps\nEpisode 3707 terminated after 30 steps\nEpisode 3708 terminated after 30 steps\nEpisode 3709 terminated after 30 steps\nEpisode 3710 terminated after 30 steps\nEpisode 3711 terminated after 30 steps\nEpisode 3712 terminated after 30 steps\nEpisode 3713 terminated after 30 steps\nEpisode 3714 terminated after 30 steps\nEpisode 3715 terminated after 30 steps\nEpisode 3716 terminated after 30 steps\nEpisode 3717 terminated after 30 steps\nEpisode 3718 terminated after 30 steps\nEpisode 3719 terminated after 30 steps\nEpisode 3720 terminated after 30 steps\nEpisode 3721 terminated after 30 steps\nEpisode 3722 terminated after 30 steps\nEpisode 3723 terminated after 30 steps\nEpisode 3724 terminated after 30 steps\nEpisode 3725 terminated after 30 steps\nEpisode 3726 terminated after 30 steps\nEpisode 3727 terminated after 30 steps\nEpisode 3728 terminated after 30 steps\nEpisode 3729 terminated after 30 steps\nEpisode 3730 terminated after 30 steps\nEpisode 3731 terminated after 30 steps\nEpisode 3732 terminated after 30 steps\nEpisode 3733 terminated after 30 steps\nEpisode 3734 terminated after 30 steps\nEpisode 3735 terminated after 30 steps\nEpisode 3736 terminated after 30 steps\nEpisode 3737 terminated after 30 steps\nEpisode 3738 terminated after 30 steps\nEpisode 3739 terminated after 30 steps\nEpisode 3740 terminated after 30 steps\nEpisode 3741 terminated after 30 steps\nEpisode 3742 terminated after 30 steps\nEpisode 3743 terminated after 30 steps\nEpisode 3744 terminated after 30 steps\nEpisode 3745 terminated after 30 steps\nEpisode 3746 terminated after 30 steps\nEpisode 3747 terminated after 30 steps\nEpisode 3748 terminated after 30 steps\nEpisode 3749 terminated after 30 steps\nEpisode 3750 terminated after 30 steps\nEpisode 3751 terminated after 30 steps\nEpisode 3752 terminated after 30 steps\nEpisode 3753 terminated after 30 steps\nEpisode 3754 terminated after 30 steps\nEpisode 3755 terminated after 30 steps\nEpisode 3756 terminated after 30 steps\nEpisode 3757 terminated after 30 steps\nEpisode 3758 terminated after 30 steps\nEpisode 3759 terminated after 30 steps\nEpisode 3760 terminated after 30 steps\nEpisode 3761 terminated after 30 steps\nEpisode 3762 terminated after 30 steps\nEpisode 3763 terminated after 30 steps\nEpisode 3764 terminated after 30 steps\nEpisode 3765 terminated after 30 steps\nEpisode 3766 terminated after 30 steps\nEpisode 3767 terminated after 30 steps\nEpisode 3768 terminated after 30 steps\nEpisode 3769 terminated after 30 steps\nEpisode 3770 terminated after 30 steps\nEpisode 3771 terminated after 30 steps\nEpisode 3772 terminated after 30 steps\nEpisode 3773 terminated after 30 steps\nEpisode 3774 terminated after 30 steps\nEpisode 3775 terminated after 30 steps\nEpisode 3776 terminated after 30 steps\nEpisode 3777 terminated after 30 steps\nEpisode 3778 terminated after 30 steps\nEpisode 3779 terminated after 30 steps\nEpisode 3780 terminated after 30 steps\nEpisode 3781 terminated after 30 steps\nEpisode 3782 terminated after 30 steps\nEpisode 3783 terminated after 30 steps\nEpisode 3784 terminated after 30 steps\nEpisode 3785 terminated after 30 steps\nEpisode 3786 terminated after 30 steps\nEpisode 3787 terminated after 30 steps\nEpisode 3788 terminated after 30 steps\nEpisode 3789 terminated after 30 steps\nEpisode 3790 terminated after 30 steps\nEpisode 3791 terminated after 30 steps\nEpisode 3792 terminated after 30 steps\nEpisode 3793 terminated after 30 steps\nEpisode 3794 terminated after 30 steps\nEpisode 3795 terminated after 30 steps\nEpisode 3796 terminated after 30 steps\nEpisode 3797 terminated after 30 steps\nEpisode 3798 terminated after 30 steps\nEpisode 3799 terminated after 30 steps\nEpisode 3800\nEpisode 3800 terminated after 30 steps\nEpisode 3801 terminated after 30 steps\nEpisode 3802 terminated after 30 steps\nEpisode 3803 terminated after 30 steps\nEpisode 3804 terminated after 30 steps\nEpisode 3805 terminated after 30 steps\nEpisode 3806 terminated after 30 steps\nEpisode 3807 terminated after 30 steps\nEpisode 3808 terminated after 30 steps\nEpisode 3809 terminated after 30 steps\nEpisode 3810 terminated after 30 steps\nEpisode 3811 terminated after 30 steps\nEpisode 3812 terminated after 30 steps\nEpisode 3813 terminated after 30 steps\nEpisode 3814 terminated after 30 steps\nEpisode 3815 terminated after 30 steps\nEpisode 3816 terminated after 30 steps\nEpisode 3817 terminated after 30 steps\nEpisode 3818 terminated after 30 steps\nEpisode 3819 terminated after 30 steps\nEpisode 3820 terminated after 30 steps\nEpisode 3821 terminated after 30 steps\nEpisode 3822 terminated after 30 steps\nEpisode 3823 terminated after 30 steps\nEpisode 3824 terminated after 30 steps\nEpisode 3825 terminated after 30 steps\nEpisode 3826 terminated after 30 steps\nEpisode 3827 terminated after 30 steps\nEpisode 3828 terminated after 30 steps\nEpisode 3829 terminated after 30 steps\nEpisode 3830 terminated after 30 steps\nEpisode 3831 terminated after 30 steps\nEpisode 3832 terminated after 30 steps\nEpisode 3833 terminated after 30 steps\nEpisode 3834 terminated after 30 steps\nEpisode 3835 terminated after 30 steps\nEpisode 3836 terminated after 30 steps\nEpisode 3837 terminated after 30 steps\nEpisode 3838 terminated after 30 steps\nEpisode 3839 terminated after 30 steps\nEpisode 3840 terminated after 30 steps\nEpisode 3841 terminated after 30 steps\nEpisode 3842 terminated after 30 steps\nEpisode 3843 terminated after 30 steps\nEpisode 3844 terminated after 30 steps\nEpisode 3845 terminated after 30 steps\nEpisode 3846 terminated after 30 steps\nEpisode 3847 terminated after 30 steps\nEpisode 3848 terminated after 30 steps\nEpisode 3849 terminated after 30 steps\nEpisode 3850 terminated after 30 steps\nEpisode 3851 terminated after 30 steps\nEpisode 3852 terminated after 30 steps\nEpisode 3853 terminated after 30 steps\nEpisode 3854 terminated after 30 steps\nEpisode 3855 terminated after 30 steps\nEpisode 3856 terminated after 30 steps\nEpisode 3857 terminated after 30 steps\nEpisode 3858 terminated after 30 steps\nEpisode 3859 terminated after 30 steps\nEpisode 3860 terminated after 30 steps\nEpisode 3861 terminated after 30 steps\nEpisode 3862 terminated after 30 steps\nEpisode 3863 terminated after 30 steps\nEpisode 3864 terminated after 30 steps\nEpisode 3865 terminated after 30 steps\nEpisode 3866 terminated after 30 steps\nEpisode 3867 terminated after 30 steps\nEpisode 3868 terminated after 30 steps\nEpisode 3869 terminated after 30 steps\nEpisode 3870 terminated after 30 steps\nEpisode 3871 terminated after 30 steps\nEpisode 3872 terminated after 30 steps\nEpisode 3873 terminated after 30 steps\nEpisode 3874 terminated after 30 steps\nEpisode 3875 terminated after 30 steps\nEpisode 3876 terminated after 30 steps\nEpisode 3877 terminated after 30 steps\nEpisode 3878 terminated after 30 steps\nEpisode 3879 terminated after 30 steps\nEpisode 3880 terminated after 30 steps\nEpisode 3881 terminated after 30 steps\nEpisode 3882 terminated after 30 steps\nEpisode 3883 terminated after 30 steps\nEpisode 3884 terminated after 30 steps\nEpisode 3885 terminated after 30 steps\nEpisode 3886 terminated after 30 steps\nEpisode 3887 terminated after 30 steps\nEpisode 3888 terminated after 30 steps\nEpisode 3889 terminated after 30 steps\nEpisode 3890 terminated after 30 steps\nEpisode 3891 terminated after 30 steps\nEpisode 3892 terminated after 30 steps\nEpisode 3893 terminated after 30 steps\nEpisode 3894 terminated after 30 steps\nEpisode 3895 terminated after 30 steps\nEpisode 3896 terminated after 30 steps\nEpisode 3897 terminated after 30 steps\nEpisode 3898 terminated after 30 steps\nEpisode 3899 terminated after 30 steps\nEpisode 3900\nEpisode 3900 terminated after 30 steps\nEpisode 3901 terminated after 30 steps\nEpisode 3902 terminated after 30 steps\nEpisode 3903 terminated after 30 steps\nEpisode 3904 terminated after 30 steps\nEpisode 3905 terminated after 30 steps\nEpisode 3906 terminated after 30 steps\nEpisode 3907 terminated after 30 steps\nEpisode 3908 terminated after 30 steps\nEpisode 3909 terminated after 30 steps\nEpisode 3910 terminated after 30 steps\nEpisode 3911 terminated after 30 steps\nEpisode 3912 terminated after 30 steps\nEpisode 3913 terminated after 30 steps\nEpisode 3914 terminated after 30 steps\nEpisode 3915 terminated after 30 steps\nEpisode 3916 terminated after 30 steps\nEpisode 3917 terminated after 30 steps\nEpisode 3918 terminated after 30 steps\nEpisode 3919 terminated after 30 steps\nEpisode 3920 terminated after 30 steps\nEpisode 3921 terminated after 30 steps\nEpisode 3922 terminated after 30 steps\nEpisode 3923 terminated after 30 steps\nEpisode 3924 terminated after 30 steps\nEpisode 3925 terminated after 30 steps\nEpisode 3926 terminated after 30 steps\nEpisode 3927 terminated after 30 steps\nEpisode 3928 terminated after 30 steps\nEpisode 3929 terminated after 30 steps\nEpisode 3930 terminated after 30 steps\nEpisode 3931 terminated after 30 steps\nEpisode 3932 terminated after 30 steps\nEpisode 3933 terminated after 30 steps\nEpisode 3934 terminated after 30 steps\nEpisode 3935 terminated after 30 steps\nEpisode 3936 terminated after 30 steps\nEpisode 3937 terminated after 30 steps\nEpisode 3938 terminated after 30 steps\nEpisode 3939 terminated after 30 steps\nEpisode 3940 terminated after 30 steps\nEpisode 3941 terminated after 30 steps\nEpisode 3942 terminated after 30 steps\nEpisode 3943 terminated after 30 steps\nEpisode 3944 terminated after 30 steps\nEpisode 3945 terminated after 30 steps\nEpisode 3946 terminated after 30 steps\nEpisode 3947 terminated after 30 steps\nEpisode 3948 terminated after 30 steps\nEpisode 3949 terminated after 30 steps\nEpisode 3950 terminated after 30 steps\nEpisode 3951 terminated after 30 steps\nEpisode 3952 terminated after 30 steps\nEpisode 3953 terminated after 30 steps\nEpisode 3954 terminated after 30 steps\nEpisode 3955 terminated after 30 steps\nEpisode 3956 terminated after 30 steps\nEpisode 3957 terminated after 30 steps\nEpisode 3958 terminated after 30 steps\nEpisode 3959 terminated after 30 steps\nEpisode 3960 terminated after 30 steps\nEpisode 3961 terminated after 30 steps\nEpisode 3962 terminated after 30 steps\nEpisode 3963 terminated after 30 steps\nEpisode 3964 terminated after 30 steps\nEpisode 3965 terminated after 30 steps\nEpisode 3966 terminated after 30 steps\nEpisode 3967 terminated after 30 steps\nEpisode 3968 terminated after 30 steps\nEpisode 3969 terminated after 30 steps\nEpisode 3970 terminated after 30 steps\nEpisode 3971 terminated after 30 steps\nEpisode 3972 terminated after 30 steps\nEpisode 3973 terminated after 30 steps\nEpisode 3974 terminated after 30 steps\nEpisode 3975 terminated after 30 steps\nEpisode 3976 terminated after 30 steps\nEpisode 3977 terminated after 30 steps\nEpisode 3978 terminated after 30 steps\nEpisode 3979 terminated after 30 steps\nEpisode 3980 terminated after 30 steps\nEpisode 3981 terminated after 30 steps\nEpisode 3982 terminated after 30 steps\nEpisode 3983 terminated after 30 steps\nEpisode 3984 terminated after 30 steps\nEpisode 3985 terminated after 30 steps\nEpisode 3986 terminated after 30 steps\nEpisode 3987 terminated after 30 steps\nEpisode 3988 terminated after 30 steps\nEpisode 3989 terminated after 30 steps\nEpisode 3990 terminated after 30 steps\nEpisode 3991 terminated after 30 steps\nEpisode 3992 terminated after 30 steps\nEpisode 3993 terminated after 30 steps\nEpisode 3994 terminated after 30 steps\nEpisode 3995 terminated after 30 steps\nEpisode 3996 terminated after 30 steps\nEpisode 3997 terminated after 30 steps\nEpisode 3998 terminated after 30 steps\nEpisode 3999 terminated after 30 steps\nEpisode 4000\nEpisode 4000 terminated after 30 steps\nEpisode 4001 terminated after 30 steps\nEpisode 4002 terminated after 30 steps\nEpisode 4003 terminated after 30 steps\nEpisode 4004 terminated after 30 steps\nEpisode 4005 terminated after 30 steps\nEpisode 4006 terminated after 30 steps\nEpisode 4007 terminated after 30 steps\nEpisode 4008 terminated after 30 steps\nEpisode 4009 terminated after 30 steps\nEpisode 4010 terminated after 30 steps\nEpisode 4011 terminated after 30 steps\nEpisode 4012 terminated after 30 steps\nEpisode 4013 terminated after 30 steps\nEpisode 4014 terminated after 30 steps\nEpisode 4015 terminated after 30 steps\nEpisode 4016 terminated after 30 steps\nEpisode 4017 terminated after 30 steps\nEpisode 4018 terminated after 30 steps\nEpisode 4019 terminated after 30 steps\nEpisode 4020 terminated after 30 steps\nEpisode 4021 terminated after 30 steps\nEpisode 4022 terminated after 30 steps\nEpisode 4023 terminated after 30 steps\nEpisode 4024 terminated after 30 steps\nEpisode 4025 terminated after 30 steps\nEpisode 4026 terminated after 30 steps\nEpisode 4027 terminated after 30 steps\nEpisode 4028 terminated after 30 steps\nEpisode 4029 terminated after 30 steps\nEpisode 4030 terminated after 30 steps\nEpisode 4031 terminated after 30 steps\nEpisode 4032 terminated after 30 steps\nEpisode 4033 terminated after 30 steps\nEpisode 4034 terminated after 30 steps\nEpisode 4035 terminated after 30 steps\nEpisode 4036 terminated after 30 steps\nEpisode 4037 terminated after 30 steps\nEpisode 4038 terminated after 30 steps\nEpisode 4039 terminated after 30 steps\nEpisode 4040 terminated after 30 steps\nEpisode 4041 terminated after 30 steps\nEpisode 4042 terminated after 30 steps\nEpisode 4043 terminated after 30 steps\nEpisode 4044 terminated after 30 steps\nEpisode 4045 terminated after 30 steps\nEpisode 4046 terminated after 30 steps\nEpisode 4047 terminated after 30 steps\nEpisode 4048 terminated after 30 steps\nEpisode 4049 terminated after 30 steps\nEpisode 4050 terminated after 30 steps\nEpisode 4051 terminated after 30 steps\nEpisode 4052 terminated after 30 steps\nEpisode 4053 terminated after 30 steps\nEpisode 4054 terminated after 30 steps\nEpisode 4055 terminated after 30 steps\nEpisode 4056 terminated after 30 steps\nEpisode 4057 terminated after 30 steps\nEpisode 4058 terminated after 30 steps\nEpisode 4059 terminated after 30 steps\nEpisode 4060 terminated after 30 steps\nEpisode 4061 terminated after 30 steps\nEpisode 4062 terminated after 30 steps\nEpisode 4063 terminated after 30 steps\nEpisode 4064 terminated after 30 steps\nEpisode 4065 terminated after 30 steps\nEpisode 4066 terminated after 30 steps\nEpisode 4067 terminated after 30 steps\nEpisode 4068 terminated after 30 steps\nEpisode 4069 terminated after 30 steps\nEpisode 4070 terminated after 30 steps\nEpisode 4071 terminated after 30 steps\nEpisode 4072 terminated after 30 steps\nEpisode 4073 terminated after 30 steps\nEpisode 4074 terminated after 30 steps\nEpisode 4075 terminated after 30 steps\nEpisode 4076 terminated after 30 steps\nEpisode 4077 terminated after 30 steps\nEpisode 4078 terminated after 30 steps\nEpisode 4079 terminated after 30 steps\nEpisode 4080 terminated after 30 steps\nEpisode 4081 terminated after 30 steps\nEpisode 4082 terminated after 30 steps\nEpisode 4083 terminated after 30 steps\nEpisode 4084 terminated after 30 steps\nEpisode 4085 terminated after 30 steps\nEpisode 4086 terminated after 30 steps\nEpisode 4087 terminated after 30 steps\nEpisode 4088 terminated after 30 steps\nEpisode 4089 terminated after 30 steps\nEpisode 4090 terminated after 30 steps\nEpisode 4091 terminated after 30 steps\nEpisode 4092 terminated after 30 steps\nEpisode 4093 terminated after 30 steps\nEpisode 4094 terminated after 30 steps\nEpisode 4095 terminated after 30 steps\nEpisode 4096 terminated after 30 steps\nEpisode 4097 terminated after 30 steps\nEpisode 4098 terminated after 30 steps\nEpisode 4099 terminated after 30 steps\nEpisode 4100\nEpisode 4100 terminated after 30 steps\nEpisode 4101 terminated after 30 steps\nEpisode 4102 terminated after 30 steps\nEpisode 4103 terminated after 30 steps\nEpisode 4104 terminated after 30 steps\nEpisode 4105 terminated after 30 steps\nEpisode 4106 terminated after 30 steps\nEpisode 4107 terminated after 30 steps\nEpisode 4108 terminated after 30 steps\nEpisode 4109 terminated after 30 steps\nEpisode 4110 terminated after 30 steps\nEpisode 4111 terminated after 30 steps\nEpisode 4112 terminated after 30 steps\nEpisode 4113 terminated after 30 steps\nEpisode 4114 terminated after 30 steps\nEpisode 4115 terminated after 30 steps\nEpisode 4116 terminated after 30 steps\nEpisode 4117 terminated after 30 steps\nEpisode 4118 terminated after 30 steps\nEpisode 4119 terminated after 30 steps\nEpisode 4120 terminated after 30 steps\nEpisode 4121 terminated after 30 steps\nEpisode 4122 terminated after 30 steps\nEpisode 4123 terminated after 30 steps\nEpisode 4124 terminated after 30 steps\nEpisode 4125 terminated after 30 steps\nEpisode 4126 terminated after 30 steps\nEpisode 4127 terminated after 30 steps\nEpisode 4128 terminated after 30 steps\nEpisode 4129 terminated after 30 steps\nEpisode 4130 terminated after 30 steps\nEpisode 4131 terminated after 30 steps\nEpisode 4132 terminated after 30 steps\nEpisode 4133 terminated after 30 steps\nEpisode 4134 terminated after 30 steps\nEpisode 4135 terminated after 30 steps\nEpisode 4136 terminated after 30 steps\nEpisode 4137 terminated after 30 steps\nEpisode 4138 terminated after 30 steps\nEpisode 4139 terminated after 30 steps\nEpisode 4140 terminated after 30 steps\nEpisode 4141 terminated after 30 steps\nEpisode 4142 terminated after 30 steps\nEpisode 4143 terminated after 30 steps\nEpisode 4144 terminated after 30 steps\nEpisode 4145 terminated after 30 steps\nEpisode 4146 terminated after 30 steps\nEpisode 4147 terminated after 30 steps\nEpisode 4148 terminated after 30 steps\nEpisode 4149 terminated after 30 steps\nEpisode 4150 terminated after 30 steps\nEpisode 4151 terminated after 30 steps\nEpisode 4152 terminated after 30 steps\nEpisode 4153 terminated after 30 steps\nEpisode 4154 terminated after 30 steps\nEpisode 4155 terminated after 30 steps\nEpisode 4156 terminated after 30 steps\nEpisode 4157 terminated after 30 steps\nEpisode 4158 terminated after 30 steps\nEpisode 4159 terminated after 30 steps\nEpisode 4160 terminated after 30 steps\nEpisode 4161 terminated after 30 steps\nEpisode 4162 terminated after 30 steps\nEpisode 4163 terminated after 30 steps\nEpisode 4164 terminated after 30 steps\nEpisode 4165 terminated after 30 steps\nEpisode 4166 terminated after 30 steps\nEpisode 4167 terminated after 30 steps\nEpisode 4168 terminated after 30 steps\nEpisode 4169 terminated after 30 steps\nEpisode 4170 terminated after 30 steps\nEpisode 4171 terminated after 30 steps\nEpisode 4172 terminated after 30 steps\nEpisode 4173 terminated after 30 steps\nEpisode 4174 terminated after 30 steps\nEpisode 4175 terminated after 30 steps\nEpisode 4176 terminated after 30 steps\nEpisode 4177 terminated after 30 steps\nEpisode 4178 terminated after 30 steps\nEpisode 4179 terminated after 30 steps\nEpisode 4180 terminated after 30 steps\nEpisode 4181 terminated after 30 steps\nEpisode 4182 terminated after 30 steps\nEpisode 4183 terminated after 30 steps\nEpisode 4184 terminated after 30 steps\nEpisode 4185 terminated after 30 steps\nEpisode 4186 terminated after 30 steps\nEpisode 4187 terminated after 30 steps\nEpisode 4188 terminated after 30 steps\nEpisode 4189 terminated after 30 steps\nEpisode 4190 terminated after 30 steps\nEpisode 4191 terminated after 30 steps\nEpisode 4192 terminated after 30 steps\nEpisode 4193 terminated after 30 steps\nEpisode 4194 terminated after 30 steps\nEpisode 4195 terminated after 30 steps\nEpisode 4196 terminated after 30 steps\nEpisode 4197 terminated after 30 steps\nEpisode 4198 terminated after 30 steps\nEpisode 4199 terminated after 30 steps\nEpisode 4200\nEpisode 4200 terminated after 30 steps\nEpisode 4201 terminated after 30 steps\nEpisode 4202 terminated after 30 steps\nEpisode 4203 terminated after 30 steps\nEpisode 4204 terminated after 30 steps\nEpisode 4205 terminated after 30 steps\nEpisode 4206 terminated after 30 steps\nEpisode 4207 terminated after 30 steps\nEpisode 4208 terminated after 30 steps\nEpisode 4209 terminated after 30 steps\nEpisode 4210 terminated after 30 steps\nEpisode 4211 terminated after 30 steps\nEpisode 4212 terminated after 30 steps\nEpisode 4213 terminated after 30 steps\nEpisode 4214 terminated after 30 steps\nEpisode 4215 terminated after 30 steps\nEpisode 4216 terminated after 30 steps\nEpisode 4217 terminated after 30 steps\nEpisode 4218 terminated after 30 steps\nEpisode 4219 terminated after 30 steps\nEpisode 4220 terminated after 30 steps\nEpisode 4221 terminated after 30 steps\nEpisode 4222 terminated after 30 steps\nEpisode 4223 terminated after 30 steps\nEpisode 4224 terminated after 30 steps\nEpisode 4225 terminated after 30 steps\nEpisode 4226 terminated after 30 steps\nEpisode 4227 terminated after 30 steps\nEpisode 4228 terminated after 30 steps\nEpisode 4229 terminated after 30 steps\nEpisode 4230 terminated after 30 steps\nEpisode 4231 terminated after 30 steps\nEpisode 4232 terminated after 30 steps\nEpisode 4233 terminated after 30 steps\nEpisode 4234 terminated after 30 steps\nEpisode 4235 terminated after 30 steps\nEpisode 4236 terminated after 30 steps\nEpisode 4237 terminated after 30 steps\nEpisode 4238 terminated after 30 steps\nEpisode 4239 terminated after 30 steps\nEpisode 4240 terminated after 30 steps\nEpisode 4241 terminated after 30 steps\nEpisode 4242 terminated after 30 steps\nEpisode 4243 terminated after 30 steps\nEpisode 4244 terminated after 30 steps\nEpisode 4245 terminated after 30 steps\nEpisode 4246 terminated after 30 steps\nEpisode 4247 terminated after 30 steps\nEpisode 4248 terminated after 30 steps\nEpisode 4249 terminated after 30 steps\nEpisode 4250 terminated after 30 steps\nEpisode 4251 terminated after 30 steps\nEpisode 4252 terminated after 30 steps\nEpisode 4253 terminated after 30 steps\nEpisode 4254 terminated after 30 steps\nEpisode 4255 terminated after 30 steps\nEpisode 4256 terminated after 30 steps\nEpisode 4257 terminated after 30 steps\nEpisode 4258 terminated after 30 steps\nEpisode 4259 terminated after 30 steps\nEpisode 4260 terminated after 30 steps\nEpisode 4261 terminated after 30 steps\nEpisode 4262 terminated after 30 steps\nEpisode 4263 terminated after 30 steps\nEpisode 4264 terminated after 30 steps\nEpisode 4265 terminated after 30 steps\nEpisode 4266 terminated after 30 steps\nEpisode 4267 terminated after 30 steps\nEpisode 4268 terminated after 30 steps\nEpisode 4269 terminated after 30 steps\nEpisode 4270 terminated after 30 steps\nEpisode 4271 terminated after 30 steps\nEpisode 4272 terminated after 30 steps\nEpisode 4273 terminated after 30 steps\nEpisode 4274 terminated after 30 steps\nEpisode 4275 terminated after 30 steps\nEpisode 4276 terminated after 30 steps\nEpisode 4277 terminated after 30 steps\nEpisode 4278 terminated after 30 steps\nEpisode 4279 terminated after 30 steps\nEpisode 4280 terminated after 30 steps\nEpisode 4281 terminated after 30 steps\nEpisode 4282 terminated after 30 steps\nEpisode 4283 terminated after 30 steps\nEpisode 4284 terminated after 30 steps\nEpisode 4285 terminated after 30 steps\nEpisode 4286 terminated after 30 steps\nEpisode 4287 terminated after 30 steps\nEpisode 4288 terminated after 30 steps\nEpisode 4289 terminated after 30 steps\nEpisode 4290 terminated after 30 steps\nEpisode 4291 terminated after 30 steps\nEpisode 4292 terminated after 30 steps\nEpisode 4293 terminated after 30 steps\nEpisode 4294 terminated after 30 steps\nEpisode 4295 terminated after 30 steps\nEpisode 4296 terminated after 30 steps\nEpisode 4297 terminated after 30 steps\nEpisode 4298 terminated after 30 steps\nEpisode 4299 terminated after 30 steps\nEpisode 4300\nEpisode 4300 terminated after 30 steps\nEpisode 4301 terminated after 30 steps\nEpisode 4302 terminated after 30 steps\nEpisode 4303 terminated after 30 steps\nEpisode 4304 terminated after 30 steps\nEpisode 4305 terminated after 30 steps\nEpisode 4306 terminated after 30 steps\nEpisode 4307 terminated after 30 steps\nEpisode 4308 terminated after 30 steps\nEpisode 4309 terminated after 30 steps\nEpisode 4310 terminated after 30 steps\nEpisode 4311 terminated after 30 steps\nEpisode 4312 terminated after 30 steps\nEpisode 4313 terminated after 30 steps\nEpisode 4314 terminated after 30 steps\nEpisode 4315 terminated after 30 steps\nEpisode 4316 terminated after 30 steps\nEpisode 4317 terminated after 30 steps\nEpisode 4318 terminated after 30 steps\nEpisode 4319 terminated after 30 steps\nEpisode 4320 terminated after 30 steps\nEpisode 4321 terminated after 30 steps\nEpisode 4322 terminated after 30 steps\nEpisode 4323 terminated after 30 steps\nEpisode 4324 terminated after 30 steps\nEpisode 4325 terminated after 30 steps\nEpisode 4326 terminated after 30 steps\nEpisode 4327 terminated after 30 steps\nEpisode 4328 terminated after 30 steps\nEpisode 4329 terminated after 30 steps\nEpisode 4330 terminated after 30 steps\nEpisode 4331 terminated after 30 steps\nEpisode 4332 terminated after 30 steps\nEpisode 4333 terminated after 30 steps\nEpisode 4334 terminated after 30 steps\nEpisode 4335 terminated after 30 steps\nEpisode 4336 terminated after 30 steps\nEpisode 4337 terminated after 30 steps\nEpisode 4338 terminated after 30 steps\nEpisode 4339 terminated after 30 steps\nEpisode 4340 terminated after 30 steps\nEpisode 4341 terminated after 30 steps\nEpisode 4342 terminated after 30 steps\nEpisode 4343 terminated after 30 steps\nEpisode 4344 terminated after 30 steps\nEpisode 4345 terminated after 30 steps\nEpisode 4346 terminated after 30 steps\nEpisode 4347 terminated after 30 steps\nEpisode 4348 terminated after 30 steps\nEpisode 4349 terminated after 30 steps\nEpisode 4350 terminated after 30 steps\nEpisode 4351 terminated after 30 steps\nEpisode 4352 terminated after 30 steps\nEpisode 4353 terminated after 30 steps\nEpisode 4354 terminated after 30 steps\nEpisode 4355 terminated after 30 steps\nEpisode 4356 terminated after 30 steps\nEpisode 4357 terminated after 30 steps\nEpisode 4358 terminated after 30 steps\nEpisode 4359 terminated after 30 steps\nEpisode 4360 terminated after 30 steps\nEpisode 4361 terminated after 30 steps\nEpisode 4362 terminated after 30 steps\nEpisode 4363 terminated after 30 steps\nEpisode 4364 terminated after 30 steps\nEpisode 4365 terminated after 30 steps\nEpisode 4366 terminated after 30 steps\nEpisode 4367 terminated after 30 steps\nEpisode 4368 terminated after 30 steps\nEpisode 4369 terminated after 30 steps\nEpisode 4370 terminated after 30 steps\nEpisode 4371 terminated after 30 steps\nEpisode 4372 terminated after 30 steps\nEpisode 4373 terminated after 30 steps\nEpisode 4374 terminated after 30 steps\nEpisode 4375 terminated after 30 steps\nEpisode 4376 terminated after 30 steps\nEpisode 4377 terminated after 30 steps\nEpisode 4378 terminated after 30 steps\nEpisode 4379 terminated after 30 steps\nEpisode 4380 terminated after 30 steps\nEpisode 4381 terminated after 30 steps\nEpisode 4382 terminated after 30 steps\nEpisode 4383 terminated after 30 steps\nEpisode 4384 terminated after 30 steps\nEpisode 4385 terminated after 30 steps\nEpisode 4386 terminated after 30 steps\nEpisode 4387 terminated after 30 steps\nEpisode 4388 terminated after 30 steps\nEpisode 4389 terminated after 30 steps\nEpisode 4390 terminated after 30 steps\nEpisode 4391 terminated after 30 steps\nEpisode 4392 terminated after 30 steps\nEpisode 4393 terminated after 30 steps\nEpisode 4394 terminated after 30 steps\nEpisode 4395 terminated after 30 steps\nEpisode 4396 terminated after 30 steps\nEpisode 4397 terminated after 30 steps\nEpisode 4398 terminated after 30 steps\nEpisode 4399 terminated after 30 steps\nEpisode 4400\nEpisode 4400 terminated after 30 steps\nEpisode 4401 terminated after 30 steps\nEpisode 4402 terminated after 30 steps\nEpisode 4403 terminated after 30 steps\nEpisode 4404 terminated after 30 steps\nEpisode 4405 terminated after 30 steps\nEpisode 4406 terminated after 30 steps\nEpisode 4407 terminated after 30 steps\nEpisode 4408 terminated after 30 steps\nEpisode 4409 terminated after 30 steps\nEpisode 4410 terminated after 30 steps\nEpisode 4411 terminated after 30 steps\nEpisode 4412 terminated after 30 steps\nEpisode 4413 terminated after 30 steps\nEpisode 4414 terminated after 30 steps\nEpisode 4415 terminated after 30 steps\nEpisode 4416 terminated after 30 steps\nEpisode 4417 terminated after 30 steps\nEpisode 4418 terminated after 30 steps\nEpisode 4419 terminated after 30 steps\nEpisode 4420 terminated after 30 steps\nEpisode 4421 terminated after 30 steps\nEpisode 4422 terminated after 30 steps\nEpisode 4423 terminated after 30 steps\nEpisode 4424 terminated after 30 steps\nEpisode 4425 terminated after 30 steps\nEpisode 4426 terminated after 30 steps\nEpisode 4427 terminated after 30 steps\nEpisode 4428 terminated after 30 steps\nEpisode 4429 terminated after 30 steps\nEpisode 4430 terminated after 30 steps\nEpisode 4431 terminated after 30 steps\nEpisode 4432 terminated after 30 steps\nEpisode 4433 terminated after 30 steps\nEpisode 4434 terminated after 30 steps\nEpisode 4435 terminated after 30 steps\nEpisode 4436 terminated after 30 steps\nEpisode 4437 terminated after 30 steps\nEpisode 4438 terminated after 30 steps\nEpisode 4439 terminated after 30 steps\nEpisode 4440 terminated after 30 steps\nEpisode 4441 terminated after 30 steps\nEpisode 4442 terminated after 30 steps\nEpisode 4443 terminated after 30 steps\nEpisode 4444 terminated after 30 steps\nEpisode 4445 terminated after 30 steps\nEpisode 4446 terminated after 30 steps\nEpisode 4447 terminated after 30 steps\nEpisode 4448 terminated after 30 steps\nEpisode 4449 terminated after 30 steps\nEpisode 4450 terminated after 30 steps\nEpisode 4451 terminated after 30 steps\nEpisode 4452 terminated after 30 steps\nEpisode 4453 terminated after 30 steps\nEpisode 4454 terminated after 30 steps\nEpisode 4455 terminated after 30 steps\nEpisode 4456 terminated after 30 steps\nEpisode 4457 terminated after 30 steps\nEpisode 4458 terminated after 30 steps\nEpisode 4459 terminated after 30 steps\nEpisode 4460 terminated after 30 steps\nEpisode 4461 terminated after 30 steps\nEpisode 4462 terminated after 30 steps\nEpisode 4463 terminated after 30 steps\nEpisode 4464 terminated after 30 steps\nEpisode 4465 terminated after 30 steps\nEpisode 4466 terminated after 30 steps\nEpisode 4467 terminated after 30 steps\nEpisode 4468 terminated after 30 steps\nEpisode 4469 terminated after 30 steps\nEpisode 4470 terminated after 30 steps\nEpisode 4471 terminated after 30 steps\nEpisode 4472 terminated after 30 steps\nEpisode 4473 terminated after 30 steps\nEpisode 4474 terminated after 30 steps\nEpisode 4475 terminated after 30 steps\nEpisode 4476 terminated after 30 steps\nEpisode 4477 terminated after 30 steps\nEpisode 4478 terminated after 30 steps\nEpisode 4479 terminated after 30 steps\nEpisode 4480 terminated after 30 steps\nEpisode 4481 terminated after 30 steps\nEpisode 4482 terminated after 30 steps\nEpisode 4483 terminated after 30 steps\nEpisode 4484 terminated after 30 steps\nEpisode 4485 terminated after 30 steps\nEpisode 4486 terminated after 30 steps\nEpisode 4487 terminated after 30 steps\nEpisode 4488 terminated after 30 steps\nEpisode 4489 terminated after 30 steps\nEpisode 4490 terminated after 30 steps\nEpisode 4491 terminated after 30 steps\nEpisode 4492 terminated after 30 steps\nEpisode 4493 terminated after 30 steps\nEpisode 4494 terminated after 30 steps\nEpisode 4495 terminated after 30 steps\nEpisode 4496 terminated after 30 steps\nEpisode 4497 terminated after 30 steps\nEpisode 4498 terminated after 30 steps\nEpisode 4499 terminated after 30 steps\nEpisode 4500\nEpisode 4500 terminated after 30 steps\nEpisode 4501 terminated after 30 steps\nEpisode 4502 terminated after 30 steps\nEpisode 4503 terminated after 30 steps\nEpisode 4504 terminated after 30 steps\nEpisode 4505 terminated after 30 steps\nEpisode 4506 terminated after 30 steps\nEpisode 4507 terminated after 30 steps\nEpisode 4508 terminated after 30 steps\nEpisode 4509 terminated after 30 steps\nEpisode 4510 terminated after 30 steps\nEpisode 4511 terminated after 30 steps\nEpisode 4512 terminated after 30 steps\nEpisode 4513 terminated after 30 steps\nEpisode 4514 terminated after 30 steps\nEpisode 4515 terminated after 30 steps\nEpisode 4516 terminated after 30 steps\nEpisode 4517 terminated after 30 steps\nEpisode 4518 terminated after 30 steps\nEpisode 4519 terminated after 30 steps\nEpisode 4520 terminated after 30 steps\nEpisode 4521 terminated after 30 steps\nEpisode 4522 terminated after 30 steps\nEpisode 4523 terminated after 30 steps\nEpisode 4524 terminated after 30 steps\nEpisode 4525 terminated after 30 steps\nEpisode 4526 terminated after 30 steps\nEpisode 4527 terminated after 30 steps\nEpisode 4528 terminated after 30 steps\nEpisode 4529 terminated after 30 steps\nEpisode 4530 terminated after 30 steps\nEpisode 4531 terminated after 30 steps\nEpisode 4532 terminated after 30 steps\nEpisode 4533 terminated after 30 steps\nEpisode 4534 terminated after 30 steps\nEpisode 4535 terminated after 30 steps\nEpisode 4536 terminated after 30 steps\nEpisode 4537 terminated after 30 steps\nEpisode 4538 terminated after 30 steps\nEpisode 4539 terminated after 30 steps\nEpisode 4540 terminated after 30 steps\nEpisode 4541 terminated after 30 steps\nEpisode 4542 terminated after 30 steps\nEpisode 4543 terminated after 30 steps\nEpisode 4544 terminated after 30 steps\nEpisode 4545 terminated after 30 steps\nEpisode 4546 terminated after 30 steps\nEpisode 4547 terminated after 30 steps\nEpisode 4548 terminated after 30 steps\nEpisode 4549 terminated after 30 steps\nEpisode 4550 terminated after 30 steps\nEpisode 4551 terminated after 30 steps\nEpisode 4552 terminated after 30 steps\nEpisode 4553 terminated after 30 steps\nEpisode 4554 terminated after 30 steps\nEpisode 4555 terminated after 30 steps\nEpisode 4556 terminated after 30 steps\nEpisode 4557 terminated after 30 steps\nEpisode 4558 terminated after 30 steps\nEpisode 4559 terminated after 30 steps\nEpisode 4560 terminated after 30 steps\nEpisode 4561 terminated after 30 steps\nEpisode 4562 terminated after 30 steps\nEpisode 4563 terminated after 30 steps\nEpisode 4564 terminated after 30 steps\nEpisode 4565 terminated after 30 steps\nEpisode 4566 terminated after 30 steps\nEpisode 4567 terminated after 30 steps\nEpisode 4568 terminated after 30 steps\nEpisode 4569 terminated after 30 steps\nEpisode 4570 terminated after 30 steps\nEpisode 4571 terminated after 30 steps\nEpisode 4572 terminated after 30 steps\nEpisode 4573 terminated after 30 steps\nEpisode 4574 terminated after 30 steps\nEpisode 4575 terminated after 30 steps\nEpisode 4576 terminated after 30 steps\nEpisode 4577 terminated after 30 steps\nEpisode 4578 terminated after 30 steps\nEpisode 4579 terminated after 30 steps\nEpisode 4580 terminated after 30 steps\nEpisode 4581 terminated after 30 steps\nEpisode 4582 terminated after 30 steps\nEpisode 4583 terminated after 30 steps\nEpisode 4584 terminated after 30 steps\nEpisode 4585 terminated after 30 steps\nEpisode 4586 terminated after 30 steps\nEpisode 4587 terminated after 30 steps\nEpisode 4588 terminated after 30 steps\nEpisode 4589 terminated after 30 steps\nEpisode 4590 terminated after 30 steps\nEpisode 4591 terminated after 30 steps\nEpisode 4592 terminated after 30 steps\nEpisode 4593 terminated after 30 steps\nEpisode 4594 terminated after 30 steps\nEpisode 4595 terminated after 30 steps\nEpisode 4596 terminated after 30 steps\nEpisode 4597 terminated after 30 steps\nEpisode 4598 terminated after 30 steps\nEpisode 4599 terminated after 30 steps\nEpisode 4600\nEpisode 4600 terminated after 30 steps\nEpisode 4601 terminated after 30 steps\nEpisode 4602 terminated after 30 steps\nEpisode 4603 terminated after 30 steps\nEpisode 4604 terminated after 30 steps\nEpisode 4605 terminated after 30 steps\nEpisode 4606 terminated after 30 steps\nEpisode 4607 terminated after 30 steps\nEpisode 4608 terminated after 30 steps\nEpisode 4609 terminated after 30 steps\nEpisode 4610 terminated after 30 steps\nEpisode 4611 terminated after 30 steps\nEpisode 4612 terminated after 30 steps\nEpisode 4613 terminated after 30 steps\nEpisode 4614 terminated after 30 steps\nEpisode 4615 terminated after 30 steps\nEpisode 4616 terminated after 30 steps\nEpisode 4617 terminated after 30 steps\nEpisode 4618 terminated after 30 steps\nEpisode 4619 terminated after 30 steps\nEpisode 4620 terminated after 30 steps\nEpisode 4621 terminated after 30 steps\nEpisode 4622 terminated after 30 steps\nEpisode 4623 terminated after 30 steps\nEpisode 4624 terminated after 30 steps\nEpisode 4625 terminated after 30 steps\nEpisode 4626 terminated after 30 steps\nEpisode 4627 terminated after 30 steps\nEpisode 4628 terminated after 30 steps\nEpisode 4629 terminated after 30 steps\nEpisode 4630 terminated after 30 steps\nEpisode 4631 terminated after 30 steps\nEpisode 4632 terminated after 30 steps\nEpisode 4633 terminated after 30 steps\nEpisode 4634 terminated after 30 steps\nEpisode 4635 terminated after 30 steps\nEpisode 4636 terminated after 30 steps\nEpisode 4637 terminated after 30 steps\nEpisode 4638 terminated after 30 steps\nEpisode 4639 terminated after 30 steps\nEpisode 4640 terminated after 30 steps\nEpisode 4641 terminated after 30 steps\nEpisode 4642 terminated after 30 steps\nEpisode 4643 terminated after 30 steps\nEpisode 4644 terminated after 30 steps\nEpisode 4645 terminated after 30 steps\nEpisode 4646 terminated after 30 steps\nEpisode 4647 terminated after 30 steps\nEpisode 4648 terminated after 30 steps\nEpisode 4649 terminated after 30 steps\nEpisode 4650 terminated after 30 steps\nEpisode 4651 terminated after 30 steps\nEpisode 4652 terminated after 30 steps\nEpisode 4653 terminated after 30 steps\nEpisode 4654 terminated after 30 steps\nEpisode 4655 terminated after 30 steps\nEpisode 4656 terminated after 30 steps\nEpisode 4657 terminated after 30 steps\nEpisode 4658 terminated after 30 steps\nEpisode 4659 terminated after 30 steps\nEpisode 4660 terminated after 30 steps\nEpisode 4661 terminated after 30 steps\nEpisode 4662 terminated after 30 steps\nEpisode 4663 terminated after 30 steps\nEpisode 4664 terminated after 30 steps\nEpisode 4665 terminated after 30 steps\nEpisode 4666 terminated after 30 steps\nEpisode 4667 terminated after 30 steps\nEpisode 4668 terminated after 30 steps\nEpisode 4669 terminated after 30 steps\nEpisode 4670 terminated after 30 steps\nEpisode 4671 terminated after 30 steps\nEpisode 4672 terminated after 30 steps\nEpisode 4673 terminated after 30 steps\nEpisode 4674 terminated after 30 steps\nEpisode 4675 terminated after 30 steps\nEpisode 4676 terminated after 30 steps\nEpisode 4677 terminated after 30 steps\nEpisode 4678 terminated after 30 steps\nEpisode 4679 terminated after 30 steps\nEpisode 4680 terminated after 30 steps\nEpisode 4681 terminated after 30 steps\nEpisode 4682 terminated after 30 steps\nEpisode 4683 terminated after 30 steps\nEpisode 4684 terminated after 30 steps\nEpisode 4685 terminated after 30 steps\nEpisode 4686 terminated after 30 steps\nEpisode 4687 terminated after 30 steps\nEpisode 4688 terminated after 30 steps\nEpisode 4689 terminated after 30 steps\nEpisode 4690 terminated after 30 steps\nEpisode 4691 terminated after 30 steps\nEpisode 4692 terminated after 30 steps\nEpisode 4693 terminated after 30 steps\nEpisode 4694 terminated after 30 steps\nEpisode 4695 terminated after 30 steps\nEpisode 4696 terminated after 30 steps\nEpisode 4697 terminated after 30 steps\nEpisode 4698 terminated after 30 steps\nEpisode 4699 terminated after 30 steps\nEpisode 4700\nEpisode 4700 terminated after 30 steps\nEpisode 4701 terminated after 30 steps\nEpisode 4702 terminated after 30 steps\nEpisode 4703 terminated after 30 steps\nEpisode 4704 terminated after 30 steps\nEpisode 4705 terminated after 30 steps\nEpisode 4706 terminated after 30 steps\nEpisode 4707 terminated after 30 steps\nEpisode 4708 terminated after 30 steps\nEpisode 4709 terminated after 30 steps\nEpisode 4710 terminated after 30 steps\nEpisode 4711 terminated after 30 steps\nEpisode 4712 terminated after 30 steps\nEpisode 4713 terminated after 30 steps\nEpisode 4714 terminated after 30 steps\nEpisode 4715 terminated after 30 steps\nEpisode 4716 terminated after 30 steps\nEpisode 4717 terminated after 30 steps\nEpisode 4718 terminated after 30 steps\nEpisode 4719 terminated after 30 steps\nEpisode 4720 terminated after 30 steps\nEpisode 4721 terminated after 30 steps\nEpisode 4722 terminated after 30 steps\nEpisode 4723 terminated after 30 steps\nEpisode 4724 terminated after 30 steps\nEpisode 4725 terminated after 30 steps\nEpisode 4726 terminated after 30 steps\nEpisode 4727 terminated after 30 steps\nEpisode 4728 terminated after 30 steps\nEpisode 4729 terminated after 30 steps\nEpisode 4730 terminated after 30 steps\nEpisode 4731 terminated after 30 steps\nEpisode 4732 terminated after 30 steps\nEpisode 4733 terminated after 30 steps\nEpisode 4734 terminated after 30 steps\nEpisode 4735 terminated after 30 steps\nEpisode 4736 terminated after 30 steps\nEpisode 4737 terminated after 30 steps\nEpisode 4738 terminated after 30 steps\nEpisode 4739 terminated after 30 steps\nEpisode 4740 terminated after 30 steps\nEpisode 4741 terminated after 30 steps\nEpisode 4742 terminated after 30 steps\nEpisode 4743 terminated after 30 steps\nEpisode 4744 terminated after 30 steps\nEpisode 4745 terminated after 30 steps\nEpisode 4746 terminated after 30 steps\nEpisode 4747 terminated after 30 steps\nEpisode 4748 terminated after 30 steps\nEpisode 4749 terminated after 30 steps\nEpisode 4750 terminated after 30 steps\nEpisode 4751 terminated after 30 steps\nEpisode 4752 terminated after 30 steps\nEpisode 4753 terminated after 30 steps\nEpisode 4754 terminated after 30 steps\nEpisode 4755 terminated after 30 steps\nEpisode 4756 terminated after 30 steps\nEpisode 4757 terminated after 30 steps\nEpisode 4758 terminated after 30 steps\nEpisode 4759 terminated after 30 steps\nEpisode 4760 terminated after 30 steps\nEpisode 4761 terminated after 30 steps\nEpisode 4762 terminated after 30 steps\nEpisode 4763 terminated after 30 steps\nEpisode 4764 terminated after 30 steps\nEpisode 4765 terminated after 30 steps\nEpisode 4766 terminated after 30 steps\nEpisode 4767 terminated after 30 steps\nEpisode 4768 terminated after 30 steps\nEpisode 4769 terminated after 30 steps\nEpisode 4770 terminated after 30 steps\nEpisode 4771 terminated after 30 steps\nEpisode 4772 terminated after 30 steps\nEpisode 4773 terminated after 30 steps\nEpisode 4774 terminated after 30 steps\nEpisode 4775 terminated after 30 steps\nEpisode 4776 terminated after 30 steps\nEpisode 4777 terminated after 30 steps\nEpisode 4778 terminated after 30 steps\nEpisode 4779 terminated after 30 steps\nEpisode 4780 terminated after 30 steps\nEpisode 4781 terminated after 30 steps\nEpisode 4782 terminated after 30 steps\nEpisode 4783 terminated after 30 steps\nEpisode 4784 terminated after 30 steps\nEpisode 4785 terminated after 30 steps\nEpisode 4786 terminated after 30 steps\nEpisode 4787 terminated after 30 steps\nEpisode 4788 terminated after 30 steps\nEpisode 4789 terminated after 30 steps\nEpisode 4790 terminated after 30 steps\nEpisode 4791 terminated after 30 steps\nEpisode 4792 terminated after 30 steps\nEpisode 4793 terminated after 30 steps\nEpisode 4794 terminated after 30 steps\nEpisode 4795 terminated after 30 steps\nEpisode 4796 terminated after 30 steps\nEpisode 4797 terminated after 30 steps\nEpisode 4798 terminated after 30 steps\nEpisode 4799 terminated after 30 steps\nEpisode 4800\nEpisode 4800 terminated after 30 steps\nEpisode 4801 terminated after 30 steps\nEpisode 4802 terminated after 30 steps\nEpisode 4803 terminated after 30 steps\nEpisode 4804 terminated after 30 steps\nEpisode 4805 terminated after 30 steps\nEpisode 4806 terminated after 30 steps\nEpisode 4807 terminated after 30 steps\nEpisode 4808 terminated after 30 steps\nEpisode 4809 terminated after 30 steps\nEpisode 4810 terminated after 30 steps\nEpisode 4811 terminated after 30 steps\nEpisode 4812 terminated after 30 steps\nEpisode 4813 terminated after 30 steps\nEpisode 4814 terminated after 30 steps\nEpisode 4815 terminated after 30 steps\nEpisode 4816 terminated after 30 steps\nEpisode 4817 terminated after 30 steps\nEpisode 4818 terminated after 30 steps\nEpisode 4819 terminated after 30 steps\nEpisode 4820 terminated after 30 steps\nEpisode 4821 terminated after 30 steps\nEpisode 4822 terminated after 30 steps\nEpisode 4823 terminated after 30 steps\nEpisode 4824 terminated after 30 steps\nEpisode 4825 terminated after 30 steps\nEpisode 4826 terminated after 30 steps\nEpisode 4827 terminated after 30 steps\nEpisode 4828 terminated after 30 steps\nEpisode 4829 terminated after 30 steps\nEpisode 4830 terminated after 30 steps\nEpisode 4831 terminated after 30 steps\nEpisode 4832 terminated after 30 steps\nEpisode 4833 terminated after 30 steps\nEpisode 4834 terminated after 30 steps\nEpisode 4835 terminated after 30 steps\nEpisode 4836 terminated after 30 steps\nEpisode 4837 terminated after 30 steps\nEpisode 4838 terminated after 30 steps\nEpisode 4839 terminated after 30 steps\nEpisode 4840 terminated after 30 steps\nEpisode 4841 terminated after 30 steps\nEpisode 4842 terminated after 30 steps\nEpisode 4843 terminated after 30 steps\nEpisode 4844 terminated after 30 steps\nEpisode 4845 terminated after 30 steps\nEpisode 4846 terminated after 30 steps\nEpisode 4847 terminated after 30 steps\nEpisode 4848 terminated after 30 steps\nEpisode 4849 terminated after 30 steps\nEpisode 4850 terminated after 30 steps\nEpisode 4851 terminated after 30 steps\nEpisode 4852 terminated after 30 steps\nEpisode 4853 terminated after 30 steps\nEpisode 4854 terminated after 30 steps\nEpisode 4855 terminated after 30 steps\nEpisode 4856 terminated after 30 steps\nEpisode 4857 terminated after 30 steps\nEpisode 4858 terminated after 30 steps\nEpisode 4859 terminated after 30 steps\nEpisode 4860 terminated after 30 steps\nEpisode 4861 terminated after 30 steps\nEpisode 4862 terminated after 30 steps\nEpisode 4863 terminated after 30 steps\nEpisode 4864 terminated after 30 steps\nEpisode 4865 terminated after 30 steps\nEpisode 4866 terminated after 30 steps\nEpisode 4867 terminated after 30 steps\nEpisode 4868 terminated after 30 steps\nEpisode 4869 terminated after 30 steps\nEpisode 4870 terminated after 30 steps\nEpisode 4871 terminated after 30 steps\nEpisode 4872 terminated after 30 steps\nEpisode 4873 terminated after 30 steps\nEpisode 4874 terminated after 30 steps\nEpisode 4875 terminated after 30 steps\nEpisode 4876 terminated after 30 steps\nEpisode 4877 terminated after 30 steps\nEpisode 4878 terminated after 30 steps\nEpisode 4879 terminated after 30 steps\nEpisode 4880 terminated after 30 steps\nEpisode 4881 terminated after 30 steps\nEpisode 4882 terminated after 30 steps\nEpisode 4883 terminated after 30 steps\nEpisode 4884 terminated after 30 steps\nEpisode 4885 terminated after 30 steps\nEpisode 4886 terminated after 30 steps\nEpisode 4887 terminated after 30 steps\nEpisode 4888 terminated after 30 steps\nEpisode 4889 terminated after 30 steps\nEpisode 4890 terminated after 30 steps\nEpisode 4891 terminated after 30 steps\nEpisode 4892 terminated after 30 steps\nEpisode 4893 terminated after 30 steps\nEpisode 4894 terminated after 30 steps\nEpisode 4895 terminated after 30 steps\nEpisode 4896 terminated after 30 steps\nEpisode 4897 terminated after 30 steps\nEpisode 4898 terminated after 30 steps\nEpisode 4899 terminated after 30 steps\nEpisode 4900\nEpisode 4900 terminated after 30 steps\nEpisode 4901 terminated after 30 steps\nEpisode 4902 terminated after 30 steps\nEpisode 4903 terminated after 30 steps\nEpisode 4904 terminated after 30 steps\nEpisode 4905 terminated after 30 steps\nEpisode 4906 terminated after 30 steps\nEpisode 4907 terminated after 30 steps\nEpisode 4908 terminated after 30 steps\nEpisode 4909 terminated after 30 steps\nEpisode 4910 terminated after 30 steps\nEpisode 4911 terminated after 30 steps\nEpisode 4912 terminated after 30 steps\nEpisode 4913 terminated after 30 steps\nEpisode 4914 terminated after 30 steps\nEpisode 4915 terminated after 30 steps\nEpisode 4916 terminated after 30 steps\nEpisode 4917 terminated after 30 steps\nEpisode 4918 terminated after 30 steps\nEpisode 4919 terminated after 30 steps\nEpisode 4920 terminated after 30 steps\nEpisode 4921 terminated after 30 steps\nEpisode 4922 terminated after 30 steps\nEpisode 4923 terminated after 30 steps\nEpisode 4924 terminated after 30 steps\nEpisode 4925 terminated after 30 steps\nEpisode 4926 terminated after 30 steps\nEpisode 4927 terminated after 30 steps\nEpisode 4928 terminated after 30 steps\nEpisode 4929 terminated after 30 steps\nEpisode 4930 terminated after 30 steps\nEpisode 4931 terminated after 30 steps\nEpisode 4932 terminated after 30 steps\nEpisode 4933 terminated after 30 steps\nEpisode 4934 terminated after 30 steps\nEpisode 4935 terminated after 30 steps\nEpisode 4936 terminated after 30 steps\nEpisode 4937 terminated after 30 steps\nEpisode 4938 terminated after 30 steps\nEpisode 4939 terminated after 30 steps\nEpisode 4940 terminated after 30 steps\nEpisode 4941 terminated after 30 steps\nEpisode 4942 terminated after 30 steps\nEpisode 4943 terminated after 30 steps\nEpisode 4944 terminated after 30 steps\nEpisode 4945 terminated after 30 steps\nEpisode 4946 terminated after 30 steps\nEpisode 4947 terminated after 30 steps\nEpisode 4948 terminated after 30 steps\nEpisode 4949 terminated after 30 steps\nEpisode 4950 terminated after 30 steps\nEpisode 4951 terminated after 30 steps\nEpisode 4952 terminated after 30 steps\nEpisode 4953 terminated after 30 steps\nEpisode 4954 terminated after 30 steps\nEpisode 4955 terminated after 30 steps\nEpisode 4956 terminated after 30 steps\nEpisode 4957 terminated after 30 steps\nEpisode 4958 terminated after 30 steps\nEpisode 4959 terminated after 30 steps\nEpisode 4960 terminated after 30 steps\nEpisode 4961 terminated after 30 steps\nEpisode 4962 terminated after 30 steps\nEpisode 4963 terminated after 30 steps\nEpisode 4964 terminated after 30 steps\nEpisode 4965 terminated after 30 steps\nEpisode 4966 terminated after 30 steps\nEpisode 4967 terminated after 30 steps\nEpisode 4968 terminated after 30 steps\nEpisode 4969 terminated after 30 steps\nEpisode 4970 terminated after 30 steps\nEpisode 4971 terminated after 30 steps\nEpisode 4972 terminated after 30 steps\nEpisode 4973 terminated after 30 steps\nEpisode 4974 terminated after 30 steps\nEpisode 4975 terminated after 30 steps\nEpisode 4976 terminated after 30 steps\nEpisode 4977 terminated after 30 steps\nEpisode 4978 terminated after 30 steps\nEpisode 4979 terminated after 30 steps\nEpisode 4980 terminated after 30 steps\nEpisode 4981 terminated after 30 steps\nEpisode 4982 terminated after 30 steps\nEpisode 4983 terminated after 30 steps\nEpisode 4984 terminated after 30 steps\nEpisode 4985 terminated after 30 steps\nEpisode 4986 terminated after 30 steps\nEpisode 4987 terminated after 30 steps\nEpisode 4988 terminated after 30 steps\nEpisode 4989 terminated after 30 steps\nEpisode 4990 terminated after 30 steps\nEpisode 4991 terminated after 30 steps\nEpisode 4992 terminated after 30 steps\nEpisode 4993 terminated after 30 steps\nEpisode 4994 terminated after 30 steps\nEpisode 4995 terminated after 30 steps\nEpisode 4996 terminated after 30 steps\nEpisode 4997 terminated after 30 steps\nEpisode 4998 terminated after 30 steps\nEpisode 4999 terminated after 30 steps\nEpisode 5000\nEpisode 5000 terminated after 30 steps\nEpisode 5001 terminated after 30 steps\nEpisode 5002 terminated after 30 steps\nEpisode 5003 terminated after 30 steps\nEpisode 5004 terminated after 30 steps\nEpisode 5005 terminated after 30 steps\nEpisode 5006 terminated after 30 steps\nEpisode 5007 terminated after 30 steps\nEpisode 5008 terminated after 30 steps\nEpisode 5009 terminated after 30 steps\nEpisode 5010 terminated after 30 steps\nEpisode 5011 terminated after 30 steps\nEpisode 5012 terminated after 30 steps\nEpisode 5013 terminated after 30 steps\nEpisode 5014 terminated after 30 steps\nEpisode 5015 terminated after 30 steps\nEpisode 5016 terminated after 30 steps\nEpisode 5017 terminated after 30 steps\nEpisode 5018 terminated after 30 steps\nEpisode 5019 terminated after 30 steps\nEpisode 5020 terminated after 30 steps\nEpisode 5021 terminated after 30 steps\nEpisode 5022 terminated after 30 steps\nEpisode 5023 terminated after 30 steps\nEpisode 5024 terminated after 30 steps\nEpisode 5025 terminated after 30 steps\nEpisode 5026 terminated after 30 steps\nEpisode 5027 terminated after 30 steps\nEpisode 5028 terminated after 30 steps\nEpisode 5029 terminated after 30 steps\nEpisode 5030 terminated after 30 steps\nEpisode 5031 terminated after 30 steps\nEpisode 5032 terminated after 30 steps\nEpisode 5033 terminated after 30 steps\nEpisode 5034 terminated after 30 steps\nEpisode 5035 terminated after 30 steps\nEpisode 5036 terminated after 30 steps\nEpisode 5037 terminated after 30 steps\nEpisode 5038 terminated after 30 steps\nEpisode 5039 terminated after 30 steps\nEpisode 5040 terminated after 30 steps\nEpisode 5041 terminated after 30 steps\nEpisode 5042 terminated after 30 steps\nEpisode 5043 terminated after 30 steps\nEpisode 5044 terminated after 30 steps\nEpisode 5045 terminated after 30 steps\nEpisode 5046 terminated after 30 steps\nEpisode 5047 terminated after 30 steps\nEpisode 5048 terminated after 30 steps\nEpisode 5049 terminated after 30 steps\nEpisode 5050 terminated after 30 steps\nEpisode 5051 terminated after 30 steps\nEpisode 5052 terminated after 30 steps\nEpisode 5053 terminated after 30 steps\nEpisode 5054 terminated after 30 steps\nEpisode 5055 terminated after 30 steps\nEpisode 5056 terminated after 30 steps\nEpisode 5057 terminated after 30 steps\nEpisode 5058 terminated after 30 steps\nEpisode 5059 terminated after 30 steps\nEpisode 5060 terminated after 30 steps\nEpisode 5061 terminated after 30 steps\nEpisode 5062 terminated after 30 steps\nEpisode 5063 terminated after 30 steps\nEpisode 5064 terminated after 30 steps\nEpisode 5065 terminated after 30 steps\nEpisode 5066 terminated after 30 steps\nEpisode 5067 terminated after 30 steps\nEpisode 5068 terminated after 30 steps\nEpisode 5069 terminated after 30 steps\nEpisode 5070 terminated after 30 steps\nEpisode 5071 terminated after 30 steps\nEpisode 5072 terminated after 30 steps\nEpisode 5073 terminated after 30 steps\nEpisode 5074 terminated after 30 steps\nEpisode 5075 terminated after 30 steps\nEpisode 5076 terminated after 30 steps\nEpisode 5077 terminated after 30 steps\nEpisode 5078 terminated after 30 steps\nEpisode 5079 terminated after 30 steps\nEpisode 5080 terminated after 30 steps\nEpisode 5081 terminated after 30 steps\nEpisode 5082 terminated after 30 steps\nEpisode 5083 terminated after 30 steps\nEpisode 5084 terminated after 30 steps\nEpisode 5085 terminated after 30 steps\nEpisode 5086 terminated after 30 steps\nEpisode 5087 terminated after 30 steps\nEpisode 5088 terminated after 30 steps\nEpisode 5089 terminated after 30 steps\nEpisode 5090 terminated after 30 steps\nEpisode 5091 terminated after 30 steps\nEpisode 5092 terminated after 30 steps\nEpisode 5093 terminated after 30 steps\nEpisode 5094 terminated after 30 steps\nEpisode 5095 terminated after 30 steps\nEpisode 5096 terminated after 30 steps\nEpisode 5097 terminated after 30 steps\nEpisode 5098 terminated after 30 steps\nEpisode 5099 terminated after 30 steps\nEpisode 5100\nEpisode 5100 terminated after 30 steps\nEpisode 5101 terminated after 30 steps\nEpisode 5102 terminated after 30 steps\nEpisode 5103 terminated after 30 steps\nEpisode 5104 terminated after 30 steps\nEpisode 5105 terminated after 30 steps\nEpisode 5106 terminated after 30 steps\nEpisode 5107 terminated after 30 steps\nEpisode 5108 terminated after 30 steps\nEpisode 5109 terminated after 30 steps\nEpisode 5110 terminated after 30 steps\nEpisode 5111 terminated after 30 steps\nEpisode 5112 terminated after 30 steps\nEpisode 5113 terminated after 30 steps\nEpisode 5114 terminated after 30 steps\nEpisode 5115 terminated after 30 steps\nEpisode 5116 terminated after 30 steps\nEpisode 5117 terminated after 30 steps\nEpisode 5118 terminated after 30 steps\nEpisode 5119 terminated after 30 steps\nEpisode 5120 terminated after 30 steps\nEpisode 5121 terminated after 30 steps\nEpisode 5122 terminated after 30 steps\nEpisode 5123 terminated after 30 steps\nEpisode 5124 terminated after 30 steps\nEpisode 5125 terminated after 30 steps\nEpisode 5126 terminated after 30 steps\nEpisode 5127 terminated after 30 steps\nEpisode 5128 terminated after 30 steps\nEpisode 5129 terminated after 30 steps\nEpisode 5130 terminated after 30 steps\nEpisode 5131 terminated after 30 steps\nEpisode 5132 terminated after 30 steps\nEpisode 5133 terminated after 30 steps\nEpisode 5134 terminated after 30 steps\nEpisode 5135 terminated after 30 steps\nEpisode 5136 terminated after 30 steps\nEpisode 5137 terminated after 30 steps\nEpisode 5138 terminated after 30 steps\nEpisode 5139 terminated after 30 steps\nEpisode 5140 terminated after 30 steps\nEpisode 5141 terminated after 30 steps\nEpisode 5142 terminated after 30 steps\nEpisode 5143 terminated after 30 steps\nEpisode 5144 terminated after 30 steps\nEpisode 5145 terminated after 30 steps\nEpisode 5146 terminated after 30 steps\nEpisode 5147 terminated after 30 steps\nEpisode 5148 terminated after 30 steps\nEpisode 5149 terminated after 30 steps\nEpisode 5150 terminated after 30 steps\nEpisode 5151 terminated after 30 steps\nEpisode 5152 terminated after 30 steps\nEpisode 5153 terminated after 30 steps\nEpisode 5154 terminated after 30 steps\nEpisode 5155 terminated after 30 steps\nEpisode 5156 terminated after 30 steps\nEpisode 5157 terminated after 30 steps\nEpisode 5158 terminated after 30 steps\nEpisode 5159 terminated after 30 steps\nEpisode 5160 terminated after 30 steps\nEpisode 5161 terminated after 30 steps\nEpisode 5162 terminated after 30 steps\nEpisode 5163 terminated after 30 steps\nEpisode 5164 terminated after 30 steps\nEpisode 5165 terminated after 30 steps\nEpisode 5166 terminated after 30 steps\nEpisode 5167 terminated after 30 steps\nEpisode 5168 terminated after 30 steps\nEpisode 5169 terminated after 30 steps\nEpisode 5170 terminated after 30 steps\nEpisode 5171 terminated after 30 steps\nEpisode 5172 terminated after 30 steps\nEpisode 5173 terminated after 30 steps\nEpisode 5174 terminated after 30 steps\nEpisode 5175 terminated after 30 steps\nEpisode 5176 terminated after 30 steps\nEpisode 5177 terminated after 30 steps\nEpisode 5178 terminated after 30 steps\nEpisode 5179 terminated after 30 steps\nEpisode 5180 terminated after 30 steps\nEpisode 5181 terminated after 30 steps\nEpisode 5182 terminated after 30 steps\nEpisode 5183 terminated after 30 steps\nEpisode 5184 terminated after 30 steps\nEpisode 5185 terminated after 30 steps\nEpisode 5186 terminated after 30 steps\nEpisode 5187 terminated after 30 steps\nEpisode 5188 terminated after 30 steps\nEpisode 5189 terminated after 30 steps\nEpisode 5190 terminated after 30 steps\nEpisode 5191 terminated after 30 steps\nEpisode 5192 terminated after 30 steps\nEpisode 5193 terminated after 30 steps\nEpisode 5194 terminated after 30 steps\nEpisode 5195 terminated after 30 steps\nEpisode 5196 terminated after 30 steps\nEpisode 5197 terminated after 30 steps\nEpisode 5198 terminated after 30 steps\nEpisode 5199 terminated after 30 steps\nEpisode 5200\nEpisode 5200 terminated after 30 steps\nEpisode 5201 terminated after 30 steps\nEpisode 5202 terminated after 30 steps\nEpisode 5203 terminated after 30 steps\nEpisode 5204 terminated after 30 steps\nEpisode 5205 terminated after 30 steps\nEpisode 5206 terminated after 30 steps\nEpisode 5207 terminated after 30 steps\nEpisode 5208 terminated after 30 steps\nEpisode 5209 terminated after 30 steps\nEpisode 5210 terminated after 30 steps\nEpisode 5211 terminated after 30 steps\nEpisode 5212 terminated after 30 steps\nEpisode 5213 terminated after 30 steps\nEpisode 5214 terminated after 30 steps\nEpisode 5215 terminated after 30 steps\nEpisode 5216 terminated after 30 steps\nEpisode 5217 terminated after 30 steps\nEpisode 5218 terminated after 30 steps\nEpisode 5219 terminated after 30 steps\nEpisode 5220 terminated after 30 steps\nEpisode 5221 terminated after 30 steps\nEpisode 5222 terminated after 30 steps\nEpisode 5223 terminated after 30 steps\nEpisode 5224 terminated after 30 steps\nEpisode 5225 terminated after 30 steps\nEpisode 5226 terminated after 30 steps\nEpisode 5227 terminated after 30 steps\nEpisode 5228 terminated after 30 steps\nEpisode 5229 terminated after 30 steps\nEpisode 5230 terminated after 30 steps\nEpisode 5231 terminated after 30 steps\nEpisode 5232 terminated after 30 steps\nEpisode 5233 terminated after 30 steps\nEpisode 5234 terminated after 30 steps\nEpisode 5235 terminated after 30 steps\nEpisode 5236 terminated after 30 steps\nEpisode 5237 terminated after 30 steps\nEpisode 5238 terminated after 30 steps\nEpisode 5239 terminated after 30 steps\nEpisode 5240 terminated after 30 steps\nEpisode 5241 terminated after 30 steps\nEpisode 5242 terminated after 30 steps\nEpisode 5243 terminated after 30 steps\nEpisode 5244 terminated after 30 steps\nEpisode 5245 terminated after 30 steps\nEpisode 5246 terminated after 30 steps\nEpisode 5247 terminated after 30 steps\nEpisode 5248 terminated after 30 steps\nEpisode 5249 terminated after 30 steps\nEpisode 5250 terminated after 30 steps\nEpisode 5251 terminated after 30 steps\nEpisode 5252 terminated after 30 steps\nEpisode 5253 terminated after 30 steps\nEpisode 5254 terminated after 30 steps\nEpisode 5255 terminated after 30 steps\nEpisode 5256 terminated after 30 steps\nEpisode 5257 terminated after 30 steps\nEpisode 5258 terminated after 30 steps\nEpisode 5259 terminated after 30 steps\nEpisode 5260 terminated after 30 steps\nEpisode 5261 terminated after 30 steps\nEpisode 5262 terminated after 30 steps\nEpisode 5263 terminated after 30 steps\nEpisode 5264 terminated after 30 steps\nEpisode 5265 terminated after 30 steps\nEpisode 5266 terminated after 30 steps\nEpisode 5267 terminated after 30 steps\nEpisode 5268 terminated after 30 steps\nEpisode 5269 terminated after 30 steps\nEpisode 5270 terminated after 30 steps\nEpisode 5271 terminated after 30 steps\nEpisode 5272 terminated after 30 steps\nEpisode 5273 terminated after 30 steps\nEpisode 5274 terminated after 30 steps\nEpisode 5275 terminated after 30 steps\nEpisode 5276 terminated after 30 steps\nEpisode 5277 terminated after 30 steps\nEpisode 5278 terminated after 30 steps\nEpisode 5279 terminated after 30 steps\nEpisode 5280 terminated after 30 steps\nEpisode 5281 terminated after 30 steps\nEpisode 5282 terminated after 30 steps\nEpisode 5283 terminated after 30 steps\nEpisode 5284 terminated after 30 steps\nEpisode 5285 terminated after 30 steps\nEpisode 5286 terminated after 30 steps\nEpisode 5287 terminated after 30 steps\nEpisode 5288 terminated after 30 steps\nEpisode 5289 terminated after 30 steps\nEpisode 5290 terminated after 30 steps\nEpisode 5291 terminated after 30 steps\nEpisode 5292 terminated after 30 steps\nEpisode 5293 terminated after 30 steps\nEpisode 5294 terminated after 30 steps\nEpisode 5295 terminated after 30 steps\nEpisode 5296 terminated after 30 steps\nEpisode 5297 terminated after 30 steps\nEpisode 5298 terminated after 30 steps\nEpisode 5299 terminated after 30 steps\nEpisode 5300\nEpisode 5300 terminated after 30 steps\nEpisode 5301 terminated after 30 steps\nEpisode 5302 terminated after 30 steps\nEpisode 5303 terminated after 30 steps\nEpisode 5304 terminated after 30 steps\nEpisode 5305 terminated after 30 steps\nEpisode 5306 terminated after 30 steps\nEpisode 5307 terminated after 30 steps\nEpisode 5308 terminated after 30 steps\nEpisode 5309 terminated after 30 steps\nEpisode 5310 terminated after 30 steps\nEpisode 5311 terminated after 30 steps\nEpisode 5312 terminated after 30 steps\nEpisode 5313 terminated after 30 steps\nEpisode 5314 terminated after 30 steps\nEpisode 5315 terminated after 30 steps\nEpisode 5316 terminated after 30 steps\nEpisode 5317 terminated after 30 steps\nEpisode 5318 terminated after 30 steps\nEpisode 5319 terminated after 30 steps\nEpisode 5320 terminated after 30 steps\nEpisode 5321 terminated after 30 steps\nEpisode 5322 terminated after 30 steps\nEpisode 5323 terminated after 30 steps\nEpisode 5324 terminated after 30 steps\nEpisode 5325 terminated after 30 steps\nEpisode 5326 terminated after 30 steps\nEpisode 5327 terminated after 30 steps\nEpisode 5328 terminated after 30 steps\nEpisode 5329 terminated after 30 steps\nEpisode 5330 terminated after 30 steps\nEpisode 5331 terminated after 30 steps\nEpisode 5332 terminated after 30 steps\nEpisode 5333 terminated after 30 steps\nEpisode 5334 terminated after 30 steps\nEpisode 5335 terminated after 30 steps\nEpisode 5336 terminated after 30 steps\nEpisode 5337 terminated after 30 steps\nEpisode 5338 terminated after 30 steps\nEpisode 5339 terminated after 30 steps\nEpisode 5340 terminated after 30 steps\nEpisode 5341 terminated after 30 steps\nEpisode 5342 terminated after 30 steps\nEpisode 5343 terminated after 30 steps\nEpisode 5344 terminated after 30 steps\nEpisode 5345 terminated after 30 steps\nEpisode 5346 terminated after 30 steps\nEpisode 5347 terminated after 30 steps\nEpisode 5348 terminated after 30 steps\nEpisode 5349 terminated after 30 steps\nEpisode 5350 terminated after 30 steps\nEpisode 5351 terminated after 30 steps\nEpisode 5352 terminated after 30 steps\nEpisode 5353 terminated after 30 steps\nEpisode 5354 terminated after 30 steps\nEpisode 5355 terminated after 30 steps\nEpisode 5356 terminated after 30 steps\nEpisode 5357 terminated after 30 steps\nEpisode 5358 terminated after 30 steps\nEpisode 5359 terminated after 30 steps\nEpisode 5360 terminated after 30 steps\nEpisode 5361 terminated after 30 steps\nEpisode 5362 terminated after 30 steps\nEpisode 5363 terminated after 30 steps\nEpisode 5364 terminated after 30 steps\nEpisode 5365 terminated after 30 steps\nEpisode 5366 terminated after 30 steps\nEpisode 5367 terminated after 30 steps\nEpisode 5368 terminated after 30 steps\nEpisode 5369 terminated after 30 steps\nEpisode 5370 terminated after 30 steps\nEpisode 5371 terminated after 30 steps\nEpisode 5372 terminated after 30 steps\nEpisode 5373 terminated after 30 steps\nEpisode 5374 terminated after 30 steps\nEpisode 5375 terminated after 30 steps\nEpisode 5376 terminated after 30 steps\nEpisode 5377 terminated after 30 steps\nEpisode 5378 terminated after 30 steps\nEpisode 5379 terminated after 30 steps\nEpisode 5380 terminated after 30 steps\nEpisode 5381 terminated after 30 steps\nEpisode 5382 terminated after 30 steps\nEpisode 5383 terminated after 30 steps\nEpisode 5384 terminated after 30 steps\nEpisode 5385 terminated after 30 steps\nEpisode 5386 terminated after 30 steps\nEpisode 5387 terminated after 30 steps\nEpisode 5388 terminated after 30 steps\nEpisode 5389 terminated after 30 steps\nEpisode 5390 terminated after 30 steps\nEpisode 5391 terminated after 30 steps\nEpisode 5392 terminated after 30 steps\nEpisode 5393 terminated after 30 steps\nEpisode 5394 terminated after 30 steps\nEpisode 5395 terminated after 30 steps\nEpisode 5396 terminated after 30 steps\nEpisode 5397 terminated after 30 steps\nEpisode 5398 terminated after 30 steps\nEpisode 5399 terminated after 30 steps\nEpisode 5400\nEpisode 5400 terminated after 30 steps\nEpisode 5401 terminated after 30 steps\nEpisode 5402 terminated after 30 steps\nEpisode 5403 terminated after 30 steps\nEpisode 5404 terminated after 30 steps\nEpisode 5405 terminated after 30 steps\nEpisode 5406 terminated after 30 steps\nEpisode 5407 terminated after 30 steps\nEpisode 5408 terminated after 30 steps\nEpisode 5409 terminated after 30 steps\nEpisode 5410 terminated after 30 steps\nEpisode 5411 terminated after 30 steps\nEpisode 5412 terminated after 30 steps\nEpisode 5413 terminated after 30 steps\nEpisode 5414 terminated after 30 steps\nEpisode 5415 terminated after 30 steps\nEpisode 5416 terminated after 30 steps\nEpisode 5417 terminated after 30 steps\nEpisode 5418 terminated after 30 steps\nEpisode 5419 terminated after 30 steps\nEpisode 5420 terminated after 30 steps\nEpisode 5421 terminated after 30 steps\nEpisode 5422 terminated after 30 steps\nEpisode 5423 terminated after 30 steps\nEpisode 5424 terminated after 30 steps\nEpisode 5425 terminated after 30 steps\nEpisode 5426 terminated after 30 steps\nEpisode 5427 terminated after 30 steps\nEpisode 5428 terminated after 30 steps\nEpisode 5429 terminated after 30 steps\nEpisode 5430 terminated after 30 steps\nEpisode 5431 terminated after 30 steps\nEpisode 5432 terminated after 30 steps\nEpisode 5433 terminated after 30 steps\nEpisode 5434 terminated after 30 steps\nEpisode 5435 terminated after 30 steps\nEpisode 5436 terminated after 30 steps\nEpisode 5437 terminated after 30 steps\nEpisode 5438 terminated after 30 steps\nEpisode 5439 terminated after 30 steps\nEpisode 5440 terminated after 30 steps\nEpisode 5441 terminated after 30 steps\nEpisode 5442 terminated after 30 steps\nEpisode 5443 terminated after 30 steps\nEpisode 5444 terminated after 30 steps\nEpisode 5445 terminated after 30 steps\nEpisode 5446 terminated after 30 steps\nEpisode 5447 terminated after 30 steps\nEpisode 5448 terminated after 30 steps\nEpisode 5449 terminated after 30 steps\nEpisode 5450 terminated after 30 steps\nEpisode 5451 terminated after 30 steps\nEpisode 5452 terminated after 30 steps\nEpisode 5453 terminated after 30 steps\nEpisode 5454 terminated after 30 steps\nEpisode 5455 terminated after 30 steps\nEpisode 5456 terminated after 30 steps\nEpisode 5457 terminated after 30 steps\nEpisode 5458 terminated after 30 steps\nEpisode 5459 terminated after 30 steps\nEpisode 5460 terminated after 30 steps\nEpisode 5461 terminated after 30 steps\nEpisode 5462 terminated after 30 steps\nEpisode 5463 terminated after 30 steps\nEpisode 5464 terminated after 30 steps\nEpisode 5465 terminated after 30 steps\nEpisode 5466 terminated after 30 steps\nEpisode 5467 terminated after 30 steps\nEpisode 5468 terminated after 30 steps\nEpisode 5469 terminated after 30 steps\nEpisode 5470 terminated after 30 steps\nEpisode 5471 terminated after 30 steps\nEpisode 5472 terminated after 30 steps\nEpisode 5473 terminated after 30 steps\nEpisode 5474 terminated after 30 steps\nEpisode 5475 terminated after 30 steps\nEpisode 5476 terminated after 30 steps\nEpisode 5477 terminated after 30 steps\nEpisode 5478 terminated after 30 steps\nEpisode 5479 terminated after 30 steps\nEpisode 5480 terminated after 30 steps\nEpisode 5481 terminated after 30 steps\nEpisode 5482 terminated after 30 steps\nEpisode 5483 terminated after 30 steps\nEpisode 5484 terminated after 30 steps\nEpisode 5485 terminated after 30 steps\nEpisode 5486 terminated after 30 steps\nEpisode 5487 terminated after 30 steps\nEpisode 5488 terminated after 30 steps\nEpisode 5489 terminated after 30 steps\nEpisode 5490 terminated after 30 steps\nEpisode 5491 terminated after 30 steps\nEpisode 5492 terminated after 30 steps\nEpisode 5493 terminated after 30 steps\nEpisode 5494 terminated after 30 steps\nEpisode 5495 terminated after 30 steps\nEpisode 5496 terminated after 30 steps\nEpisode 5497 terminated after 30 steps\nEpisode 5498 terminated after 30 steps\nEpisode 5499 terminated after 30 steps\nEpisode 5500\nEpisode 5500 terminated after 30 steps\nEpisode 5501 terminated after 30 steps\nEpisode 5502 terminated after 30 steps\nEpisode 5503 terminated after 30 steps\nEpisode 5504 terminated after 30 steps\nEpisode 5505 terminated after 30 steps\nEpisode 5506 terminated after 30 steps\nEpisode 5507 terminated after 30 steps\nEpisode 5508 terminated after 30 steps\nEpisode 5509 terminated after 30 steps\nEpisode 5510 terminated after 30 steps\nEpisode 5511 terminated after 30 steps\nEpisode 5512 terminated after 30 steps\nEpisode 5513 terminated after 30 steps\nEpisode 5514 terminated after 30 steps\nEpisode 5515 terminated after 30 steps\nEpisode 5516 terminated after 30 steps\nEpisode 5517 terminated after 30 steps\nEpisode 5518 terminated after 30 steps\nEpisode 5519 terminated after 30 steps\nEpisode 5520 terminated after 30 steps\nEpisode 5521 terminated after 30 steps\nEpisode 5522 terminated after 30 steps\nEpisode 5523 terminated after 30 steps\nEpisode 5524 terminated after 30 steps\nEpisode 5525 terminated after 30 steps\nEpisode 5526 terminated after 30 steps\nEpisode 5527 terminated after 30 steps\nEpisode 5528 terminated after 30 steps\nEpisode 5529 terminated after 30 steps\nEpisode 5530 terminated after 30 steps\nEpisode 5531 terminated after 30 steps\nEpisode 5532 terminated after 30 steps\nEpisode 5533 terminated after 30 steps\nEpisode 5534 terminated after 30 steps\nEpisode 5535 terminated after 30 steps\nEpisode 5536 terminated after 30 steps\nEpisode 5537 terminated after 30 steps\nEpisode 5538 terminated after 30 steps\nEpisode 5539 terminated after 30 steps\nEpisode 5540 terminated after 30 steps\nEpisode 5541 terminated after 30 steps\nEpisode 5542 terminated after 30 steps\nEpisode 5543 terminated after 30 steps\nEpisode 5544 terminated after 30 steps\nEpisode 5545 terminated after 30 steps\nEpisode 5546 terminated after 30 steps\nEpisode 5547 terminated after 30 steps\nEpisode 5548 terminated after 30 steps\nEpisode 5549 terminated after 30 steps\nEpisode 5550 terminated after 30 steps\nEpisode 5551 terminated after 30 steps\nEpisode 5552 terminated after 30 steps\nEpisode 5553 terminated after 30 steps\nEpisode 5554 terminated after 30 steps\nEpisode 5555 terminated after 30 steps\nEpisode 5556 terminated after 30 steps\nEpisode 5557 terminated after 30 steps\nEpisode 5558 terminated after 30 steps\nEpisode 5559 terminated after 30 steps\nEpisode 5560 terminated after 30 steps\nEpisode 5561 terminated after 30 steps\nEpisode 5562 terminated after 30 steps\nEpisode 5563 terminated after 30 steps\nEpisode 5564 terminated after 30 steps\nEpisode 5565 terminated after 30 steps\nEpisode 5566 terminated after 30 steps\nEpisode 5567 terminated after 30 steps\nEpisode 5568 terminated after 30 steps\nEpisode 5569 terminated after 30 steps\nEpisode 5570 terminated after 30 steps\nEpisode 5571 terminated after 30 steps\nEpisode 5572 terminated after 30 steps\nEpisode 5573 terminated after 30 steps\nEpisode 5574 terminated after 30 steps\nEpisode 5575 terminated after 30 steps\nEpisode 5576 terminated after 30 steps\nEpisode 5577 terminated after 30 steps\nEpisode 5578 terminated after 30 steps\nEpisode 5579 terminated after 30 steps\nEpisode 5580 terminated after 30 steps\nEpisode 5581 terminated after 30 steps\nEpisode 5582 terminated after 30 steps\nEpisode 5583 terminated after 30 steps\nEpisode 5584 terminated after 30 steps\nEpisode 5585 terminated after 30 steps\nEpisode 5586 terminated after 30 steps\nEpisode 5587 terminated after 30 steps\nEpisode 5588 terminated after 30 steps\nEpisode 5589 terminated after 30 steps\nEpisode 5590 terminated after 30 steps\nEpisode 5591 terminated after 30 steps\nEpisode 5592 terminated after 30 steps\nEpisode 5593 terminated after 30 steps\nEpisode 5594 terminated after 30 steps\nEpisode 5595 terminated after 30 steps\nEpisode 5596 terminated after 30 steps\nEpisode 5597 terminated after 30 steps\nEpisode 5598 terminated after 30 steps\nEpisode 5599 terminated after 30 steps\nEpisode 5600\nEpisode 5600 terminated after 30 steps\nEpisode 5601 terminated after 30 steps\nEpisode 5602 terminated after 30 steps\nEpisode 5603 terminated after 30 steps\nEpisode 5604 terminated after 30 steps\nEpisode 5605 terminated after 30 steps\nEpisode 5606 terminated after 30 steps\nEpisode 5607 terminated after 30 steps\nEpisode 5608 terminated after 30 steps\nEpisode 5609 terminated after 30 steps\nEpisode 5610 terminated after 30 steps\nEpisode 5611 terminated after 30 steps\nEpisode 5612 terminated after 30 steps\nEpisode 5613 terminated after 30 steps\nEpisode 5614 terminated after 30 steps\nEpisode 5615 terminated after 30 steps\nEpisode 5616 terminated after 30 steps\nEpisode 5617 terminated after 30 steps\nEpisode 5618 terminated after 30 steps\nEpisode 5619 terminated after 30 steps\nEpisode 5620 terminated after 30 steps\nEpisode 5621 terminated after 30 steps\nEpisode 5622 terminated after 30 steps\nEpisode 5623 terminated after 30 steps\nEpisode 5624 terminated after 30 steps\nEpisode 5625 terminated after 30 steps\nEpisode 5626 terminated after 30 steps\nEpisode 5627 terminated after 30 steps\nEpisode 5628 terminated after 30 steps\nEpisode 5629 terminated after 30 steps\nEpisode 5630 terminated after 30 steps\nEpisode 5631 terminated after 30 steps\nEpisode 5632 terminated after 30 steps\nEpisode 5633 terminated after 30 steps\nEpisode 5634 terminated after 30 steps\nEpisode 5635 terminated after 30 steps\nEpisode 5636 terminated after 30 steps\nEpisode 5637 terminated after 30 steps\nEpisode 5638 terminated after 30 steps\nEpisode 5639 terminated after 30 steps\nEpisode 5640 terminated after 30 steps\nEpisode 5641 terminated after 30 steps\nEpisode 5642 terminated after 30 steps\nEpisode 5643 terminated after 30 steps\nEpisode 5644 terminated after 30 steps\nEpisode 5645 terminated after 30 steps\nEpisode 5646 terminated after 30 steps\nEpisode 5647 terminated after 30 steps\nEpisode 5648 terminated after 30 steps\nEpisode 5649 terminated after 30 steps\nEpisode 5650 terminated after 30 steps\nEpisode 5651 terminated after 30 steps\nEpisode 5652 terminated after 30 steps\nEpisode 5653 terminated after 30 steps\nEpisode 5654 terminated after 30 steps\nEpisode 5655 terminated after 30 steps\nEpisode 5656 terminated after 30 steps\nEpisode 5657 terminated after 30 steps\nEpisode 5658 terminated after 30 steps\nEpisode 5659 terminated after 30 steps\nEpisode 5660 terminated after 30 steps\nEpisode 5661 terminated after 30 steps\nEpisode 5662 terminated after 30 steps\nEpisode 5663 terminated after 30 steps\nEpisode 5664 terminated after 30 steps\nEpisode 5665 terminated after 30 steps\nEpisode 5666 terminated after 30 steps\nEpisode 5667 terminated after 30 steps\nEpisode 5668 terminated after 30 steps\nEpisode 5669 terminated after 30 steps\nEpisode 5670 terminated after 30 steps\nEpisode 5671 terminated after 30 steps\nEpisode 5672 terminated after 30 steps\nEpisode 5673 terminated after 30 steps\nEpisode 5674 terminated after 30 steps\nEpisode 5675 terminated after 30 steps\nEpisode 5676 terminated after 30 steps\nEpisode 5677 terminated after 30 steps\nEpisode 5678 terminated after 30 steps\nEpisode 5679 terminated after 30 steps\nEpisode 5680 terminated after 30 steps\nEpisode 5681 terminated after 30 steps\nEpisode 5682 terminated after 30 steps\nEpisode 5683 terminated after 30 steps\nEpisode 5684 terminated after 30 steps\nEpisode 5685 terminated after 30 steps\nEpisode 5686 terminated after 30 steps\nEpisode 5687 terminated after 30 steps\nEpisode 5688 terminated after 30 steps\nEpisode 5689 terminated after 30 steps\nEpisode 5690 terminated after 30 steps\nEpisode 5691 terminated after 30 steps\nEpisode 5692 terminated after 30 steps\nEpisode 5693 terminated after 30 steps\nEpisode 5694 terminated after 30 steps\nEpisode 5695 terminated after 30 steps\nEpisode 5696 terminated after 30 steps\nEpisode 5697 terminated after 30 steps\nEpisode 5698 terminated after 30 steps\nEpisode 5699 terminated after 30 steps\nEpisode 5700\nEpisode 5700 terminated after 30 steps\nEpisode 5701 terminated after 30 steps\nEpisode 5702 terminated after 30 steps\nEpisode 5703 terminated after 30 steps\nEpisode 5704 terminated after 30 steps\nEpisode 5705 terminated after 30 steps\nEpisode 5706 terminated after 30 steps\nEpisode 5707 terminated after 30 steps\nEpisode 5708 terminated after 30 steps\nEpisode 5709 terminated after 30 steps\nEpisode 5710 terminated after 30 steps\nEpisode 5711 terminated after 30 steps\nEpisode 5712 terminated after 30 steps\nEpisode 5713 terminated after 30 steps\nEpisode 5714 terminated after 30 steps\nEpisode 5715 terminated after 30 steps\nEpisode 5716 terminated after 30 steps\nEpisode 5717 terminated after 30 steps\nEpisode 5718 terminated after 30 steps\nEpisode 5719 terminated after 30 steps\nEpisode 5720 terminated after 30 steps\nEpisode 5721 terminated after 30 steps\nEpisode 5722 terminated after 30 steps\nEpisode 5723 terminated after 30 steps\nEpisode 5724 terminated after 30 steps\nEpisode 5725 terminated after 30 steps\nEpisode 5726 terminated after 30 steps\nEpisode 5727 terminated after 30 steps\nEpisode 5728 terminated after 30 steps\nEpisode 5729 terminated after 30 steps\nEpisode 5730 terminated after 30 steps\nEpisode 5731 terminated after 30 steps\nEpisode 5732 terminated after 30 steps\nEpisode 5733 terminated after 30 steps\nEpisode 5734 terminated after 30 steps\nEpisode 5735 terminated after 30 steps\nEpisode 5736 terminated after 30 steps\nEpisode 5737 terminated after 30 steps\nEpisode 5738 terminated after 30 steps\nEpisode 5739 terminated after 30 steps\nEpisode 5740 terminated after 30 steps\nEpisode 5741 terminated after 30 steps\nEpisode 5742 terminated after 30 steps\nEpisode 5743 terminated after 30 steps\nEpisode 5744 terminated after 30 steps\nEpisode 5745 terminated after 30 steps\nEpisode 5746 terminated after 30 steps\nEpisode 5747 terminated after 30 steps\nEpisode 5748 terminated after 30 steps\nEpisode 5749 terminated after 30 steps\nEpisode 5750 terminated after 30 steps\nEpisode 5751 terminated after 30 steps\nEpisode 5752 terminated after 30 steps\nEpisode 5753 terminated after 30 steps\nEpisode 5754 terminated after 30 steps\nEpisode 5755 terminated after 30 steps\nEpisode 5756 terminated after 30 steps\nEpisode 5757 terminated after 30 steps\nEpisode 5758 terminated after 30 steps\nEpisode 5759 terminated after 30 steps\nEpisode 5760 terminated after 30 steps\nEpisode 5761 terminated after 30 steps\nEpisode 5762 terminated after 30 steps\nEpisode 5763 terminated after 30 steps\nEpisode 5764 terminated after 30 steps\nEpisode 5765 terminated after 30 steps\nEpisode 5766 terminated after 30 steps\nEpisode 5767 terminated after 30 steps\nEpisode 5768 terminated after 30 steps\nEpisode 5769 terminated after 30 steps\nEpisode 5770 terminated after 30 steps\nEpisode 5771 terminated after 30 steps\nEpisode 5772 terminated after 30 steps\nEpisode 5773 terminated after 30 steps\nEpisode 5774 terminated after 30 steps\nEpisode 5775 terminated after 30 steps\nEpisode 5776 terminated after 30 steps\nEpisode 5777 terminated after 30 steps\nEpisode 5778 terminated after 30 steps\nEpisode 5779 terminated after 30 steps\nEpisode 5780 terminated after 30 steps\nEpisode 5781 terminated after 30 steps\nEpisode 5782 terminated after 30 steps\nEpisode 5783 terminated after 30 steps\nEpisode 5784 terminated after 30 steps\nEpisode 5785 terminated after 30 steps\nEpisode 5786 terminated after 30 steps\nEpisode 5787 terminated after 30 steps\nEpisode 5788 terminated after 30 steps\nEpisode 5789 terminated after 30 steps\nEpisode 5790 terminated after 30 steps\nEpisode 5791 terminated after 30 steps\nEpisode 5792 terminated after 30 steps\nEpisode 5793 terminated after 30 steps\nEpisode 5794 terminated after 30 steps\nEpisode 5795 terminated after 30 steps\nEpisode 5796 terminated after 30 steps\nEpisode 5797 terminated after 30 steps\nEpisode 5798 terminated after 30 steps\nEpisode 5799 terminated after 30 steps\nEpisode 5800\nEpisode 5800 terminated after 30 steps\nEpisode 5801 terminated after 30 steps\nEpisode 5802 terminated after 30 steps\nEpisode 5803 terminated after 30 steps\nEpisode 5804 terminated after 30 steps\nEpisode 5805 terminated after 30 steps\nEpisode 5806 terminated after 30 steps\nEpisode 5807 terminated after 30 steps\nEpisode 5808 terminated after 30 steps\nEpisode 5809 terminated after 30 steps\nEpisode 5810 terminated after 30 steps\nEpisode 5811 terminated after 30 steps\nEpisode 5812 terminated after 30 steps\nEpisode 5813 terminated after 30 steps\nEpisode 5814 terminated after 30 steps\nEpisode 5815 terminated after 30 steps\nEpisode 5816 terminated after 30 steps\nEpisode 5817 terminated after 30 steps\nEpisode 5818 terminated after 30 steps\nEpisode 5819 terminated after 30 steps\nEpisode 5820 terminated after 30 steps\nEpisode 5821 terminated after 30 steps\nEpisode 5822 terminated after 30 steps\nEpisode 5823 terminated after 30 steps\nEpisode 5824 terminated after 30 steps\nEpisode 5825 terminated after 30 steps\nEpisode 5826 terminated after 30 steps\nEpisode 5827 terminated after 30 steps\nEpisode 5828 terminated after 30 steps\nEpisode 5829 terminated after 30 steps\nEpisode 5830 terminated after 30 steps\nEpisode 5831 terminated after 30 steps\nEpisode 5832 terminated after 30 steps\nEpisode 5833 terminated after 30 steps\nEpisode 5834 terminated after 30 steps\nEpisode 5835 terminated after 30 steps\nEpisode 5836 terminated after 30 steps\nEpisode 5837 terminated after 30 steps\nEpisode 5838 terminated after 30 steps\nEpisode 5839 terminated after 30 steps\nEpisode 5840 terminated after 30 steps\nEpisode 5841 terminated after 30 steps\nEpisode 5842 terminated after 30 steps\nEpisode 5843 terminated after 30 steps\nEpisode 5844 terminated after 30 steps\nEpisode 5845 terminated after 30 steps\nEpisode 5846 terminated after 30 steps\nEpisode 5847 terminated after 30 steps\nEpisode 5848 terminated after 30 steps\nEpisode 5849 terminated after 30 steps\nEpisode 5850 terminated after 30 steps\nEpisode 5851 terminated after 30 steps\nEpisode 5852 terminated after 30 steps\nEpisode 5853 terminated after 30 steps\nEpisode 5854 terminated after 30 steps\nEpisode 5855 terminated after 30 steps\nEpisode 5856 terminated after 30 steps\nEpisode 5857 terminated after 30 steps\nEpisode 5858 terminated after 30 steps\nEpisode 5859 terminated after 30 steps\nEpisode 5860 terminated after 30 steps\nEpisode 5861 terminated after 30 steps\nEpisode 5862 terminated after 30 steps\nEpisode 5863 terminated after 30 steps\nEpisode 5864 terminated after 30 steps\nEpisode 5865 terminated after 30 steps\nEpisode 5866 terminated after 30 steps\nEpisode 5867 terminated after 30 steps\nEpisode 5868 terminated after 30 steps\nEpisode 5869 terminated after 30 steps\nEpisode 5870 terminated after 30 steps\nEpisode 5871 terminated after 30 steps\nEpisode 5872 terminated after 30 steps\nEpisode 5873 terminated after 30 steps\nEpisode 5874 terminated after 30 steps\nEpisode 5875 terminated after 30 steps\nEpisode 5876 terminated after 30 steps\nEpisode 5877 terminated after 30 steps\nEpisode 5878 terminated after 30 steps\nEpisode 5879 terminated after 30 steps\nEpisode 5880 terminated after 30 steps\nEpisode 5881 terminated after 30 steps\nEpisode 5882 terminated after 30 steps\nEpisode 5883 terminated after 30 steps\nEpisode 5884 terminated after 30 steps\nEpisode 5885 terminated after 30 steps\nEpisode 5886 terminated after 30 steps\nEpisode 5887 terminated after 30 steps\nEpisode 5888 terminated after 30 steps\nEpisode 5889 terminated after 30 steps\nEpisode 5890 terminated after 30 steps\nEpisode 5891 terminated after 30 steps\nEpisode 5892 terminated after 30 steps\nEpisode 5893 terminated after 30 steps\nEpisode 5894 terminated after 30 steps\nEpisode 5895 terminated after 30 steps\nEpisode 5896 terminated after 30 steps\nEpisode 5897 terminated after 30 steps\nEpisode 5898 terminated after 30 steps\nEpisode 5899 terminated after 30 steps\nEpisode 5900\nEpisode 5900 terminated after 30 steps\nEpisode 5901 terminated after 30 steps\nEpisode 5902 terminated after 30 steps\nEpisode 5903 terminated after 30 steps\nEpisode 5904 terminated after 30 steps\nEpisode 5905 terminated after 30 steps\nEpisode 5906 terminated after 30 steps\nEpisode 5907 terminated after 30 steps\nEpisode 5908 terminated after 30 steps\nEpisode 5909 terminated after 30 steps\nEpisode 5910 terminated after 30 steps\nEpisode 5911 terminated after 30 steps\nEpisode 5912 terminated after 30 steps\nEpisode 5913 terminated after 30 steps\nEpisode 5914 terminated after 30 steps\nEpisode 5915 terminated after 30 steps\nEpisode 5916 terminated after 30 steps\nEpisode 5917 terminated after 30 steps\nEpisode 5918 terminated after 30 steps\nEpisode 5919 terminated after 30 steps\nEpisode 5920 terminated after 30 steps\nEpisode 5921 terminated after 30 steps\nEpisode 5922 terminated after 30 steps\nEpisode 5923 terminated after 30 steps\nEpisode 5924 terminated after 30 steps\nEpisode 5925 terminated after 30 steps\nEpisode 5926 terminated after 30 steps\nEpisode 5927 terminated after 30 steps\nEpisode 5928 terminated after 30 steps\nEpisode 5929 terminated after 30 steps\nEpisode 5930 terminated after 30 steps\nEpisode 5931 terminated after 30 steps\nEpisode 5932 terminated after 30 steps\nEpisode 5933 terminated after 30 steps\nEpisode 5934 terminated after 30 steps\nEpisode 5935 terminated after 30 steps\nEpisode 5936 terminated after 30 steps\nEpisode 5937 terminated after 30 steps\nEpisode 5938 terminated after 30 steps\nEpisode 5939 terminated after 30 steps\nEpisode 5940 terminated after 30 steps\nEpisode 5941 terminated after 30 steps\nEpisode 5942 terminated after 30 steps\nEpisode 5943 terminated after 30 steps\nEpisode 5944 terminated after 30 steps\nEpisode 5945 terminated after 30 steps\nEpisode 5946 terminated after 30 steps\nEpisode 5947 terminated after 30 steps\nEpisode 5948 terminated after 30 steps\nEpisode 5949 terminated after 30 steps\nEpisode 5950 terminated after 30 steps\nEpisode 5951 terminated after 30 steps\nEpisode 5952 terminated after 30 steps\nEpisode 5953 terminated after 30 steps\nEpisode 5954 terminated after 30 steps\nEpisode 5955 terminated after 30 steps\nEpisode 5956 terminated after 30 steps\nEpisode 5957 terminated after 30 steps\nEpisode 5958 terminated after 30 steps\nEpisode 5959 terminated after 30 steps\nEpisode 5960 terminated after 30 steps\nEpisode 5961 terminated after 30 steps\nEpisode 5962 terminated after 30 steps\nEpisode 5963 terminated after 30 steps\nEpisode 5964 terminated after 30 steps\nEpisode 5965 terminated after 30 steps\nEpisode 5966 terminated after 30 steps\nEpisode 5967 terminated after 30 steps\nEpisode 5968 terminated after 30 steps\nEpisode 5969 terminated after 30 steps\nEpisode 5970 terminated after 30 steps\nEpisode 5971 terminated after 30 steps\nEpisode 5972 terminated after 30 steps\nEpisode 5973 terminated after 30 steps\nEpisode 5974 terminated after 30 steps\nEpisode 5975 terminated after 30 steps\nEpisode 5976 terminated after 30 steps\nEpisode 5977 terminated after 30 steps\nEpisode 5978 terminated after 30 steps\nEpisode 5979 terminated after 30 steps\nEpisode 5980 terminated after 30 steps\nEpisode 5981 terminated after 30 steps\nEpisode 5982 terminated after 30 steps\nEpisode 5983 terminated after 30 steps\nEpisode 5984 terminated after 30 steps\nEpisode 5985 terminated after 30 steps\nEpisode 5986 terminated after 30 steps\nEpisode 5987 terminated after 30 steps\nEpisode 5988 terminated after 30 steps\nEpisode 5989 terminated after 30 steps\nEpisode 5990 terminated after 30 steps\nEpisode 5991 terminated after 30 steps\nEpisode 5992 terminated after 30 steps\nEpisode 5993 terminated after 30 steps\nEpisode 5994 terminated after 30 steps\nEpisode 5995 terminated after 30 steps\nEpisode 5996 terminated after 30 steps\nEpisode 5997 terminated after 30 steps\nEpisode 5998 terminated after 30 steps\nEpisode 5999 terminated after 30 steps\nEpisode 6000\nEpisode 6000 terminated after 30 steps\nEpisode 6001 terminated after 30 steps\nEpisode 6002 terminated after 30 steps\nEpisode 6003 terminated after 30 steps\nEpisode 6004 terminated after 30 steps\nEpisode 6005 terminated after 30 steps\nEpisode 6006 terminated after 30 steps\nEpisode 6007 terminated after 30 steps\nEpisode 6008 terminated after 30 steps\nEpisode 6009 terminated after 30 steps\nEpisode 6010 terminated after 30 steps\nEpisode 6011 terminated after 30 steps\nEpisode 6012 terminated after 30 steps\nEpisode 6013 terminated after 30 steps\nEpisode 6014 terminated after 30 steps\nEpisode 6015 terminated after 30 steps\nEpisode 6016 terminated after 30 steps\nEpisode 6017 terminated after 30 steps\nEpisode 6018 terminated after 30 steps\nEpisode 6019 terminated after 30 steps\nEpisode 6020 terminated after 30 steps\nEpisode 6021 terminated after 30 steps\nEpisode 6022 terminated after 30 steps\nEpisode 6023 terminated after 30 steps\nEpisode 6024 terminated after 30 steps\nEpisode 6025 terminated after 30 steps\nEpisode 6026 terminated after 30 steps\nEpisode 6027 terminated after 30 steps\nEpisode 6028 terminated after 30 steps\nEpisode 6029 terminated after 30 steps\nEpisode 6030 terminated after 30 steps\nEpisode 6031 terminated after 30 steps\nEpisode 6032 terminated after 30 steps\nEpisode 6033 terminated after 30 steps\nEpisode 6034 terminated after 30 steps\nEpisode 6035 terminated after 30 steps\nEpisode 6036 terminated after 30 steps\nEpisode 6037 terminated after 30 steps\nEpisode 6038 terminated after 30 steps\nEpisode 6039 terminated after 30 steps\nEpisode 6040 terminated after 30 steps\nEpisode 6041 terminated after 30 steps\nEpisode 6042 terminated after 30 steps\nEpisode 6043 terminated after 30 steps\nEpisode 6044 terminated after 30 steps\nEpisode 6045 terminated after 30 steps\nEpisode 6046 terminated after 30 steps\nEpisode 6047 terminated after 30 steps\nEpisode 6048 terminated after 30 steps\nEpisode 6049 terminated after 30 steps\nEpisode 6050 terminated after 30 steps\nEpisode 6051 terminated after 30 steps\nEpisode 6052 terminated after 30 steps\nEpisode 6053 terminated after 30 steps\nEpisode 6054 terminated after 30 steps\nEpisode 6055 terminated after 30 steps\nEpisode 6056 terminated after 30 steps\nEpisode 6057 terminated after 30 steps\nEpisode 6058 terminated after 30 steps\nEpisode 6059 terminated after 30 steps\nEpisode 6060 terminated after 30 steps\nEpisode 6061 terminated after 30 steps\nEpisode 6062 terminated after 30 steps\nEpisode 6063 terminated after 30 steps\nEpisode 6064 terminated after 30 steps\nEpisode 6065 terminated after 30 steps\nEpisode 6066 terminated after 30 steps\nEpisode 6067 terminated after 30 steps\nEpisode 6068 terminated after 30 steps\nEpisode 6069 terminated after 30 steps\nEpisode 6070 terminated after 30 steps\nEpisode 6071 terminated after 30 steps\nEpisode 6072 terminated after 30 steps\nEpisode 6073 terminated after 30 steps\nEpisode 6074 terminated after 30 steps\nEpisode 6075 terminated after 30 steps\nEpisode 6076 terminated after 30 steps\nEpisode 6077 terminated after 30 steps\nEpisode 6078 terminated after 30 steps\nEpisode 6079 terminated after 30 steps\nEpisode 6080 terminated after 30 steps\nEpisode 6081 terminated after 30 steps\nEpisode 6082 terminated after 30 steps\nEpisode 6083 terminated after 30 steps\nEpisode 6084 terminated after 30 steps\nEpisode 6085 terminated after 30 steps\nEpisode 6086 terminated after 30 steps\nEpisode 6087 terminated after 30 steps\nEpisode 6088 terminated after 30 steps\nEpisode 6089 terminated after 30 steps\nEpisode 6090 terminated after 30 steps\nEpisode 6091 terminated after 30 steps\nEpisode 6092 terminated after 30 steps\nEpisode 6093 terminated after 30 steps\nEpisode 6094 terminated after 30 steps\nEpisode 6095 terminated after 30 steps\nEpisode 6096 terminated after 30 steps\nEpisode 6097 terminated after 30 steps\nEpisode 6098 terminated after 30 steps\nEpisode 6099 terminated after 30 steps\nEpisode 6100\nEpisode 6100 terminated after 30 steps\nEpisode 6101 terminated after 30 steps\nEpisode 6102 terminated after 30 steps\nEpisode 6103 terminated after 30 steps\nEpisode 6104 terminated after 30 steps\nEpisode 6105 terminated after 30 steps\nEpisode 6106 terminated after 30 steps\nEpisode 6107 terminated after 30 steps\nEpisode 6108 terminated after 30 steps\nEpisode 6109 terminated after 30 steps\nEpisode 6110 terminated after 30 steps\nEpisode 6111 terminated after 30 steps\nEpisode 6112 terminated after 30 steps\nEpisode 6113 terminated after 30 steps\nEpisode 6114 terminated after 30 steps\nEpisode 6115 terminated after 30 steps\nEpisode 6116 terminated after 30 steps\nEpisode 6117 terminated after 30 steps\nEpisode 6118 terminated after 30 steps\nEpisode 6119 terminated after 30 steps\nEpisode 6120 terminated after 30 steps\nEpisode 6121 terminated after 30 steps\nEpisode 6122 terminated after 30 steps\nEpisode 6123 terminated after 30 steps\nEpisode 6124 terminated after 30 steps\nEpisode 6125 terminated after 30 steps\nEpisode 6126 terminated after 30 steps\nEpisode 6127 terminated after 30 steps\nEpisode 6128 terminated after 30 steps\nEpisode 6129 terminated after 30 steps\nEpisode 6130 terminated after 30 steps\nEpisode 6131 terminated after 30 steps\nEpisode 6132 terminated after 30 steps\nEpisode 6133 terminated after 30 steps\nEpisode 6134 terminated after 30 steps\nEpisode 6135 terminated after 30 steps\nEpisode 6136 terminated after 30 steps\nEpisode 6137 terminated after 30 steps\nEpisode 6138 terminated after 30 steps\nEpisode 6139 terminated after 30 steps\nEpisode 6140 terminated after 30 steps\nEpisode 6141 terminated after 30 steps\nEpisode 6142 terminated after 30 steps\nEpisode 6143 terminated after 30 steps\nEpisode 6144 terminated after 30 steps\nEpisode 6145 terminated after 30 steps\nEpisode 6146 terminated after 30 steps\nEpisode 6147 terminated after 30 steps\nEpisode 6148 terminated after 30 steps\nEpisode 6149 terminated after 30 steps\nEpisode 6150 terminated after 30 steps\nEpisode 6151 terminated after 30 steps\nEpisode 6152 terminated after 30 steps\nEpisode 6153 terminated after 30 steps\nEpisode 6154 terminated after 30 steps\nEpisode 6155 terminated after 30 steps\nEpisode 6156 terminated after 30 steps\nEpisode 6157 terminated after 30 steps\nEpisode 6158 terminated after 30 steps\nEpisode 6159 terminated after 30 steps\nEpisode 6160 terminated after 30 steps\nEpisode 6161 terminated after 30 steps\nEpisode 6162 terminated after 30 steps\nEpisode 6163 terminated after 30 steps\nEpisode 6164 terminated after 30 steps\nEpisode 6165 terminated after 30 steps\nEpisode 6166 terminated after 30 steps\nEpisode 6167 terminated after 30 steps\nEpisode 6168 terminated after 30 steps\nEpisode 6169 terminated after 30 steps\nEpisode 6170 terminated after 30 steps\nEpisode 6171 terminated after 30 steps\nEpisode 6172 terminated after 30 steps\nEpisode 6173 terminated after 30 steps\nEpisode 6174 terminated after 30 steps\nEpisode 6175 terminated after 30 steps\nEpisode 6176 terminated after 30 steps\nEpisode 6177 terminated after 30 steps\nEpisode 6178 terminated after 30 steps\nEpisode 6179 terminated after 30 steps\nEpisode 6180 terminated after 30 steps\nEpisode 6181 terminated after 30 steps\nEpisode 6182 terminated after 30 steps\nEpisode 6183 terminated after 30 steps\nEpisode 6184 terminated after 30 steps\nEpisode 6185 terminated after 30 steps\nEpisode 6186 terminated after 30 steps\nEpisode 6187 terminated after 30 steps\nEpisode 6188 terminated after 30 steps\nEpisode 6189 terminated after 30 steps\nEpisode 6190 terminated after 30 steps\nEpisode 6191 terminated after 30 steps\nEpisode 6192 terminated after 30 steps\nEpisode 6193 terminated after 30 steps\nEpisode 6194 terminated after 30 steps\nEpisode 6195 terminated after 30 steps\nEpisode 6196 terminated after 30 steps\nEpisode 6197 terminated after 30 steps\nEpisode 6198 terminated after 30 steps\nEpisode 6199 terminated after 30 steps\nEpisode 6200\nEpisode 6200 terminated after 30 steps\nEpisode 6201 terminated after 30 steps\nEpisode 6202 terminated after 30 steps\nEpisode 6203 terminated after 30 steps\nEpisode 6204 terminated after 30 steps\nEpisode 6205 terminated after 30 steps\nEpisode 6206 terminated after 30 steps\nEpisode 6207 terminated after 30 steps\nEpisode 6208 terminated after 30 steps\nEpisode 6209 terminated after 30 steps\nEpisode 6210 terminated after 30 steps\nEpisode 6211 terminated after 30 steps\nEpisode 6212 terminated after 30 steps\nEpisode 6213 terminated after 30 steps\nEpisode 6214 terminated after 30 steps\nEpisode 6215 terminated after 30 steps\nEpisode 6216 terminated after 30 steps\nEpisode 6217 terminated after 30 steps\nEpisode 6218 terminated after 30 steps\nEpisode 6219 terminated after 30 steps\nEpisode 6220 terminated after 30 steps\nEpisode 6221 terminated after 30 steps\nEpisode 6222 terminated after 30 steps\nEpisode 6223 terminated after 30 steps\nEpisode 6224 terminated after 30 steps\nEpisode 6225 terminated after 30 steps\nEpisode 6226 terminated after 30 steps\nEpisode 6227 terminated after 30 steps\nEpisode 6228 terminated after 30 steps\nEpisode 6229 terminated after 30 steps\nEpisode 6230 terminated after 30 steps\nEpisode 6231 terminated after 30 steps\nEpisode 6232 terminated after 30 steps\nEpisode 6233 terminated after 30 steps\nEpisode 6234 terminated after 30 steps\nEpisode 6235 terminated after 30 steps\nEpisode 6236 terminated after 30 steps\nEpisode 6237 terminated after 30 steps\nEpisode 6238 terminated after 30 steps\nEpisode 6239 terminated after 30 steps\nEpisode 6240 terminated after 30 steps\nEpisode 6241 terminated after 30 steps\nEpisode 6242 terminated after 30 steps\nEpisode 6243 terminated after 30 steps\nEpisode 6244 terminated after 30 steps\nEpisode 6245 terminated after 30 steps\nEpisode 6246 terminated after 30 steps\nEpisode 6247 terminated after 30 steps\nEpisode 6248 terminated after 30 steps\nEpisode 6249 terminated after 30 steps\nEpisode 6250 terminated after 30 steps\nEpisode 6251 terminated after 30 steps\nEpisode 6252 terminated after 30 steps\nEpisode 6253 terminated after 30 steps\nEpisode 6254 terminated after 30 steps\nEpisode 6255 terminated after 30 steps\nEpisode 6256 terminated after 30 steps\nEpisode 6257 terminated after 30 steps\nEpisode 6258 terminated after 30 steps\nEpisode 6259 terminated after 30 steps\nEpisode 6260 terminated after 30 steps\nEpisode 6261 terminated after 30 steps\nEpisode 6262 terminated after 30 steps\nEpisode 6263 terminated after 30 steps\nEpisode 6264 terminated after 30 steps\nEpisode 6265 terminated after 30 steps\nEpisode 6266 terminated after 30 steps\nEpisode 6267 terminated after 30 steps\nEpisode 6268 terminated after 30 steps\nEpisode 6269 terminated after 30 steps\nEpisode 6270 terminated after 30 steps\nEpisode 6271 terminated after 30 steps\nEpisode 6272 terminated after 30 steps\nEpisode 6273 terminated after 30 steps\nEpisode 6274 terminated after 30 steps\nEpisode 6275 terminated after 30 steps\nEpisode 6276 terminated after 30 steps\nEpisode 6277 terminated after 30 steps\nEpisode 6278 terminated after 30 steps\nEpisode 6279 terminated after 30 steps\nEpisode 6280 terminated after 30 steps\nEpisode 6281 terminated after 30 steps\nEpisode 6282 terminated after 30 steps\nEpisode 6283 terminated after 30 steps\nEpisode 6284 terminated after 30 steps\nEpisode 6285 terminated after 30 steps\nEpisode 6286 terminated after 30 steps\nEpisode 6287 terminated after 30 steps\nEpisode 6288 terminated after 30 steps\nEpisode 6289 terminated after 30 steps\nEpisode 6290 terminated after 30 steps\nEpisode 6291 terminated after 30 steps\nEpisode 6292 terminated after 30 steps\nEpisode 6293 terminated after 30 steps\nEpisode 6294 terminated after 30 steps\nEpisode 6295 terminated after 30 steps\nEpisode 6296 terminated after 30 steps\nEpisode 6297 terminated after 30 steps\nEpisode 6298 terminated after 30 steps\nEpisode 6299 terminated after 30 steps\nEpisode 6300\nEpisode 6300 terminated after 30 steps\nEpisode 6301 terminated after 30 steps\nEpisode 6302 terminated after 30 steps\nEpisode 6303 terminated after 30 steps\nEpisode 6304 terminated after 30 steps\nEpisode 6305 terminated after 30 steps\nEpisode 6306 terminated after 30 steps\nEpisode 6307 terminated after 30 steps\nEpisode 6308 terminated after 30 steps\nEpisode 6309 terminated after 30 steps\nEpisode 6310 terminated after 30 steps\nEpisode 6311 terminated after 30 steps\nEpisode 6312 terminated after 30 steps\nEpisode 6313 terminated after 30 steps\nEpisode 6314 terminated after 30 steps\nEpisode 6315 terminated after 30 steps\nEpisode 6316 terminated after 30 steps\nEpisode 6317 terminated after 30 steps\nEpisode 6318 terminated after 30 steps\nEpisode 6319 terminated after 30 steps\nEpisode 6320 terminated after 30 steps\nEpisode 6321 terminated after 30 steps\nEpisode 6322 terminated after 30 steps\nEpisode 6323 terminated after 30 steps\nEpisode 6324 terminated after 30 steps\nEpisode 6325 terminated after 30 steps\nEpisode 6326 terminated after 30 steps\nEpisode 6327 terminated after 30 steps\nEpisode 6328 terminated after 30 steps\nEpisode 6329 terminated after 30 steps\nEpisode 6330 terminated after 30 steps\nEpisode 6331 terminated after 30 steps\nEpisode 6332 terminated after 30 steps\nEpisode 6333 terminated after 30 steps\nEpisode 6334 terminated after 30 steps\nEpisode 6335 terminated after 30 steps\nEpisode 6336 terminated after 30 steps\nEpisode 6337 terminated after 30 steps\nEpisode 6338 terminated after 30 steps\nEpisode 6339 terminated after 30 steps\nEpisode 6340 terminated after 30 steps\nEpisode 6341 terminated after 30 steps\nEpisode 6342 terminated after 30 steps\nEpisode 6343 terminated after 30 steps\nEpisode 6344 terminated after 30 steps\nEpisode 6345 terminated after 30 steps\nEpisode 6346 terminated after 30 steps\nEpisode 6347 terminated after 30 steps\nEpisode 6348 terminated after 30 steps\nEpisode 6349 terminated after 30 steps\nEpisode 6350 terminated after 30 steps\nEpisode 6351 terminated after 30 steps\nEpisode 6352 terminated after 30 steps\nEpisode 6353 terminated after 30 steps\nEpisode 6354 terminated after 30 steps\nEpisode 6355 terminated after 30 steps\nEpisode 6356 terminated after 30 steps\nEpisode 6357 terminated after 30 steps\nEpisode 6358 terminated after 30 steps\nEpisode 6359 terminated after 30 steps\nEpisode 6360 terminated after 30 steps\nEpisode 6361 terminated after 30 steps\nEpisode 6362 terminated after 30 steps\nEpisode 6363 terminated after 30 steps\nEpisode 6364 terminated after 30 steps\nEpisode 6365 terminated after 30 steps\nEpisode 6366 terminated after 30 steps\nEpisode 6367 terminated after 30 steps\nEpisode 6368 terminated after 30 steps\nEpisode 6369 terminated after 30 steps\nEpisode 6370 terminated after 30 steps\nEpisode 6371 terminated after 30 steps\nEpisode 6372 terminated after 30 steps\nEpisode 6373 terminated after 30 steps\nEpisode 6374 terminated after 30 steps\nEpisode 6375 terminated after 30 steps\nEpisode 6376 terminated after 30 steps\nEpisode 6377 terminated after 30 steps\nEpisode 6378 terminated after 30 steps\nEpisode 6379 terminated after 30 steps\nEpisode 6380 terminated after 30 steps\nEpisode 6381 terminated after 30 steps\nEpisode 6382 terminated after 30 steps\nEpisode 6383 terminated after 30 steps\nEpisode 6384 terminated after 30 steps\nEpisode 6385 terminated after 30 steps\nEpisode 6386 terminated after 30 steps\nEpisode 6387 terminated after 30 steps\nEpisode 6388 terminated after 30 steps\nEpisode 6389 terminated after 30 steps\nEpisode 6390 terminated after 30 steps\nEpisode 6391 terminated after 30 steps\nEpisode 6392 terminated after 30 steps\nEpisode 6393 terminated after 30 steps\nEpisode 6394 terminated after 30 steps\nEpisode 6395 terminated after 30 steps\nEpisode 6396 terminated after 30 steps\nEpisode 6397 terminated after 30 steps\nEpisode 6398 terminated after 30 steps\nEpisode 6399 terminated after 30 steps\nEpisode 6400\nEpisode 6400 terminated after 30 steps\nEpisode 6401 terminated after 30 steps\nEpisode 6402 terminated after 30 steps\nEpisode 6403 terminated after 30 steps\nEpisode 6404 terminated after 30 steps\nEpisode 6405 terminated after 30 steps\nEpisode 6406 terminated after 30 steps\nEpisode 6407 terminated after 30 steps\nEpisode 6408 terminated after 30 steps\nEpisode 6409 terminated after 30 steps\nEpisode 6410 terminated after 30 steps\nEpisode 6411 terminated after 30 steps\nEpisode 6412 terminated after 30 steps\nEpisode 6413 terminated after 30 steps\nEpisode 6414 terminated after 30 steps\nEpisode 6415 terminated after 30 steps\nEpisode 6416 terminated after 30 steps\nEpisode 6417 terminated after 30 steps\nEpisode 6418 terminated after 30 steps\nEpisode 6419 terminated after 30 steps\nEpisode 6420 terminated after 30 steps\nEpisode 6421 terminated after 30 steps\nEpisode 6422 terminated after 30 steps\nEpisode 6423 terminated after 30 steps\nEpisode 6424 terminated after 30 steps\nEpisode 6425 terminated after 30 steps\nEpisode 6426 terminated after 30 steps\nEpisode 6427 terminated after 30 steps\nEpisode 6428 terminated after 30 steps\nEpisode 6429 terminated after 30 steps\nEpisode 6430 terminated after 30 steps\nEpisode 6431 terminated after 30 steps\nEpisode 6432 terminated after 30 steps\nEpisode 6433 terminated after 30 steps\nEpisode 6434 terminated after 30 steps\nEpisode 6435 terminated after 30 steps\nEpisode 6436 terminated after 30 steps\nEpisode 6437 terminated after 30 steps\nEpisode 6438 terminated after 30 steps\nEpisode 6439 terminated after 30 steps\nEpisode 6440 terminated after 30 steps\nEpisode 6441 terminated after 30 steps\nEpisode 6442 terminated after 30 steps\nEpisode 6443 terminated after 30 steps\nEpisode 6444 terminated after 30 steps\nEpisode 6445 terminated after 30 steps\nEpisode 6446 terminated after 30 steps\nEpisode 6447 terminated after 30 steps\nEpisode 6448 terminated after 30 steps\nEpisode 6449 terminated after 30 steps\nEpisode 6450 terminated after 30 steps\nEpisode 6451 terminated after 30 steps\nEpisode 6452 terminated after 30 steps\nEpisode 6453 terminated after 30 steps\nEpisode 6454 terminated after 30 steps\nEpisode 6455 terminated after 30 steps\nEpisode 6456 terminated after 30 steps\nEpisode 6457 terminated after 30 steps\nEpisode 6458 terminated after 30 steps\nEpisode 6459 terminated after 30 steps\nEpisode 6460 terminated after 30 steps\nEpisode 6461 terminated after 30 steps\nEpisode 6462 terminated after 30 steps\nEpisode 6463 terminated after 30 steps\nEpisode 6464 terminated after 30 steps\nEpisode 6465 terminated after 30 steps\nEpisode 6466 terminated after 30 steps\nEpisode 6467 terminated after 30 steps\nEpisode 6468 terminated after 30 steps\nEpisode 6469 terminated after 30 steps\nEpisode 6470 terminated after 30 steps\nEpisode 6471 terminated after 30 steps\nEpisode 6472 terminated after 30 steps\nEpisode 6473 terminated after 30 steps\nEpisode 6474 terminated after 30 steps\nEpisode 6475 terminated after 30 steps\nEpisode 6476 terminated after 30 steps\nEpisode 6477 terminated after 30 steps\nEpisode 6478 terminated after 30 steps\nEpisode 6479 terminated after 30 steps\nEpisode 6480 terminated after 30 steps\nEpisode 6481 terminated after 30 steps\nEpisode 6482 terminated after 30 steps\nEpisode 6483 terminated after 30 steps\nEpisode 6484 terminated after 30 steps\nEpisode 6485 terminated after 30 steps\nEpisode 6486 terminated after 30 steps\nEpisode 6487 terminated after 30 steps\nEpisode 6488 terminated after 30 steps\nEpisode 6489 terminated after 30 steps\nEpisode 6490 terminated after 30 steps\nEpisode 6491 terminated after 30 steps\nEpisode 6492 terminated after 30 steps\nEpisode 6493 terminated after 30 steps\nEpisode 6494 terminated after 30 steps\nEpisode 6495 terminated after 30 steps\nEpisode 6496 terminated after 30 steps\nEpisode 6497 terminated after 30 steps\nEpisode 6498 terminated after 30 steps\nEpisode 6499 terminated after 30 steps\nEpisode 6500\nEpisode 6500 terminated after 30 steps\nEpisode 6501 terminated after 30 steps\nEpisode 6502 terminated after 30 steps\nEpisode 6503 terminated after 30 steps\nEpisode 6504 terminated after 30 steps\nEpisode 6505 terminated after 30 steps\nEpisode 6506 terminated after 30 steps\nEpisode 6507 terminated after 30 steps\nEpisode 6508 terminated after 30 steps\nEpisode 6509 terminated after 30 steps\nEpisode 6510 terminated after 30 steps\nEpisode 6511 terminated after 30 steps\nEpisode 6512 terminated after 30 steps\nEpisode 6513 terminated after 30 steps\nEpisode 6514 terminated after 30 steps\nEpisode 6515 terminated after 30 steps\nEpisode 6516 terminated after 30 steps\nEpisode 6517 terminated after 30 steps\nEpisode 6518 terminated after 30 steps\nEpisode 6519 terminated after 30 steps\nEpisode 6520 terminated after 30 steps\nEpisode 6521 terminated after 30 steps\nEpisode 6522 terminated after 30 steps\nEpisode 6523 terminated after 30 steps\nEpisode 6524 terminated after 30 steps\nEpisode 6525 terminated after 30 steps\nEpisode 6526 terminated after 30 steps\nEpisode 6527 terminated after 30 steps\nEpisode 6528 terminated after 30 steps\nEpisode 6529 terminated after 30 steps\nEpisode 6530 terminated after 30 steps\nEpisode 6531 terminated after 30 steps\nEpisode 6532 terminated after 30 steps\nEpisode 6533 terminated after 30 steps\nEpisode 6534 terminated after 30 steps\nEpisode 6535 terminated after 30 steps\nEpisode 6536 terminated after 30 steps\nEpisode 6537 terminated after 30 steps\nEpisode 6538 terminated after 30 steps\nEpisode 6539 terminated after 30 steps\nEpisode 6540 terminated after 30 steps\nEpisode 6541 terminated after 30 steps\nEpisode 6542 terminated after 30 steps\nEpisode 6543 terminated after 30 steps\nEpisode 6544 terminated after 30 steps\nEpisode 6545 terminated after 30 steps\nEpisode 6546 terminated after 30 steps\nEpisode 6547 terminated after 30 steps\nEpisode 6548 terminated after 30 steps\nEpisode 6549 terminated after 30 steps\nEpisode 6550 terminated after 30 steps\nEpisode 6551 terminated after 30 steps\nEpisode 6552 terminated after 30 steps\nEpisode 6553 terminated after 30 steps\nEpisode 6554 terminated after 30 steps\nEpisode 6555 terminated after 30 steps\nEpisode 6556 terminated after 30 steps\nEpisode 6557 terminated after 30 steps\nEpisode 6558 terminated after 30 steps\nEpisode 6559 terminated after 30 steps\nEpisode 6560 terminated after 30 steps\nEpisode 6561 terminated after 30 steps\nEpisode 6562 terminated after 30 steps\nEpisode 6563 terminated after 30 steps\nEpisode 6564 terminated after 30 steps\nEpisode 6565 terminated after 30 steps\nEpisode 6566 terminated after 30 steps\nEpisode 6567 terminated after 30 steps\nEpisode 6568 terminated after 30 steps\nEpisode 6569 terminated after 30 steps\nEpisode 6570 terminated after 30 steps\nEpisode 6571 terminated after 30 steps\nEpisode 6572 terminated after 30 steps\nEpisode 6573 terminated after 30 steps\nEpisode 6574 terminated after 30 steps\nEpisode 6575 terminated after 30 steps\nEpisode 6576 terminated after 30 steps\nEpisode 6577 terminated after 30 steps\nEpisode 6578 terminated after 30 steps\nEpisode 6579 terminated after 30 steps\nEpisode 6580 terminated after 30 steps\nEpisode 6581 terminated after 30 steps\nEpisode 6582 terminated after 30 steps\nEpisode 6583 terminated after 30 steps\nEpisode 6584 terminated after 30 steps\nEpisode 6585 terminated after 30 steps\nEpisode 6586 terminated after 30 steps\nEpisode 6587 terminated after 30 steps\nEpisode 6588 terminated after 30 steps\nEpisode 6589 terminated after 30 steps\nEpisode 6590 terminated after 30 steps\nEpisode 6591 terminated after 30 steps\nEpisode 6592 terminated after 30 steps\nEpisode 6593 terminated after 30 steps\nEpisode 6594 terminated after 30 steps\nEpisode 6595 terminated after 30 steps\nEpisode 6596 terminated after 30 steps\nEpisode 6597 terminated after 30 steps\nEpisode 6598 terminated after 30 steps\nEpisode 6599 terminated after 30 steps\nEpisode 6600\nEpisode 6600 terminated after 30 steps\nEpisode 6601 terminated after 30 steps\nEpisode 6602 terminated after 30 steps\nEpisode 6603 terminated after 30 steps\nEpisode 6604 terminated after 30 steps\nEpisode 6605 terminated after 30 steps\nEpisode 6606 terminated after 30 steps\nEpisode 6607 terminated after 30 steps\nEpisode 6608 terminated after 30 steps\nEpisode 6609 terminated after 30 steps\nEpisode 6610 terminated after 30 steps\nEpisode 6611 terminated after 30 steps\nEpisode 6612 terminated after 30 steps\nEpisode 6613 terminated after 30 steps\nEpisode 6614 terminated after 30 steps\nEpisode 6615 terminated after 30 steps\nEpisode 6616 terminated after 30 steps\nEpisode 6617 terminated after 30 steps\nEpisode 6618 terminated after 30 steps\nEpisode 6619 terminated after 30 steps\nEpisode 6620 terminated after 30 steps\nEpisode 6621 terminated after 30 steps\nEpisode 6622 terminated after 30 steps\nEpisode 6623 terminated after 30 steps\nEpisode 6624 terminated after 30 steps\nEpisode 6625 terminated after 30 steps\nEpisode 6626 terminated after 30 steps\nEpisode 6627 terminated after 30 steps\nEpisode 6628 terminated after 30 steps\nEpisode 6629 terminated after 30 steps\nEpisode 6630 terminated after 30 steps\nEpisode 6631 terminated after 30 steps\nEpisode 6632 terminated after 30 steps\nEpisode 6633 terminated after 30 steps\nEpisode 6634 terminated after 30 steps\nEpisode 6635 terminated after 30 steps\nEpisode 6636 terminated after 30 steps\nEpisode 6637 terminated after 30 steps\nEpisode 6638 terminated after 30 steps\nEpisode 6639 terminated after 30 steps\nEpisode 6640 terminated after 30 steps\nEpisode 6641 terminated after 30 steps\nEpisode 6642 terminated after 30 steps\nEpisode 6643 terminated after 30 steps\nEpisode 6644 terminated after 30 steps\nEpisode 6645 terminated after 30 steps\nEpisode 6646 terminated after 30 steps\nEpisode 6647 terminated after 30 steps\nEpisode 6648 terminated after 30 steps\nEpisode 6649 terminated after 30 steps\nEpisode 6650 terminated after 30 steps\nEpisode 6651 terminated after 30 steps\nEpisode 6652 terminated after 30 steps\nEpisode 6653 terminated after 30 steps\nEpisode 6654 terminated after 30 steps\nEpisode 6655 terminated after 30 steps\nEpisode 6656 terminated after 30 steps\nEpisode 6657 terminated after 30 steps\nEpisode 6658 terminated after 30 steps\nEpisode 6659 terminated after 30 steps\nEpisode 6660 terminated after 30 steps\nEpisode 6661 terminated after 30 steps\nEpisode 6662 terminated after 30 steps\nEpisode 6663 terminated after 30 steps\nEpisode 6664 terminated after 30 steps\nEpisode 6665 terminated after 30 steps\nEpisode 6666 terminated after 30 steps\nEpisode 6667 terminated after 30 steps\nEpisode 6668 terminated after 30 steps\nEpisode 6669 terminated after 30 steps\nEpisode 6670 terminated after 30 steps\nEpisode 6671 terminated after 30 steps\nEpisode 6672 terminated after 30 steps\nEpisode 6673 terminated after 30 steps\nEpisode 6674 terminated after 30 steps\nEpisode 6675 terminated after 30 steps\nEpisode 6676 terminated after 30 steps\nEpisode 6677 terminated after 30 steps\nEpisode 6678 terminated after 30 steps\nEpisode 6679 terminated after 30 steps\nEpisode 6680 terminated after 30 steps\nEpisode 6681 terminated after 30 steps\nEpisode 6682 terminated after 30 steps\nEpisode 6683 terminated after 30 steps\nEpisode 6684 terminated after 30 steps\nEpisode 6685 terminated after 30 steps\nEpisode 6686 terminated after 30 steps\nEpisode 6687 terminated after 30 steps\nEpisode 6688 terminated after 30 steps\nEpisode 6689 terminated after 30 steps\nEpisode 6690 terminated after 30 steps\nEpisode 6691 terminated after 30 steps\nEpisode 6692 terminated after 30 steps\nEpisode 6693 terminated after 30 steps\nEpisode 6694 terminated after 30 steps\nEpisode 6695 terminated after 30 steps\nEpisode 6696 terminated after 30 steps\nEpisode 6697 terminated after 30 steps\nEpisode 6698 terminated after 30 steps\nEpisode 6699 terminated after 30 steps\nEpisode 6700\nEpisode 6700 terminated after 30 steps\nEpisode 6701 terminated after 30 steps\nEpisode 6702 terminated after 30 steps\nEpisode 6703 terminated after 30 steps\nEpisode 6704 terminated after 30 steps\nEpisode 6705 terminated after 30 steps\nEpisode 6706 terminated after 30 steps\nEpisode 6707 terminated after 30 steps\nEpisode 6708 terminated after 30 steps\nEpisode 6709 terminated after 30 steps\nEpisode 6710 terminated after 30 steps\nEpisode 6711 terminated after 30 steps\nEpisode 6712 terminated after 30 steps\nEpisode 6713 terminated after 30 steps\nEpisode 6714 terminated after 30 steps\nEpisode 6715 terminated after 30 steps\nEpisode 6716 terminated after 30 steps\nEpisode 6717 terminated after 30 steps\nEpisode 6718 terminated after 30 steps\nEpisode 6719 terminated after 30 steps\nEpisode 6720 terminated after 30 steps\nEpisode 6721 terminated after 30 steps\nEpisode 6722 terminated after 30 steps\nEpisode 6723 terminated after 30 steps\nEpisode 6724 terminated after 30 steps\nEpisode 6725 terminated after 30 steps\nEpisode 6726 terminated after 30 steps\nEpisode 6727 terminated after 30 steps\nEpisode 6728 terminated after 30 steps\nEpisode 6729 terminated after 30 steps\nEpisode 6730 terminated after 30 steps\nEpisode 6731 terminated after 30 steps\nEpisode 6732 terminated after 30 steps\nEpisode 6733 terminated after 30 steps\nEpisode 6734 terminated after 30 steps\nEpisode 6735 terminated after 30 steps\nEpisode 6736 terminated after 30 steps\nEpisode 6737 terminated after 30 steps\nEpisode 6738 terminated after 30 steps\nEpisode 6739 terminated after 30 steps\nEpisode 6740 terminated after 30 steps\nEpisode 6741 terminated after 30 steps\nEpisode 6742 terminated after 30 steps\nEpisode 6743 terminated after 30 steps\nEpisode 6744 terminated after 30 steps\nEpisode 6745 terminated after 30 steps\nEpisode 6746 terminated after 30 steps\nEpisode 6747 terminated after 30 steps\nEpisode 6748 terminated after 30 steps\nEpisode 6749 terminated after 30 steps\nEpisode 6750 terminated after 30 steps\nEpisode 6751 terminated after 30 steps\nEpisode 6752 terminated after 30 steps\nEpisode 6753 terminated after 30 steps\nEpisode 6754 terminated after 30 steps\nEpisode 6755 terminated after 30 steps\nEpisode 6756 terminated after 30 steps\nEpisode 6757 terminated after 30 steps\nEpisode 6758 terminated after 30 steps\nEpisode 6759 terminated after 30 steps\nEpisode 6760 terminated after 30 steps\nEpisode 6761 terminated after 30 steps\nEpisode 6762 terminated after 30 steps\nEpisode 6763 terminated after 30 steps\nEpisode 6764 terminated after 30 steps\nEpisode 6765 terminated after 30 steps\nEpisode 6766 terminated after 30 steps\nEpisode 6767 terminated after 30 steps\nEpisode 6768 terminated after 30 steps\nEpisode 6769 terminated after 30 steps\nEpisode 6770 terminated after 30 steps\nEpisode 6771 terminated after 30 steps\nEpisode 6772 terminated after 30 steps\nEpisode 6773 terminated after 30 steps\nEpisode 6774 terminated after 30 steps\nEpisode 6775 terminated after 30 steps\nEpisode 6776 terminated after 30 steps\nEpisode 6777 terminated after 30 steps\nEpisode 6778 terminated after 30 steps\nEpisode 6779 terminated after 30 steps\nEpisode 6780 terminated after 30 steps\nEpisode 6781 terminated after 30 steps\nEpisode 6782 terminated after 30 steps\nEpisode 6783 terminated after 30 steps\nEpisode 6784 terminated after 30 steps\nEpisode 6785 terminated after 30 steps\nEpisode 6786 terminated after 30 steps\nEpisode 6787 terminated after 30 steps\nEpisode 6788 terminated after 30 steps\nEpisode 6789 terminated after 30 steps\nEpisode 6790 terminated after 30 steps\nEpisode 6791 terminated after 30 steps\nEpisode 6792 terminated after 30 steps\nEpisode 6793 terminated after 30 steps\nEpisode 6794 terminated after 30 steps\nEpisode 6795 terminated after 30 steps\nEpisode 6796 terminated after 30 steps\nEpisode 6797 terminated after 30 steps\nEpisode 6798 terminated after 30 steps\nEpisode 6799 terminated after 30 steps\nEpisode 6800\nEpisode 6800 terminated after 30 steps\nEpisode 6801 terminated after 30 steps\nEpisode 6802 terminated after 30 steps\nEpisode 6803 terminated after 30 steps\nEpisode 6804 terminated after 30 steps\nEpisode 6805 terminated after 30 steps\nEpisode 6806 terminated after 30 steps\nEpisode 6807 terminated after 30 steps\nEpisode 6808 terminated after 30 steps\nEpisode 6809 terminated after 30 steps\nEpisode 6810 terminated after 30 steps\nEpisode 6811 terminated after 30 steps\nEpisode 6812 terminated after 30 steps\nEpisode 6813 terminated after 30 steps\nEpisode 6814 terminated after 30 steps\nEpisode 6815 terminated after 30 steps\nEpisode 6816 terminated after 30 steps\nEpisode 6817 terminated after 30 steps\nEpisode 6818 terminated after 30 steps\nEpisode 6819 terminated after 30 steps\nEpisode 6820 terminated after 30 steps\nEpisode 6821 terminated after 30 steps\nEpisode 6822 terminated after 30 steps\nEpisode 6823 terminated after 30 steps\nEpisode 6824 terminated after 30 steps\nEpisode 6825 terminated after 30 steps\nEpisode 6826 terminated after 30 steps\nEpisode 6827 terminated after 30 steps\nEpisode 6828 terminated after 30 steps\nEpisode 6829 terminated after 30 steps\nEpisode 6830 terminated after 30 steps\nEpisode 6831 terminated after 30 steps\nEpisode 6832 terminated after 30 steps\nEpisode 6833 terminated after 30 steps\nEpisode 6834 terminated after 30 steps\nEpisode 6835 terminated after 30 steps\nEpisode 6836 terminated after 30 steps\nEpisode 6837 terminated after 30 steps\nEpisode 6838 terminated after 30 steps\nEpisode 6839 terminated after 30 steps\nEpisode 6840 terminated after 30 steps\nEpisode 6841 terminated after 30 steps\nEpisode 6842 terminated after 30 steps\nEpisode 6843 terminated after 30 steps\nEpisode 6844 terminated after 30 steps\nEpisode 6845 terminated after 30 steps\nEpisode 6846 terminated after 30 steps\nEpisode 6847 terminated after 30 steps\nEpisode 6848 terminated after 30 steps\nEpisode 6849 terminated after 30 steps\nEpisode 6850 terminated after 30 steps\nEpisode 6851 terminated after 30 steps\nEpisode 6852 terminated after 30 steps\nEpisode 6853 terminated after 30 steps\nEpisode 6854 terminated after 30 steps\nEpisode 6855 terminated after 30 steps\nEpisode 6856 terminated after 30 steps\nEpisode 6857 terminated after 30 steps\nEpisode 6858 terminated after 30 steps\nEpisode 6859 terminated after 30 steps\nEpisode 6860 terminated after 30 steps\nEpisode 6861 terminated after 30 steps\nEpisode 6862 terminated after 30 steps\nEpisode 6863 terminated after 30 steps\nEpisode 6864 terminated after 30 steps\nEpisode 6865 terminated after 30 steps\nEpisode 6866 terminated after 30 steps\nEpisode 6867 terminated after 30 steps\nEpisode 6868 terminated after 30 steps\nEpisode 6869 terminated after 30 steps\nEpisode 6870 terminated after 30 steps\nEpisode 6871 terminated after 30 steps\nEpisode 6872 terminated after 30 steps\nEpisode 6873 terminated after 30 steps\nEpisode 6874 terminated after 30 steps\nEpisode 6875 terminated after 30 steps\nEpisode 6876 terminated after 30 steps\nEpisode 6877 terminated after 30 steps\nEpisode 6878 terminated after 30 steps\nEpisode 6879 terminated after 30 steps\nEpisode 6880 terminated after 30 steps\nEpisode 6881 terminated after 30 steps\nEpisode 6882 terminated after 30 steps\nEpisode 6883 terminated after 30 steps\nEpisode 6884 terminated after 30 steps\nEpisode 6885 terminated after 30 steps\nEpisode 6886 terminated after 30 steps\nEpisode 6887 terminated after 30 steps\nEpisode 6888 terminated after 30 steps\nEpisode 6889 terminated after 30 steps\nEpisode 6890 terminated after 30 steps\nEpisode 6891 terminated after 30 steps\nEpisode 6892 terminated after 30 steps\nEpisode 6893 terminated after 30 steps\nEpisode 6894 terminated after 30 steps\nEpisode 6895 terminated after 30 steps\nEpisode 6896 terminated after 30 steps\nEpisode 6897 terminated after 30 steps\nEpisode 6898 terminated after 30 steps\nEpisode 6899 terminated after 30 steps\nEpisode 6900\nEpisode 6900 terminated after 30 steps\nEpisode 6901 terminated after 30 steps\nEpisode 6902 terminated after 30 steps\nEpisode 6903 terminated after 30 steps\nEpisode 6904 terminated after 30 steps\nEpisode 6905 terminated after 30 steps\nEpisode 6906 terminated after 30 steps\nEpisode 6907 terminated after 30 steps\nEpisode 6908 terminated after 30 steps\nEpisode 6909 terminated after 30 steps\nEpisode 6910 terminated after 30 steps\nEpisode 6911 terminated after 30 steps\nEpisode 6912 terminated after 30 steps\nEpisode 6913 terminated after 30 steps\nEpisode 6914 terminated after 30 steps\nEpisode 6915 terminated after 30 steps\nEpisode 6916 terminated after 30 steps\nEpisode 6917 terminated after 30 steps\nEpisode 6918 terminated after 30 steps\nEpisode 6919 terminated after 30 steps\nEpisode 6920 terminated after 30 steps\nEpisode 6921 terminated after 30 steps\nEpisode 6922 terminated after 30 steps\nEpisode 6923 terminated after 30 steps\nEpisode 6924 terminated after 30 steps\nEpisode 6925 terminated after 30 steps\nEpisode 6926 terminated after 30 steps\nEpisode 6927 terminated after 30 steps\nEpisode 6928 terminated after 30 steps\nEpisode 6929 terminated after 30 steps\nEpisode 6930 terminated after 30 steps\nEpisode 6931 terminated after 30 steps\nEpisode 6932 terminated after 30 steps\nEpisode 6933 terminated after 30 steps\nEpisode 6934 terminated after 30 steps\nEpisode 6935 terminated after 30 steps\nEpisode 6936 terminated after 30 steps\nEpisode 6937 terminated after 30 steps\nEpisode 6938 terminated after 30 steps\nEpisode 6939 terminated after 30 steps\nEpisode 6940 terminated after 30 steps\nEpisode 6941 terminated after 30 steps\nEpisode 6942 terminated after 30 steps\nEpisode 6943 terminated after 30 steps\nEpisode 6944 terminated after 30 steps\nEpisode 6945 terminated after 30 steps\nEpisode 6946 terminated after 30 steps\nEpisode 6947 terminated after 30 steps\nEpisode 6948 terminated after 30 steps\nEpisode 6949 terminated after 30 steps\nEpisode 6950 terminated after 30 steps\nEpisode 6951 terminated after 30 steps\nEpisode 6952 terminated after 30 steps\nEpisode 6953 terminated after 30 steps\nEpisode 6954 terminated after 30 steps\nEpisode 6955 terminated after 30 steps\nEpisode 6956 terminated after 30 steps\nEpisode 6957 terminated after 30 steps\nEpisode 6958 terminated after 30 steps\nEpisode 6959 terminated after 30 steps\nEpisode 6960 terminated after 30 steps\nEpisode 6961 terminated after 30 steps\nEpisode 6962 terminated after 30 steps\nEpisode 6963 terminated after 30 steps\nEpisode 6964 terminated after 30 steps\nEpisode 6965 terminated after 30 steps\nEpisode 6966 terminated after 30 steps\nEpisode 6967 terminated after 30 steps\nEpisode 6968 terminated after 30 steps\nEpisode 6969 terminated after 30 steps\nEpisode 6970 terminated after 30 steps\nEpisode 6971 terminated after 30 steps\nEpisode 6972 terminated after 30 steps\nEpisode 6973 terminated after 30 steps\nEpisode 6974 terminated after 30 steps\nEpisode 6975 terminated after 30 steps\nEpisode 6976 terminated after 30 steps\nEpisode 6977 terminated after 30 steps\nEpisode 6978 terminated after 30 steps\nEpisode 6979 terminated after 30 steps\nEpisode 6980 terminated after 30 steps\nEpisode 6981 terminated after 30 steps\nEpisode 6982 terminated after 30 steps\nEpisode 6983 terminated after 30 steps\nEpisode 6984 terminated after 30 steps\nEpisode 6985 terminated after 30 steps\nEpisode 6986 terminated after 30 steps\nEpisode 6987 terminated after 30 steps\nEpisode 6988 terminated after 30 steps\nEpisode 6989 terminated after 30 steps\nEpisode 6990 terminated after 30 steps\nEpisode 6991 terminated after 30 steps\nEpisode 6992 terminated after 30 steps\nEpisode 6993 terminated after 30 steps\nEpisode 6994 terminated after 30 steps\nEpisode 6995 terminated after 30 steps\nEpisode 6996 terminated after 30 steps\nEpisode 6997 terminated after 30 steps\nEpisode 6998 terminated after 30 steps\nEpisode 6999 terminated after 30 steps\nEpisode 7000\nEpisode 7000 terminated after 30 steps\nEpisode 7001 terminated after 30 steps\nEpisode 7002 terminated after 30 steps\nEpisode 7003 terminated after 30 steps\nEpisode 7004 terminated after 30 steps\nEpisode 7005 terminated after 30 steps\nEpisode 7006 terminated after 30 steps\nEpisode 7007 terminated after 30 steps\nEpisode 7008 terminated after 30 steps\nEpisode 7009 terminated after 30 steps\nEpisode 7010 terminated after 30 steps\nEpisode 7011 terminated after 30 steps\nEpisode 7012 terminated after 30 steps\nEpisode 7013 terminated after 30 steps\nEpisode 7014 terminated after 30 steps\nEpisode 7015 terminated after 30 steps\nEpisode 7016 terminated after 30 steps\nEpisode 7017 terminated after 30 steps\nEpisode 7018 terminated after 30 steps\nEpisode 7019 terminated after 30 steps\nEpisode 7020 terminated after 30 steps\nEpisode 7021 terminated after 30 steps\nEpisode 7022 terminated after 30 steps\nEpisode 7023 terminated after 30 steps\nEpisode 7024 terminated after 30 steps\nEpisode 7025 terminated after 30 steps\nEpisode 7026 terminated after 30 steps\nEpisode 7027 terminated after 30 steps\nEpisode 7028 terminated after 30 steps\nEpisode 7029 terminated after 30 steps\nEpisode 7030 terminated after 30 steps\nEpisode 7031 terminated after 30 steps\nEpisode 7032 terminated after 30 steps\nEpisode 7033 terminated after 30 steps\nEpisode 7034 terminated after 30 steps\nEpisode 7035 terminated after 30 steps\nEpisode 7036 terminated after 30 steps\nEpisode 7037 terminated after 30 steps\nEpisode 7038 terminated after 30 steps\nEpisode 7039 terminated after 30 steps\nEpisode 7040 terminated after 30 steps\nEpisode 7041 terminated after 30 steps\nEpisode 7042 terminated after 30 steps\nEpisode 7043 terminated after 30 steps\nEpisode 7044 terminated after 30 steps\nEpisode 7045 terminated after 30 steps\nEpisode 7046 terminated after 30 steps\nEpisode 7047 terminated after 30 steps\nEpisode 7048 terminated after 30 steps\nEpisode 7049 terminated after 30 steps\nEpisode 7050 terminated after 30 steps\nEpisode 7051 terminated after 30 steps\nEpisode 7052 terminated after 30 steps\nEpisode 7053 terminated after 30 steps\nEpisode 7054 terminated after 30 steps\nEpisode 7055 terminated after 30 steps\nEpisode 7056 terminated after 30 steps\nEpisode 7057 terminated after 30 steps\nEpisode 7058 terminated after 30 steps\nEpisode 7059 terminated after 30 steps\nEpisode 7060 terminated after 30 steps\nEpisode 7061 terminated after 30 steps\nEpisode 7062 terminated after 30 steps\nEpisode 7063 terminated after 30 steps\nEpisode 7064 terminated after 30 steps\nEpisode 7065 terminated after 30 steps\nEpisode 7066 terminated after 30 steps\nEpisode 7067 terminated after 30 steps\nEpisode 7068 terminated after 30 steps\nEpisode 7069 terminated after 30 steps\nEpisode 7070 terminated after 30 steps\nEpisode 7071 terminated after 30 steps\nEpisode 7072 terminated after 30 steps\nEpisode 7073 terminated after 30 steps\nEpisode 7074 terminated after 30 steps\nEpisode 7075 terminated after 30 steps\nEpisode 7076 terminated after 30 steps\nEpisode 7077 terminated after 30 steps\nEpisode 7078 terminated after 30 steps\nEpisode 7079 terminated after 30 steps\nEpisode 7080 terminated after 30 steps\nEpisode 7081 terminated after 30 steps\nEpisode 7082 terminated after 30 steps\nEpisode 7083 terminated after 30 steps\nEpisode 7084 terminated after 30 steps\nEpisode 7085 terminated after 30 steps\nEpisode 7086 terminated after 30 steps\nEpisode 7087 terminated after 30 steps\nEpisode 7088 terminated after 30 steps\nEpisode 7089 terminated after 30 steps\nEpisode 7090 terminated after 30 steps\nEpisode 7091 terminated after 30 steps\nEpisode 7092 terminated after 30 steps\nEpisode 7093 terminated after 30 steps\nEpisode 7094 terminated after 30 steps\nEpisode 7095 terminated after 30 steps\nEpisode 7096 terminated after 30 steps\nEpisode 7097 terminated after 30 steps\nEpisode 7098 terminated after 30 steps\nEpisode 7099 terminated after 30 steps\nEpisode 7100\nEpisode 7100 terminated after 30 steps\nEpisode 7101 terminated after 30 steps\nEpisode 7102 terminated after 30 steps\nEpisode 7103 terminated after 30 steps\nEpisode 7104 terminated after 30 steps\nEpisode 7105 terminated after 30 steps\nEpisode 7106 terminated after 30 steps\nEpisode 7107 terminated after 30 steps\nEpisode 7108 terminated after 30 steps\nEpisode 7109 terminated after 30 steps\nEpisode 7110 terminated after 30 steps\nEpisode 7111 terminated after 30 steps\nEpisode 7112 terminated after 30 steps\nEpisode 7113 terminated after 30 steps\nEpisode 7114 terminated after 30 steps\nEpisode 7115 terminated after 30 steps\nEpisode 7116 terminated after 30 steps\nEpisode 7117 terminated after 30 steps\nEpisode 7118 terminated after 30 steps\nEpisode 7119 terminated after 30 steps\nEpisode 7120 terminated after 30 steps\nEpisode 7121 terminated after 30 steps\nEpisode 7122 terminated after 30 steps\nEpisode 7123 terminated after 30 steps\nEpisode 7124 terminated after 30 steps\nEpisode 7125 terminated after 30 steps\nEpisode 7126 terminated after 30 steps\nEpisode 7127 terminated after 30 steps\nEpisode 7128 terminated after 30 steps\nEpisode 7129 terminated after 30 steps\nEpisode 7130 terminated after 30 steps\nEpisode 7131 terminated after 30 steps\nEpisode 7132 terminated after 30 steps\nEpisode 7133 terminated after 30 steps\nEpisode 7134 terminated after 30 steps\nEpisode 7135 terminated after 30 steps\nEpisode 7136 terminated after 30 steps\nEpisode 7137 terminated after 30 steps\nEpisode 7138 terminated after 30 steps\nEpisode 7139 terminated after 30 steps\nEpisode 7140 terminated after 30 steps\nEpisode 7141 terminated after 30 steps\nEpisode 7142 terminated after 30 steps\nEpisode 7143 terminated after 30 steps\nEpisode 7144 terminated after 30 steps\nEpisode 7145 terminated after 30 steps\nEpisode 7146 terminated after 30 steps\nEpisode 7147 terminated after 30 steps\nEpisode 7148 terminated after 30 steps\nEpisode 7149 terminated after 30 steps\nEpisode 7150 terminated after 30 steps\nEpisode 7151 terminated after 30 steps\nEpisode 7152 terminated after 30 steps\nEpisode 7153 terminated after 30 steps\nEpisode 7154 terminated after 30 steps\nEpisode 7155 terminated after 30 steps\nEpisode 7156 terminated after 30 steps\nEpisode 7157 terminated after 30 steps\nEpisode 7158 terminated after 30 steps\nEpisode 7159 terminated after 30 steps\nEpisode 7160 terminated after 30 steps\nEpisode 7161 terminated after 30 steps\nEpisode 7162 terminated after 30 steps\nEpisode 7163 terminated after 30 steps\nEpisode 7164 terminated after 30 steps\nEpisode 7165 terminated after 30 steps\nEpisode 7166 terminated after 30 steps\nEpisode 7167 terminated after 30 steps\nEpisode 7168 terminated after 30 steps\nEpisode 7169 terminated after 30 steps\nEpisode 7170 terminated after 30 steps\nEpisode 7171 terminated after 30 steps\nEpisode 7172 terminated after 30 steps\nEpisode 7173 terminated after 30 steps\nEpisode 7174 terminated after 30 steps\nEpisode 7175 terminated after 30 steps\nEpisode 7176 terminated after 30 steps\nEpisode 7177 terminated after 30 steps\nEpisode 7178 terminated after 30 steps\nEpisode 7179 terminated after 30 steps\nEpisode 7180 terminated after 30 steps\nEpisode 7181 terminated after 30 steps\nEpisode 7182 terminated after 30 steps\nEpisode 7183 terminated after 30 steps\nEpisode 7184 terminated after 30 steps\nEpisode 7185 terminated after 30 steps\nEpisode 7186 terminated after 30 steps\nEpisode 7187 terminated after 30 steps\nEpisode 7188 terminated after 30 steps\nEpisode 7189 terminated after 30 steps\nEpisode 7190 terminated after 30 steps\nEpisode 7191 terminated after 30 steps\nEpisode 7192 terminated after 30 steps\nEpisode 7193 terminated after 30 steps\nEpisode 7194 terminated after 30 steps\nEpisode 7195 terminated after 30 steps\nEpisode 7196 terminated after 30 steps\nEpisode 7197 terminated after 30 steps\nEpisode 7198 terminated after 30 steps\nEpisode 7199 terminated after 30 steps\nEpisode 7200\nEpisode 7200 terminated after 30 steps\nEpisode 7201 terminated after 30 steps\nEpisode 7202 terminated after 30 steps\nEpisode 7203 terminated after 30 steps\nEpisode 7204 terminated after 30 steps\nEpisode 7205 terminated after 30 steps\nEpisode 7206 terminated after 30 steps\nEpisode 7207 terminated after 30 steps\nEpisode 7208 terminated after 30 steps\nEpisode 7209 terminated after 30 steps\nEpisode 7210 terminated after 30 steps\nEpisode 7211 terminated after 30 steps\nEpisode 7212 terminated after 30 steps\nEpisode 7213 terminated after 30 steps\nEpisode 7214 terminated after 30 steps\nEpisode 7215 terminated after 30 steps\nEpisode 7216 terminated after 30 steps\nEpisode 7217 terminated after 30 steps\nEpisode 7218 terminated after 30 steps\nEpisode 7219 terminated after 30 steps\nEpisode 7220 terminated after 30 steps\nEpisode 7221 terminated after 30 steps\nEpisode 7222 terminated after 30 steps\nEpisode 7223 terminated after 30 steps\nEpisode 7224 terminated after 30 steps\nEpisode 7225 terminated after 30 steps\nEpisode 7226 terminated after 30 steps\nEpisode 7227 terminated after 30 steps\nEpisode 7228 terminated after 30 steps\nEpisode 7229 terminated after 30 steps\nEpisode 7230 terminated after 30 steps\nEpisode 7231 terminated after 30 steps\nEpisode 7232 terminated after 30 steps\nEpisode 7233 terminated after 30 steps\nEpisode 7234 terminated after 30 steps\nEpisode 7235 terminated after 30 steps\nEpisode 7236 terminated after 30 steps\nEpisode 7237 terminated after 30 steps\nEpisode 7238 terminated after 30 steps\nEpisode 7239 terminated after 30 steps\nEpisode 7240 terminated after 30 steps\nEpisode 7241 terminated after 30 steps\nEpisode 7242 terminated after 30 steps\nEpisode 7243 terminated after 30 steps\nEpisode 7244 terminated after 30 steps\nEpisode 7245 terminated after 30 steps\nEpisode 7246 terminated after 30 steps\nEpisode 7247 terminated after 30 steps\nEpisode 7248 terminated after 30 steps\nEpisode 7249 terminated after 30 steps\nEpisode 7250 terminated after 30 steps\nEpisode 7251 terminated after 30 steps\nEpisode 7252 terminated after 30 steps\nEpisode 7253 terminated after 30 steps\nEpisode 7254 terminated after 30 steps\nEpisode 7255 terminated after 30 steps\nEpisode 7256 terminated after 30 steps\nEpisode 7257 terminated after 30 steps\nEpisode 7258 terminated after 30 steps\nEpisode 7259 terminated after 30 steps\nEpisode 7260 terminated after 30 steps\nEpisode 7261 terminated after 30 steps\nEpisode 7262 terminated after 30 steps\nEpisode 7263 terminated after 30 steps\nEpisode 7264 terminated after 30 steps\nEpisode 7265 terminated after 30 steps\nEpisode 7266 terminated after 30 steps\nEpisode 7267 terminated after 30 steps\nEpisode 7268 terminated after 30 steps\nEpisode 7269 terminated after 30 steps\nEpisode 7270 terminated after 30 steps\nEpisode 7271 terminated after 30 steps\nEpisode 7272 terminated after 30 steps\nEpisode 7273 terminated after 30 steps\nEpisode 7274 terminated after 30 steps\nEpisode 7275 terminated after 30 steps\nEpisode 7276 terminated after 30 steps\nEpisode 7277 terminated after 30 steps\nEpisode 7278 terminated after 30 steps\nEpisode 7279 terminated after 30 steps\nEpisode 7280 terminated after 30 steps\nEpisode 7281 terminated after 30 steps\nEpisode 7282 terminated after 30 steps\nEpisode 7283 terminated after 30 steps\nEpisode 7284 terminated after 30 steps\nEpisode 7285 terminated after 30 steps\nEpisode 7286 terminated after 30 steps\nEpisode 7287 terminated after 30 steps\nEpisode 7288 terminated after 30 steps\nEpisode 7289 terminated after 30 steps\nEpisode 7290 terminated after 30 steps\nEpisode 7291 terminated after 30 steps\nEpisode 7292 terminated after 30 steps\nEpisode 7293 terminated after 30 steps\nEpisode 7294 terminated after 30 steps\nEpisode 7295 terminated after 30 steps\nEpisode 7296 terminated after 30 steps\nEpisode 7297 terminated after 30 steps\nEpisode 7298 terminated after 30 steps\nEpisode 7299 terminated after 30 steps\nEpisode 7300\nEpisode 7300 terminated after 30 steps\nEpisode 7301 terminated after 30 steps\nEpisode 7302 terminated after 30 steps\nEpisode 7303 terminated after 30 steps\nEpisode 7304 terminated after 30 steps\nEpisode 7305 terminated after 30 steps\nEpisode 7306 terminated after 30 steps\nEpisode 7307 terminated after 30 steps\nEpisode 7308 terminated after 30 steps\nEpisode 7309 terminated after 30 steps\nEpisode 7310 terminated after 30 steps\nEpisode 7311 terminated after 30 steps\nEpisode 7312 terminated after 30 steps\nEpisode 7313 terminated after 30 steps\nEpisode 7314 terminated after 30 steps\nEpisode 7315 terminated after 30 steps\nEpisode 7316 terminated after 30 steps\nEpisode 7317 terminated after 30 steps\nEpisode 7318 terminated after 30 steps\nEpisode 7319 terminated after 30 steps\nEpisode 7320 terminated after 30 steps\nEpisode 7321 terminated after 30 steps\nEpisode 7322 terminated after 30 steps\nEpisode 7323 terminated after 30 steps\nEpisode 7324 terminated after 30 steps\nEpisode 7325 terminated after 30 steps\nEpisode 7326 terminated after 30 steps\nEpisode 7327 terminated after 30 steps\nEpisode 7328 terminated after 30 steps\nEpisode 7329 terminated after 30 steps\nEpisode 7330 terminated after 30 steps\nEpisode 7331 terminated after 30 steps\nEpisode 7332 terminated after 30 steps\nEpisode 7333 terminated after 30 steps\nEpisode 7334 terminated after 30 steps\nEpisode 7335 terminated after 30 steps\nEpisode 7336 terminated after 30 steps\nEpisode 7337 terminated after 30 steps\nEpisode 7338 terminated after 30 steps\nEpisode 7339 terminated after 30 steps\nEpisode 7340 terminated after 30 steps\nEpisode 7341 terminated after 30 steps\nEpisode 7342 terminated after 30 steps\nEpisode 7343 terminated after 30 steps\nEpisode 7344 terminated after 30 steps\nEpisode 7345 terminated after 30 steps\nEpisode 7346 terminated after 30 steps\nEpisode 7347 terminated after 30 steps\nEpisode 7348 terminated after 30 steps\nEpisode 7349 terminated after 30 steps\nEpisode 7350 terminated after 30 steps\nEpisode 7351 terminated after 30 steps\nEpisode 7352 terminated after 30 steps\nEpisode 7353 terminated after 30 steps\nEpisode 7354 terminated after 30 steps\nEpisode 7355 terminated after 30 steps\nEpisode 7356 terminated after 30 steps\nEpisode 7357 terminated after 30 steps\nEpisode 7358 terminated after 30 steps\nEpisode 7359 terminated after 30 steps\nEpisode 7360 terminated after 30 steps\nEpisode 7361 terminated after 30 steps\nEpisode 7362 terminated after 30 steps\nEpisode 7363 terminated after 30 steps\nEpisode 7364 terminated after 30 steps\nEpisode 7365 terminated after 30 steps\nEpisode 7366 terminated after 30 steps\nEpisode 7367 terminated after 30 steps\nEpisode 7368 terminated after 30 steps\nEpisode 7369 terminated after 30 steps\nEpisode 7370 terminated after 30 steps\nEpisode 7371 terminated after 30 steps\nEpisode 7372 terminated after 30 steps\nEpisode 7373 terminated after 30 steps\nEpisode 7374 terminated after 30 steps\nEpisode 7375 terminated after 30 steps\nEpisode 7376 terminated after 30 steps\nEpisode 7377 terminated after 30 steps\nEpisode 7378 terminated after 30 steps\nEpisode 7379 terminated after 30 steps\nEpisode 7380 terminated after 30 steps\nEpisode 7381 terminated after 30 steps\nEpisode 7382 terminated after 30 steps\nEpisode 7383 terminated after 30 steps\nEpisode 7384 terminated after 30 steps\nEpisode 7385 terminated after 30 steps\nEpisode 7386 terminated after 30 steps\nEpisode 7387 terminated after 30 steps\nEpisode 7388 terminated after 30 steps\nEpisode 7389 terminated after 30 steps\nEpisode 7390 terminated after 30 steps\nEpisode 7391 terminated after 30 steps\nEpisode 7392 terminated after 30 steps\nEpisode 7393 terminated after 30 steps\nEpisode 7394 terminated after 30 steps\nEpisode 7395 terminated after 30 steps\nEpisode 7396 terminated after 30 steps\nEpisode 7397 terminated after 30 steps\nEpisode 7398 terminated after 30 steps\nEpisode 7399 terminated after 30 steps\nEpisode 7400\nEpisode 7400 terminated after 30 steps\nEpisode 7401 terminated after 30 steps\nEpisode 7402 terminated after 30 steps\nEpisode 7403 terminated after 30 steps\nEpisode 7404 terminated after 30 steps\nEpisode 7405 terminated after 30 steps\nEpisode 7406 terminated after 30 steps\nEpisode 7407 terminated after 30 steps\nEpisode 7408 terminated after 30 steps\nEpisode 7409 terminated after 30 steps\nEpisode 7410 terminated after 30 steps\nEpisode 7411 terminated after 30 steps\nEpisode 7412 terminated after 30 steps\nEpisode 7413 terminated after 30 steps\nEpisode 7414 terminated after 30 steps\nEpisode 7415 terminated after 30 steps\nEpisode 7416 terminated after 30 steps\nEpisode 7417 terminated after 30 steps\nEpisode 7418 terminated after 30 steps\nEpisode 7419 terminated after 30 steps\nEpisode 7420 terminated after 30 steps\nEpisode 7421 terminated after 30 steps\nEpisode 7422 terminated after 30 steps\nEpisode 7423 terminated after 30 steps\nEpisode 7424 terminated after 30 steps\nEpisode 7425 terminated after 30 steps\nEpisode 7426 terminated after 30 steps\nEpisode 7427 terminated after 30 steps\nEpisode 7428 terminated after 30 steps\nEpisode 7429 terminated after 30 steps\nEpisode 7430 terminated after 30 steps\nEpisode 7431 terminated after 30 steps\nEpisode 7432 terminated after 30 steps\nEpisode 7433 terminated after 30 steps\nEpisode 7434 terminated after 30 steps\nEpisode 7435 terminated after 30 steps\nEpisode 7436 terminated after 30 steps\nEpisode 7437 terminated after 30 steps\nEpisode 7438 terminated after 30 steps\nEpisode 7439 terminated after 30 steps\nEpisode 7440 terminated after 30 steps\nEpisode 7441 terminated after 30 steps\nEpisode 7442 terminated after 30 steps\nEpisode 7443 terminated after 30 steps\nEpisode 7444 terminated after 30 steps\nEpisode 7445 terminated after 30 steps\nEpisode 7446 terminated after 30 steps\nEpisode 7447 terminated after 30 steps\nEpisode 7448 terminated after 30 steps\nEpisode 7449 terminated after 30 steps\nEpisode 7450 terminated after 30 steps\nEpisode 7451 terminated after 30 steps\nEpisode 7452 terminated after 30 steps\nEpisode 7453 terminated after 30 steps\nEpisode 7454 terminated after 30 steps\nEpisode 7455 terminated after 30 steps\nEpisode 7456 terminated after 30 steps\nEpisode 7457 terminated after 30 steps\nEpisode 7458 terminated after 30 steps\nEpisode 7459 terminated after 30 steps\nEpisode 7460 terminated after 30 steps\nEpisode 7461 terminated after 30 steps\nEpisode 7462 terminated after 30 steps\nEpisode 7463 terminated after 30 steps\nEpisode 7464 terminated after 30 steps\nEpisode 7465 terminated after 30 steps\nEpisode 7466 terminated after 30 steps\nEpisode 7467 terminated after 30 steps\nEpisode 7468 terminated after 30 steps\nEpisode 7469 terminated after 30 steps\nEpisode 7470 terminated after 30 steps\nEpisode 7471 terminated after 30 steps\nEpisode 7472 terminated after 30 steps\nEpisode 7473 terminated after 30 steps\nEpisode 7474 terminated after 30 steps\nEpisode 7475 terminated after 30 steps\nEpisode 7476 terminated after 30 steps\nEpisode 7477 terminated after 30 steps\nEpisode 7478 terminated after 30 steps\nEpisode 7479 terminated after 30 steps\nEpisode 7480 terminated after 30 steps\nEpisode 7481 terminated after 30 steps\nEpisode 7482 terminated after 30 steps\nEpisode 7483 terminated after 30 steps\nEpisode 7484 terminated after 30 steps\nEpisode 7485 terminated after 30 steps\nEpisode 7486 terminated after 30 steps\nEpisode 7487 terminated after 30 steps\nEpisode 7488 terminated after 30 steps\nEpisode 7489 terminated after 30 steps\nEpisode 7490 terminated after 30 steps\nEpisode 7491 terminated after 30 steps\nEpisode 7492 terminated after 30 steps\nEpisode 7493 terminated after 30 steps\nEpisode 7494 terminated after 30 steps\nEpisode 7495 terminated after 30 steps\nEpisode 7496 terminated after 30 steps\nEpisode 7497 terminated after 30 steps\nEpisode 7498 terminated after 30 steps\nEpisode 7499 terminated after 30 steps\nEpisode 7500\nEpisode 7500 terminated after 30 steps\nEpisode 7501 terminated after 30 steps\nEpisode 7502 terminated after 30 steps\nEpisode 7503 terminated after 30 steps\nEpisode 7504 terminated after 30 steps\nEpisode 7505 terminated after 30 steps\nEpisode 7506 terminated after 30 steps\nEpisode 7507 terminated after 30 steps\nEpisode 7508 terminated after 30 steps\nEpisode 7509 terminated after 30 steps\nEpisode 7510 terminated after 30 steps\nEpisode 7511 terminated after 30 steps\nEpisode 7512 terminated after 30 steps\nEpisode 7513 terminated after 30 steps\nEpisode 7514 terminated after 30 steps\nEpisode 7515 terminated after 30 steps\nEpisode 7516 terminated after 30 steps\nEpisode 7517 terminated after 30 steps\nEpisode 7518 terminated after 30 steps\nEpisode 7519 terminated after 30 steps\nEpisode 7520 terminated after 30 steps\nEpisode 7521 terminated after 30 steps\nEpisode 7522 terminated after 30 steps\nEpisode 7523 terminated after 30 steps\nEpisode 7524 terminated after 30 steps\nEpisode 7525 terminated after 30 steps\nEpisode 7526 terminated after 30 steps\nEpisode 7527 terminated after 30 steps\nEpisode 7528 terminated after 30 steps\nEpisode 7529 terminated after 30 steps\nEpisode 7530 terminated after 30 steps\nEpisode 7531 terminated after 30 steps\nEpisode 7532 terminated after 30 steps\nEpisode 7533 terminated after 30 steps\nEpisode 7534 terminated after 30 steps\nEpisode 7535 terminated after 30 steps\nEpisode 7536 terminated after 30 steps\nEpisode 7537 terminated after 30 steps\nEpisode 7538 terminated after 30 steps\nEpisode 7539 terminated after 30 steps\nEpisode 7540 terminated after 30 steps\nEpisode 7541 terminated after 30 steps\nEpisode 7542 terminated after 30 steps\nEpisode 7543 terminated after 30 steps\nEpisode 7544 terminated after 30 steps\nEpisode 7545 terminated after 30 steps\nEpisode 7546 terminated after 30 steps\nEpisode 7547 terminated after 30 steps\nEpisode 7548 terminated after 30 steps\nEpisode 7549 terminated after 30 steps\nEpisode 7550 terminated after 30 steps\nEpisode 7551 terminated after 30 steps\nEpisode 7552 terminated after 30 steps\nEpisode 7553 terminated after 30 steps\nEpisode 7554 terminated after 30 steps\nEpisode 7555 terminated after 30 steps\nEpisode 7556 terminated after 30 steps\nEpisode 7557 terminated after 30 steps\nEpisode 7558 terminated after 30 steps\nEpisode 7559 terminated after 30 steps\nEpisode 7560 terminated after 30 steps\nEpisode 7561 terminated after 30 steps\nEpisode 7562 terminated after 30 steps\nEpisode 7563 terminated after 30 steps\nEpisode 7564 terminated after 30 steps\nEpisode 7565 terminated after 30 steps\nEpisode 7566 terminated after 30 steps\nEpisode 7567 terminated after 30 steps\nEpisode 7568 terminated after 30 steps\nEpisode 7569 terminated after 30 steps\nEpisode 7570 terminated after 30 steps\nEpisode 7571 terminated after 30 steps\nEpisode 7572 terminated after 30 steps\nEpisode 7573 terminated after 30 steps\nEpisode 7574 terminated after 30 steps\nEpisode 7575 terminated after 30 steps\nEpisode 7576 terminated after 30 steps\nEpisode 7577 terminated after 30 steps\nEpisode 7578 terminated after 30 steps\nEpisode 7579 terminated after 30 steps\nEpisode 7580 terminated after 30 steps\nEpisode 7581 terminated after 30 steps\nEpisode 7582 terminated after 30 steps\nEpisode 7583 terminated after 30 steps\nEpisode 7584 terminated after 30 steps\nEpisode 7585 terminated after 30 steps\nEpisode 7586 terminated after 30 steps\nEpisode 7587 terminated after 30 steps\nEpisode 7588 terminated after 30 steps\nEpisode 7589 terminated after 30 steps\nEpisode 7590 terminated after 30 steps\nEpisode 7591 terminated after 30 steps\nEpisode 7592 terminated after 30 steps\nEpisode 7593 terminated after 30 steps\nEpisode 7594 terminated after 30 steps\nEpisode 7595 terminated after 30 steps\nEpisode 7596 terminated after 30 steps\nEpisode 7597 terminated after 30 steps\nEpisode 7598 terminated after 30 steps\nEpisode 7599 terminated after 30 steps\nEpisode 7600\nEpisode 7600 terminated after 30 steps\nEpisode 7601 terminated after 30 steps\nEpisode 7602 terminated after 30 steps\nEpisode 7603 terminated after 30 steps\nEpisode 7604 terminated after 30 steps\nEpisode 7605 terminated after 30 steps\nEpisode 7606 terminated after 30 steps\nEpisode 7607 terminated after 30 steps\nEpisode 7608 terminated after 30 steps\nEpisode 7609 terminated after 30 steps\nEpisode 7610 terminated after 30 steps\nEpisode 7611 terminated after 30 steps\nEpisode 7612 terminated after 30 steps\nEpisode 7613 terminated after 30 steps\nEpisode 7614 terminated after 30 steps\nEpisode 7615 terminated after 30 steps\nEpisode 7616 terminated after 30 steps\nEpisode 7617 terminated after 30 steps\nEpisode 7618 terminated after 30 steps\nEpisode 7619 terminated after 30 steps\nEpisode 7620 terminated after 30 steps\nEpisode 7621 terminated after 30 steps\nEpisode 7622 terminated after 30 steps\nEpisode 7623 terminated after 30 steps\nEpisode 7624 terminated after 30 steps\nEpisode 7625 terminated after 30 steps\nEpisode 7626 terminated after 30 steps\nEpisode 7627 terminated after 30 steps\nEpisode 7628 terminated after 30 steps\nEpisode 7629 terminated after 30 steps\nEpisode 7630 terminated after 30 steps\nEpisode 7631 terminated after 30 steps\nEpisode 7632 terminated after 30 steps\nEpisode 7633 terminated after 30 steps\nEpisode 7634 terminated after 30 steps\nEpisode 7635 terminated after 30 steps\nEpisode 7636 terminated after 30 steps\nEpisode 7637 terminated after 30 steps\nEpisode 7638 terminated after 30 steps\nEpisode 7639 terminated after 30 steps\nEpisode 7640 terminated after 30 steps\nEpisode 7641 terminated after 30 steps\nEpisode 7642 terminated after 30 steps\nEpisode 7643 terminated after 30 steps\nEpisode 7644 terminated after 30 steps\nEpisode 7645 terminated after 30 steps\nEpisode 7646 terminated after 30 steps\nEpisode 7647 terminated after 30 steps\nEpisode 7648 terminated after 30 steps\nEpisode 7649 terminated after 30 steps\nEpisode 7650 terminated after 30 steps\nEpisode 7651 terminated after 30 steps\nEpisode 7652 terminated after 30 steps\nEpisode 7653 terminated after 30 steps\nEpisode 7654 terminated after 30 steps\nEpisode 7655 terminated after 30 steps\nEpisode 7656 terminated after 30 steps\nEpisode 7657 terminated after 30 steps\nEpisode 7658 terminated after 30 steps\nEpisode 7659 terminated after 30 steps\nEpisode 7660 terminated after 30 steps\nEpisode 7661 terminated after 30 steps\nEpisode 7662 terminated after 30 steps\nEpisode 7663 terminated after 30 steps\nEpisode 7664 terminated after 30 steps\nEpisode 7665 terminated after 30 steps\nEpisode 7666 terminated after 30 steps\nEpisode 7667 terminated after 30 steps\nEpisode 7668 terminated after 30 steps\nEpisode 7669 terminated after 30 steps\nEpisode 7670 terminated after 30 steps\nEpisode 7671 terminated after 30 steps\nEpisode 7672 terminated after 30 steps\nEpisode 7673 terminated after 30 steps\nEpisode 7674 terminated after 30 steps\nEpisode 7675 terminated after 30 steps\nEpisode 7676 terminated after 30 steps\nEpisode 7677 terminated after 30 steps\nEpisode 7678 terminated after 30 steps\nEpisode 7679 terminated after 30 steps\nEpisode 7680 terminated after 30 steps\nEpisode 7681 terminated after 30 steps\nEpisode 7682 terminated after 30 steps\nEpisode 7683 terminated after 30 steps\nEpisode 7684 terminated after 30 steps\nEpisode 7685 terminated after 30 steps\nEpisode 7686 terminated after 30 steps\nEpisode 7687 terminated after 30 steps\nEpisode 7688 terminated after 30 steps\nEpisode 7689 terminated after 30 steps\nEpisode 7690 terminated after 30 steps\nEpisode 7691 terminated after 30 steps\nEpisode 7692 terminated after 30 steps\nEpisode 7693 terminated after 30 steps\nEpisode 7694 terminated after 30 steps\nEpisode 7695 terminated after 30 steps\nEpisode 7696 terminated after 30 steps\nEpisode 7697 terminated after 30 steps\nEpisode 7698 terminated after 30 steps\nEpisode 7699 terminated after 30 steps\nEpisode 7700\nEpisode 7700 terminated after 30 steps\nEpisode 7701 terminated after 30 steps\nEpisode 7702 terminated after 30 steps\nEpisode 7703 terminated after 30 steps\nEpisode 7704 terminated after 30 steps\nEpisode 7705 terminated after 30 steps\nEpisode 7706 terminated after 30 steps\nEpisode 7707 terminated after 30 steps\nEpisode 7708 terminated after 30 steps\nEpisode 7709 terminated after 30 steps\nEpisode 7710 terminated after 30 steps\nEpisode 7711 terminated after 30 steps\nEpisode 7712 terminated after 30 steps\nEpisode 7713 terminated after 30 steps\nEpisode 7714 terminated after 30 steps\nEpisode 7715 terminated after 30 steps\nEpisode 7716 terminated after 30 steps\nEpisode 7717 terminated after 30 steps\nEpisode 7718 terminated after 30 steps\nEpisode 7719 terminated after 30 steps\nEpisode 7720 terminated after 30 steps\nEpisode 7721 terminated after 30 steps\nEpisode 7722 terminated after 30 steps\nEpisode 7723 terminated after 30 steps\nEpisode 7724 terminated after 30 steps\nEpisode 7725 terminated after 30 steps\nEpisode 7726 terminated after 30 steps\nEpisode 7727 terminated after 30 steps\nEpisode 7728 terminated after 30 steps\nEpisode 7729 terminated after 30 steps\nEpisode 7730 terminated after 30 steps\nEpisode 7731 terminated after 30 steps\nEpisode 7732 terminated after 30 steps\nEpisode 7733 terminated after 30 steps\nEpisode 7734 terminated after 30 steps\nEpisode 7735 terminated after 30 steps\nEpisode 7736 terminated after 30 steps\nEpisode 7737 terminated after 30 steps\nEpisode 7738 terminated after 30 steps\nEpisode 7739 terminated after 30 steps\nEpisode 7740 terminated after 30 steps\nEpisode 7741 terminated after 30 steps\nEpisode 7742 terminated after 30 steps\nEpisode 7743 terminated after 30 steps\nEpisode 7744 terminated after 30 steps\nEpisode 7745 terminated after 30 steps\nEpisode 7746 terminated after 30 steps\nEpisode 7747 terminated after 30 steps\nEpisode 7748 terminated after 30 steps\nEpisode 7749 terminated after 30 steps\nEpisode 7750 terminated after 30 steps\nEpisode 7751 terminated after 30 steps\nEpisode 7752 terminated after 30 steps\nEpisode 7753 terminated after 30 steps\nEpisode 7754 terminated after 30 steps\nEpisode 7755 terminated after 30 steps\nEpisode 7756 terminated after 30 steps\nEpisode 7757 terminated after 30 steps\nEpisode 7758 terminated after 30 steps\nEpisode 7759 terminated after 30 steps\nEpisode 7760 terminated after 30 steps\nEpisode 7761 terminated after 30 steps\nEpisode 7762 terminated after 30 steps\nEpisode 7763 terminated after 30 steps\nEpisode 7764 terminated after 30 steps\nEpisode 7765 terminated after 30 steps\nEpisode 7766 terminated after 30 steps\nEpisode 7767 terminated after 30 steps\nEpisode 7768 terminated after 30 steps\nEpisode 7769 terminated after 30 steps\nEpisode 7770 terminated after 30 steps\nEpisode 7771 terminated after 30 steps\nEpisode 7772 terminated after 30 steps\nEpisode 7773 terminated after 30 steps\nEpisode 7774 terminated after 30 steps\nEpisode 7775 terminated after 30 steps\nEpisode 7776 terminated after 30 steps\nEpisode 7777 terminated after 30 steps\nEpisode 7778 terminated after 30 steps\nEpisode 7779 terminated after 30 steps\nEpisode 7780 terminated after 30 steps\nEpisode 7781 terminated after 30 steps\nEpisode 7782 terminated after 30 steps\nEpisode 7783 terminated after 30 steps\nEpisode 7784 terminated after 30 steps\nEpisode 7785 terminated after 30 steps\nEpisode 7786 terminated after 30 steps\nEpisode 7787 terminated after 30 steps\nEpisode 7788 terminated after 30 steps\nEpisode 7789 terminated after 30 steps\nEpisode 7790 terminated after 30 steps\nEpisode 7791 terminated after 30 steps\nEpisode 7792 terminated after 30 steps\nEpisode 7793 terminated after 30 steps\nEpisode 7794 terminated after 30 steps\nEpisode 7795 terminated after 30 steps\nEpisode 7796 terminated after 30 steps\nEpisode 7797 terminated after 30 steps\nEpisode 7798 terminated after 30 steps\nEpisode 7799 terminated after 30 steps\nEpisode 7800\nEpisode 7800 terminated after 30 steps\nEpisode 7801 terminated after 30 steps\nEpisode 7802 terminated after 30 steps\nEpisode 7803 terminated after 30 steps\nEpisode 7804 terminated after 30 steps\nEpisode 7805 terminated after 30 steps\nEpisode 7806 terminated after 30 steps\nEpisode 7807 terminated after 30 steps\nEpisode 7808 terminated after 30 steps\nEpisode 7809 terminated after 30 steps\nEpisode 7810 terminated after 30 steps\nEpisode 7811 terminated after 30 steps\nEpisode 7812 terminated after 30 steps\nEpisode 7813 terminated after 30 steps\nEpisode 7814 terminated after 30 steps\nEpisode 7815 terminated after 30 steps\nEpisode 7816 terminated after 30 steps\nEpisode 7817 terminated after 30 steps\nEpisode 7818 terminated after 30 steps\nEpisode 7819 terminated after 30 steps\nEpisode 7820 terminated after 30 steps\nEpisode 7821 terminated after 30 steps\nEpisode 7822 terminated after 30 steps\nEpisode 7823 terminated after 30 steps\nEpisode 7824 terminated after 30 steps\nEpisode 7825 terminated after 30 steps\nEpisode 7826 terminated after 30 steps\nEpisode 7827 terminated after 30 steps\nEpisode 7828 terminated after 30 steps\nEpisode 7829 terminated after 30 steps\nEpisode 7830 terminated after 30 steps\nEpisode 7831 terminated after 30 steps\nEpisode 7832 terminated after 30 steps\nEpisode 7833 terminated after 30 steps\nEpisode 7834 terminated after 30 steps\nEpisode 7835 terminated after 30 steps\nEpisode 7836 terminated after 30 steps\nEpisode 7837 terminated after 30 steps\nEpisode 7838 terminated after 30 steps\nEpisode 7839 terminated after 30 steps\nEpisode 7840 terminated after 30 steps\nEpisode 7841 terminated after 30 steps\nEpisode 7842 terminated after 30 steps\nEpisode 7843 terminated after 30 steps\nEpisode 7844 terminated after 30 steps\nEpisode 7845 terminated after 30 steps\nEpisode 7846 terminated after 30 steps\nEpisode 7847 terminated after 30 steps\nEpisode 7848 terminated after 30 steps\nEpisode 7849 terminated after 30 steps\nEpisode 7850 terminated after 30 steps\nEpisode 7851 terminated after 30 steps\nEpisode 7852 terminated after 30 steps\nEpisode 7853 terminated after 30 steps\nEpisode 7854 terminated after 30 steps\nEpisode 7855 terminated after 30 steps\nEpisode 7856 terminated after 30 steps\nEpisode 7857 terminated after 30 steps\nEpisode 7858 terminated after 30 steps\nEpisode 7859 terminated after 30 steps\nEpisode 7860 terminated after 30 steps\nEpisode 7861 terminated after 30 steps\nEpisode 7862 terminated after 30 steps\nEpisode 7863 terminated after 30 steps\nEpisode 7864 terminated after 30 steps\nEpisode 7865 terminated after 30 steps\nEpisode 7866 terminated after 30 steps\nEpisode 7867 terminated after 30 steps\nEpisode 7868 terminated after 30 steps\nEpisode 7869 terminated after 30 steps\nEpisode 7870 terminated after 30 steps\nEpisode 7871 terminated after 30 steps\nEpisode 7872 terminated after 30 steps\nEpisode 7873 terminated after 30 steps\nEpisode 7874 terminated after 30 steps\nEpisode 7875 terminated after 30 steps\nEpisode 7876 terminated after 30 steps\nEpisode 7877 terminated after 30 steps\nEpisode 7878 terminated after 30 steps\nEpisode 7879 terminated after 30 steps\nEpisode 7880 terminated after 30 steps\nEpisode 7881 terminated after 30 steps\nEpisode 7882 terminated after 30 steps\nEpisode 7883 terminated after 30 steps\nEpisode 7884 terminated after 30 steps\nEpisode 7885 terminated after 30 steps\nEpisode 7886 terminated after 30 steps\nEpisode 7887 terminated after 30 steps\nEpisode 7888 terminated after 30 steps\nEpisode 7889 terminated after 30 steps\nEpisode 7890 terminated after 30 steps\nEpisode 7891 terminated after 30 steps\nEpisode 7892 terminated after 30 steps\nEpisode 7893 terminated after 30 steps\nEpisode 7894 terminated after 30 steps\nEpisode 7895 terminated after 30 steps\nEpisode 7896 terminated after 30 steps\nEpisode 7897 terminated after 30 steps\nEpisode 7898 terminated after 30 steps\nEpisode 7899 terminated after 30 steps\nEpisode 7900\nEpisode 7900 terminated after 30 steps\nEpisode 7901 terminated after 30 steps\nEpisode 7902 terminated after 30 steps\nEpisode 7903 terminated after 30 steps\nEpisode 7904 terminated after 30 steps\nEpisode 7905 terminated after 30 steps\nEpisode 7906 terminated after 30 steps\nEpisode 7907 terminated after 30 steps\nEpisode 7908 terminated after 30 steps\nEpisode 7909 terminated after 30 steps\nEpisode 7910 terminated after 30 steps\nEpisode 7911 terminated after 30 steps\nEpisode 7912 terminated after 30 steps\nEpisode 7913 terminated after 30 steps\nEpisode 7914 terminated after 30 steps\nEpisode 7915 terminated after 30 steps\nEpisode 7916 terminated after 30 steps\nEpisode 7917 terminated after 30 steps\nEpisode 7918 terminated after 30 steps\nEpisode 7919 terminated after 30 steps\nEpisode 7920 terminated after 30 steps\nEpisode 7921 terminated after 30 steps\nEpisode 7922 terminated after 30 steps\nEpisode 7923 terminated after 30 steps\nEpisode 7924 terminated after 30 steps\nEpisode 7925 terminated after 30 steps\nEpisode 7926 terminated after 30 steps\nEpisode 7927 terminated after 30 steps\nEpisode 7928 terminated after 30 steps\nEpisode 7929 terminated after 30 steps\nEpisode 7930 terminated after 30 steps\nEpisode 7931 terminated after 30 steps\nEpisode 7932 terminated after 30 steps\nEpisode 7933 terminated after 30 steps\nEpisode 7934 terminated after 30 steps\nEpisode 7935 terminated after 30 steps\nEpisode 7936 terminated after 30 steps\nEpisode 7937 terminated after 30 steps\nEpisode 7938 terminated after 30 steps\nEpisode 7939 terminated after 30 steps\nEpisode 7940 terminated after 30 steps\nEpisode 7941 terminated after 30 steps\nEpisode 7942 terminated after 30 steps\nEpisode 7943 terminated after 30 steps\nEpisode 7944 terminated after 30 steps\nEpisode 7945 terminated after 30 steps\nEpisode 7946 terminated after 30 steps\nEpisode 7947 terminated after 30 steps\nEpisode 7948 terminated after 30 steps\nEpisode 7949 terminated after 30 steps\nEpisode 7950 terminated after 30 steps\nEpisode 7951 terminated after 30 steps\nEpisode 7952 terminated after 30 steps\nEpisode 7953 terminated after 30 steps\nEpisode 7954 terminated after 30 steps\nEpisode 7955 terminated after 30 steps\nEpisode 7956 terminated after 30 steps\nEpisode 7957 terminated after 30 steps\nEpisode 7958 terminated after 30 steps\nEpisode 7959 terminated after 30 steps\nEpisode 7960 terminated after 30 steps\nEpisode 7961 terminated after 30 steps\nEpisode 7962 terminated after 30 steps\nEpisode 7963 terminated after 30 steps\nEpisode 7964 terminated after 30 steps\nEpisode 7965 terminated after 30 steps\nEpisode 7966 terminated after 30 steps\nEpisode 7967 terminated after 30 steps\nEpisode 7968 terminated after 30 steps\nEpisode 7969 terminated after 30 steps\nEpisode 7970 terminated after 30 steps\nEpisode 7971 terminated after 30 steps\nEpisode 7972 terminated after 30 steps\nEpisode 7973 terminated after 30 steps\nEpisode 7974 terminated after 30 steps\nEpisode 7975 terminated after 30 steps\nEpisode 7976 terminated after 30 steps\nEpisode 7977 terminated after 30 steps\nEpisode 7978 terminated after 30 steps\nEpisode 7979 terminated after 30 steps\nEpisode 7980 terminated after 30 steps\nEpisode 7981 terminated after 30 steps\nEpisode 7982 terminated after 30 steps\nEpisode 7983 terminated after 30 steps\nEpisode 7984 terminated after 30 steps\nEpisode 7985 terminated after 30 steps\nEpisode 7986 terminated after 30 steps\nEpisode 7987 terminated after 30 steps\nEpisode 7988 terminated after 30 steps\nEpisode 7989 terminated after 30 steps\nEpisode 7990 terminated after 30 steps\nEpisode 7991 terminated after 30 steps\nEpisode 7992 terminated after 30 steps\nEpisode 7993 terminated after 30 steps\nEpisode 7994 terminated after 30 steps\nEpisode 7995 terminated after 30 steps\nEpisode 7996 terminated after 30 steps\nEpisode 7997 terminated after 30 steps\nEpisode 7998 terminated after 30 steps\nEpisode 7999 terminated after 30 steps\n\n\n\nepsilon\n\n1.206581828445788e-35\n\n\n\nplt.plot(rewards)\n\nplt.plot(pd.Series(rewards).rolling(100).mean())\n\n\n\n\n\n\n\n\n\n# now play in human mode using the trained Q-table\n\nenv = gym.make(\"FlappyBird-v0\", render_mode=\"human\", use_lidar=False)\n\nobs, _ = env.reset()\n\nfor i in range(300):\n    env.render()\n    obs = obs[3:-3]\n    obs_idx = discretize_observation(obs)\n    action = np.argmax(q_table_np[obs_idx])\n    action = int(i%16==0)\n    obs, reward, terminated, truncated, info = env.step(action)\n    print(i, action)\n    if terminated:\n        break\n\n0 1\n1 0\n2 0\n3 0\n4 0\n5 0\n6 0\n7 0\n8 0\n9 0\n10 0\n11 0\n12 0\n13 0\n14 0\n15 0\n16 1\n17 0\n18 0\n19 0\n20 0\n21 0\n22 0\n23 0\n24 0\n25 0\n26 0\n27 0\n28 0\n29 0\n30 0\n31 0\n32 1\n33 0\n34 0\n35 0\n36 0\n37 0\n38 0\n39 0\n40 0\n41 0\n42 0\n43 0\n44 0\n45 0\n46 0\n47 0\n48 1\n49 0\n50 0\n51 0\n52 0\n53 0\n54 0\n55 0\n56 0\n57 0\n58 0\n59 0\n60 0\n61 0\n62 0\n63 0\n64 1\n65 0\n66 0\n67 0\n68 0\n69 0\n\n\n\nThe Kernel crashed while executing code in the current cell or a previous cell. \n\nPlease review the code in the cell(s) to identify a possible cause of the failure. \n\nClick &lt;a href='https://aka.ms/vscodeJupyterKernelCrash'&gt;here&lt;/a&gt; for more info. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\n# find mean for action\n\n\nq_table_np[..., 0].mean(), q_table_np[..., 1].mean()\n\n(-1.664908695922577e-06, -6.468030410554288e-06)"
  },
  {
    "objectID": "notebooks/svm-kernel-understanding.html",
    "href": "notebooks/svm-kernel-understanding.html",
    "title": "SVM Kernels Understanding",
    "section": "",
    "text": "%pip install plotly\n\nCollecting plotly\n  Downloading plotly-5.20.0-py3-none-any.whl (15.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/15.7 MB 451.4 kB/s eta 0:00:0000:0100:01\nRequirement already satisfied: tenacity&gt;=6.2.0 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from plotly) (8.2.3)\nRequirement already satisfied: packaging in /Users/nipun/mambaforge/lib/python3.10/site-packages (from plotly) (23.1)\nInstalling collected packages: plotly\nSuccessfully installed plotly-5.20.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n%config InlineBackend.figure_format = 'retina'\n\n\ndef linear_kernel(x1, x2):\n    return np.dot(x1, x2)\n\ndef polynomial_kernel(x, y, p=3):\n    return (1 + np.dot(x, y)) ** p\n\ndef gaussian_kernel(x, y, gamma=0.5):\n    return np.exp(-gamma * np.linalg.norm(x-y)**2)\n\n\nx_test = np.linspace(-5, 5, 100)\nx_point = np.array([1.0])\n\nK_linear_point = np.array([linear_kernel(x_point, xi) for xi in x_test])\nK_polynomial_point = np.array([polynomial_kernel(x_point, xi) for xi in x_test])\nK_gaussian_point = np.array([gaussian_kernel(x_point, xi) for xi in x_test])\nK_gaussian_high_gamma_point = np.array([gaussian_kernel(x_point, xi, gamma=2.0) for xi in x_test])\n\n\nfig, ax = plt.subplots(nrows=4, sharex=True, figsize=(6, 6))\nax[0].plot(x_test, K_linear_point)\nax[0].set_title('Linear kernel')\nax[1].plot(x_test, K_polynomial_point)\nax[1].set_title('Polynomial kernel')\nax[2].plot(x_test, K_gaussian_point)\nax[2].set_title('Gaussian kernel')\nax[3].plot(x_test, K_gaussian_high_gamma_point)\nax[3].set_title('Gaussian kernel with high $\\gamma$')\nplt.tight_layout()\n\nfor i in range(4):\n    ax[i].axvline(x=x_point[0], color='k', linestyle='--', label=r'$x_{sv}$')\n\n\nplt.legend()\n\n# add ylabel to figure as K(x, x_{sv})\nfor i in range(4):\n    ax[i].set_ylabel(r'$K(x, x_{sv})$')\n\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5, 5, 100)\n\nK_lin = np.array([[linear_kernel(xi, xj) for xi in x] for xj in x])\nK_poly = np.array([[polynomial_kernel(xi, xj) for xi in x] for xj in x])\nK_gauss = np.array([[gaussian_kernel(xi, xj) for xi in x] for xj in x])\n\n\ntrace_lin = go.Heatmap(z=K_lin, x=x, y=x, colorscale='Viridis', name='Linear Kernel')\nfig = go.Figure(data=[trace_lin])\nfig.update_layout(title='Linear Kernel', width=500, height=500)\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\ntrace_rbf = go.Heatmap(z=K_gauss, x=x, y=x, colorscale='Viridis', name='Gaussian Kernel')\nfig = go.Figure(data=[trace_rbf])\nfig.update_layout(title='Gaussian Kernel', width=500, height=500)\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n# Plot 1 row 3 cols for various value of gamma in Gaussian kernel using plotly\ngammas = [0.1, 1, 5]\n\nfig = make_subplots(rows=1, cols=len(gammas), subplot_titles=[f'Gamma={gamma}' for gamma in gammas])\n\nfor i, gamma in enumerate(gammas, 1):\n    K_gauss = np.array([[gaussian_kernel(xi, xj, gamma) for xi in x] for xj in x])\n    fig.add_trace(go.Heatmap(z=K_gauss, x=x, y=x, colorscale='Viridis'), row=1, col=i)\n\nfig.update_layout(title_text=\"Gaussian Kernel Heatmaps for Various Gamma Values\")\nfig.show()\n\n    \n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n### Understanding how the \"similarity\" from support vectors to the test point is calculated\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.svm import SVC\nX, y = make_blobs(n_samples=100, centers=2, random_state=6)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n\nclf = SVC(kernel='linear', C = 1e6)\nclf.fit(X, y)\n\nSVC(C=1000000.0, kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  SVC?Documentation for SVCiFittedSVC(C=1000000.0, kernel='linear') \n\n\n\n\n\n\n\n\n\n\nclf.support_vectors_\n\narray([[ 7.27059007, -4.84225716],\n       [ 6.49868019, -7.13530714]])\n\n\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n\n# plot scatter with black edge color for support vectors\nplt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n            linewidth=1, facecolors='none', edgecolors='k')\n\n# test point\ntest_point = np.array([6, -4])\n\ndef decision_function_test(clf, test_point):\n    decision_function = 0\n    print(f\"Test point: {test_point}\")\n    print(\"---\"*20)\n    for i in range(len(clf.support_vectors_)):\n        print(f\"{i}th support vector: {clf.support_vectors_[i]}\")\n        print(f'alpha_{i} = {alpha[i]}')\n        print(f'y_{clf.support_}[{i}] = {y_plus_minus[clf.support_][i]}')\n        print(f'K(x_{i}, x_test) = {linear_kernel(clf.support_vectors_[i], test_point[0])}')\n        \n        out_i = alpha[i] * y_plus_minus[clf.support_][i] * linear_kernel(clf.support_vectors_[i], test_point[0])\n        print(f'alpha_{i} * y_{clf.support_}[{i}] * K(x_{i}, x_test) = {out_i}')\n        decision_function += out_i\n        print(\"---\"*10)\n    print(f\"b = {clf.intercept_}\")\n    print(f\"Decision function: {decision_function + clf.intercept_} = {decision_function} + {clf.intercept_}\")\n    decision_function += clf.intercept_\n    return decision_function\n\n\n\n\n\n\n\n\n\nclf.dual_coef_\n\narray([[-0.341651,  0.341651]])\n\n\n\ntest_point = np.array([[7.0, -8.0]])\n\ndecision_function_test(clf, test_point)\n\nTest point: [[ 7. -8.]]\n------------------------------------------------------------\n0th support vector: [ 7.27059007 -4.84225716]\nalpha_0 = 0.3416510021250007\ny_[48 31][0] = -1\nK(x_0, x_test) = 89.63218778443229\nalpha_0 * y_[48 31][0] * K(x_0, x_test) = -30.62292677920754\n------------------------------\n1th support vector: [ 6.49868019 -7.13530714]\nalpha_1 = 0.3416510021250007\ny_[48 31][1] = 1\nK(x_1, x_test) = 102.57321844738127\nalpha_1 * y_[48 31][1] * K(x_1, x_test) = 35.04424287373442\n------------------------------\nb = [-2.87610651]\nDecision function: [1.54520958] = 4.42131609452688 + [-2.87610651]\n\n\narray([1.54520958])\n\n\n\nclf.decision_function(test_point)\n\narray([1.54520958])"
  },
  {
    "objectID": "notebooks/Gradient Descent.html",
    "href": "notebooks/Gradient Descent.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nsns.despine()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\ninit_x = 2\ninit_y = 5\n\n\n%matplotlib notebook\n\n\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D  \n# Axes3D import has side effects, it enables using projection='3d' in add_subplot\nimport matplotlib.pyplot as plt\nimport random\n\ndef fun(x, y):\n    x = np.array(x)\n    y = np.array(y)\n    return 14+3*(x**2) + 14*(y**2) - 12*x- 28*y + 12*x*y\n\n\nlst_x = []\nlst_y = []\nx_ = init_x\ny_ = init_y\nalpha = 0.005\n\nlst_x.append(x_)\nlst_y.append(y_)\n\nfor i in range(10):\n\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111, projection='3d')\n    x = y = np.arange(-4.0, 4.0, 0.05)\n    X, Y = np.meshgrid(x, y)\n    zs = np.array(fun(np.ravel(X), np.ravel(Y)))\n    Z = zs.reshape(X.shape)\n    x_ = lst_x[-1]\n    y_ = lst_y[-1]\n#     ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='Greens')\n#     print (lst_x,lst_y,fun(lst_x,lst_y))\n    ax.scatter3D(lst_x,lst_y,fun(lst_x,lst_y),lw=10,alpha=1,cmap='hsv')\n    ax.plot_surface(X, Y, Z,color='orange',cmap='hsv')\n\n    ax.set_xlabel('X Label')\n    ax.set_ylabel('Y Label')\n    ax.set_zlabel('Z Label')\n    plt.title(\"Iteration \"+str(i+1))\n    lst_x.append(x_ - alpha * (3*x_ - 12 + 12*y_))\n    lst_y.append(y_  - alpha *(14*y_ -28 + 12*x_))\n    \n\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,1000) y = x**2 plt.plot(x,y) plt.title(“Cost Function”)\n\nplt.rcParams['axes.facecolor'] = '#fafafa'\n\n\n\np = 4.1\nalpha = 0.05\niterations = 20\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"iteration-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    \n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Gradient Descent}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-10.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-11.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-12.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-13.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-14.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-15.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-16.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-17.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-18.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-19.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-20.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\ndef func(x):\n    return np.sin(x) + np.sin(x/2) + np.sin(x/3)\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\nplt.savefig(\"local-minima.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .95\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"overshooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Overshooting}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Slow Convergence}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(1,10,100)\ny = 1/x\nplt.plot(y,label=\"GD\")\nnoise = np.random.random((len(x)))\nnoise[0] = 0\nnoise[1] = 0\nnoise[2] = 0\nplt.plot(y+0.2*(noise-0.5),label=\"SGD\")\nplt.legend()\nplt.title(\"Iterations vs Cost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\nplt.savefig(\"gd-sgd.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nval = 4.1\nalpha = 0.05\n\nfor i in range(10):\n    val = val - alpha * 2* val\n    print (val)\n\n3.6899999999999995\n3.3209999999999997\n2.9888999999999997\n2.6900099999999996\n2.4210089999999997\n2.1789080999999997\n1.9610172899999996\n1.7649155609999996\n1.5884240048999996\n1.4295816044099996"
  },
  {
    "objectID": "notebooks/basis2.html",
    "href": "notebooks/basis2.html",
    "title": "Basis Expansion in Linear Regression",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\nx = np.linspace(-1, 1, 100)\n\n\nfrom sklearn.kernel_approximation import RBFSampler\n\n\nRBFSampler?\n\nInit signature: RBFSampler(*, gamma=1.0, n_components=100, random_state=None)\nDocstring:     \nApproximate a RBF kernel feature map using random Fourier features.\n\nIt implements a variant of Random Kitchen Sinks.[1]\n\nRead more in the :ref:`User Guide &lt;rbf_kernel_approx&gt;`.\n\nParameters\n----------\ngamma : 'scale' or float, default=1.0\n    Parameter of RBF kernel: exp(-gamma * x^2).\n    If ``gamma='scale'`` is passed then it uses\n    1 / (n_features * X.var()) as value of gamma.\n\n    .. versionadded:: 1.2\n       The option `\"scale\"` was added in 1.2.\n\nn_components : int, default=100\n    Number of Monte Carlo samples per original feature.\n    Equals the dimensionality of the computed feature space.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the generation of the random\n    weights and random offset when fitting the training data.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary &lt;random_state&gt;`.\n\nAttributes\n----------\nrandom_offset_ : ndarray of shape (n_components,), dtype={np.float64, np.float32}\n    Random offset used to compute the projection in the `n_components`\n    dimensions of the feature space.\n\nrandom_weights_ : ndarray of shape (n_features, n_components),        dtype={np.float64, np.float32}\n    Random projection directions drawn from the Fourier transform\n    of the RBF kernel.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nAdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\nNystroem : Approximate a kernel map using a subset of the training data.\nPolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch.\nSkewedChi2Sampler : Approximate feature map for\n    \"skewed chi-squared\" kernel.\nsklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\nNotes\n-----\nSee \"Random Features for Large-Scale Kernel Machines\" by A. Rahimi and\nBenjamin Recht.\n\n[1] \"Weighted Sums of Random Kitchen Sinks: Replacing\nminimization with randomization in learning\" by A. Rahimi and\nBenjamin Recht.\n(https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)\n\nExamples\n--------\n&gt;&gt;&gt; from sklearn.kernel_approximation import RBFSampler\n&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier\n&gt;&gt;&gt; X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n&gt;&gt;&gt; y = [0, 0, 1, 1]\n&gt;&gt;&gt; rbf_feature = RBFSampler(gamma=1, random_state=1)\n&gt;&gt;&gt; X_features = rbf_feature.fit_transform(X)\n&gt;&gt;&gt; clf = SGDClassifier(max_iter=5, tol=1e-3)\n&gt;&gt;&gt; clf.fit(X_features, y)\nSGDClassifier(max_iter=5)\n&gt;&gt;&gt; clf.score(X_features, y)\n1.0\nFile:           ~/miniforge3/lib/python3.9/site-packages/sklearn/kernel_approximation.py\nType:           type\nSubclasses:     \n\n\n\nr= RBFSampler(n_components=5)\n\n\nplt.plot(x, r.fit_transform(x.reshape(-1,1)))\n\n\n\n\n\n\n\n\n\nr = RBFSampler(n_components=4, gamma=0.1)\nplt.plot(x, r.fit_transform(x.reshape(-1,1)))\n\n\n\n\n\n\n\n\n\nr = RBFSampler(n_components=4, gamma=20)\nplt.plot(x, r.fit_transform(x.reshape(-1,1)))"
  },
  {
    "objectID": "notebooks/text_to_image.html",
    "href": "notebooks/text_to_image.html",
    "title": "Using Diffusion to Generate Images from Text",
    "section": "",
    "text": "References\n\nPromptHero guide\n\n\n%pip install --upgrade \\\n  diffusers \\\n  transformers \\\n  safetensors \\\n  sentencepiece \\\n  accelerate \\\n  bitsandbytes \\\n  torch \\\n  huggingface_hub --quiet\n\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 2.1.2 which is incompatible.\ntorchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 2.1.2 which is incompatible.\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom huggingface_hub import login\n\nlogin()\n\n\n\n\n\nBasic Imports\n\n# Display the images in a grid\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nfrom PIL import Image\nimport io\n\n\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler, AutoencoderKL\nimport torch\npipe = DiffusionPipeline.from_pretrained(\n    \"prompthero/openjourney\", \n    torch_dtype=torch.float16\n)\nvae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-mse\", torch_dtype=torch.float16).to(\"cuda\")\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.vae = vae\npipe = pipe.to(\"cuda\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n\n\n\n\n\n\n\n\n\ndef generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions):\n    random_seeds = [random.randint(0, 65000) for _ in range(num_variations)]\n    images = pipe(prompt= num_variations * [prompt],\n              num_inference_steps=num_steps,\n              guidance_scale=prompt_guidance,\n              height = dimensions[0],\n              width = dimensions[1],\n              generator = [torch.Generator('cuda').manual_seed(i) for i in random_seeds]\n             ).images\n    return images\n\n\nimport random\n\n\n# Setting for image generation\nprompt = 'Small happy dog anf owner learning to walk on a rainy day. Colored photography. Leica lens. Hi-res. hd 8k --ar 2:3'\nnum_steps = 150\nnum_variations = 4\nprompt_guidance = 8\ndimensions = (400, 600) # (width, height) tuple\nrandom_seeds = [random.randint(0, 65000) for _ in range(num_variations)]\n\n\nimages = generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions)\n\n\n\n\n\n\n\ndef display_images(images, num_variations, dimensions):\n    fig = plt.figure(figsize=(dimensions[0]/10, dimensions[1]/10))\n    columns = num_variations\n    rows = 1\n    for i in range(1, columns*rows +1):\n        img = images[i-1]\n        fig.add_subplot(rows, columns, i)\n        plt.imshow(img)\n        # hide axes\n        plt.axis('off')\n    plt.show()\n    \ndisplay_images(images, num_variations, dimensions)\n\n\n\n\n\n\n\n\n\nprompt = \"Batman, cinematic lighting, dark background, very high resolution 3D render.\"\n\n\nimages = generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions)\ndisplay_images(images, num_variations, dimensions)\n\n\n\n\n\n\n\n\n\n\n\n\nprompt = \"A logo for a research group in India called Sustainability lab that works on AI for sustainability. Show AI as the central theme. Show applications in health, air quality\"\n\nimages = generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions)\ndisplay_images(images, num_variations, dimensions)\n\n\n\n\n\n\n\n\n\n\n\n\nprompt = \"Portrait of a small kid with a big smile.\"\n\nimages = generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions)\ndisplay_images(images, num_variations, dimensions)\n\n\n\n\n\n\n\n\n\n\n\n\nprompt = \"A photorealistic render of an academic campus in India, on the edges of a river, low-light, evening.\"\n\nimages = generate_images(prompt, num_steps, num_variations, prompt_guidance, dimensions)\ndisplay_images(images, num_variations, dimensions)"
  },
  {
    "objectID": "notebooks/ZeroShot_FewShot.html",
    "href": "notebooks/ZeroShot_FewShot.html",
    "title": "Notebook to demonstrate Zero shot and Few shot Learning",
    "section": "",
    "text": "%pip install langchain langchain_groq\n\nCollecting langchain\n  Downloading langchain-0.2.13-py3-none-any.whl (997 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 997.8/997.8 kB 3.7 MB/s eta 0:00:00a 0:00:01\nCollecting langchain_groq\n  Downloading langchain_groq-0.1.9-py3-none-any.whl (14 kB)\nRequirement already satisfied: PyYAML&gt;=5.3 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from langchain) (6.0)\nCollecting SQLAlchemy&lt;3,&gt;=1.4 (from langchain)\n  Downloading SQLAlchemy-2.0.32-cp310-cp310-macosx_11_0_arm64.whl (2.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 6.9 MB/s eta 0:00:00a 0:00:01\nCollecting aiohttp&lt;4.0.0,&gt;=3.8.3 (from langchain)\n  Downloading aiohttp-3.10.3-cp310-cp310-macosx_11_0_arm64.whl (388 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.5/388.5 kB 6.9 MB/s eta 0:00:00a 0:00:01\nCollecting async-timeout&lt;5.0.0,&gt;=4.0.0 (from langchain)\n  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nCollecting langchain-core&lt;0.3.0,&gt;=0.2.30 (from langchain)\n  Downloading langchain_core-0.2.30-py3-none-any.whl (384 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 384.8/384.8 kB 7.8 MB/s eta 0:00:00a 0:00:01\nCollecting langchain-text-splitters&lt;0.3.0,&gt;=0.2.0 (from langchain)\n  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\nCollecting langsmith&lt;0.2.0,&gt;=0.1.17 (from langchain)\n  Downloading langsmith-0.1.99-py3-none-any.whl (140 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.4/140.4 kB 13.7 MB/s eta 0:00:00\nRequirement already satisfied: numpy&lt;2,&gt;=1 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from langchain) (1.24.3)\nCollecting pydantic&lt;3,&gt;=1 (from langchain)\n  Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 423.9/423.9 kB 8.0 MB/s eta 0:00:00a 0:00:01\nRequirement already satisfied: requests&lt;3,&gt;=2 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from langchain) (2.29.0)\nRequirement already satisfied: tenacity!=8.4.0,&lt;9.0.0,&gt;=8.1.0 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from langchain) (8.2.3)\nCollecting groq&lt;1,&gt;=0.4.1 (from langchain_groq)\n  Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.5/103.5 kB 2.5 MB/s eta 0:00:0000:01\nCollecting aiohappyeyeballs&gt;=2.3.0 (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain)\n  Downloading aiohappyeyeballs-2.3.5-py3-none-any.whl (12 kB)\nCollecting aiosignal&gt;=1.1.2 (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain)\n  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nRequirement already satisfied: attrs&gt;=17.3.0 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain) (23.1.0)\nCollecting frozenlist&gt;=1.1.1 (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain)\n  Downloading frozenlist-1.4.1-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.2/52.2 kB 7.1 MB/s eta 0:00:00\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain) (6.0.4)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain) (1.9.2)\nRequirement already satisfied: anyio&lt;5,&gt;=3.5.0 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from groq&lt;1,&gt;=0.4.1-&gt;langchain_groq) (3.6.2)\nCollecting distro&lt;2,&gt;=1.7.0 (from groq&lt;1,&gt;=0.4.1-&gt;langchain_groq)\n  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\nRequirement already satisfied: httpx&lt;1,&gt;=0.23.0 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from groq&lt;1,&gt;=0.4.1-&gt;langchain_groq) (0.23.3)\nRequirement already satisfied: sniffio in /Users/nipun/mambaforge/lib/python3.10/site-packages (from groq&lt;1,&gt;=0.4.1-&gt;langchain_groq) (1.3.0)\nCollecting typing-extensions&lt;5,&gt;=4.7 (from groq&lt;1,&gt;=0.4.1-&gt;langchain_groq)\n  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nCollecting jsonpatch&lt;2.0,&gt;=1.33 (from langchain-core&lt;0.3.0,&gt;=0.2.30-&gt;langchain)\n  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\nCollecting packaging&lt;25,&gt;=23.2 (from langchain-core&lt;0.3.0,&gt;=0.2.30-&gt;langchain)\n  Downloading packaging-24.1-py3-none-any.whl (53 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 1.1 MB/s eta 0:00:00ta 0:00:01\nCollecting orjson&lt;4.0.0,&gt;=3.9.14 (from langsmith&lt;0.2.0,&gt;=0.1.17-&gt;langchain)\n  Downloading orjson-3.10.7-cp310-cp310-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (251 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.3/251.3 kB 9.1 MB/s eta 0:00:00\nCollecting annotated-types&gt;=0.4.0 (from pydantic&lt;3,&gt;=1-&gt;langchain)\n  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nCollecting pydantic-core==2.20.1 (from pydantic&lt;3,&gt;=1-&gt;langchain)\n  Downloading pydantic_core-2.20.1-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 6.0 MB/s eta 0:00:00a 0:00:01\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from requests&lt;3,&gt;=2-&gt;langchain) (3.1.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from requests&lt;3,&gt;=2-&gt;langchain) (3.4)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from requests&lt;3,&gt;=2-&gt;langchain) (1.26.15)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from requests&lt;3,&gt;=2-&gt;langchain) (2023.5.7)\nRequirement already satisfied: httpcore&lt;0.17.0,&gt;=0.15.0 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from httpx&lt;1,&gt;=0.23.0-&gt;groq&lt;1,&gt;=0.4.1-&gt;langchain_groq) (0.16.3)\nRequirement already satisfied: rfc3986[idna2008]&lt;2,&gt;=1.3 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from httpx&lt;1,&gt;=0.23.0-&gt;groq&lt;1,&gt;=0.4.1-&gt;langchain_groq) (1.5.0)\nRequirement already satisfied: jsonpointer&gt;=1.9 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from jsonpatch&lt;2.0,&gt;=1.33-&gt;langchain-core&lt;0.3.0,&gt;=0.2.30-&gt;langchain) (2.0)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from httpcore&lt;0.17.0,&gt;=0.15.0-&gt;httpx&lt;1,&gt;=0.23.0-&gt;groq&lt;1,&gt;=0.4.1-&gt;langchain_groq) (0.14.0)\nInstalling collected packages: typing-extensions, packaging, orjson, jsonpatch, frozenlist, distro, async-timeout, annotated-types, aiohappyeyeballs, SQLAlchemy, pydantic-core, aiosignal, pydantic, aiohttp, langsmith, groq, langchain-core, langchain-text-splitters, langchain_groq, langchain\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.5.0\n    Uninstalling typing_extensions-4.5.0:\n      Successfully uninstalled typing_extensions-4.5.0\n  Attempting uninstall: packaging\n    Found existing installation: packaging 23.1\n    Uninstalling packaging-23.1:\n      Successfully uninstalled packaging-23.1\n  Attempting uninstall: jsonpatch\n    Found existing installation: jsonpatch 1.32\n    Uninstalling jsonpatch-1.32:\n      Successfully uninstalled jsonpatch-1.32\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyter 1.0.0 requires qtconsole, which is not installed.\nstreamlit 1.31.0 requires packaging&lt;24,&gt;=16.8, but you have packaging 24.1 which is incompatible.\nSuccessfully installed SQLAlchemy-2.0.32 aiohappyeyeballs-2.3.5 aiohttp-3.10.3 aiosignal-1.3.1 annotated-types-0.7.0 async-timeout-4.0.3 distro-1.9.0 frozenlist-1.4.1 groq-0.9.0 jsonpatch-1.33 langchain-0.2.13 langchain-core-0.2.30 langchain-text-splitters-0.2.2 langchain_groq-0.1.9 langsmith-0.1.99 orjson-3.10.7 packaging-24.1 pydantic-2.8.2 pydantic-core-2.20.1 typing-extensions-4.12.2\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport pandas as pd \nfrom langchain_groq.chat_models import ChatGroq\nimport json\n\n\nwith open(\"../secrets.json\") as f:\n    secrets = json.load(f)   \nGroq_Token = secrets[\"groq\"]\n\n\n# Groq API and Models \n\ngroq_models = {\"llama3-70b\": \"llama3-70b-8192\", \"mixtral\": \"mixtral-8x7b-32768\", \"gemma-7b\": \"gemma-7b-it\",\"llama3.1-70b\":\"llama-3.1-70b-versatile\",\"llama3-8b\":\"llama3-8b-8192\",\"llama3.1-8b\":\"llama-3.1-8b-instant\",\"gemma-9b\":\"gemma2-9b-it\"}\n\nNOTE : DO NOT SHARE THE API KEY WITH ANYONE. DO NOT COMMIT THE API KEY TO GITHUB.\nAlways do a sanity check before committing the code to github. If the key is found in the code, you will be penalized with a 0.5 marks deduction.\n\nZero Shot\n\n# Statement \nsentence = \"The product quality is amazing but the delivery was delayed. However I am happy with the customer service.\"\n\nsentence_2 = \"The product quality is amazing and the delivery was not delayed. I am happy with the customer service.\"\n# System Prompts \nquery = f\"\"\"\n* You are a sentiment analysis model. \n* Your task is to analyze the sentiment expressed in the given text and classify it as 'positive', 'negative', or 'neutral'. \n* Provide the sentiment label and, if necessary, a brief explanation of your reasoning.\n\nSentence: {sentence}\n\"\"\" \n\n\n# To use Groq LLMs \nmodel_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\nllm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0)\nanswer = llm.invoke(query)\n\nprint(answer.content)\n\nSentiment label: Neutral\n\nExplanation: The sentence expresses mixed sentiments. The words \"amazing\" and \"happy\" convey a positive sentiment, indicating satisfaction with the product quality and customer service. However, the phrase \"delivery was delayed\" expresses a negative sentiment, indicating dissatisfaction with the delivery experience. Overall, the positive and negative sentiments balance each other out, resulting in a neutral sentiment label.\n\n\n\n\nFew Shot\n\n# Statement \nsentence = \"The product quality is amazing but the delivery was delayed. However I am happy with the customer service.\"\n\n# System Prompts \nquery = f\"\"\"\n* You are a sentiment analysis model. \n* Your task is to analyze the sentiment expressed in the given text and classify it as 'positive', 'negative', or 'neutral'. \n* Provide the sentiment label and, if necessary, a brief explanation of your reasoning.\n\nHere are few examples:\n1. Sentence: 'The customer service was excellent, and I received my order quickly.'\nSentiment: Positive\n\n2. Sentence: 'The food was bland and the service was slow.'\nSentiment: Negative\n\n3. Sentence: 'The product is okay, but it's not worth the price.'\nSentiment: Neutral\n\nSentence: {sentence}\n\"\"\" \n\n# To use Groq LLMs \nmodel_name = \"llama3-70b\" # We can choose any model from the groq_models dictionary\nllm = ChatGroq(model=groq_models[model_name], api_key=Groq_Token, temperature=0)\nanswer = llm.invoke(query)\n\nprint(answer.content)\n\nSentiment: Positive\n\nExplanation: Although the sentence mentions a negative aspect (\"the delivery was delayed\"), the positive sentiments (\"The product quality is amazing\" and \"I am happy with the customer service\") outweigh the negative one, resulting in an overall positive sentiment. The use of the word \"amazing\" and \"happy\" also indicates a strong positive emotion, which contributes to the positive sentiment classification."
  },
  {
    "objectID": "notebooks/SFS_and_BFS.html",
    "href": "notebooks/SFS_and_BFS.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score as acc\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom sklearn.datasets import fetch_california_housing\n\n\n\nfrom mlxtend.feature_selection import Sequential\n\n\n\n# Read data\ndata =  fetch_california_housing()\nX = data['data']\ny = data['target']\n\n\n# Train/test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.25,\n    random_state=42)\n\ny_train = y_train.ravel()\ny_test = y_test.ravel()\n\nprint('Training dataset shape:', X_train.shape, y_train.shape)\nprint('Testing dataset shape:', X_test.shape, y_test.shape)\n\nTraining dataset shape: (15480, 8) (15480,)\nTesting dataset shape: (5160, 8) (5160,)\n\n\n\nclf = DecisionTreeRegressor()\n# clf = DecisionTreeClassifier()\n\n# Build step forward feature selection\nsfs1 = sfs(clf,\n           k_features=6,\n           forward=True,\n           floating=False,\n           verbose=2,\n          #  scoring='accuracy',\n           scoring='neg_root_mean_squared_error',\n           cv=5)\n\n# Perform SFFS\nsfs1 = sfs1.fit(X_train, y_train)\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    1.1s finished\n\n[2020-01-18 12:14:41] Features: 1/6 -- score: -0.9799855743872914[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    1.4s finished\n\n[2020-01-18 12:14:42] Features: 2/6 -- score: -0.6329184295861372[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    1.6s finished\n\n[2020-01-18 12:14:44] Features: 3/6 -- score: -0.6522227062026185[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.9s finished\n\n[2020-01-18 12:14:45] Features: 4/6 -- score: -0.6627208539646012[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.9s finished\n\n[2020-01-18 12:14:47] Features: 5/6 -- score: -0.6800772168838566[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.7s finished\n\n[2020-01-18 12:14:49] Features: 6/6 -- score: -0.6988981779449276\n\n\n\nfeat_cols = list(sfs1.k_feature_idx_)\n\n# data['features']\n\n\nnp.array(data['feature_names'])[feat_cols]\n\narray(['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'Latitude',\n       'Longitude'], dtype='&lt;U10')\n\n\n\ndata['feature_names']\n\n['MedInc',\n 'HouseAge',\n 'AveRooms',\n 'AveBedrms',\n 'Population',\n 'AveOccup',\n 'Latitude',\n 'Longitude']\n\n\n\nclf = DecisionTreeRegressor()\n# clf = DecisionTreeClassifier()\n\n# Build step forward feature selection\nsbs = sfs(clf,\n           k_features=1,\n           forward=False,\n           floating=False,\n           verbose=2,\n          #  scoring='accuracy',\n           scoring='neg_root_mean_squared_error',\n           cv=5)\n\n# Perform SFFS\nsfs1 = sbs.fit(X_train, y_train)\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.7s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    5.2s finished\n\n[2020-01-18 12:26:50] Features: 7/1 -- score: -0.7097202737310716[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.6s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    3.9s finished\n\n[2020-01-18 12:26:54] Features: 6/1 -- score: -0.6985180206966245[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    2.8s finished\n\n[2020-01-18 12:26:57] Features: 5/1 -- score: -0.6766309576585353[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.8s finished\n\n[2020-01-18 12:26:58] Features: 4/1 -- score: -0.6683220592138266[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.2s finished\n\n[2020-01-18 12:27:00] Features: 3/1 -- score: -0.6613854217987167[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.6s finished\n\n[2020-01-18 12:27:00] Features: 2/1 -- score: -0.6320510743499647[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s finished\n\n[2020-01-18 12:27:00] Features: 1/1 -- score: -0.9799855743872914"
  },
  {
    "objectID": "notebooks/contour.html",
    "href": "notebooks/contour.html",
    "title": "Contour and Surface Plots",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify(columns=2, fig_height=4.5)\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nimport torch\n\ndef create_XYZ(f):\n    # Generate data\n    x = np.linspace(-5, 5, 100)\n    y = np.linspace(-5, 5, 100)\n    X, Y = np.meshgrid(x, y)\n    \n    # Convert to PyTorch tensors\n    X_torch = torch.from_numpy(X)\n    Y_torch = torch.from_numpy(Y)\n\n    # Evaluate the function\n    Z = f(X_torch, Y_torch)\n    return X, Y, Z, X_torch, Y_torch\n\ndef create_contour(X, Y, Z, ax2, alpha, scatter_pts=None, filled=True, levels=10, mark_levels=False):\n    if filled:\n        scatter_color='white'\n        contour = ax2.contourf(X, Y, Z.detach().numpy(), levels=levels, cmap='magma', alpha=alpha)\n    else:\n        scatter_color='black'\n        contour = ax2.contour(X, Y, Z.detach().numpy(), levels=levels, cmap='magma', alpha=alpha)\n    if scatter_pts is not None:\n        ax2.scatter(scatter_pts[0], scatter_pts[1], s=10, c=scatter_color)\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('y')\n    ax2.set_title('Contour Plot')\n    \n\n    # Add a colorbar in between the subplots\n    divider = make_axes_locatable(ax2)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n    cbar = plt.colorbar(contour, cax=cax)\n    return ax2, contour\n\ndef plot_surface_and_contour(f, function_name, uv = None, stride=4, alpha=1, scatter_pts=None, filled=True, levels=10):\n    X, Y, Z, X_torch, Y_torch = create_XYZ(f)\n\n    # Create the single figure with two subplots\n    fig = plt.figure()\n\n    # Plot the 3D surface on the first subplot\n    ax1 = fig.add_subplot(121, projection='3d')\n    ax1.plot_surface(X, Y, Z.detach().numpy(), cmap='magma', edgecolor='none', alpha=alpha)  # Remove grid lines\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y')\n    ax1.set_zlabel('z')\n    ax1.grid(False)\n    ax1.xaxis.pane.fill = False\n    ax1.yaxis.pane.fill = False\n    ax1.zaxis.pane.fill = False\n    ax1.view_init(elev=30, azim=30)\n    ax1.set_title('Surface Plot')\n    if scatter_pts is not None:\n        ax1.scatter(scatter_pts[0], scatter_pts[1], f(scatter_pts[0], scatter_pts[1]), s=100, c='black')\n    \n\n    # Plot the contour plot on the second subplot\n    ax2 = fig.add_subplot(122, aspect='equal')  # Set 1:1 aspect ratio\n    \n    ax2, contour = create_contour(X, Y, Z, ax2, alpha, scatter_pts, filled, levels)\n    file_name = f\"../figures/mml/contour-{function_name}.pdf\"\n    if uv is not None:\n        u = uv[0](X_torch, Y_torch)\n        v = uv[1](X_torch, Y_torch)\n        # Quiver plot for gradient\n        ax2.quiver(X[::stride, ::stride], Y[::stride, ::stride], u[::stride, ::stride].detach().numpy(),\n                   v[::stride, ::stride].detach().numpy(), scale=140)\n        # for c in contour, set alpha\n        for c in contour.collections:\n            c.set_alpha(0.5)\n        \n        \n        file_name = f\"../figures/mml/contour-{function_name}-with-gradient.pdf\"\n        \n\n    \n    # Save the figure\n    plt.tight_layout(pad=1.0, w_pad=1.0)\n    fig.savefig(file_name, bbox_inches=\"tight\")\n\n# Example usage:\n# Define your function f(x, y) and its gradient g(x, y)\n#f = lambda x, y: x**2 + y**2\n#g = lambda x, y: (2*x, 2*y)\n#plot_surface_and_contour(f, \"x_squared_plus_y_squared\", uv=(lambda x, y: 2*x, lambda x, y: 2*y))\n\n\nplot_surface_and_contour(lambda x, y: x**2 + y**2, \"x_squared_plus_y_squared_quiver\", \n                         uv=(lambda x, y: 2*x, lambda x, y: 2*y)\n                         ,stride=5)\n\n\n\n\n\n\n\n\n\nplot_surface_and_contour(lambda x, y: x**2 + y**2, \"x_squared_plus_y_squared\")\n\n\n\n\n\n\n\n\n\nplot_surface_and_contour(lambda x, y: x**2, \"x_squared\")\n\n\n\n\n\n\n\n\n\nplot_surface_and_contour(lambda x, y: torch.abs(x) + torch.abs(y), \"mod_x_plus_mod_y\")\n\n\n\n\n\n\n\n\n\nplot_surface_and_contour(lambda x, y: (x**2) * y, \"x_square_times_y\")\n\n\n\n\n\n\n\n\n\nplot_surface_and_contour(lambda x, y: x * y, \"x_times_y\")\n\n\n\n\n\n\n\n\n\ndef f(x, y):\n    return (14 + 3*x**2 +14*y**2 -12*x - 28*y + 12*x*y)/3\n\nx0, y0 = torch.tensor(4.0), torch.tensor(0.0)\n\n\n\ndel_x, del_y = torch.func.grad(f, argnums=(0, 1))(x0, y0)\nprint(del_x, del_y)\n\ntensor(4.) tensor(6.6667)\n\n\n\nlatexify(columns=2, fig_width=6.5, fig_height=3)\nX, Y, Z, X_torch, Y_torch = create_XYZ(f)\nlevels = [0, 10, 25, 50, 100, 200, 300, 400]\n\nxi = x0\nyi = y0\nalpha = 0.1\nfor i in range(50):\n    fig, ax = plt.subplots(ncols=2)\n    _, _ = create_contour(X, Y, Z, ax[0], alpha=0.8, scatter_pts=(xi, yi), filled=True, levels=levels)\n    # Mark the minima with horizontal and vertical lines\n    ax[0].axhline(y=yi, color='red', linestyle='--', alpha=0.7)\n    ax[0].axvline(x=xi, color='red', linestyle='--', alpha=0.7)\n    ax[0].scatter([0], [1], s=10, c='pink', marker='x', label='Minima')\n    del_x, del_y = torch.func.grad(f, argnums=(0, 1))(xi, yi)\n    xi = xi - alpha * del_x\n    yi = yi - alpha * del_y\n    print(xi, yi)\n    \n    # plot the line fit \n    x_line = np.linspace(0, 5, 100)\n    y_line = xi + yi*x_line\n    ax[1].plot(x_line, y_line, label='Line fit')\n    ax[1].set_ylim(-1, 7)\n    ax[1].plot(x_line, x_line, label='Actual line')\n    ax[1].legend()\n    \n    plt.tight_layout()\n    plt.savefig(f\"../figures/mml/gradient-descent-{i}.pdf\", bbox_inches=\"tight\")\n\n\n\n\ntensor(3.6000) tensor(-0.6667)\ntensor(3.5467) tensor(-0.5511)\ntensor(3.4578) tensor(-0.5221)\ntensor(3.3751) tensor(-0.4846)\ntensor(3.2939) tensor(-0.4490)\ntensor(3.2147) tensor(-0.4141)\ntensor(3.1374) tensor(-0.3802)\ntensor(3.0620) tensor(-0.3470)\ntensor(2.9884) tensor(-0.3146)\ntensor(2.9165) tensor(-0.2830)\ntensor(2.8464) tensor(-0.2522)\ntensor(2.7780) tensor(-0.2221)\ntensor(2.7112) tensor(-0.1927)\ntensor(2.6461) tensor(-0.1640)\ntensor(2.5824) tensor(-0.1360)\ntensor(2.5204) tensor(-0.1087)\ntensor(2.4598) tensor(-0.0821)\ntensor(2.4006) tensor(-0.0560)\ntensor(2.3429) tensor(-0.0307)\ntensor(2.2866) tensor(-0.0059)\n\n\n/tmp/ipykernel_4080399/2912499836.py:9: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n  fig, ax = plt.subplots(ncols=2)\n\n\ntensor(2.2316) tensor(0.0183)\ntensor(2.1780) tensor(0.0419)\ntensor(2.1256) tensor(0.0649)\ntensor(2.0745) tensor(0.0874)\ntensor(2.0247) tensor(0.1093)\ntensor(1.9760) tensor(0.1308)\ntensor(1.9285) tensor(0.1517)\ntensor(1.8821) tensor(0.1720)\ntensor(1.8369) tensor(0.1919)\ntensor(1.7927) tensor(0.2114)\ntensor(1.7496) tensor(0.2303)\ntensor(1.7076) tensor(0.2488)\ntensor(1.6665) tensor(0.2669)\ntensor(1.6265) tensor(0.2845)\ntensor(1.5874) tensor(0.3017)\ntensor(1.5492) tensor(0.3185)\ntensor(1.5120) tensor(0.3349)\ntensor(1.4756) tensor(0.3509)\ntensor(1.4401) tensor(0.3665)\ntensor(1.4055) tensor(0.3817)\ntensor(1.3717) tensor(0.3966)\ntensor(1.3388) tensor(0.4111)\ntensor(1.3066) tensor(0.4252)\ntensor(1.2752) tensor(0.4391)\ntensor(1.2445) tensor(0.4525)\ntensor(1.2146) tensor(0.4657)\ntensor(1.1854) tensor(0.4785)\ntensor(1.1569) tensor(0.4911)\ntensor(1.1291) tensor(0.5033)\ntensor(1.1020) tensor(0.5153)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlevels = 10\ndef f1(x, y):\n    return (2-x-2*y)**2\n\ndef f2(x, y):\n    return (3-x-3*y)**2\n\ndef f3(x, y):\n    return (1-x-y)**2\n\nx0, y0 = torch.tensor(4.0), torch.tensor(0.0)\n\n\nfig, ax = plt.subplots(ncols=3, figsize=(8, 3))\nX, Y, Z, X_torch, Y_torch = create_XYZ(f1)\n_, _ = create_contour(X, Y, Z, ax[0], alpha=0.8, scatter_pts=(x0, y0), filled=True, levels=levels)\n\nX, Y, Z, X_torch, Y_torch = create_XYZ(f2)\n_, _ = create_contour(X, Y, Z, ax[1], alpha=0.8, scatter_pts=(x0, y0), filled=True, levels=levels)\n\nX, Y, Z, X_torch, Y_torch = create_XYZ(f3)\n_, _ = create_contour(X, Y, Z, ax[2], alpha=0.8, scatter_pts=(x0, y0), filled=True, levels=levels)\n\nfig.tight_layout()\nplt.savefig(f\"../figures/mml/gradient-descent-3-functions.pdf\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nX, Y, Z, X_torch, Y_torch = create_XYZ(f1)\nalpha = 0.1\n\n\ndel_x, del_y = torch.func.grad(f1, argnums=(0, 1))(x0, y0)\nx1 = x0 - alpha * del_x\ny1 = y0 - alpha * del_y\n\nprint(x1, y1)\n\ntensor(3.6000) tensor(-0.8000)\n\n\n\ndel_x, del_y = torch.func.grad(f2, argnums=(0, 1))(x1, y1)\nx2 = x1 - alpha * del_x\ny2 = y1 - alpha * del_y\n\nprint(x2, y2)\n\ntensor(3.9600) tensor(0.2800)\n\n\n\ndel_x, del_y = torch.func.grad(f3, argnums=(0, 1))(x2, y2)\nx3 = x2 - alpha * del_x\ny3 = y2 - alpha * del_y\n\nprint(x3, y3)\n\ntensor(3.3120) tensor(-0.3680)"
  },
  {
    "objectID": "notebooks/ensemble-feature-importance.html",
    "href": "notebooks/ensemble-feature-importance.html",
    "title": "Random Forest Feature Importance",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.ensemble import RandomForestClassifier\nfrom IPython.display import Image\n\n# To plot trees in forest via graphviz\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\ntry:\n    from latexify import latexify, format_axes\n    latexify(columns=2)\nexcept:\n    pass\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Load IRIS dataset from Seaborn\niris = sns.load_dataset('iris')\niris\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n# classes\niris.species.unique()\n\narray(['setosa', 'versicolor', 'virginica'], dtype=object)\n\n\n\nplt.hist(iris.sepal_width, bins=20)\n\n(array([ 1.,  3.,  4.,  3.,  8., 14., 14., 10., 26., 11., 19., 12.,  6.,\n         4.,  9.,  2.,  1.,  1.,  1.,  1.]),\n array([2.  , 2.12, 2.24, 2.36, 2.48, 2.6 , 2.72, 2.84, 2.96, 3.08, 3.2 ,\n        3.32, 3.44, 3.56, 3.68, 3.8 , 3.92, 4.04, 4.16, 4.28, 4.4 ]),\n &lt;BarContainer object of 20 artists&gt;)\n\n\n\n\n\n\n\n\n\n\nsns.kdeplot(data=iris, x=\"sepal_length\")\n\n\n\n\n\n\n\n\n\nsns.displot(iris.sepal_length.values, kind='kde')\n\n\n\n\n\n\n\n\n\nsns.displot(data=iris, x=\"sepal_length\", kind='kde')\n\n\n\n\n\n\n\n\n\niris.groupby(\"species\")[\"petal_length\"].mean()\n\nspecies\nsetosa        1.462\nversicolor    4.260\nvirginica     5.552\nName: petal_length, dtype: float64\n\n\n\n# Pairplot\nsns.pairplot(iris, hue=\"species\")\n\n\n\n\n\n\n\n\n\n# Divide dataset into X and y\nX, y = iris.iloc[:, :-1], iris.iloc[:, -1]\nrf = RandomForestClassifier(n_estimators=10,random_state=0, criterion='entropy', bootstrap=True)\nrf.fit(X, y)\n\nRandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0) \n\n\n\n# Visualize ``each tree in the Random Forest\nfor i, tree in enumerate(rf.estimators_):\n    # Create DOT data for the i-th tree\n    dot_data = export_graphviz(tree, out_file=None, \n                               feature_names=iris.columns[:-1],  \n                               class_names=iris.species.unique(),\n                               filled=True, rounded=True,\n                               special_characters=True,\n                               impurity=True,\n                               node_ids=True)\n    \n    # Use Graphviz to render the DOT data into a graph\n    graph = graphviz.Source(dot_data)\n    \n    # Save or display the graph (change the format as needed)\n    graph.render(filename=f'../figures/ensemble/feature-imp-{i}', format='pdf', cleanup=True)\n    graph.render(filename=f'../figures/ensemble/feature-imp-{i}', format='png', cleanup=True)\n\n\n# Visualize the tree\nImage(filename='../figures/ensemble/feature-imp-0.png')\n\n\n\n\n\n\n\n\n\nrf.feature_importances_\n\narray([0.09864748, 0.03396026, 0.32312193, 0.54427033])\n\n\n\n\\(t\\) = node\n\\(N_t\\) = number of observations at node \\(t\\)\n\\(N_{t_L}\\) = number of observations in the left child node of node \\(t\\)\n\\(N_{t_R}\\) = number of observations in the right child node of node \\(t\\)\n\\(p(t)=N_t/N\\) = proportion of observations in node \\(t\\)\n\\(X_j\\) = feature \\(j\\)\n\\(j_t\\) = feature used at node \\(t\\) for splitting\n\\(i(t)\\) = impurity at node \\(t\\) (impurity = entropy in this case)\n\\(M\\) = number of trees in the forest\n\n\nFor a particular node:\n\nInformation gain at node \\(t\\) = Impurity reduction at node \\(t\\) = entropy(parent) - weighted entropy(children)\n\n\\(\\Delta i(t) = i(t) - \\frac{N_{t_L}}{N_t} i(t_L) - \\frac{N_{t_r}}{N_t} i(t_R)\\)\n\n\nFor a tree:\nImportance of feature \\(X_j\\) is given by:\n\\(\\text{Imp}(X_j) = \\sum_{t \\in \\varphi_{m}} 1(j_t = j) \\Big[ p(t) \\Delta i(t) \\Big]\\)\n\n\nFor a forest:\nImportance of feature \\(X_j\\) for an ensemble of \\(M\\) trees \\(\\varphi_{m}\\) is:\n\\[\\begin{equation*}\n  \\text{Imp}(X_j) = \\frac{1}{M} \\sum_{m=1}^M \\sum_{t \\in \\varphi_{m}} 1(j_t = j) \\Big[ p(t) \\Delta i(t) \\Big]\n\\end{equation*}\\]\n\n1-1/np.e\n\n0.6321205588285577\n\n\n\nN = 150\n(1-1/np.e)*N\n\n94.81808382428365\n\n\n\nrf.feature_importances_\n\narray([0.09864748, 0.03396026, 0.32312193, 0.54427033])\n\n\n\nlist_of_trees_in_rf = rf.estimators_\n\n\ntree_0 = list_of_trees_in_rf[0]\n\n\ntree_0.feature_importances_\n\narray([0.00397339, 0.01375245, 0.35802357, 0.6242506 ])\n\n\n\ns = []\nfor tree in rf.estimators_:\n    s.append(tree.feature_importances_)\n\n\nnp.array(s).mean(axis=0)\n\narray([0.09864748, 0.03396026, 0.32312193, 0.54427033])\n\n\n\nrf.feature_importances_\n\narray([0.09864748, 0.03396026, 0.32312193, 0.54427033])\n\n\n\ntree_0 = rf.estimators_[0]\ntree_0.feature_importances_\n\narray([0.00397339, 0.01375245, 0.35802357, 0.6242506 ])\n\n\n\n# take one tree \ntree = rf.estimators_[0].tree_\n\n\ntree.feature\n\narray([ 3, -2,  2,  3, -2,  1, -2, -2,  0, -2,  2, -2, -2], dtype=int64)\n\n\n\n\n\n# Creating a mapping of feature names to the feature indices\nmapping = {-2: 'Leaf', 0: 'sepal_length', 1: 'sepal_width', 2: 'petal_length', 3: 'petal_width'}\n\n# print the node number along with the corresponding feature name\nfor node in range(tree.node_count):\n    print(f'Node {node}: {mapping[tree.feature[node]]}')\n\nNode 0: petal_width\nNode 1: Leaf\nNode 2: petal_length\nNode 3: petal_width\nNode 4: Leaf\nNode 5: sepal_width\nNode 6: Leaf\nNode 7: Leaf\nNode 8: sepal_length\nNode 9: Leaf\nNode 10: petal_length\nNode 11: Leaf\nNode 12: Leaf\n\n\n\nid = 2\ntree.children_left[id], tree.children_right[id]\n\n(3, 8)\n\n\n\ndef print_child_id(tree, node):\n    '''\n    Prints the child node ids of a given node.\n    tree: tree object\n    node: int\n    '''\n\n    # check if leaf\n    l, r = tree.children_left[node], tree.children_right[node]\n    if l == -1 and r == -1:\n        return None, None\n    return tree.children_left[node], tree.children_right[node]\n\nprint_child_id(tree, 0)\n\n(1, 2)\n\n\n\ntree.impurity\n\narray([1.57310798, 0.        , 0.98464683, 0.34781691, 0.        ,\n       0.81127812, 0.        , 0.        , 0.12741851, 0.        ,\n       0.2108423 , 0.        , 0.        ])\n\n\n\ndef all_data(tree, node):\n    '''\n    Returns all the data required to calculate the information gain.\n    '''\n\n    # get the child nodes\n    left, right = print_child_id(tree, node)\n\n    # check if leaf, then return None\n    if left is None:\n        return None\n    \n    # get the data\n    entropy_node = tree.impurity[node]\n    entropy_left = tree.impurity[left]\n    entropy_right = tree.impurity[right]\n\n    # N = total number of samples considered during bagging, therefore, it is equal to the number of samples at the root node\n    N = tree.n_node_samples[0]\n\n    # n_l = number of samples at the left child node\n    n_l = tree.n_node_samples[left]\n\n    # n_r = number of samples at the right child node\n    n_r = tree.n_node_samples[right]\n\n    # n_t = total number of samples at the node\n    n_t = n_l + n_r\n\n    feature = mapping[tree.feature[node]]\n    \n    # calculate the information gain\n    info_gain_t = entropy_node - (n_l/n_t * entropy_left + n_r/n_t * entropy_right)\n    \n    return info_gain_t, N, n_l, n_r, n_t, feature\n\n\n# Calculate the importance of each features using the information gain for a tree\n\nscores = {}\nfor node in range(tree.node_count):\n    # Add the information gain of the node to the dictionary if it is not a leaf node\n    try:\n        ig, N, n_l, n_r, n_t, feature = all_data(tree, node)\n        p_t = n_t / N\n        scores[feature] = scores.get(feature, 0) + p_t * ig\n\n    # Skip if it is a leaf node\n    except:\n        continue\n\nser = pd.Series(scores) \ninfo_gain_tree = ser/ser.sum()\ninfo_gain_tree.sort_values(ascending=False)\n\npetal_width     0.639307\npetal_length    0.340335\nsepal_width     0.016459\nsepal_length    0.003899\ndtype: float64\n\n\n\n# Feature importance using sklearn for a tree\nsklearn_imp = tree.compute_feature_importances()\npd.Series(sklearn_imp, index=iris.columns[:-1]).sort_values(ascending=False)\n\npetal_width     0.624251\npetal_length    0.358024\nsepal_width     0.013752\nsepal_length    0.003973\ndtype: float64\n\n\n\n# Feature importance using sklearn for the forest\nsklearn_imp_forest = np.array([x.tree_.compute_feature_importances() for x in rf.estimators_]).mean(axis=0)\npd.Series(sklearn_imp_forest, index=iris.columns[:-1]).sort_values(ascending=False)\n\npetal_width     0.544270\npetal_length    0.323122\nsepal_length    0.098647\nsepal_width     0.033960\ndtype: float64\n\n\n\nser = pd.Series(sklearn_imp_forest, index=iris.columns[:-1])\nser.plot(kind='bar', rot=0)\nformat_axes(plt.gca())\nplt.savefig('../figures/ensemble/feature-imp-forest.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n\nAside:\ntree.tree_.feature returns the feature used at each node to divide the node into two child nodes with the below given mapping. The sequence of the features is the same as the column sequence of the input data.\n\n-2: leaf node\n0: sepal_length\n1: sepal_width\n2: petal_length\n3: petal_width\n\ntree.tree_.children_left[node] returns the node number of the left child of the node\ntree.tree_.children_right[node] returns the node number of the right child of the node\nif there is no left or right child, it returns -1\n\n\nBootstrap code:\nin the random_forest.fit() function\n\n\n\nbootstrap_code"
  },
  {
    "objectID": "notebooks/hyperparameter-optimisation.html",
    "href": "notebooks/hyperparameter-optimisation.html",
    "title": "Hyperparameter Tuning",
    "section": "",
    "text": "import numpy as np \nimport matplotlib.pyplot as plt\nimport pandas as pd\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nfrom latexify import latexify, format_axes\n\nMakeMoons Dataset\n\n# Import necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_moons\n\n# Generate the dataset\nX, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\n\n# Split the data into training, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n\nlatexify(fig_width=5, fig_height=4)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, label='Train') \nformat_axes(plt.gca())\nplt.show()\n\n\n\n\n\n\n\n\n\n#Define the hyperparameters' possible values\nmax_depth_values = [1,2,3,4,5,6,7,8,9,10]\nmin_samples_split_values = [2,3,4,5,6,7,8]\ncriteria_values = ['gini', 'entropy']\n\nNested For Loops\n\nbest_accuracy = 0\nbest_params = {}\n\nfor max_depth in max_depth_values:\n    for min_samples_split in min_samples_split_values:\n        for criterion in criteria_values:\n            # Define the Decision Tree Classifier\n            dt_classifier = DecisionTreeClassifier(\n                max_depth=max_depth,\n                min_samples_split=min_samples_split,\n                criterion=criterion,\n                random_state=42\n            )\n            dt_classifier.fit(X_train, y_train)\n            \n            # Evaluate on the validation set\n            val_accuracy = dt_classifier.score(X_val, y_val)\n            \n            # Check if this combination gives a better accuracy\n            if val_accuracy &gt; best_accuracy:\n                best_accuracy = val_accuracy\n                best_params = {\n                    'max_depth': max_depth,\n                    'min_samples_split': min_samples_split,\n                    'criterion': criterion\n                }\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best Validation Accuracy:\", best_accuracy)\n\n# Train the model with the best hyperparameters\nbest_dt_classifier = DecisionTreeClassifier(**best_params)\nbest_dt_classifier.fit(X_train, y_train)\n\n# Evaluate on the test set\ntest_accuracy = best_dt_classifier.score(X_test, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\nBest Hyperparameters: {'max_depth': 7, 'min_samples_split': 2, 'criterion': 'entropy'}\nBest Validation Accuracy: 0.925\nTest Accuracy: 0.8950\n\n\nUsing Itertools\n\nfrom itertools import product\n\nbest_accuracy = 0\nbest_params = {}\n\n# Use itertools.product for a more succinct code\nfor max_depth, min_samples_split, criterion in product(max_depth_values, min_samples_split_values, criteria_values):\n    # Define the Decision Tree Classifier\n    dt_classifier = DecisionTreeClassifier(\n        max_depth=max_depth,\n        min_samples_split=min_samples_split,\n        criterion=criterion,\n        random_state=42\n    )\n    dt_classifier.fit(X_train, y_train)\n    \n    # Evaluate on the validation set\n    val_accuracy = dt_classifier.score(X_val, y_val)\n    \n    # Check if this combination gives a better accuracy\n    if val_accuracy &gt; best_accuracy:\n        best_accuracy = val_accuracy\n        best_params = {\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'criterion': criterion\n        }\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best Validation Accuracy:\", best_accuracy)\n\n# Train the model with the best hyperparameters\nbest_dt_classifier = DecisionTreeClassifier(**best_params)\nbest_dt_classifier.fit(X_train, y_train)\n\n# Evaluate on the test set\ntest_accuracy = best_dt_classifier.score(X_test, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\nBest Hyperparameters: {'max_depth': 7, 'min_samples_split': 2, 'criterion': 'entropy'}\nBest Validation Accuracy: 0.925\nTest Accuracy: 0.8950\n\n\nUsing Sklearn Grid Search (5 fold Cross-Validation)\n\nfrom sklearn.model_selection import GridSearchCV\n# Define the Decision Tree Classifier\ndt_classifier = DecisionTreeClassifier(random_state=42)\n\n# Define the hyperparameters to tune\nparam_grid = {\n    'max_depth': max_depth_values,\n    'min_samples_split': min_samples_split_values,\n    'criterion': criteria_values\n}\n\nX_train_val = np.concatenate([X_train, X_val], axis=0)\ny_train_val = np.concatenate([y_train, y_val], axis=0)\n\n# Use GridSearchCV for hyperparameter tuning\nnum_inner_folds = 5\ngrid_search = GridSearchCV(dt_classifier, param_grid, scoring='accuracy', cv=num_inner_folds)\ngrid_search.fit(X_train_val, y_train_val)\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\", grid_search.best_params_)\n\n# Evaluate on the test set\ntest_accuracy = grid_search.best_estimator_.score(X_test, y_test)\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\nBest Hyperparameters: {'criterion': 'entropy', 'max_depth': 8, 'min_samples_split': 8}\nTest Accuracy: 0.9000"
  },
  {
    "objectID": "notebooks/geometric-linear-regression.html",
    "href": "notebooks/geometric-linear-regression.html",
    "title": "Geometric interpretation of Linear Regression",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport numpy as np\n\n\nfrom latexify import latexify, format_axes\nlatexify(columns=2)\n\n\n\n\n# Define points\nA = (1, 3)\nB = (2, 1)\nOrigin = (0, 0)\n\n# Plot vectors\nplt.quiver(*Origin, *A, angles='xy', scale_units='xy', scale=1, color='b', label='$v_1$')\nplt.quiver(*Origin, *B, angles='xy', scale_units='xy', scale=1, color='r', label='$v_2$')\n\n# Set axis limits\nplt.xlim(-2, 4)\nplt.ylim(-2, 4)\n\n# Add legend\nplt.legend()\n\n# Show plot\nplt.grid(alpha=0.1)\n\nax = plt.gca()\nformat_axes(ax)\nax.set_aspect('equal')\nplt.savefig('../figures/linear-regression/geoemetric-span-1.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n# Now, create v3 = v1 + v2 and v4 = v1 - v2 and plot\nC = (A[0] + B[0], A[1] + B[1])\nD = (A[0] - B[0], A[1] - B[1])\n\n# Set axis limits\nplt.xlim(-2, 4)\nplt.ylim(-2, 4)\n\nplt.quiver(*Origin, *A, angles='xy', scale_units='xy', scale=1, color='b', label='$v_1$')\nplt.quiver(*Origin, *B, angles='xy', scale_units='xy', scale=1, color='r', label='$v_2$')\nplt.quiver(*Origin, *C, angles='xy', scale_units='xy', scale=1, color='g', label='$v_3$')\nplt.quiver(*Origin, *D, angles='xy', scale_units='xy', scale=1, color='y', label='$v_4$')\n\nax = plt.gca()\nformat_axes(ax)\nplt.legend()\n\nax.set_aspect('equal')\nplt.savefig('../figures/linear-regression/geoemetric-span-2.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nA_arr = np.array(A)\nB_arr = np.array(B)\n\nAB_matrix = np.zeros((2, 2))\n# First column is A, second column is B\nAB_matrix[:, 0] = A_arr\nAB_matrix[:, 1] = B_arr\n\nprint(AB_matrix)\n\n[[1. 2.]\n [3. 1.]]\n\n\n\ndef new_vector(AB_matrix, alpha):\n    return AB_matrix @ alpha\n\nprint(new_vector(AB_matrix, np.array([1, -1])))\n\n[-1.  2.]\n\n\n\n# Generate a bunch of alphas\nalphas = np.random.uniform(-3, 3, size=(30000, 2))\nnew_vecs = []\nfor i, alpha in enumerate(alphas):\n    new_vecs.append(new_vector(AB_matrix, alpha))\n\nnew_vecs = np.array(new_vecs)\n\n\nt = new_vecs\nplt.scatter(t[:, 0], t[:, 1], alpha=0.2)\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nax = plt.gca()\nformat_axes(ax)\nax.set_aspect('equal')\nplt.savefig('../figures/linear-regression/geoemetric-span-3.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nA = np.array([1, 2])\nB = np.array([2, 4])\n\nAB_matrix = np.zeros((2, 2))\n# First column is A, second column is B\nAB_matrix[:, 0] = A\nAB_matrix[:, 1] = B\n\nprint(AB_matrix)\n\n[[1. 2.]\n [2. 4.]]\n\n\n\n# Generate a bunch of alphas\nalphas = np.random.uniform(-3, 3, size=(10000, 2))\nnew_vecs = []\nfor i, alpha in enumerate(alphas):\n    new_vecs.append(new_vector(AB_matrix, alpha))\n\nnew_vecs = np.array(new_vecs)\n\nplt.scatter(new_vecs[:, 0], new_vecs[:, 1], alpha=0.2, s=2)\nax = plt.gca()\nplt.xlim(-3, 3)\nplt.ylim(-3, 3)\nformat_axes(ax)\nax.set_aspect('equal')\nplt.savefig('../figures/linear-regression/geoemetric-span-4.pdf', bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Create a figure and 3D axis\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Define the surface plot\nX = np.linspace(0, 2.2, 100)\nY = np.linspace(-2.2, 1, 100)\nX, Y = np.meshgrid(X, Y)\nZ = X\n\n# Plot the surface\nsurf = ax.plot_surface(X, Y, Z,  alpha=0.3, rstride=100, cstride=100)\n\n# Define points\nA = np.array([1, 1, 1])\nD = np.array([0, 0, 0])\nB = np.array([2, -2, 2])\n\n# Mark the origin\nax.scatter(*D, color='black', label='Origin')\n\n# Plot vectors with labels including the vector\nax.quiver(D[0], D[1], D[2], A[0], A[1], A[2], color='b', label=f'$X_1 = {A.tolist()}$', arrow_length_ratio=0.1)\nax.quiver(D[0], D[1], D[2], B[0], B[1], B[2], color='r', label=f'$X_2 = {B.tolist()}$', arrow_length_ratio=0.1)\n\n# Set axis labels\nax.set_xlabel('$x$', fontsize=12)\nax.set_ylabel('$y$', fontsize=12)\nax.set_zlabel('$z$', fontsize=12)\n\n# Set legend\nax.legend()\n\n# Adjust view angle\nax.view_init(elev=15, azim=-35)\n\n# Customize grid lines\nax.grid(linestyle='dashed', color='white', alpha=0.2)  # Adjust color here\n\n\nplt.savefig(\"../figures/linear-regression/geometric-1.pdf\", bbox_inches='tight')\n\n\n\n\n\n\n\n\n\n\n\n# Create a figure and 3D axis\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Define the surface plot\nX = np.linspace(0, 10.2, 300)\nY = np.linspace(-5, 5, 300)\nX, Y = np.meshgrid(X, Y)\nZ = X\n\n# Plot the surface\nsurf = ax.plot_surface(X, Y, Z, alpha=0.3, rstride=100, cstride=100,)\n\n# Define points\nA = np.array([1, 1, 1])\nD = np.array([0, 0, 0])\nB = np.array([2, -2, 2])\ny_vec = np.array([8.8957, 0.6130, 1.7761])\n\n# Mark the origin\nax.scatter(*D, color='black', label='Origin')\n\n# Plot vectors with labels including the vector\nax.quiver(D[0], D[1], D[2], A[0], A[1], A[2], color='b', label=f'$X_1 = {A.tolist()}$', arrow_length_ratio=0.1)\nax.quiver(D[0], D[1], D[2], B[0], B[1], B[2], color='r', label=f'$X_2 = {B.tolist()}$', arrow_length_ratio=0.1)\nax.quiver(D[0], D[1], D[2], y_vec[0], y_vec[1], y_vec[2], color='g', label=f'$y = {y_vec.tolist()}$', arrow_length_ratio=0.1)\n\n# Set axis labels\nax.set_xlabel('$x$', fontsize=12)\nax.set_ylabel('$y$', fontsize=12)\nax.set_zlabel('$z$', fontsize=12)\n\n# Set legend\nax.legend()\n\n# Adjust view angle\nax.view_init(elev=15, azim=-35)\n\n# Customize grid lines\nax.grid(linestyle='dashed', color='white', alpha=0.2)  # Adjust color here\n\n\nplt.savefig(\"../figures/linear-regression/geometric-2.pdf\", bbox_inches=\"tight\")\n\n\nX_matrix = np.zeros((3, 2))\nX_matrix[:, 0] = A\nX_matrix[:, 1] = B\n\nprint(X_matrix)\n\ntheta_hat = np.linalg.inv(X_matrix.T @ X_matrix) @ X_matrix.T @ y_vec\n\nprint(theta_hat)\n\ny_hat = X_matrix @ theta_hat\nprint(y_hat)\n\n\n# Plot y_hat vector\nax.quiver(D[0], D[1], D[2], y_hat[0], y_hat[1], y_hat[2], color='y', label=f'$\\hat y = {list(map(lambda x: round(x, 4), y_hat))}$', arrow_length_ratio=0.1)\nplt.legend()\nplt.savefig(\"../figures/linear-regression/geometric-3.pdf\", bbox_inches=\"tight\")\n\n\n# perpendiculat vector\nperp_vec = y_vec - y_hat\n# Plot perp vector with y_hat as origin\nax.quiver(y_hat[0], y_hat[1], y_hat[2], perp_vec[0], perp_vec[1], perp_vec[2], color='m', label=f'$y - \\hat y = {list(map(lambda x: round(x, 4), perp_vec))}$', arrow_length_ratio=0.1)\nplt.legend()\nplt.savefig(\"../figures/linear-regression/geometric-4.pdf\", bbox_inches=\"tight\")\n\n[[ 1.  2.]\n [ 1. -2.]\n [ 1.  2.]]\n[2.97445  1.180725]\n[5.3359 0.613  5.3359]\n\n\n\n\n\n\n\n\n\n\nperp_vec\n\narray([ 3.5598,  0.    , -3.5598])\n\n\n\nX_matrix[:, 0]\n\narray([1., 1., 1.])\n\n\n\n\nperp_vec@X_matrix[:, 0]\n\n-1.3322676295501878e-15\n\n\n\nperp_vec@X_matrix[:, 1]\n\n-2.6645352591003757e-15\n\n\n\nX_matrix.T @ perp_vec\n\narray([-1.33226763e-15, -2.66453526e-15])"
  },
  {
    "objectID": "notebooks/1d-cnn.html",
    "href": "notebooks/1d-cnn.html",
    "title": "1d CNN",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, TensorDataset\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Set random seed for reproducibility\ntorch.manual_seed(0)\n\n# Torch version\ntorch.__version__\n\n'2.0.0+cu118'\n\n\n\n# Simple 1d dataset\n\ny = torch.Tensor([1, 1, 1, 0, -1, -1, -1, 0, 1, 1, 1, 1, 0, -1, -1, -1, 0, 1, 1, 1])\nx = torch.arange(0, len(y))\n\nplt.plot(x, y, 'o-')\n\n\n\n\n\n\n\n\n\n# Filter 1 (detect silence)\n\nwith torch.no_grad():\n    f1 = nn.Conv1d(1, 1, 3, padding=1)\n    f1.weight.data = torch.Tensor([[[-1, -1, -1]]])\n    f1.bias.data = torch.Tensor([0])\n    y1 = F.relu(f1(y.view(1, 1, -1))).view(-1)\n\nplt.plot(x, y, 'o-', label='original')\nplt.plot(x, y1, 'o-', label='filtered f1 (silence)')\nplt.legend()\n\n\n\n\n\n\n\n\n\n# Filter 2 (detect falling edge)\n\nwith torch.no_grad():\n    f2 = nn.Conv1d(1, 1, 3, padding=1)\n    f2.weight.data = torch.Tensor([[[1, 0, -1]]])\n    f2.bias.data = torch.Tensor([0])\n    y2 = F.relu(f2(y.view(1, 1, -1))).view(-1)\n\nplt.plot(x, y, 'o-', label='original')\nplt.plot(x, y2, 'o-', label='filtered f2 (falling edge)')\nplt.legend()\n\n\n\n\n\n\n\n\n\n# Filter 3 (detect rising edge)\n\nwith torch.no_grad():\n    f3 = nn.Conv1d(1, 1, 3, padding=1)\n    f3.weight.data = torch.Tensor([[[-1, 0, 1]]])\n    f3.bias.data = torch.Tensor([0])\n    y3 = F.relu(f3(y.view(1, 1, -1))).view(-1)\n\nplt.plot(x, y, 'o-', label='original')\nplt.plot(x, y3, 'o-', label='filtered f3 (rising edge)')\nplt.legend()\n\n\n\n\n\n\n\n\n\n# Filter 4 (detect high amplitude)\n\nwith torch.no_grad():\n    f4 = nn.Conv1d(1, 1, 3, padding=1)\n    f4.weight.data = torch.Tensor([[[1, 1, 1]]])\n    f4.bias.data = torch.Tensor([0])\n    y4 = F.relu(f4(y.view(1, 1, -1))).view(-1)\n\nplt.plot(x, y, 'o-', label='original') \nplt.plot(x, y4, 'o-', label='filtered f4 (high amplitude)')\nplt.legend()"
  },
  {
    "objectID": "notebooks/hyperparams-experiments.html",
    "href": "notebooks/hyperparams-experiments.html",
    "title": "Hyperparams Tuning Strategies Experimentation",
    "section": "",
    "text": "import numpy as np \nnp.random.seed(20)\nimport matplotlib.pyplot as plt\nimport pandas as pd\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nfrom latexify import latexify, format_axes\n\nMakeMoons Dataset\n1.1 Fixed Train-Test (70:30) split ; No Tuning\n\n# Import necessary libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import make_moons\n\n# Generate the dataset\nX, y = make_moons(n_samples=1000, noise=0.3, random_state=42)\n\n# Split the data into training, validation, and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n\n\nlatexify(fig_width=5, fig_height=4)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, label='Train') \nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker='x', label='Test')\nformat_axes(plt.gca())\nplt.legend()\n\n\n\n\n\n\n\n\n\nlen(X_train), len(X_test)\n\n(700, 300)\n\n\n\n#hyperparameters take their default values\ndt_classifier = DecisionTreeClassifier(random_state=42)\ndt_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ntest_accuracy = dt_classifier.score(X_test, y_test)\nprint(\"Test set accuracy: {:.4f}\".format(test_accuracy))\n\nTest set accuracy: 0.8933\n\n\n1.2 Multiple Random Train-Test splits\n\n# Initialize an empty list to store the accuracy metrics\naccuracy_metrics = []\nall_test_sets = []\nall_predictions = []\n\nX_tests = []\n\n# Perform 10 random train-test splits and repeat the fit\nfor _ in range(10):\n    # Split the data into training and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=np.random.randint(100))\n    X_tests.append(X_test)\n    # Create and fit the decision tree classifier\n    dt_classifier = DecisionTreeClassifier(random_state=42)\n    dt_classifier.fit(X_train, y_train)\n\n    current_predictions = dt_classifier.predict(X_test)\n    all_predictions.append(current_predictions)\n    current_accuracy = np.mean(current_predictions == y_test)\n    all_test_sets.append(y_test)\n    \n    # Calculate the accuracy on the test set\n    test_accuracy = dt_classifier.score(X_test, y_test)\n    \n    # Append the accuracy to the list\n    accuracy_metrics.append(test_accuracy)\n\n# Calculate the mean and standard deviation of the accuracy metrics\nmean_accuracy = np.mean(accuracy_metrics)\nstd_accuracy = np.std(accuracy_metrics)\n\n# Print the mean and standard deviation\nprint(\"Mean accuracy: {:.4f}\".format(mean_accuracy))\nprint(\"Standard deviation: {:.4f}\".format(std_accuracy))\n\n# Print minimum and maximum accuracies\nprint(\"Minimum accuracy: {:.4f}\".format(min(accuracy_metrics)))\nprint(\"Maximum accuracy: {:.4f}\".format(max(accuracy_metrics)))\n\nMean accuracy: 0.8800\nStandard deviation: 0.0211\nMinimum accuracy: 0.8400\nMaximum accuracy: 0.9133\n\n\n\n# Find number of unique element in X_tests\nfound_unique_test_samples = len(np.unique(np.concatenate(X_tests), axis=0))\nprint(f\"Number of unique test samples: {found_unique_test_samples}\")\nprint(f\"Ideally we wanted {len(X)} unique test samples\")\n\nNumber of unique test samples: 964\nIdeally we wanted 1000 unique test samples\n\n\n1.3 K-Fold Cross Validation\n\nimport numpy as np\n# Define the number of folds (k)\nk = 5\n\n# Initialize lists to store predictions and accuracies\npredictions = {}\naccuracies = []\n\n# Calculate the size of each fold\nfold_size = len(X) // k\n\n# Perform k-fold cross-validation\nfor i in range(k):\n    # Split the data into training and test sets\n    test_start = i * fold_size\n    test_end = (i + 1) * fold_size\n    test_set = X[test_start:test_end]\n    test_labels = y[test_start:test_end]\n    \n    training_set = np.concatenate((X[:test_start], X[test_end:]), axis=0)\n    print(len(test_set))\n    training_labels = np.concatenate((y[:test_start], y[test_end:]), axis=0)\n    \n    # Train the model\n    dt_classifier = DecisionTreeClassifier(random_state=42)\n    dt_classifier.fit(training_set, training_labels)\n    \n    # Make predictions on the validation set\n    fold_predictions = dt_classifier.predict(test_set)\n    \n    # Calculate the accuracy of the fold\n    fold_accuracy = np.mean(fold_predictions == test_labels)\n    \n    # Store the predictions and accuracy of the fold\n    predictions[i] = fold_predictions\n    accuracies.append(fold_accuracy)\n\n# Print the predictions and accuracies of each fold\nfor i in range(k):\n    print(\"Fold {}: Accuracy: {:.4f}\".format(i+1, accuracies[i]))\n\n200\n200\n200\n200\n200\nFold 1: Accuracy: 0.8700\nFold 2: Accuracy: 0.8850\nFold 3: Accuracy: 0.9300\nFold 4: Accuracy: 0.8650\nFold 5: Accuracy: 0.8850\n\n\n\nfrom cgi import test\nfrom sklearn.model_selection import KFold\n\n# Define the number of folds (k)\nk = 5\n\n# Initialize lists to store predictions and accuracies\npredictions = {}\naccuracies = []\n\n# Create a KFold instance\nkf = KFold(n_splits=k, shuffle=False)\n\n# Perform k-fold cross-validation\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    # Split the data into training and test sets\n    training_set, test_set = X[train_index], X[test_index]\n    print(len(test_set))\n    training_labels, test_labels = y[train_index], y[test_index]\n    \n    # Train the model\n    dt_classifier = DecisionTreeClassifier(random_state=42)\n    dt_classifier.fit(training_set, training_labels)\n    \n    # Make predictions on the validation set\n    fold_predictions = dt_classifier.predict(test_set)\n    \n    # Calculate the accuracy of the fold\n    fold_accuracy = np.mean(fold_predictions == test_labels)\n    \n    # Store the predictions and accuracy of the fold\n    predictions[i] = fold_predictions\n    accuracies.append(fold_accuracy)\n\n    # Print the predictions and accuracy of each fold\n    print(\"Fold {}: Accuracy: {:.4f}\".format(i+1, fold_accuracy))\n\n200\nFold 1: Accuracy: 0.8700\n200\nFold 2: Accuracy: 0.8850\n200\nFold 3: Accuracy: 0.9300\n200\nFold 4: Accuracy: 0.8650\n200\nFold 5: Accuracy: 0.8850\n\n\n\nMicro and Macro Averaging\n\nfrom sklearn.metrics import accuracy_score\n\n# Method 1 for computing accuracy\naccuracy_1 = accuracy_score(y, np.concatenate(list(predictions.values())))\n\n# Calculate macro-averaged accuracy\naccuracy_2 = np.mean(accuracies)\n\n# Print the micro and macro averaged accuracy\nprint(\"Method 1 accuracy: {:.4f}\".format(accuracy_1))\nprint(\"Method2 accuracy: {:.4f}\".format(accuracy_2))\n\nMethod 1 accuracy: 0.8870\nMethod2 accuracy: 0.8870\n\n\n2.1 Fixed Train-Test Split (hyperparameters tuned on Validation set)\n2.1.1 Validation Set as fixed Subset of Training Set\n\nlen(X)\n\n1000\n\n\n\n# Step 1: Split the data into training, validation, and test sets\nX_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.285, random_state=42)\n\n\nprint(\"Number of training examples: {}\".format(len(X_train)))\nprint(\"Number of validation examples: {}\".format(len(X_val)))\nprint(\"Number of testing examples: {}\".format(len(X_test)))\n\nNumber of training examples: 500\nNumber of validation examples: 200\nNumber of testing examples: 300\n\n\n\nhyperparameters = {}\nhyperparameters['max_depth'] = [1,2,3,4,5,6,7,8,9,10]\nhyperparameters['min_samples_split'] = [2,3,4,5,6,7,8]\nhyperparameters['criteria_values'] = ['gini', 'entropy']\n\nbest_accuracy = 0\nbest_hyperparameters = {}\n\nout = {}\ncount = 0\nfor max_depth in hyperparameters['max_depth']:\n    for min_samples_split in hyperparameters['min_samples_split']:\n        for criterion in hyperparameters['criteria_values']:\n            # Create and fit the decision tree classifier with the current hyperparameters\n            dt_classifier = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion, random_state=42)\n            dt_classifier.fit(X_train, y_train)\n            \n            # Evaluate the performance on the validation set\n            val_accuracy = dt_classifier.score(X_val, y_val)\n            out[count] = {'max_depth': max_depth, 'min_samples_split': min_samples_split, 'criterion': criterion, 'val_accuracy': val_accuracy}\n            count += 1\n\n\nhparam_df = pd.DataFrame(out).T\nhparam_df\n\n\n\n\n\n\n\n\nmax_depth\nmin_samples_split\ncriterion\nval_accuracy\n\n\n\n\n0\n1\n2\ngini\n0.785\n\n\n1\n1\n2\nentropy\n0.785\n\n\n2\n1\n3\ngini\n0.785\n\n\n3\n1\n3\nentropy\n0.785\n\n\n4\n1\n4\ngini\n0.785\n\n\n...\n...\n...\n...\n...\n\n\n135\n10\n6\nentropy\n0.895\n\n\n136\n10\n7\ngini\n0.89\n\n\n137\n10\n7\nentropy\n0.895\n\n\n138\n10\n8\ngini\n0.885\n\n\n139\n10\n8\nentropy\n0.895\n\n\n\n\n140 rows × 4 columns\n\n\n\n\nhparam_df.sort_values(by='val_accuracy', ascending=False).head(10)\n\n\n\n\n\n\n\n\nmax_depth\nmin_samples_split\ncriterion\nval_accuracy\n\n\n\n\n76\n6\n5\ngini\n0.925\n\n\n77\n6\n5\nentropy\n0.925\n\n\n78\n6\n6\ngini\n0.925\n\n\n79\n6\n6\nentropy\n0.925\n\n\n80\n6\n7\ngini\n0.925\n\n\n81\n6\n7\nentropy\n0.925\n\n\n83\n6\n8\nentropy\n0.925\n\n\n70\n6\n2\ngini\n0.92\n\n\n82\n6\n8\ngini\n0.92\n\n\n90\n7\n5\ngini\n0.915\n\n\n\n\n\n\n\n\n# Ensure dtype of val_accuracy is float\nhparam_df['val_accuracy'] = hparam_df['val_accuracy'].astype(float)\n\n\nbest_hyperparameters_row = hparam_df.iloc[hparam_df['val_accuracy'].idxmax()]\nbest_accuracy = best_hyperparameters_row['val_accuracy']\nbest_hyperparameters = best_hyperparameters_row[['max_depth', 'min_samples_split', 'criterion']].to_dict()\n\n\nbest_hyperparameters\n\n{'max_depth': 6, 'min_samples_split': 5, 'criterion': 'gini'}\n\n\n\n# Evaluate the performance of the selected hyperparameter combination on the test set\ndt_classifier = DecisionTreeClassifier(max_depth=best_hyperparameters['max_depth'], \n                                       min_samples_split=best_hyperparameters['min_samples_split'], \n                                       criterion=best_hyperparameters['criterion'], \n                                       random_state=42)\ndt_classifier.fit(X_train_val, y_train_val)\ntest_accuracy = dt_classifier.score(X_test, y_test)\n\nprint(\"Best Hyperparameters:\", best_hyperparameters)\nprint(\"Validation Set accuracy: {:.4f}\".format(best_accuracy))\nprint(\"Test Set accuracy: {:.4f}\".format(test_accuracy))\n\nBest Hyperparameters: {'max_depth': 6, 'min_samples_split': 5, 'criterion': 'gini'}\nValidation Set accuracy: 0.9250\nTest Set accuracy: 0.9067\n\n\nAvoiding nested loops by using itertools.product\nfor max_depth in hyperparameters['max_depth']:\n    for min_samples_split in hyperparameters['min_samples_split']:\n        for criterion in hyperparameters['criteria_values']:\n            # Create and fit the decision tree classifier with the current hyperparameters\n            dt_classifier = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion, random_state=42)\n            dt_classifier.fit(X_train, y_train)\n            \n            # Evaluate the performance on the validation set\n            val_accuracy = dt_classifier.score(X_val, y_val)\n            out[count] = {'max_depth': max_depth, 'min_samples_split': min_samples_split, 'criterion': criterion, 'val_accuracy': val_accuracy}\n            count += 1\n\nfrom itertools import product\n\nfor max_depth, min_samples_split, criterion in product(hyperparameters['max_depth'], hyperparameters['min_samples_split'], hyperparameters['criteria_values']):\n    # Define the Decision Tree Classifier\n    dt_classifier = DecisionTreeClassifier(\n        max_depth=max_depth,\n        min_samples_split=min_samples_split,\n        criterion=criterion,\n        random_state=42\n    )\n    dt_classifier.fit(X_train, y_train)\n\n2.1.2 Multiple random subsets of Training Set used as Validation Set\n\n# Initialize a list to store the optimal hyperparameters for each validation set\noptimal_hyperparameters = {}\ntest_accuracies = []\n\n# Set the number of subsets and iterations\nnum_subsets = 5\n\n# Make a pandas dataframe with columns as the hyperparameters, subset number, and validation accuracy\nhyperparameters_df = pd.DataFrame(columns=['max_depth', 'min_samples_split', 'criterion', 'subset', 'validation accuracy'])\n\n# Iterate over the subsets\nfor i in range(num_subsets):\n    # Split the data into training and validation sets\n    X_train_subset, X_val_subset, y_train_subset, y_val_subset = train_test_split(X_train_val, y_train_val, test_size=0.285, random_state=i)\n    \n    # Initialize variables to store the best hyperparameters and accuracy for the current subset\n    best_accuracy = 0\n    best_hyperparameters = {}\n    \n    # Iterate over the hyperparameter values\n\n    for max_depth in hyperparameters['max_depth']:\n        for min_samples_split in hyperparameters['min_samples_split']:\n            for criterion in hyperparameters['criteria_values']:\n                # Initialize and train the model with the current hyperparameters\n                dt_classifier = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion, random_state=42)\n                dt_classifier.fit(X_train_subset, y_train_subset)\n                \n                # Evaluate the model on the validation set\n                val_accuracy = dt_classifier.score(X_val_subset, y_val_subset)\n                hyperparameters_df.loc[len(hyperparameters_df)] = [max_depth, min_samples_split, criterion, i+1, val_accuracy]\n                \n                # Update the best accuracy and hyperparameters\n                if val_accuracy &gt; best_accuracy:\n                    best_accuracy = val_accuracy\n                    best_hyperparameters = {\n                        'max_depth': max_depth,\n                        'min_samples_split': min_samples_split,\n                        'criterion': criterion\n                    }\n    \n    optimal_hyperparameters[i] = best_hyperparameters\n\n    # Evaluate the model with the best hyperparameters on the test set\n    dt_classifier = DecisionTreeClassifier(max_depth=best_hyperparameters['max_depth'], min_samples_split=best_hyperparameters['min_samples_split'], criterion=best_hyperparameters['criterion'], random_state=42)\n    dt_classifier.fit(X_train_val, y_train_val)\n    test_accuracy = dt_classifier.score(X_test, y_test)\n    test_accuracies.append(test_accuracy)\n\n\n\nprint(\"Optimal hyperparameters for {} inner folds/validation sets\".format(num_subsets))\nprint()\n# Print the optimal hyperparameters for each validation set\nfor i in range(num_subsets):\n    print(\"Optimal hyperparameters for validation set {}: {}\".format(i+1, optimal_hyperparameters[i]))\n    print(\"Test Accuracy for validation set {}: {:.4f}\".format(i+1, test_accuracies[i]))\n\nOptimal hyperparameters for 5 inner folds/validation sets\n\nOptimal hyperparameters for validation set 1: {'max_depth': 7, 'min_samples_split': 6, 'criterion': 'entropy'}\nTest Accuracy for validation set 1: 0.9000\nOptimal hyperparameters for validation set 2: {'max_depth': 5, 'min_samples_split': 7, 'criterion': 'gini'}\nTest Accuracy for validation set 2: 0.9033\nOptimal hyperparameters for validation set 3: {'max_depth': 6, 'min_samples_split': 2, 'criterion': 'entropy'}\nTest Accuracy for validation set 3: 0.9233\nOptimal hyperparameters for validation set 4: {'max_depth': 7, 'min_samples_split': 4, 'criterion': 'entropy'}\nTest Accuracy for validation set 4: 0.9000\nOptimal hyperparameters for validation set 5: {'max_depth': 6, 'min_samples_split': 2, 'criterion': 'entropy'}\nTest Accuracy for validation set 5: 0.9233\n\n\n\nhyperparameters_df\n\n\n\n\n\n\n\n\nmax_depth\nmin_samples_split\ncriterion\nsubset\nvalidation accuracy\n\n\n\n\n0\n1\n2\ngini\n1\n0.790\n\n\n1\n1\n2\nentropy\n1\n0.790\n\n\n2\n1\n3\ngini\n1\n0.790\n\n\n3\n1\n3\nentropy\n1\n0.790\n\n\n4\n1\n4\ngini\n1\n0.790\n\n\n...\n...\n...\n...\n...\n...\n\n\n695\n10\n6\nentropy\n5\n0.900\n\n\n696\n10\n7\ngini\n5\n0.905\n\n\n697\n10\n7\nentropy\n5\n0.900\n\n\n698\n10\n8\ngini\n5\n0.905\n\n\n699\n10\n8\nentropy\n5\n0.900\n\n\n\n\n700 rows × 5 columns\n\n\n\n\ngrouped_df = hyperparameters_df.groupby(['max_depth', 'min_samples_split', 'criterion']).mean()['validation accuracy']\ngrouped_df\n\nmax_depth  min_samples_split  criterion\n1          2                  entropy      0.769\n                              gini         0.771\n           3                  entropy      0.769\n                              gini         0.771\n           4                  entropy      0.769\n                                           ...  \n10         6                  gini         0.889\n           7                  entropy      0.902\n                              gini         0.894\n           8                  entropy      0.904\n                              gini         0.893\nName: validation accuracy, Length: 140, dtype: float64\n\n\n\ngrouped_df.sort_values(ascending=False).head(10)\n\nmax_depth  min_samples_split  criterion\n6          7                  entropy      0.914\n           8                  entropy      0.914\n7          7                  entropy      0.912\n6          6                  entropy      0.912\n7          8                  entropy      0.912\n           6                  entropy      0.910\n6          4                  entropy      0.910\n           5                  entropy      0.910\n7          4                  entropy      0.909\n           5                  entropy      0.909\nName: validation accuracy, dtype: float64\n\n\n\noptimal_hyperparams = grouped_df.idxmax()\noptimal_hyperparams\n\n(6, 7, 'entropy')\n\n\n\ndf_classifier = DecisionTreeClassifier(max_depth=optimal_hyperparams[0], min_samples_split=optimal_hyperparams[1], criterion=optimal_hyperparams[2], random_state=42)\ndf_classifier.fit(X_train_val, y_train_val)\ntest_accuracy = df_classifier.score(X_test, y_test)\nprint(\"Test accuracy: {:.4f}\".format(test_accuracy))\n\nTest accuracy: 0.9233\n\n\n2.2 Nested Cross-Validation\n\nhyperparameters['max_depth'] = [1,2,3,4,5,6,7,8,9,10]\nhyperparameters['min_samples_split'] = [2,3,4,5,6,7,8]\nhyperparameters['criteria_values'] = ['gini', 'entropy']\n\n\nnum_outer_folds = 5\nnum_inner_folds = 5\n\nkf_outer = KFold(n_splits=num_outer_folds, shuffle=False)\nkf_inner = KFold(n_splits=num_inner_folds, shuffle=False)\n\n# Initialize lists to store the accuracies for the outer and inner loops\nouter_loop_accuracies = []\ninner_loop_accuracies = []\n\nresults= {}\nouter_count = 0\noverall_count = 0\n# Iterate over the outer folds\nfor outer_train_index, outer_test_index in kf_outer.split(X):\n    # Split the data into outer training and test sets\n    X_outer_train, X_outer_test = X[outer_train_index], X[outer_test_index]\n    y_outer_train, y_outer_test = y[outer_train_index], y[outer_test_index]\n    \n    \n    inner_count = 0\n    \n    for innner_train_index, inner_test_index in kf_inner.split(X_outer_train):\n        print(\"*****\"*20)\n        print(\"Outer Fold {}, Inner Fold {}\".format(outer_count+1, inner_count+1))\n        # Split the data into inner training and test sets\n        X_inner_train, X_inner_test = X_outer_train[innner_train_index], X_outer_train[inner_test_index]\n        y_inner_train, y_inner_test = y_outer_train[innner_train_index], y_outer_train[inner_test_index]\n        \n        for max_depth, min_samples_split, criterion in product(hyperparameters['max_depth'],\n                                                               hyperparameters['min_samples_split'],\n                                                               hyperparameters['criteria_values']):\n            \n            #print(max_depth, min_samples_split, criterion)\n            # Initialize and train the model with the current hyperparameters\n            dt_classifier = DecisionTreeClassifier(max_depth=max_depth, \n                                                   min_samples_split=min_samples_split, \n                                                   criterion=criterion, random_state=42)\n            dt_classifier.fit(X_inner_train, y_inner_train)\n            \n            # Evaluate the model on the inner test set\n            val_accuracy = dt_classifier.score(X_inner_test, y_inner_test)\n            \n            results[overall_count] = {'outer_fold': outer_count, \n                                      'inner_fold': inner_count, \n                                      'max_depth': max_depth, \n                                      'min_samples_split': min_samples_split, \n                                      'criterion': criterion, \n                                      'val_accuracy': val_accuracy}\n            overall_count += 1\n\n        inner_count += 1\n    outer_count += 1\n    \n            \n            \n\n****************************************************************************************************\nOuter Fold 1, Inner Fold 1\n****************************************************************************************************\nOuter Fold 1, Inner Fold 2\n****************************************************************************************************\nOuter Fold 1, Inner Fold 3\n****************************************************************************************************\nOuter Fold 1, Inner Fold 4\n****************************************************************************************************\nOuter Fold 1, Inner Fold 5\n****************************************************************************************************\nOuter Fold 2, Inner Fold 1\n****************************************************************************************************\nOuter Fold 2, Inner Fold 2\n****************************************************************************************************\nOuter Fold 2, Inner Fold 3\n****************************************************************************************************\nOuter Fold 2, Inner Fold 4\n****************************************************************************************************\nOuter Fold 2, Inner Fold 5\n****************************************************************************************************\nOuter Fold 3, Inner Fold 1\n****************************************************************************************************\nOuter Fold 3, Inner Fold 2\n****************************************************************************************************\nOuter Fold 3, Inner Fold 3\n****************************************************************************************************\nOuter Fold 3, Inner Fold 4\n****************************************************************************************************\nOuter Fold 3, Inner Fold 5\n****************************************************************************************************\nOuter Fold 4, Inner Fold 1\n****************************************************************************************************\nOuter Fold 4, Inner Fold 2\n****************************************************************************************************\nOuter Fold 4, Inner Fold 3\n****************************************************************************************************\nOuter Fold 4, Inner Fold 4\n****************************************************************************************************\nOuter Fold 4, Inner Fold 5\n****************************************************************************************************\nOuter Fold 5, Inner Fold 1\n****************************************************************************************************\nOuter Fold 5, Inner Fold 2\n****************************************************************************************************\nOuter Fold 5, Inner Fold 3\n****************************************************************************************************\nOuter Fold 5, Inner Fold 4\n****************************************************************************************************\nOuter Fold 5, Inner Fold 5\n\n\n\noverall_results = pd.DataFrame(results).T\n\n\noverall_results\n\n\n\n\n\n\n\n\nouter_fold\ninner_fold\nmax_depth\nmin_samples_split\ncriterion\nval_accuracy\n\n\n\n\n0\n0\n0\n1\n2\ngini\n0.7625\n\n\n1\n0\n0\n1\n2\nentropy\n0.7625\n\n\n2\n0\n0\n1\n3\ngini\n0.7625\n\n\n3\n0\n0\n1\n3\nentropy\n0.7625\n\n\n4\n0\n0\n1\n4\ngini\n0.7625\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n3495\n4\n4\n10\n6\nentropy\n0.9\n\n\n3496\n4\n4\n10\n7\ngini\n0.91875\n\n\n3497\n4\n4\n10\n7\nentropy\n0.9\n\n\n3498\n4\n4\n10\n8\ngini\n0.925\n\n\n3499\n4\n4\n10\n8\nentropy\n0.9\n\n\n\n\n3500 rows × 6 columns\n\n\n\nFind the best hyperparameters for each outer fold\n\nouter_fold = 0\nouter_fold_df = overall_results.query('outer_fold == @outer_fold')\nouter_fold_df\n\n\n\n\n\n\n\n\nouter_fold\ninner_fold\nmax_depth\nmin_samples_split\ncriterion\nval_accuracy\n\n\n\n\n0\n0\n0\n1\n2\ngini\n0.7625\n\n\n1\n0\n0\n1\n2\nentropy\n0.7625\n\n\n2\n0\n0\n1\n3\ngini\n0.7625\n\n\n3\n0\n0\n1\n3\nentropy\n0.7625\n\n\n4\n0\n0\n1\n4\ngini\n0.7625\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n695\n0\n4\n10\n6\nentropy\n0.85\n\n\n696\n0\n4\n10\n7\ngini\n0.86875\n\n\n697\n0\n4\n10\n7\nentropy\n0.85625\n\n\n698\n0\n4\n10\n8\ngini\n0.86875\n\n\n699\n0\n4\n10\n8\nentropy\n0.85625\n\n\n\n\n700 rows × 6 columns\n\n\n\nAggregate the validation accuracies for each hyperparameter combination across all inner folds\n\nouter_fold_df.groupby(['max_depth', 'min_samples_split', 'criterion']).mean()['val_accuracy'].sort_values(ascending=False).head(10)\n\nmax_depth  min_samples_split  criterion\n6          7                  gini          0.9175\n           8                  gini          0.9175\n           6                  gini          0.9175\n           4                  gini         0.91625\n           3                  gini         0.91625\n           2                  gini         0.91625\n           5                  gini         0.91625\n7          6                  gini         0.91625\n           7                  gini         0.91625\n           8                  gini           0.915\nName: val_accuracy, dtype: object"
  },
  {
    "objectID": "notebooks/rl-1.html",
    "href": "notebooks/rl-1.html",
    "title": "Reinforcement Learning 1 Gym Environments",
    "section": "",
    "text": "Reference\n\nDetailed Explanation and Python Implementation of Q-Learning Algorithm in OpenAI Gym (Cart-Pole)\n\n\nBasic Imports\nhttps://www.gymlibrary.dev/environments/classic_control/mountain_car/\n\nimport matplotlib.pyplot as plt\nimport torch\ntry:\n    import gymnasium as gym\nexcept ImportError:\n    %pip install gymnasium[classic-control] -q\n    import gymnasium as gym\nimport numpy as np\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\n\n# List of environments\nlist(gym.envs.registry.keys())\n\n['CartPole-v0',\n 'CartPole-v1',\n 'MountainCar-v0',\n 'MountainCarContinuous-v0',\n 'Pendulum-v1',\n 'Acrobot-v1',\n 'phys2d/CartPole-v0',\n 'phys2d/CartPole-v1',\n 'phys2d/Pendulum-v0',\n 'LunarLander-v2',\n 'LunarLanderContinuous-v2',\n 'BipedalWalker-v3',\n 'BipedalWalkerHardcore-v3',\n 'CarRacing-v2',\n 'Blackjack-v1',\n 'FrozenLake-v1',\n 'FrozenLake8x8-v1',\n 'CliffWalking-v0',\n 'Taxi-v3',\n 'tabular/Blackjack-v0',\n 'tabular/CliffWalking-v0',\n 'Reacher-v2',\n 'Reacher-v4',\n 'Pusher-v2',\n 'Pusher-v4',\n 'InvertedPendulum-v2',\n 'InvertedPendulum-v4',\n 'InvertedDoublePendulum-v2',\n 'InvertedDoublePendulum-v4',\n 'HalfCheetah-v2',\n 'HalfCheetah-v3',\n 'HalfCheetah-v4',\n 'Hopper-v2',\n 'Hopper-v3',\n 'Hopper-v4',\n 'Swimmer-v2',\n 'Swimmer-v3',\n 'Swimmer-v4',\n 'Walker2d-v2',\n 'Walker2d-v3',\n 'Walker2d-v4',\n 'Ant-v2',\n 'Ant-v3',\n 'Ant-v4',\n 'Humanoid-v2',\n 'Humanoid-v3',\n 'Humanoid-v4',\n 'HumanoidStandup-v2',\n 'HumanoidStandup-v4',\n 'GymV21Environment-v0',\n 'GymV26Environment-v0']\n\n\n\nenv = gym.make('MountainCar-v0', render_mode='human')\n\n\nobservation, info = env.reset(seed=42)\n\n\nobservation\n\narray([-0.4452088,  0.       ], dtype=float32)\n\n\n\ninfo\n\n{}\n\n\n\nenv\n\n&lt;TimeLimit&lt;OrderEnforcing&lt;PassiveEnvChecker&lt;MountainCarEnv&lt;MountainCar-v0&gt;&gt;&gt;&gt;&gt;\n\n\n\nenv.action_space\n\nDiscrete(3)\n\n\n\nenv.action_space.n\n\n3\n\n\n\nenv.reward_range\n\n(-inf, inf)\n\n\n\nfor i in range(10):\n    print(i, env.action_space.sample())\n\n0 1\n1 0\n2 0\n3 0\n4 2\n5 0\n6 2\n7 2\n8 2\n9 2\n\n\n\nenv.observation_space  \n\nBox([-1.2  -0.07], [0.6  0.07], (2,), float32)\n\n\n\nenv.observation_space.high\n\narray([0.6 , 0.07], dtype=float32)\n\n\n\nenv.observation_space.sample()\n\narray([-0.2961844, -0.034966 ], dtype=float32)\n\n\n\nenv.reset(seed=42)\nfor i in range(100):\n    env.render()\n    action = env.action_space.sample()\n    observation, reward, terminated, truncated, info = env.step(action)\n    print(i, observation, reward, terminated, truncated, info)\n    if terminated:\n        break\n    \n\n0 [-0.4457913  -0.00058252] -1.0 False False {}\n1 [-0.4469521  -0.00116079] -1.0 False False {}\n2 [-0.4486827  -0.00173059] -1.0 False False {}\n3 [-0.45097044 -0.00228774] -1.0 False False {}\n4 [-0.45479858 -0.00382815] -1.0 False False {}\n5 [-0.4601391 -0.0053405] -1.0 False False {}\n6 [-0.46495268 -0.00481358] -1.0 False False {}\n7 [-0.47120383 -0.00625116] -1.0 False False {}\n8 [-0.47784632 -0.0066425 ] -1.0 False False {}\n9 [-0.4858309  -0.00798457] -1.0 False False {}\n10 [-0.4950981  -0.00926722] -1.0 False False {}\n11 [-0.5045788  -0.00948072] -1.0 False False {}\n12 [-0.5142021  -0.00962329] -1.0 False False {}\n13 [-0.5228959  -0.00869376] -1.0 False False {}\n14 [-0.5325949  -0.00969903] -1.0 False False {}\n15 [-0.5432265  -0.01063157] -1.0 False False {}\n16 [-0.55371094 -0.01048444] -1.0 False False {}\n17 [-0.56296986 -0.00925891] -1.0 False False {}\n18 [-0.57293415 -0.00996431] -1.0 False False {}\n19 [-0.5825298  -0.00959565] -1.0 False False {}\n20 [-0.5916858  -0.00915596] -1.0 False False {}\n21 [-0.6003346  -0.00864885] -1.0 False False {}\n22 [-0.60841304 -0.0080784 ] -1.0 False False {}\n23 [-0.6168622  -0.00844914] -1.0 False False {}\n24 [-0.6236209  -0.00675875] -1.0 False False {}\n25 [-0.6296407  -0.00601979] -1.0 False False {}\n26 [-0.6338785  -0.00423783] -1.0 False False {}\n27 [-0.63630426 -0.00242574] -1.0 False False {}\n28 [-6.3690072e-01 -5.9645844e-04] -1.0 False False {}\n29 [-6.3666368e-01  2.3703734e-04] -1.0 False False {}\n30 [-0.63559484  0.00106886] -1.0 False False {}\n31 [-0.63370174  0.00189311] -1.0 False False {}\n32 [-0.6309978   0.00270395] -1.0 False False {}\n33 [-0.6285022   0.00249558] -1.0 False False {}\n34 [-0.62523276  0.00326943] -1.0 False False {}\n35 [-0.62021285  0.00501993] -1.0 False False {}\n36 [-0.6134784   0.00673443] -1.0 False False {}\n37 [-0.606078    0.00740039] -1.0 False False {}\n38 [-0.5990653   0.00701269] -1.0 False False {}\n39 [-0.59249145  0.00657387] -1.0 False False {}\n40 [-0.58640456  0.00608689] -1.0 False False {}\n41 [-0.5788494   0.00755515] -1.0 False False {}\n42 [-0.5708818   0.00796764] -1.0 False False {}\n43 [-0.5615607   0.00932107] -1.0 False False {}\n44 [-0.5529555   0.00860517] -1.0 False False {}\n45 [-0.54513043  0.00782506] -1.0 False False {}\n46 [-0.538144    0.00698644] -1.0 False False {}\n47 [-0.5300485   0.00809549] -1.0 False False {}\n48 [-0.52190465  0.00814386] -1.0 False False {}\n49 [-0.5147735   0.00713116] -1.0 False False {}\n50 [-0.50770855  0.00706498] -1.0 False False {}\n51 [-0.5007627   0.00694584] -1.0 False False {}\n52 [-0.49398798  0.00677471] -1.0 False False {}\n53 [-0.48743504  0.00655292] -1.0 False False {}\n54 [-0.48115283  0.00628222] -1.0 False False {}\n55 [-0.4751881   0.00596474] -1.0 False False {}\n56 [-0.47058517  0.00460293] -1.0 False False {}\n57 [-0.46637815  0.004207  ] -1.0 False False {}\n58 [-0.46359822  0.00277995] -1.0 False False {}\n59 [-0.46126583  0.00233238] -1.0 False False {}\n60 [-0.45839822  0.0028676 ] -1.0 False False {}\n61 [-0.45501652  0.00338171] -1.0 False False {}\n62 [-0.45214558  0.00287096] -1.0 False False {}\n63 [-0.44980642  0.00233916] -1.0 False False {}\n64 [-0.4480162   0.00179022] -1.0 False False {}\n65 [-4.4778800e-01  2.2819887e-04] -1.0 False False {}\n66 [-0.44912347 -0.00133549] -1.0 False False {}\n67 [-0.4520129  -0.00288942] -1.0 False False {}\n68 [-0.4554351 -0.0034222] -1.0 False False {}\n69 [-0.45836496 -0.00292987] -1.0 False False {}\n70 [-0.46278098 -0.00441601] -1.0 False False {}\n71 [-0.4666506  -0.00386961] -1.0 False False {}\n72 [-0.46994525 -0.00329465] -1.0 False False {}\n73 [-0.47264057 -0.00269532] -1.0 False False {}\n74 [-0.4757166  -0.00307602] -1.0 False False {}\n75 [-0.4781505 -0.0024339] -1.0 False False {}\n76 [-0.4809242  -0.00277371] -1.0 False False {}\n77 [-0.4850171  -0.00409289] -1.0 False False {}\n78 [-0.4893987  -0.00438161] -1.0 False False {}\n79 [-0.49403635 -0.00463766] -1.0 False False {}\n80 [-0.49989542 -0.00585909] -1.0 False False {}\n81 [-0.50593215 -0.00603671] -1.0 False False {}\n82 [-0.5131013  -0.00716915] -1.0 False False {}\n83 [-0.5213492  -0.00824787] -1.0 False False {}\n84 [-0.5306139  -0.00926474] -1.0 False False {}\n85 [-0.53982604 -0.00921213] -1.0 False False {}\n86 [-0.5499165  -0.01009047] -1.0 False False {}\n87 [-0.5608098 -0.0108933] -1.0 False False {}\n88 [-0.5704246  -0.00961479] -1.0 False False {}\n89 [-0.57868934 -0.00826475] -1.0 False False {}\n90 [-0.5865428  -0.00785345] -1.0 False False {}\n91 [-0.59392697 -0.00738417] -1.0 False False {}\n92 [-0.6007876  -0.00686062] -1.0 False False {}\n93 [-0.60607445 -0.00528686] -1.0 False False {}\n94 [-0.61174905 -0.00567458] -1.0 False False {}\n95 [-0.6177702  -0.00602114] -1.0 False False {}\n96 [-0.62409437 -0.00632421] -1.0 False False {}\n97 [-0.6296762  -0.00558186] -1.0 False False {}\n98 [-0.6344759  -0.00479964] -1.0 False False {}\n99 [-0.6384592  -0.00398331] -1.0 False False {}\n\n\n\n# Does it help to always go right?\nenv.reset(seed=42)\nfor i in range(100):\n    env.render()\n    action = 2\n    observation, reward, terminated, truncated, info = env.step(action)\n    print(i, observation, reward, terminated, truncated, info)\n    if terminated:\n        break\n\n0 [-4.4479132e-01  4.1747934e-04] -1.0 False False {}\n1 [-0.4439594   0.00083191] -1.0 False False {}\n2 [-0.4427191   0.00124029] -1.0 False False {}\n3 [-0.4410795   0.00163962] -1.0 False False {}\n4 [-0.43905246  0.00202703] -1.0 False False {}\n5 [-0.43665275  0.00239971] -1.0 False False {}\n6 [-0.43389776  0.00275498] -1.0 False False {}\n7 [-0.43080744  0.00309032] -1.0 False False {}\n8 [-0.4274041   0.00340333] -1.0 False False {}\n9 [-0.42371225  0.00369185] -1.0 False False {}\n10 [-0.4197584   0.00395386] -1.0 False False {}\n11 [-0.41557083  0.00418759] -1.0 False False {}\n12 [-0.41117933  0.00439149] -1.0 False False {}\n13 [-0.40661508  0.00456424] -1.0 False False {}\n14 [-0.40191033  0.00470476] -1.0 False False {}\n15 [-0.3970981   0.00481224] -1.0 False False {}\n16 [-0.392212    0.00488609] -1.0 False False {}\n17 [-0.38728598  0.00492601] -1.0 False False {}\n18 [-0.38235408  0.00493192] -1.0 False False {}\n19 [-0.37745008  0.004904  ] -1.0 False False {}\n20 [-0.3726074   0.00484267] -1.0 False False {}\n21 [-0.36785883  0.00474856] -1.0 False False {}\n22 [-0.36323628  0.00462255] -1.0 False False {}\n23 [-0.3587706   0.00446569] -1.0 False False {}\n24 [-0.35449135  0.00427925] -1.0 False False {}\n25 [-0.3504267   0.00406465] -1.0 False False {}\n26 [-0.3466032  0.0038235] -1.0 False False {}\n27 [-0.34304565  0.00355754] -1.0 False False {}\n28 [-0.33977702  0.00326864] -1.0 False False {}\n29 [-0.33681822  0.0029588 ] -1.0 False False {}\n30 [-0.3341881   0.00263011] -1.0 False False {}\n31 [-0.33190334  0.00228476] -1.0 False False {}\n32 [-0.32997838  0.00192499] -1.0 False False {}\n33 [-0.32842523  0.00155313] -1.0 False False {}\n34 [-0.3272537   0.00117154] -1.0 False False {}\n35 [-0.32647103  0.00078265] -1.0 False False {}\n36 [-0.32608217  0.00038887] -1.0 False False {}\n37 [-3.2608950e-01 -7.3227225e-06] -1.0 False False {}\n38 [-0.32649297 -0.00040347] -1.0 False False {}\n39 [-0.3272901  -0.00079711] -1.0 False False {}\n40 [-0.32847586 -0.00118578] -1.0 False False {}\n41 [-0.3300429  -0.00156705] -1.0 False False {}\n42 [-0.33198142 -0.0019385 ] -1.0 False False {}\n43 [-0.33427918 -0.00229778] -1.0 False False {}\n44 [-0.33692175 -0.00264256] -1.0 False False {}\n45 [-0.33989236 -0.00297059] -1.0 False False {}\n46 [-0.34317204 -0.0032797 ] -1.0 False False {}\n47 [-0.34673983 -0.00356778] -1.0 False False {}\n48 [-0.35057268 -0.00383286] -1.0 False False {}\n49 [-0.35464573 -0.00407306] -1.0 False False {}\n50 [-0.35893238 -0.00428664] -1.0 False False {}\n51 [-0.3634044  -0.00447202] -1.0 False False {}\n52 [-0.36803216 -0.00462776] -1.0 False False {}\n53 [-0.37278476 -0.00475261] -1.0 False False {}\n54 [-0.3776303  -0.00484552] -1.0 False False {}\n55 [-0.3825359  -0.00490563] -1.0 False False {}\n56 [-0.38746822 -0.0049323 ] -1.0 False False {}\n57 [-0.39239335 -0.00492514] -1.0 False False {}\n58 [-0.39727733 -0.00488397] -1.0 False False {}\n59 [-0.40208617 -0.00480886] -1.0 False False {}\n60 [-0.40678635 -0.00470015] -1.0 False False {}\n61 [-0.41134477 -0.00455843] -1.0 False False {}\n62 [-0.41572928 -0.00438451] -1.0 False False {}\n63 [-0.41990876 -0.00417948] -1.0 False False {}\n64 [-0.42385343 -0.00394468] -1.0 False False {}\n65 [-0.4275351  -0.00368165] -1.0 False False {}\n66 [-0.43092728 -0.0033922 ] -1.0 False False {}\n67 [-0.4340056  -0.00307832] -1.0 False False {}\n68 [-0.4367478 -0.0027422] -1.0 False False {}\n69 [-0.43913403 -0.00238624] -1.0 False False {}\n70 [-0.441147   -0.00201297] -1.0 False False {}\n71 [-0.4427721  -0.00162507] -1.0 False False {}\n72 [-0.4439974  -0.00122535] -1.0 False False {}\n73 [-0.44481412 -0.0008167 ] -1.0 False False {}\n74 [-4.452162e-01 -4.020977e-04] -1.0 False False {}\n75 [-4.4520080e-01  1.5435844e-05] -1.0 False False {}\n76 [-4.4476792e-01  4.3285682e-04] -1.0 False False {}\n77 [-0.44392082  0.00084712] -1.0 False False {}\n78 [-0.4426656   0.00125521] -1.0 False False {}\n79 [-0.44101143  0.00165416] -1.0 False False {}\n80 [-0.43897036  0.00204107] -1.0 False False {}\n81 [-0.4365572   0.00241315] -1.0 False False {}\n82 [-0.4337895   0.00276774] -1.0 False False {}\n83 [-0.4306872   0.00310229] -1.0 False False {}\n84 [-0.42727277  0.00341444] -1.0 False False {}\n85 [-0.42357075  0.00370201] -1.0 False False {}\n86 [-0.41960773  0.003963  ] -1.0 False False {}\n87 [-0.41541207  0.00419566] -1.0 False False {}\n88 [-0.41101363  0.00439843] -1.0 False False {}\n89 [-0.40644366  0.00457001] -1.0 False False {}\n90 [-0.40173432  0.00470932] -1.0 False False {}\n91 [-0.39691874  0.00481556] -1.0 False False {}\n92 [-0.3920306   0.00488817] -1.0 False False {}\n93 [-0.38710377  0.00492683] -1.0 False False {}\n94 [-0.38217226  0.00493149] -1.0 False False {}\n95 [-0.37726995  0.00490233] -1.0 False False {}\n96 [-0.37243018  0.00483977] -1.0 False False {}\n97 [-0.3676857   0.00474447] -1.0 False False {}\n98 [-0.3630684  0.0046173] -1.0 False False {}\n99 [-0.35860908  0.00445932] -1.0 False False {}\n\n\n\n# Go left first K iterations and then go right N - K iterations\nenv.reset(seed=42)\nK = 50\nN = 250\n\nfor i in range(N):\n    env.render()\n    action = 0 if i &lt; K else 2\n    observation, reward, terminated, truncated, info = env.step(action)\n    print(i, observation, reward, terminated, truncated, info)\n    if terminated:\n        break\n\n0 [-0.44679132 -0.00158252] -1.0 False False {}\n1 [-0.4499448  -0.00315349] -1.0 False False {}\n2 [-0.45464623 -0.00470141] -1.0 False False {}\n3 [-0.4608611  -0.00621488] -1.0 False False {}\n4 [-0.46854374 -0.00768264] -1.0 False False {}\n5 [-0.4776374  -0.00909367] -1.0 False False {}\n6 [-0.4880747  -0.01043729] -1.0 False False {}\n7 [-0.4997779  -0.01170322] -1.0 False False {}\n8 [-0.51265967 -0.01288173] -1.0 False False {}\n9 [-0.52662337 -0.01396375] -1.0 False False {}\n10 [-0.54156446 -0.01494107] -1.0 False False {}\n11 [-0.55737084 -0.01580639] -1.0 False False {}\n12 [-0.5739244  -0.01655353] -1.0 False False {}\n13 [-0.59110194 -0.01717752] -1.0 False False {}\n14 [-0.6087766 -0.0176747] -1.0 False False {}\n15 [-0.62681943 -0.0180428 ] -1.0 False False {}\n16 [-0.64510036 -0.01828096] -1.0 False False {}\n17 [-0.6634901  -0.01838974] -1.0 False False {}\n18 [-0.6818612  -0.01837108] -1.0 False False {}\n19 [-0.7000894  -0.01822821] -1.0 False False {}\n20 [-0.71805495 -0.01796552] -1.0 False False {}\n21 [-0.7356433  -0.01758842] -1.0 False False {}\n22 [-0.7527465 -0.0171032] -1.0 False False {}\n23 [-0.7692633  -0.01651679] -1.0 False False {}\n24 [-0.7851     -0.01583663] -1.0 False False {}\n25 [-0.8001704  -0.01507044] -1.0 False False {}\n26 [-0.8143965  -0.01422609] -1.0 False False {}\n27 [-0.82770795 -0.01331142] -1.0 False False {}\n28 [-0.840042   -0.01233409] -1.0 False False {}\n29 [-0.8513436  -0.01130153] -1.0 False False {}\n30 [-0.86156434 -0.01022079] -1.0 False False {}\n31 [-0.87066287 -0.00909855] -1.0 False False {}\n32 [-0.87860394 -0.00794103] -1.0 False False {}\n33 [-0.885358   -0.00675404] -1.0 False False {}\n34 [-0.8909009  -0.00554296] -1.0 False False {}\n35 [-0.8952137  -0.00431278] -1.0 False False {}\n36 [-0.8982819  -0.00306818] -1.0 False False {}\n37 [-0.9000954  -0.00181353] -1.0 False False {}\n38 [-9.006485e-01 -5.530463e-04] -1.0 False False {}\n39 [-8.9993924e-01  7.0920831e-04] -1.0 False False {}\n40 [-0.8979701   0.00196919] -1.0 False False {}\n41 [-0.8947472   0.00322283] -1.0 False False {}\n42 [-0.8902813   0.00446589] -1.0 False False {}\n43 [-0.88458735  0.00569396] -1.0 False False {}\n44 [-0.877685    0.00690234] -1.0 False False {}\n45 [-0.86959904  0.00808598] -1.0 False False {}\n46 [-0.8603596   0.00923946] -1.0 False False {}\n47 [-0.85000265  0.01035692] -1.0 False False {}\n48 [-0.8385706   0.01143206] -1.0 False False {}\n49 [-0.82611245  0.01245818] -1.0 False False {}\n50 [-0.81068426  0.01542816] -1.0 False False {}\n51 [-0.7923595   0.01832481] -1.0 False False {}\n52 [-0.77123034  0.02112911] -1.0 False False {}\n53 [-0.7474102   0.02382017] -1.0 False False {}\n54 [-0.72103477  0.02637544] -1.0 False False {}\n55 [-0.6922636   0.02877113] -1.0 False False {}\n56 [-0.66128075  0.03098283] -1.0 False False {}\n57 [-0.62829447  0.03298633] -1.0 False False {}\n58 [-0.5935357  0.0347587] -1.0 False False {}\n59 [-0.55725634  0.03627939] -1.0 False False {}\n60 [-0.51972497  0.03753139] -1.0 False False {}\n61 [-0.48122263  0.03850234] -1.0 False False {}\n62 [-0.44203725  0.03918537] -1.0 False False {}\n63 [-0.4024575   0.03957975] -1.0 False False {}\n64 [-0.36276644  0.03969106] -1.0 False False {}\n65 [-0.32323536  0.03953107] -1.0 False False {}\n66 [-0.28411815  0.03911722] -1.0 False False {}\n67 [-0.24564646  0.03847169] -1.0 False False {}\n68 [-0.2080261   0.03762037] -1.0 False False {}\n69 [-0.17143448  0.03659161] -1.0 False False {}\n70 [-0.13601945  0.03541502] -1.0 False False {}\n71 [-0.10189916  0.03412029] -1.0 False False {}\n72 [-0.06916296  0.0327362 ] -1.0 False False {}\n73 [-0.03787315  0.03128982] -1.0 False False {}\n74 [-0.00806721  0.02980594] -1.0 False False {}\n75 [0.02023946 0.02830667] -1.0 False False {}\n76 [0.04705074 0.02681128] -1.0 False False {}\n77 [0.07238688 0.02533614] -1.0 False False {}\n78 [0.09628174 0.02389486] -1.0 False False {}\n79 [0.11878016 0.02249843] -1.0 False False {}\n80 [0.13993563 0.02115547] -1.0 False False {}\n81 [0.15980819 0.01987256] -1.0 False False {}\n82 [0.1784626 0.0186544] -1.0 False False {}\n83 [0.19596682 0.01750423] -1.0 False False {}\n84 [0.21239078 0.01642396] -1.0 False False {}\n85 [0.22780529 0.01541451] -1.0 False False {}\n86 [0.24228124 0.01447596] -1.0 False False {}\n87 [0.255889   0.01360777] -1.0 False False {}\n88 [0.26869795 0.01280894] -1.0 False False {}\n89 [0.28077608 0.01207813] -1.0 False False {}\n90 [0.2921899  0.01141381] -1.0 False False {}\n91 [0.30300424 0.01081433] -1.0 False False {}\n92 [0.31328225 0.01027802] -1.0 False False {}\n93 [0.3230855  0.00980324] -1.0 False False {}\n94 [0.33247393 0.00938846] -1.0 False False {}\n95 [0.34150624 0.00903228] -1.0 False False {}\n96 [0.35023972 0.00873351] -1.0 False False {}\n97 [0.35873088 0.00849114] -1.0 False False {}\n98 [0.3670353  0.00830443] -1.0 False False {}\n99 [0.37520823 0.00817291] -1.0 False False {}\n100 [0.3833046  0.00809638] -1.0 False False {}\n101 [0.39137957 0.00807496] -1.0 False False {}\n102 [0.3994887  0.00810912] -1.0 False False {}\n103 [0.40768832 0.00819965] -1.0 False False {}\n104 [0.41603607 0.00834773] -1.0 False False {}\n105 [0.424591   0.00855494] -1.0 False False {}\n106 [0.43341425 0.00882325] -1.0 False False {}\n107 [0.44256935 0.00915509] -1.0 False False {}\n108 [0.4521227  0.00955334] -1.0 False False {}\n109 [0.46214405 0.01002137] -1.0 False False {}\n110 [0.47270712 0.01056306] -1.0 False False {}\n111 [0.48388997 0.01118286] -1.0 False False {}\n112 [0.49577573 0.01188574] -1.0 False False {}\n113 [0.508453   0.01267731] -1.0 True False {}\n\n\n\nenv.close()\n\n\n%pip install flappy-bird-gymnasium -q\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport flappy_bird_gymnasium\nenv = gym.make(\"FlappyBird-v0\", render_mode=\"human\", use_lidar=False)\n\n\nobs, _ = env.reset()\n\n\nobs\n\narray([ 1.       ,  0.234375 ,  0.4296875,  1.       ,  0.       ,\n        1.       ,  1.       ,  0.       ,  1.       ,  0.4765625,\n       -0.9      ,  0.5      ])\n\n\n\nenv.observation_space\n\nBox(-1.0, 1.0, (12,), float64)\n\n\n\nThe Kernel crashed while executing code in the current cell or a previous cell. \n\nPlease review the code in the cell(s) to identify a possible cause of the failure. \n\nClick &lt;a href='https://aka.ms/vscodeJupyterKernelCrash'&gt;here&lt;/a&gt; for more info. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n\n\n\n\nn_bins = 6\nimport pandas as pd\nobs_low = env.observation_space.low\nobs_high = env.observation_space.high\n\n# Discretize the observation space\ndef discretize_observation(observation):\n    bins = np.linspace(obs_low, obs_high, n_bins)\n    return tuple(np.digitize(observation, bins))\n\n# Define the variables for the MultiIndex\nvariables = [\n    \"last_pipe_h_pos\",\n    \"last_top_pipe_v_pos\",\n    \"last_bottom_pipe_v_pos\",\n    \"next_pipe_h_pos\",\n    \"next_top_pipe_v_pos\",\n    \"next_bottom_pipe_v_pos\",\n    \"next_next_pipe_h_pos\",\n    \"next_next_top_pipe_v_pos\",\n    \"next_next_bottom_pipe_v_pos\",\n    \"player_v_pos\",\n    \"player_v_vel\",\n    \"player_rotation\",\n]\n\n# leave out the first three and last three variables\nvar_consider = variables[3:-3]\n\nq_table_np = np.zeros([n_bins] * len(var_consider) + [env.action_space.n])"
  },
  {
    "objectID": "notebooks/decision-tree-real-input-real-output.html",
    "href": "notebooks/decision-tree-real-input-real-output.html",
    "title": "Decision Trees [Real I/P Real O/P, Bias vs Variance]",
    "section": "",
    "text": "import numpy as np \nimport matplotlib.pyplot as plt\nimport pandas as pd\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\nfrom latexify import latexify, format_axes\nFIG_WIDTH = 5\nFIG_HEIGHT = 4\n\n\n# Create dataset\nx = np.array([1, 2, 3, 4, 5, 6])\ny = np.array([0, 0, 1, 1, 2, 2])\n\n# plot data\nlatexify(columns=2)\nplt.scatter(x, y, color='k')\nformat_axes(plt.gca()) \nplt.savefig(\"../figures/decision-trees/ri-ro-dataset.pdf\")\n\n\n\n\n\n\n\n\n\n# Depth 0 tree\n# Average of all y values\ny_pred = np.mean(y)\n# Plot data\nlatexify(columns=2)\nplt.scatter(x, y, color='C1', label='data')\n# Plot prediction\nplt.plot([0, 7], [y_pred, y_pred], color='k', linestyle='-', label='Prediction')\nformat_axes(plt.gca())\nplt.legend()\nplt.savefig(\"../figures/decision-trees/ri-ro-depth-0.pdf\")\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef create_DT_Regressor(x, y, depth, filename):\n    dt = DecisionTreeRegressor(max_depth=depth)\n    dt.fit(x.reshape(-1, 1), y)\n\n    # Plot data\n    latexify(columns=2)\n    plt.scatter(x, y, color='C1', label='Data')\n\n    x_test = np.linspace(0, 7, 500)\n    y_test = dt.predict(x_test.reshape(-1, 1))\n    plt.plot(x_test, y_test, color='k', label='Prediction')\n    format_axes(plt.gca())\n    plt.legend()\n    plt.savefig(f\"../figures/decision-trees/{filename}.pdf\")\n    return dt\n    \n\n\ndt_one = create_DT_Regressor(x, y, 1, \"ri-ro-depth-1\")\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import export_graphviz\nimport graphviz\ndef create_graph(dt, filename, feature_names=['x']):\n    dot_data = export_graphviz(dt, out_file=None, feature_names=feature_names, filled=True)\n    graph = graphviz.Source(dot_data)\n    graph.format = 'pdf'\n    graph.render(f\"../figures/decision-trees/{filename}\")\n    return graph\n\n\ncreate_graph(dt_one, \"ri-ro-depth-1-sklearn\")\n\n\n\n\n\n\n\n\n\ndt_two = create_DT_Regressor(x, y, 2, \"ri-ro-depth-2\")\n\n\n\n\n\n\n\n\n\ncreate_graph(dt_two, \"ri-ro-depth-2-sklearn\")\n\n\n\n\n\n\n\n\n\ndt_three = create_DT_Regressor(x, y, 3, \"ri-ro-depth-3\")\n\ncreate_graph(dt_three, \"ri-ro-depth-3-sklearn\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSine Dataset\n\n### Sine daatset\nx = np.linspace(0, 2*np.pi, 200)\ny = np.sin(x)\n\nlatexify(columns=2)\nplt.scatter(x, y, color='k', s=1)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/sine-dataset.pdf\")\n\n\n\n\n\n\n\n\n\ndt_sine_one = create_DT_Regressor(x, y, 1, \"sine-depth-1\")\n\n\n\n\n\n\n\n\n\nmean_y = np.mean(y)\nerror_vector = y - mean_y\nsquared_error = np.sum(error_vector**2)\nmean_squared_error = squared_error / len(y)\nprint(f\"Mean squared error: {mean_squared_error:0.4f}\")\n\nMean squared error: 0.4975\n\n\n\ncreate_graph(dt_sine_one, \"sine-depth-1-sklearn\")\n\n\n\n\n\n\n\n\n\nsplit = np.pi \nleft = y[x &lt; split]\nright = y[x &gt;= split]\n\nmean_left = np.mean(left)\nmean_right = np.mean(right)\n\nerror_vector_left = left - mean_left\nerror_vector_right = right - mean_right\n\nsquared_error_left = np.sum(error_vector_left**2)\nsquared_error_right = np.sum(error_vector_right**2)\n\nmean_squared_error_left = squared_error_left / len(left)\nmean_squared_error_right = squared_error_right / len(right)\n\nprint(f\"Mean squared error left: {mean_squared_error_left:0.4f}\")\nprint(f\"Mean value left: {mean_left:0.4f}\")  \nprint(f\"Number of samples in left: {len(left)}\")\nprint(\"---\"*20)\nprint(f\"Mean squared error right: {mean_squared_error_right:0.4f}\")\nprint(f\"Mean value right: {mean_right:0.4f}\")\nprint(f\"Number of samples in right: {len(right)}\")\n\nweighted_error = len(left) / len(y) * mean_squared_error_left + len(right) / len(y) * mean_squared_error_right\n\nprint(\"---\"*20)\nprint(f\"Weighted error: {weighted_error:0.4f}\")\nreduction = mean_squared_error - weighted_error\nprint(f\"Reduction: {reduction:0.4f}\")\n\nMean squared error left: 0.0963\nMean value left: 0.6334\nNumber of samples in left: 100\n------------------------------------------------------------\nMean squared error right: 0.0963\nMean value right: -0.6334\nNumber of samples in right: 100\n------------------------------------------------------------\nWeighted error: 0.0963\nReduction: 0.4012\n\n\n\ndt_sine_two = create_DT_Regressor(x, y, 2, \"sine-depth-2\")\n\n\n\n\n\n\n\n\n\ncreate_graph(dt_sine_two, \"sine-depth-2-sklearn\")\n\n\n\n\n\n\n\n\n\ndt_sine_four = create_DT_Regressor(x, y, 4, \"sine-depth-4\")\n\n\n\n\n\n\n\n\nBias-Variance Tradeoff - Dataset I\n\n### Dataset for showing bias-variance tradeoff\nX = np.array([[1, 1],[2, 1],[3, 1],[5, 1],\n              [6, 1],[7, 1],[1, 2],[2, 2],\n              [6, 2],[7, 2],[1, 4],[7, 4]])\ny = np.array([0, 0, 0, 1, 1, 1, 0, 1, 0, 1 ,0, 1])\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.xlabel(\"$x_1$\")\nplt.ylabel(\"$x_2$\")\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset.pdf\")\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\ndef create_DT_Classifier(X,y,depth,filename):\n    dt = DecisionTreeClassifier(max_depth=depth)\n    dt.fit(X, y)\n\n    # Predict in entire 2d space and contour plot\n    x1 = np.linspace(0, 8, 100)\n    x2 = np.linspace(0, 5, 100)\n\n    X1, X2 = np.meshgrid(x1, x2)\n    X_test = np.stack([X1.flatten(), X2.flatten()], axis=1)\n    y_test = dt.predict(X_test)\n    plt.scatter(X[:, 0], X[:, 1], c=y)\n    plt.contourf(X1, X2, y_test.reshape(X1.shape), alpha=0.1, cmap='coolwarm')\n    format_axes(plt.gca())\n    plt.savefig(f\"../figures/decision-trees/{filename}.pdf\")\n    return dt\n\n\ndt_bias_variance_one = create_DT_Classifier(X, y, 1, \"bias-variance-depth-1\")\n\n\n\n\n\n\n\n\n\ncreate_graph(dt_bias_variance_one, \"bias-variance-depth-1-sklearn\", feature_names=['x1', 'x2'])\n\n\n\n\n\n\n\n\n\ndt_bias_variance_full_depth = create_DT_Classifier(X, y, None, \"bias-variance-full-depth\")\n\n\n\n\n\n\n\n\n\ncreate_graph(dt_bias_variance_full_depth, \"bias-variance-full-depth-sklearn\", feature_names=['x1', 'x2'])\n\n\n\n\n\n\n\n\nBias-Variance Tradeoff - Dataset II\n\n# Bias variance dataset 2\n# X is all integers from (1, 1) to (6, 6)\nX = np.array([[i, j] for i in range(1, 7) for j in range(1, 7)])\ny = np.zeros(len(X), dtype=int)\ny[(2 &lt;= X[:, 0]) & (X[:, 0] &lt;= 5) & (2 &lt;= X[:, 1]) & (X[:, 1] &lt;= 5)] = 1\nplt.scatter(X[:, 0], X[:, 1], c=y)\n\n\n\n\n\n\n\n\n\nspecial_condition = (X[:, 0] == 3) & (X[:, 1] == 3) | (X[:, 0] == 4) & (X[:, 1] == 4)\ny[special_condition] = 0\n\nplt.scatter(X[:, 0], X[:, 1], c=y) \nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset-2.pdf\")\n\n\n\n\n\n\n\n\n\n# X_test random uniform frmo (1, 1) to (6, 6) of size 1000\nX_test = np.random.uniform(1, 6, size=(1000, 2))\ny_test = np.zeros(len(X_test), dtype=int)\ny_test[(2 &lt;= X_test[:, 0]) & (X_test[:, 0] &lt;= 5) & (2 &lt;= X_test[:, 1]) & (X_test[:, 1] &lt;= 5)] = 1\n\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, alpha=0.1)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset-2-test.pdf\")\n\n\n\n\n\n\n\n\n\ndef create_DT_Classifier_with_graph(X,y,depth,filename):\n    dt = DecisionTreeClassifier(max_depth=depth)\n    dt.fit(X, y)\n\n    # Predict in entire 2d space and contour plot\n    x1 = np.linspace(0.5, 6.5, 100)\n    x2 = np.linspace(0.5, 6.5, 100)\n\n    X1, X2 = np.meshgrid(x1, x2)\n    X_contour = np.stack([X1.flatten(), X2.flatten()], axis=1)\n    y_contour = dt.predict(X_contour)\n\n    plt.scatter(X[:, 0], X[:, 1], c=y)\n    plt.contourf(X1, X2, y_contour.reshape(X1.shape), alpha=0.1, cmap='coolwarm')\n    format_axes(plt.gca())\n    plt.savefig(f\"../figures/decision-trees/{filename}.pdf\")\n\n    # Export tree\n    dot_data = export_graphviz(dt, out_file=None, feature_names=['x1', 'x2'], filled=True)\n    graph = graphviz.Source(dot_data)\n    graph.format = 'pdf'\n    graph.render(f\"../figures/decision-trees/{filename}-sklearn\")\n\n\n#Underfitting\ncreate_DT_Classifier_with_graph(X, y, 2, \"bias-variance-depth-2\")\n\n\n\n\n\n\n\n\n\n#Overfitting\ncreate_DT_Classifier_with_graph(X, y, None, \"bias-variance-full-depth\")\n\n\n\n\n\n\n\n\n\n#Good Fit\ncreate_DT_Classifier_with_graph(X, y, 4, \"bias-variance-good-fit\")\n\n\n\n\n\n\n\n\nTest Accuracies\n\nfrom sklearn.metrics import accuracy_score\n### Train and test accuracy vs depth\ndepths = np.arange(2, 10)\ntrain_accs = {}\ntest_accs = {}\nfor depth in depths:\n    dt = DecisionTreeClassifier(max_depth=depth)\n    dt.fit(X, y)\n    train_accs[depth] = accuracy_score(y, dt.predict(X))\n    test_accs[depth] = accuracy_score(y_test, dt.predict(X_test))\n\n\ntrain_accs = pd.Series(train_accs)\ntest_accs = pd.Series(test_accs)\n\n\ntrain_accs\n\n2    0.722222\n3    0.833333\n4    0.944444\n5    0.944444\n6    0.944444\n7    0.944444\n8    0.944444\n9    0.944444\ndtype: float64\n\n\n\n\n\nax = train_accs.plot(label='Train')\ntest_accs.plot(label='Test', ax=ax)\nplt.xlabel(\"Depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.ylim(0, 1.1)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth.pdf\")\n\n# Highlight area of underfitting (depth &lt; 4) fill with green \nplt.fill_between(depths, 0, 1, where=depths &lt;= 4, color='g', alpha=0.1, label='Underfitting')\nplt.legend()\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth-underfitting.pdf\")\n\n\n# Highlight area of overfitting (depth &gt;7 4) fill with red\nplt.fill_between(depths, 0, 1, where=depths &gt;= 7, color='r', alpha=0.1, label='Overfitting')\nplt.legend()\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth-overfitting.pdf\")\n\n\n# Highlight good fit area (4 &lt; depth &lt; 7) fill with blue\nplt.fill_between(depths, 0, 1, where=(depths &gt;= 4) & (depths &lt;= 7), color='b', alpha=0.1, label='Good fit')\nplt.legend()\nplt.savefig(\"../figures/decision-trees/bias-variance-accuracy-vs-depth-good-fit.pdf\")\n\n\n\n\n\n\n\n\n\n# Slight variation of the dataset leads to a completely different tree\ny = np.zeros(len(X), dtype=int)\ny[(2 &lt;= X[:, 0]) & (X[:, 0] &lt;= 5) & (2 &lt;= X[:, 1]) & (X[:, 1] &lt;= 5)] = 1\nspecial_condition = (X[:, 0] == 3) & (X[:, 1] == 3) | (X[:, 0] == 4) & (X[:, 1] == 3)\ny[special_condition] = 0\n\nplt.scatter(X[:, 0], X[:, 1], c=y)\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/bias-variance-dataset-2-2.pdf\")\n\n\n\n\n\n\n\n\n\ncreate_DT_Classifier_with_graph(X, y, None, \"bias-variance-full-depth-2\")"
  },
  {
    "objectID": "notebooks/pca.html",
    "href": "notebooks/pca.html",
    "title": "PCA",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n%matplotlib inline\n# Retina \n%config InlineBackend.figure_format = 'retina'\n\n\nN = 500\nD = 2\n\ntorch.manual_seed(1)\nX = torch.distributions.MultivariateNormal(torch.zeros(D), torch.tensor([[2.0, 0.5], [0.5, 0.2]])).sample((N,))\n\nX = X - X.mean(dim=0)\nplt.scatter(X[:,0], X[:,1])\nplt.gca().set_aspect('equal', adjustable='box')\n\n\n\n\n\n\n\n\n\ncovariance_matrix = torch.matmul(X.t(), X) / (X.size(0)-1)\ncovariance_matrix.shape\n\ntorch.Size([2, 2])\n\n\n\ncovariance_matrix\n\ntensor([[1.9740, 0.5022],\n        [0.5022, 0.2127]])\n\n\n\neigenvalues, eigenvectors = torch.linalg.eig(covariance_matrix)\n\n\neigenvalues = eigenvalues.real\neigenvalues\n\ntensor([2.1071, 0.0796])\n\n\n\neigenvectors = eigenvectors.real\neigenvectors\n\ntensor([[ 0.9666, -0.2562],\n        [ 0.2562,  0.9666]])\n\n\n\n# Plot the eigenvectors along with length of eigenvalues\nplt.scatter(X[:,0], X[:,1])\nplt.gca().set_aspect('equal', adjustable='box')\nplt.quiver(0, 0, eigenvectors[0, 0], eigenvectors[1, 0], scale=eigenvalues[0].sqrt().item(), color='red', label='Eigenvector 1')\nplt.quiver(0, 0, eigenvectors[0, 1], eigenvectors[1, 1], scale=eigenvalues[1].sqrt().item(), color='green', label='Eigenvector 2')\nplt.legend()\n\n\n\n\n\n\n\n\n\nk = 1\nZ = torch.matmul(X, eigenvectors[:, :k])\nZ.shape\n\ntorch.Size([500, 1])\n\n\n\nplt.plot(Z, torch.zeros_like(Z), 'o')\n\n\n\n\n\n\n\n\n\n# Step 4: Sort eigenvectors based on eigenvalues\nsorted_indices = torch.argsort(eigenvalues[:, 0], descending=True)\nsorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\nCell In[93], line 2\n      1 # Step 4: Sort eigenvectors based on eigenvalues\n----&gt; 2 sorted_indices = torch.argsort(eigenvalues[:, 0], descending=True)\n      3 sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\nIndexError: too many indices for tensor of dimension 1\n\n\n\n\nsorted\n\n\nX.mean(dim=0), X.var(dim=0)\n\n(tensor([-5.7220e-09,  0.0000e+00]), tensor([1.9740, 0.2127]))\n\n\n\nk = 1  # Number of principal components\nB = nn.Parameter(torch.randn(D, k))\n\n\nB.shape, X.shape\n\n(torch.Size([2, 1]), torch.Size([500, 2]))\n\n\n\nmse_loss = nn.MSELoss()\n\noptimizer = optim.Adam([B], lr=0.001) \nnum_epochs = 2000  # Number of optimization epochs\n\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    \n    # Project data onto the current projection matrix\n    z = torch.matmul(X, B)\n    x_reconstructed = torch.matmul(z, B.t())\n    \n    # Compute reconstruction loss\n    loss = mse_loss(x_reconstructed, X)\n    \n    # Perform backpropagation and optimization step\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch + 1) % 100 == 0:\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n\nEpoch [100/2000], Loss: 0.7209210395812988\nEpoch [200/2000], Loss: 0.6048457026481628\nEpoch [300/2000], Loss: 0.5128658413887024\nEpoch [400/2000], Loss: 0.42206627130508423\nEpoch [500/2000], Loss: 0.3365266025066376\nEpoch [600/2000], Loss: 0.26167812943458557\nEpoch [700/2000], Loss: 0.2000642865896225\nEpoch [800/2000], Loss: 0.1518339067697525\nEpoch [900/2000], Loss: 0.11567910760641098\nEpoch [1000/2000], Loss: 0.08960824459791183\nEpoch [1100/2000], Loss: 0.07147730886936188\nEpoch [1200/2000], Loss: 0.059300925582647324\nEpoch [1300/2000], Loss: 0.0514017716050148\nEpoch [1400/2000], Loss: 0.0464538112282753\nEpoch [1500/2000], Loss: 0.04346401244401932\nEpoch [1600/2000], Loss: 0.04172360897064209\nEpoch [1700/2000], Loss: 0.0407492071390152\nEpoch [1800/2000], Loss: 0.040225498378276825\nEpoch [1900/2000], Loss: 0.03995582088828087\nEpoch [2000/2000], Loss: 0.039823081344366074\n\n\n\n# Project data onto the learned projection matrix\nZ = torch.matmul(X, B)\n\n\nB\n\nParameter containing:\ntensor([[-0.9645],\n        [-0.2663]], requires_grad=True)\n\n\n\ndef train_linear_autoencoder(X, K=2, lr=0.001, num_epochs=2000):\n    D = X.size(1)\n    B = nn.Parameter(torch.randn(D, K))\n    \n    mse_loss = nn.MSELoss()\n    optimizer = optim.Adam([B], lr=lr)\n    num_epochs = num_epochs\n    \n    for epoch in range(num_epochs):\n        optimizer.zero_grad()\n        \n        z = torch.matmul(X, B)\n        x_reconstructed = torch.sigmoid(torch.matmul(z, B.t()))\n        \n        loss = mse_loss(x_reconstructed, X)\n        \n        loss.backward()\n        optimizer.step()\n        \n        if (epoch + 1) % 100 == 0:\n            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}')\n    \n    return B\n\n\n# Get MNIST data\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\nmnist_train = datasets.MNIST(root='~/.datasets', train=True, transform=ToTensor(), download=True)\n\n\nX = mnist_train.data.float()\nX = X.view(X.size(0), -1)\nX = X / 255.0\n\nX.shape\n\ntorch.Size([60000, 784])\n\n\n\n# Work with first 1000 samples\nX = X[:3000]\n\n\nX.min(), X.max()\n\n(tensor(0.), tensor(1.))\n\n\n\n# Plot the first 10 images\nfig, ax = plt.subplots(1, 10, figsize=(10, 1))\nfor i in range(10):\n    ax[i].imshow(X[i].view(28, 28).numpy(), cmap='gray')\n    ax[i].axis('off')\n\n\n\n\n\n\n\n\n\nB = train_linear_autoencoder(X, K=5, lr=0.001, num_epochs=2000)\n\nEpoch [100/2000], Loss: 0.4135352075099945\nEpoch [200/2000], Loss: 0.3644017279148102\nEpoch [300/2000], Loss: 0.3089104890823364\nEpoch [400/2000], Loss: 0.26013466715812683\nEpoch [500/2000], Loss: 0.22550225257873535\nEpoch [600/2000], Loss: 0.20584215223789215\nEpoch [700/2000], Loss: 0.19726672768592834\nEpoch [800/2000], Loss: 0.1943318247795105\nEpoch [900/2000], Loss: 0.1914585679769516\nEpoch [1000/2000], Loss: 0.18916504085063934\nEpoch [1100/2000], Loss: 0.18805715441703796\nEpoch [1200/2000], Loss: 0.18697765469551086\nEpoch [1300/2000], Loss: 0.18595127761363983\nEpoch [1400/2000], Loss: 0.18386924266815186\nEpoch [1500/2000], Loss: 0.1826687455177307\nEpoch [1600/2000], Loss: 0.18168936669826508\nEpoch [1700/2000], Loss: 0.18077248334884644\nEpoch [1800/2000], Loss: 0.17985987663269043\nEpoch [1900/2000], Loss: 0.17898057401180267\nEpoch [2000/2000], Loss: 0.17811761796474457\n\n\n\nB.shape\n\ntorch.Size([784, 5])\n\n\n\n# Project data onto the 2d space\nwith torch.no_grad():\n    Z = torch.matmul(X, B)\n    X_reconstructed = torch.sigmoid(torch.matmul(Z, B.t()))\n\n\n\n# Plot the first 10 images and their reconstructions\nfig, ax = plt.subplots(2, 10, figsize=(10, 2))\nfor i in range(10):\n    ax[0, i].imshow(X[i].view(28, 28).numpy(), cmap='gray')\n    ax[0, i].axis('off')\n    ax[1, i].imshow(X_reconstructed[i].view(28, 28).numpy(), cmap='gray')\n    ax[1, i].axis('off')\n\n\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=32)\n\nZ_pca = pca.fit_transform(X.numpy())\n\nX_reconstructed_pca = pca.inverse_transform(Z_pca)\n\n# Plot the first 10 images and their reconstructions\nfig, ax = plt.subplots(2, 10, figsize=(10, 2))\nfor i in range(10):\n    ax[0, i].imshow(X[i].view(28, 28).numpy(), cmap='gray')\n    ax[0, i].axis('off')\n    ax[1, i].imshow(X_reconstructed_pca[i].reshape(28, 28), cmap='gray')\n    ax[1, i].axis('off')\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\ndf = pd.DataFrame({\"X\": X[0], \"X_reconstructed\": X_reconstructed[0], \"X_reconstructed_pca\": X_reconstructed_pca[0]})\n\n\ndf.plot()\n\n\n\n\n\n\n\n\n\n# Plot the original data and the projected data\nplt.figure(figsize=(6, 6))\nplt.scatter(X[:,0], X[:,1], label='Original data')\nplt.scatter(Z[:,0], Z[:,0], label='Projected data')\n\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[25], line 4\n      2 plt.figure(figsize=(6, 6))\n      3 plt.scatter(X[:,0], X[:,1], label='Original data')\n----&gt; 4 plt.scatter(Z[:,0], Z[:,0], label='Projected data')\n\nFile ~/miniconda3/lib/python3.9/site-packages/matplotlib/pyplot.py:3684, in scatter(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, data, **kwargs)\n   3665 @_copy_docstring_and_deprecators(Axes.scatter)\n   3666 def scatter(\n   3667     x: float | ArrayLike,\n   (...)\n   3682     **kwargs,\n   3683 ) -&gt; PathCollection:\n-&gt; 3684     __ret = gca().scatter(\n   3685         x,\n   3686         y,\n   3687         s=s,\n   3688         c=c,\n   3689         marker=marker,\n   3690         cmap=cmap,\n   3691         norm=norm,\n   3692         vmin=vmin,\n   3693         vmax=vmax,\n   3694         alpha=alpha,\n   3695         linewidths=linewidths,\n   3696         edgecolors=edgecolors,\n   3697         plotnonfinite=plotnonfinite,\n   3698         **({\"data\": data} if data is not None else {}),\n   3699         **kwargs,\n   3700     )\n   3701     sci(__ret)\n   3702     return __ret\n\nFile ~/miniconda3/lib/python3.9/site-packages/matplotlib/__init__.py:1465, in _preprocess_data.&lt;locals&gt;.inner(ax, data, *args, **kwargs)\n   1462 @functools.wraps(func)\n   1463 def inner(ax, *args, data=None, **kwargs):\n   1464     if data is None:\n-&gt; 1465         return func(ax, *map(sanitize_sequence, args), **kwargs)\n   1467     bound = new_sig.bind(ax, *args, **kwargs)\n   1468     auto_label = (bound.arguments.get(label_namer)\n   1469                   or bound.kwargs.get(label_namer))\n\nFile ~/miniconda3/lib/python3.9/site-packages/matplotlib/axes/_axes.py:4649, in Axes.scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\n   4646 x, y = self._process_unit_info([(\"x\", x), (\"y\", y)], kwargs)\n   4647 # np.ma.ravel yields an ndarray, not a masked array,\n   4648 # unless its argument is a masked array.\n-&gt; 4649 x = np.ma.ravel(x)\n   4650 y = np.ma.ravel(y)\n   4651 if x.size != y.size:\n\nFile ~/miniconda3/lib/python3.9/site-packages/numpy/ma/core.py:6773, in _frommethod.__call__(self, a, *args, **params)\n   6770     args = list(args)\n   6771     a, args[0] = args[0], a\n-&gt; 6773 marr = asanyarray(a)\n   6774 method_name = self.__name__\n   6775 method = getattr(type(marr), method_name, None)\n\nFile ~/miniconda3/lib/python3.9/site-packages/numpy/ma/core.py:8005, in asanyarray(a, dtype)\n   8003 if isinstance(a, MaskedArray) and (dtype is None or dtype == a.dtype):\n   8004     return a\n-&gt; 8005 return masked_array(a, dtype=dtype, copy=False, keep_mask=True, subok=True)\n\nFile ~/miniconda3/lib/python3.9/site-packages/numpy/ma/core.py:2826, in MaskedArray.__new__(cls, data, mask, dtype, copy, subok, ndmin, fill_value, keep_mask, hard_mask, shrink, order)\n   2817 \"\"\"\n   2818 Create a new masked array from scratch.\n   2819 \n   (...)\n   2823 \n   2824 \"\"\"\n   2825 # Process data.\n-&gt; 2826 _data = np.array(data, dtype=dtype, copy=copy,\n   2827                  order=order, subok=True, ndmin=ndmin)\n   2828 _baseclass = getattr(data, '_baseclass', type(_data))\n   2829 # Check that we're not erasing the mask.\n\nFile ~/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:1030, in Tensor.__array__(self, dtype)\n   1028     return handle_torch_function(Tensor.__array__, (self,), self, dtype=dtype)\n   1029 if dtype is None:\n-&gt; 1030     return self.numpy()\n   1031 else:\n   1032     return self.numpy().astype(dtype, copy=False)\n\nRuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
  },
  {
    "objectID": "notebooks/Maths for ML-2.html",
    "href": "notebooks/Maths for ML-2.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nplt.get_cmap('gnuplot2')\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2 + Y**2)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Filled Contours Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-1.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-2.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(np.abs(X) + np.abs(Y))\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-3.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-2.0, 8.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(Y*(X**2))\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-4.eps\", format='eps',transparent=True)\nplt.show()\n\nC:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in sqrt\n  after removing the cwd from sys.path.\n\n\n\n\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X*Y)\nfig,ax=plt.subplots(1,1)\ncp = ax.contour(X, Y, Z)\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\nplt.savefig(\"contour-plot-5.eps\", format='eps',transparent=True)\nplt.show()\n\nC:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in sqrt\n  after removing the cwd from sys.path.\n\n\n\n\n\n\n\n\n\n\nfrom matplotlib.pyplot import cm\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\n# Contour Plot\nX, Y = np.mgrid[-1:1:100j, -1:1:100j]\nZ = X**2 + Y**2 \ncp = plt.contour(X, Y, Z)\n# cb = plt.colorbar(cp)\n\n# Vector Field\nY, X = np.mgrid[-1:1:30j, -1:1:30j]\nU = 2*X\nV = 2*Y\nspeed = np.sqrt(U**2 + V**2)\nUN = U/speed\nVN = V/speed\nquiv = plt.quiver(X, Y, UN, VN,  # assign to var\n           color='Teal', \n           headlength=7)\nplt.savefig(\"gradient-field.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nxlist = np.linspace(-5.0, 5.0, 100)\nylist = np.linspace(-5.0, 5.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2+ Y**2)\n\nfig, ax=plt.subplots(1,1,figsize=(4,4))\ncp = ax.contour(X, Y, Z,levels=[1,2,3,4,5,6,7,8,9])\n\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\n\nd = np.linspace(0.11,4.5,600)\nplt.plot(d,0.5/d)\n# ax.arrow(1.13 , 1/1.13**2, 0, 0, head_width=0.05, head_length=0.1, fc='k', ec='k')\nplt.savefig(\"contour-plot-6.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\nfor i in np.linspace(0,2,100):\n    print (i,0.5 - i**2 + 1/i**2)\n\n0.0 inf\n0.020202020202020204 2450.7495918783793\n0.04040404040404041 613.0608675135189\n0.06060606060606061 272.74632690541773\n0.08080808080808081 153.63409505407608\n0.10101010101010102 98.4997969594939\n0.12121212121212122 68.54780762167124\n0.14141414141414144 50.4851040814244\n0.16161616161616163 38.75903646630445\n0.18181818181818182 30.71694214876033\n0.20202020202020204 24.96168783797571\n0.22222222222222224 20.700617283950617\n0.24242424242424243 17.45685548668503\n0.26262626262626265 14.929548156238129\n0.2828282828282829 12.92128367263648\n0.30303030303030304 11.298172635445361\n0.32323232323232326 9.966809927717833\n0.3434343434343435 8.860426554171964\n0.36363636363636365 7.930268595041323\n0.38383838383838387 7.1400642169759925\n0.4040404040404041 6.462376351902866\n0.42424242424242425 5.876140814452502\n0.4444444444444445 5.364969135802469\n0.4646464646464647 4.9159562148764175\n0.48484848484848486 4.518828196740127\n0.5050505050505051 4.165323987348229\n0.5252525252525253 3.848739962230637\n0.5454545454545455 3.5635904499540856\n0.5656565656565657 3.305351527280638\n0.5858585858585859 3.0702655556635308\n0.6060606060606061 2.8551905417814507\n0.6262626262626263 2.65748294812874\n0.6464646464646465 2.474905726496339\n0.6666666666666667 2.305555555555555\n0.686868686868687 2.1478048326048214\n0.7070707070707072 2.0002550968351827\n0.7272727272727273 1.8616993801652892\n0.7474747474747475 1.7310915822381832\n0.7676767676767677 1.6075214108402638\n0.787878787878788 1.4901937611727818\n0.8080808080808082 1.3784116576114678\n0.8282828282828284 1.27156207154134\n0.8484848484848485 1.1691040741365415\n0.8686868686868687 1.0705588948578604\n0.888888888888889 0.9755015432098765\n0.9090909090909092 0.8835537190082641\n0.9292929292929294 0.7943777895623855\n0.9494949494949496 0.7076716541475031\n0.9696969696969697 0.6231643494605139\n0.98989898989899 0.5406122763442311\n1.0101010101010102 0.4597959493929188\n1.0303030303030305 0.3805171882397417\n1.0505050505050506 0.302596683242079\n1.0707070707070707 0.22587187959584054\n1.090909090909091 0.15019513314967814\n1.1111111111111112 0.07543209876543189\n1.1313131313131315 0.0014603183062319447\n1.1515151515151516 -0.07183201951522311\n1.1717171717171717 -0.14454717092494984\n1.191919191919192 -0.2167788004559923\n1.2121212121212122 -0.2886128328741967\n1.2323232323232325 -0.36012820815496915\n1.2525252525252526 -0.4313975519179223\n1.272727272727273 -0.5024877719682923\n1.292929292929293 -0.5734605901083917\n1.3131313131313131 -0.6443730171236\n1.3333333333333335 -0.7152777777777782\n1.3535353535353536 -0.7862236917418948\n1.373737373737374 -0.8572560156014735\n1.393939393939394 -0.9284167504222499\n1.4141414141414144 -0.9997449187817161\n1.4343434343434345 -1.0712768146824043\n1.4545454545454546 -1.143046229338843\n1.474747474747475 -1.2150846554637709\n1.494949494949495 -1.2874214723620951\n1.5151515151515154 -1.3600841138659328\n1.5353535353535355 -1.4330982209048717\n1.5555555555555556 -1.5064877802973042\n1.575757575757576 -1.58027525116686\n1.595959595959596 -1.6544816802290594\n1.6161616161616164 -1.729126807054128\n1.6363636363636365 -1.8042291602897669\n1.6565656565656568 -1.8798061457204203\n1.676767676767677 -1.9558741269450486\n1.696969696969697 -2.032448499372201\n1.7171717171717173 -2.1095437581575784\n1.7373737373737375 -2.187173560644274\n1.7575757575757578 -2.2653507838082487\n1.777777777777778 -2.344087577160494\n1.7979797979797982 -2.423395411511965\n1.8181818181818183 -2.503285123966943\n1.8383838383838385 -2.583766959474586\n1.8585858585858588 -2.664850609236279\n1.878787878787879 -2.7465452462378015\n1.8989898989898992 -2.828859558149688\n1.9191919191919193 -2.9118017778162164\n1.9393939393939394 -2.9953797115329435\n1.9595959595959598 -3.079600765294187\n1.97979797979798 -3.1644719691753447\n2.0 -3.25\n\n\nC:\\Users\\Hp\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in double_scalars\n  \n\n\n\nxlist = np.linspace(-2.0, 2.0, 100)\nylist = np.linspace(-2.0, 2.0, 100)\nX, Y = np.meshgrid(xlist, ylist)\nZ = np.sqrt(X**2+ Y**2)\n\nfig, ax=plt.subplots(1,1,figsize=(4,4))\ncontours = ax.contour(X, Y, Z,levels=[0.1,0.2,.3,.4,.5,.6,.7,.8,.9,1])#,levels=[.5**(.5),2,3,4,5,6,7,8,9])\nplt.clabel(contours, inline=True, fontsize=8)\nax.clabel(cp, inline=1, fontsize=10)\n# fig.colorbar(cp) # Add a colorbar to a plot\nax.set_title('Contour Plot')\n#ax.set_xlabel('x (cm)')\n# ax.set_ylabel('y (cm)')\n\nd = np.linspace(-4,5,600)\nplt.plot(d,1-d)\nplt.xlim(-1,1)\nplt.ylim(-1,1)\n# ax.arrow(1.13 , 1/1.13**2, 0, 0, head_width=0.05, head_length=0.1, fc='k', ec='k')\nplt.savefig(\"contour-plot-7.eps\", format='eps',transparent=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n(2*0.5**2)**(.5)\n\n0.7071067811865476"
  },
  {
    "objectID": "notebooks/boosting-explanation.html",
    "href": "notebooks/boosting-explanation.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom math import sqrt\nSPINE_COLOR = 'gray'\n\n\nx = np.linspace(0.04, 0.51, 1000)\n\n\ndef latexify(fig_width=None, fig_height=None, columns=1):\n    \"\"\"Set up matplotlib's RC params for LaTeX plotting.\n    Call this before plotting a figure.\n\n    Parameters\n    ----------\n    fig_width : float, optional, inches\n    fig_height : float,  optional, inches\n    columns : {1, 2}\n    \"\"\"\n\n    # code adapted from http://www.scipy.org/Cookbook/Matplotlib/LaTeX_Examples\n\n    # Width and max height in inches for IEEE journals taken from\n    # computer.org/cms/Computer.org/Journal%20templates/transactions_art_guide.pdf\n\n    assert(columns in [1,2])\n\n    if fig_width is None:\n        fig_width = 3.39 if columns==1 else 6.9 # width in inches\n\n    if fig_height is None:\n        golden_mean = (sqrt(5)-1.0)/2.0    # Aesthetic ratio\n        fig_height = fig_width*golden_mean # height in inches\n\n    MAX_HEIGHT_INCHES = 8.0\n    if fig_height &gt; MAX_HEIGHT_INCHES:\n        print(\"WARNING: fig_height too large:\" + fig_height + \n              \"so will reduce to\" + MAX_HEIGHT_INCHES + \"inches.\")\n        fig_height = MAX_HEIGHT_INCHES\n\n    params = {'backend': 'ps',\n              'axes.labelsize': 8, # fontsize for x and y labels (was 10)\n              'axes.titlesize': 8,\n              'legend.fontsize': 8, # was 10\n              'xtick.labelsize': 8,\n              'ytick.labelsize': 8,\n              'text.usetex': True,\n              'figure.figsize': [fig_width,fig_height],\n              'font.family': 'serif'\n    }\n\n    matplotlib.rcParams.update(params)\n\n\ndef format_axes(ax):\n\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n\n    for spine in ['left', 'bottom']:\n        ax.spines[spine].set_color(SPINE_COLOR)\n        ax.spines[spine].set_linewidth(0.5)\n\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n    for axis in [ax.xaxis, ax.yaxis]:\n        axis.set_tick_params(direction='out', color=SPINE_COLOR)\n\n    return ax\n\n\nplt.plot(x, 0.5*np.log((1-x)/x), color='k', linewidth=3)\nplt.axvline(0.5, color='r', linewidth=0.9)\nplt.axhline(0.0, color='r', linewidth=0.9)\nplt.xlabel(r\"$err_m$\")\nplt.ylabel(r\"$\\alpha_m$\")\n\nText(0, 0.5, '$\\\\alpha_m$')\n\n\n\n\n\n\n\n\n\n\nlatexify()\nplt.rcParams.update({'font.size': 40})\nplt.plot(x, 0.5*np.log((1-x)/x), color='k', linewidth=2)\nplt.axvline(0.5, color='r', linewidth=0.9)\nplt.axhline(0.0, color='r', linewidth=0.9)\nplt.xlabel(r\"$err_m$\")\nplt.ylabel(r\"$\\alpha_m$\")\nformat_axes(plt.gca())\nplt.savefig(\"../figures/ensemble/alpha-boosting.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nlatexify()\nplt.rcParams.update({'font.size': 40})\nplt.plot(x, np.exp(0.5*np.log((1-x)/x)), color='r', linewidth=2, label=r'$e^{\\alpha_m}$ ')\nplt.plot(x, np.exp(-0.5*np.log((1-x)/x)), color='g', linewidth=2, label=r'$e^{-\\alpha_m}$')\n\nplt.xlabel(r\"$err_m$\")\nplt.ylabel(\"Weight Multiplier\")\nformat_axes(plt.gca())\nplt.legend()\nplt.savefig(\"../figures/ensemble/alpha-boosting-weight.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "notebooks/Sklearn_on_GPU.html",
    "href": "notebooks/Sklearn_on_GPU.html",
    "title": "Sklearn on GPU",
    "section": "",
    "text": "https://scikit-learn.org/stable/modules/array_api.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\nhttps://github.com/data-apis/array-api-compat\nhttps://labs.quansight.org/blog/array-api-support-scikit-learn"
  },
  {
    "objectID": "notebooks/Sklearn_on_GPU.html#references",
    "href": "notebooks/Sklearn_on_GPU.html#references",
    "title": "Sklearn on GPU",
    "section": "",
    "text": "https://scikit-learn.org/stable/modules/array_api.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\nhttps://github.com/data-apis/array-api-compat\nhttps://labs.quansight.org/blog/array-api-support-scikit-learn"
  },
  {
    "objectID": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html",
    "href": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html",
    "title": "Introduction to Neural Networks",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F"
  },
  {
    "objectID": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#imports",
    "href": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#imports",
    "title": "Introduction to Neural Networks",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F"
  },
  {
    "objectID": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#convention",
    "href": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#convention",
    "title": "Introduction to Neural Networks",
    "section": "Convention",
    "text": "Convention\n\nn0 = 3\nn1 = 2\nlayer = nn.Linear(n0, n1)\nlayer\n\nLinear(in_features=3, out_features=2, bias=True)\n\n\n\nlayer.weight.shape\n\ntorch.Size([2, 3])\n\n\n\nlayer.bias.shape\n\ntorch.Size([2])\n\n\n\nfor i in range(n0):\n    print(layer.weight[0, i])\n\ntensor(-0.0279, grad_fn=&lt;SelectBackward0&gt;)\ntensor(0.0743, grad_fn=&lt;SelectBackward0&gt;)\ntensor(-0.1339, grad_fn=&lt;SelectBackward0&gt;)\n\n\n\nA 2-layer network\n\nmlp_2layers = nn.Sequential(\n    nn.Linear(4, 3),\n    nn.ReLU(),\n    nn.Linear(3, 2),\n    nn.ReLU(),\n    nn.Linear(2, 1)\n)\n\n\nparams = dict(mlp_2layers.named_parameters())\n{name: param.shape for name, param in params.items()}\n\n{'0.weight': torch.Size([3, 4]),\n '0.bias': torch.Size([3]),\n '2.weight': torch.Size([2, 3]),\n '2.bias': torch.Size([2]),\n '4.weight': torch.Size([1, 2]),\n '4.bias': torch.Size([1])}"
  },
  {
    "objectID": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#vectorization-with-torch.vmap",
    "href": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#vectorization-with-torch.vmap",
    "title": "Introduction to Neural Networks",
    "section": "Vectorization with torch.vmap",
    "text": "Vectorization with torch.vmap\n\nfrom jaxtyping import Float\nfrom torch import Tensor\nfrom beartype import beartype\n\n\nQuick intro to jaxtyping\n\nScalars\n\nscalar_type = Float[Tensor, \"\"]\n\n\nscalar = torch.tensor(1.0)\nscalar.shape\n\ntorch.Size([])\n\n\n\nisinstance(scalar, scalar_type)\n\nTrue\n\n\n\nnon_scalar = torch.tensor([1.0])\nnon_scalar.shape\n\ntorch.Size([1])\n\n\n\nisinstance(non_scalar, scalar_type)\n\nFalse\n\n\n\n\nVectors\n\nvector_type = Float[Tensor, \"n0\"]\n\n\nisinstance(non_scalar, vector_type)\n\nTrue\n\n\n\nvector = torch.tensor([1.0, 2.0])\nvector.shape\n\ntorch.Size([2])\n\n\n\nisinstance(vector, vector_type)\n\nTrue\n\n\n\n\nMatrices\n\nmatrix_type = Float[Tensor, \"n0 n1\"]\n\n\nisinstance(vector, matrix_type)\n\nFalse\n\n\n\nmatrix = torch.tensor([[1.0, 2.0]])\nmatrix.shape\n\ntorch.Size([1, 2])\n\n\n\nisinstance(matrix, matrix_type)\n\nTrue\n\n\n\nanother_matrix = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nanother_matrix.shape\n\ntorch.Size([2, 3])\n\n\n\nisinstance(another_matrix, matrix_type)\n\nTrue\n\n\n\n\nTensors\n\ntensor_type = Float[Tensor, \"n0 n1 n2\"]\n\n\ntensor = torch.tensor([[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]])\ntensor.shape\n\ntorch.Size([2, 2, 2])\n\n\n\nisinstance(tensor, tensor_type)\n\nTrue\n\n\n\n\n\nQuick intro to beartype\n\ndef call_my_name(name):\n    return f\"Hello, {name}!\"\n\n\ncall_my_name(\"John\")\n\n'Hello, John!'\n\n\n\ncall_my_name(123)\n\n'Hello, 123!'\n\n\n\n@beartype\ndef secured_call_my_name(name: str) -&gt; str:\n    return f\"Hello, {name}!\"\n\n\nsecured_call_my_name(\"John\")\n\n'Hello, John!'\n\n\n\nsecured_call_my_name(123)\n\n\n---------------------------------------------------------------------------\nBeartypeCallHintParamViolation            Traceback (most recent call last)\nCell In[39], line 1\n----&gt; 1 secured_call_my_name(123)\n\nFile &lt;@beartype(__main__.secured_call_my_name) at 0x7fdbb0613ce0&gt;:28, in secured_call_my_name(__beartype_get_violation, __beartype_conf, __beartype_func, *args, **kwargs)\n\nBeartypeCallHintParamViolation: Function __main__.secured_call_my_name() parameter name=123 violates type hint &lt;class 'str'&gt;, as int 123 not instance of str.\n\n\n\n\n\nVectorization\nOn which dimensions should we apply the vectorization? - Current layer’s neurons size - Number of examples\n\nn = 50\na = torch.rand(n, n0)\na.shape\n\ntorch.Size([50, 3])\n\n\n\nactivation = F.relu\n\n@beartype\ndef forward(a: Float[Tensor, \"n0\"], w: Float[Tensor, \"n0\"], b: Float[Tensor, \"\"]) -&gt; Float[Tensor, \"\"]:\n    z = (a * w).sum() + b  # () + () -&gt; ()\n    a = activation(z)  # () -&gt; ()\n    return a  # ()\n\n\ndummy_a = torch.rand(n0)\ndummy_w = torch.rand(n0)\ndummy_b = torch.rand(())\nprint(dummy_a.shape, dummy_w.shape, dummy_b.shape)\n\ntorch.Size([3]) torch.Size([3]) torch.Size([])\n\n\n\nforward(dummy_a, dummy_w, dummy_b).shape\n\ntorch.Size([])\n\n\n\nforward(a[0], layer.weight[0], layer.bias[0])\n\ntensor(0.1544, grad_fn=&lt;ReluBackward0&gt;)\n\n\n\nVectorization over current layer’s neurons size\n\n\n\ninput\nshape in forward\nshape in vectorized forward\n\n\n\n\na\n[n0=3]\n[n0=3]\n\n\nw\n[n0=3]\n[n1=2, n0=3]\n\n\nb\n[]\n[n1=2]\n\n\noutput\n[]\n[n1=2]\n\n\n\n\nv1_forward = torch.vmap(forward, in_dims=(None, 0, 0), out_dims=0)\n\n\nlayer.weight.shape\n\ntorch.Size([2, 3])\n\n\n\nlayer.bias.shape\n\ntorch.Size([2])\n\n\n\nout = v1_forward(a[0], layer.weight, layer.bias)\nout.shape\n\ntorch.Size([2])\n\n\n\n\nVectorization over number of examples\n\n\n\ninput\nshape in forward\nshape in vectorized forward\n\n\n\n\na\n[n0=3]\n[n=50, n0=3]\n\n\nw\n[n1=2, n0=3]\n[n1=2, n0=3]\n\n\nb\n[n1=2]\n[n1=2]\n\n\noutput\n[n1=2]\n[n=50, n1=2]\n\n\n\n\nv2_forward = torch.vmap(v1_forward, in_dims=(0, None, None), out_dims=0)\n\n\nfinal_out = v2_forward(a, layer.weight, layer.bias)\nfinal_out.shape\n\ntorch.Size([50, 2])\n\n\n\n\nComparing with torch model forward pass\n\nlayer_out = F.relu(layer(a))\ntorch.allclose(final_out, layer_out)\n\nTrue"
  },
  {
    "objectID": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#xor-example",
    "href": "notebooks/nn_vectorization_with_vmap_jaxtyping_and_beartype_zeel.html#xor-example",
    "title": "Introduction to Neural Networks",
    "section": "XOR example",
    "text": "XOR example\n\nDefine inputs, outputs, weights and biases\n\nxor_x = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\nprint(xor_x)\nprint(xor_x.shape)\n\ntensor([[0., 0.],\n        [0., 1.],\n        [1., 0.],\n        [1., 1.]])\ntorch.Size([4, 2])\n\n\n\nxor_y = torch.tensor([[0.0], [1.0], [1.0], [0.0]])\nprint(xor_y)\nprint(xor_y.shape)\n\ntensor([[0.],\n        [1.],\n        [1.],\n        [0.]])\ntorch.Size([4, 1])\n\n\n\nW_1 = torch.tensor([[1.0, 1.0], [1.0, 1.0]])\nb_1 = torch.tensor([0.0, -1.0])\nprint(W_1)\nprint(b_1)\nprint(W_1.shape, b_1.shape)\n\ntensor([[1., 1.],\n        [1., 1.]])\ntensor([ 0., -1.])\ntorch.Size([2, 2]) torch.Size([2])\n\n\n\nW_2 = torch.tensor([[1.0, -2.0]])\nb_2 = torch.tensor([0.0])\nprint(W_2)\nprint(b_2)\nprint(W_2.shape, b_2.shape)\n\ntensor([[ 1., -2.]])\ntensor([0.])\ntorch.Size([1, 2]) torch.Size([1])\n\n\n\n\nForward pass\n\na1 = v2_forward(xor_x, W_1, b_1)\na1\n\ntensor([[0., 0.],\n        [1., 0.],\n        [1., 0.],\n        [2., 1.]])\n\n\n\na2 = v2_forward(a1, W_2, b_2)\na2\n\ntensor([[0.],\n        [1.],\n        [1.],\n        [0.]])\n\n\n\n\nForward pass with torch neural network\n\nmodel = nn.Sequential(nn.Linear(2, 2), nn.ReLU(), nn.Linear(2, 1))\nprint(model[0])\nprint(model[1])\nprint(model[2])\n\nLinear(in_features=2, out_features=2, bias=True)\nReLU()\nLinear(in_features=2, out_features=1, bias=True)\n\n\n\nmodel[0].weight.data = W_1\nmodel[0].bias.data = b_1\nmodel[2].weight.data = W_2\nmodel[2].bias.data = b_2\n\n\nmodel_out = model(xor_x)\nmodel_out\n\ntensor([[0.],\n        [1.],\n        [1.],\n        [0.]], grad_fn=&lt;AddmmBackward0&gt;)"
  },
  {
    "objectID": "notebooks/logistic-circular.html",
    "href": "notebooks/logistic-circular.html",
    "title": "Logistic Regression - Basis",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom latexify import *\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.patches as mpatches\n%config InlineBackend.figure_format = 'retina'\n\n\n# Choose some points between\n\n\nnp.random.seed(0)\nx1 = np.random.randn(1, 100)\nx2 = np.random.randn(1, 100)\n\n\ny = x1**2 + x2**2\n\n\nx1\n\narray([[ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799,\n        -0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ,\n         0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323,\n         0.33367433,  1.49407907, -0.20515826,  0.3130677 , -0.85409574,\n        -2.55298982,  0.6536186 ,  0.8644362 , -0.74216502,  2.26975462,\n        -1.45436567,  0.04575852, -0.18718385,  1.53277921,  1.46935877,\n         0.15494743,  0.37816252, -0.88778575, -1.98079647, -0.34791215,\n         0.15634897,  1.23029068,  1.20237985, -0.38732682, -0.30230275,\n        -1.04855297, -1.42001794, -1.70627019,  1.9507754 , -0.50965218,\n        -0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028,\n        -0.89546656,  0.3869025 , -0.51080514, -1.18063218, -0.02818223,\n         0.42833187,  0.06651722,  0.3024719 , -0.63432209, -0.36274117,\n        -0.67246045, -0.35955316, -0.81314628, -1.7262826 ,  0.17742614,\n        -0.40178094, -1.63019835,  0.46278226, -0.90729836,  0.0519454 ,\n         0.72909056,  0.12898291,  1.13940068, -1.23482582,  0.40234164,\n        -0.68481009, -0.87079715, -0.57884966, -0.31155253,  0.05616534,\n        -1.16514984,  0.90082649,  0.46566244, -1.53624369,  1.48825219,\n         1.89588918,  1.17877957, -0.17992484, -1.07075262,  1.05445173,\n        -0.40317695,  1.22244507,  0.20827498,  0.97663904,  0.3563664 ,\n         0.70657317,  0.01050002,  1.78587049,  0.12691209,  0.40198936]])\n\n\n\ny[y&gt;1] = 1\ny[y&lt;1] = 0\n\nc = 0\nfor i in range(100):\n    if y[0, i] == 1:\n        y[0, i] = 0\n        c += 1\n    if c == 10:\n        break\n\n\nlatexify()\nplt.scatter(x1, x2, c=y,s=5)\n\n\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch])\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.gca().set_aspect('equal')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/logistic-regression/logisitic-circular-data.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nnp.vstack((x1, x2)).shape\n\n(2, 100)\n\n\n\nclf_1 = LogisticRegression(penalty='none',solver='newton-cg')\nclf_1.fit(np.vstack((x1, x2)).T, y.T)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression(penalty='none', solver='newton-cg')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='none', solver='newton-cg')\n\n\n\nX = np.vstack((x1, x2)).T\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3\ny_min, y_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf_1.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nlatexify()\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\npink_patch = mpatches.Patch(color='darksalmon', label='Predict oranges')\nlblue_patch = mpatches.Patch(color='lightblue', label='Predict tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch, pink_patch, lblue_patch], loc='upper center',\n           bbox_to_anchor=(0.5, 1.25),\n          ncol=2, fancybox=True, shadow=True)\n\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.4)\nplt.gca().set_aspect('equal')\nplt.scatter(x1, x2, c=y,s=5)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.gca().set_aspect('equal')\nplt.savefig(\"../figures/logistic-regression/logisitic-linear-prediction.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nnew_x = np.zeros((4, 100))\n\n\nnew_x[0] = x1\nnew_x[1] = x2\nnew_x[2] = x1**2\nnew_x[3] = x2**2\n\n\nclf = LogisticRegression(penalty='none',solver='newton-cg')\n\n\nclf.fit(new_x.T, y.T)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\nLogisticRegression(penalty='none', solver='newton-cg')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='none', solver='newton-cg')\n\n\n\nclf.coef_\n\narray([[-0.50464855, -0.30337009,  1.08937351,  0.73697949]])\n\n\n\nnew_x.T[:, 0]\n\narray([ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799,\n       -0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ,\n        0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323,\n        0.33367433,  1.49407907, -0.20515826,  0.3130677 , -0.85409574,\n       -2.55298982,  0.6536186 ,  0.8644362 , -0.74216502,  2.26975462,\n       -1.45436567,  0.04575852, -0.18718385,  1.53277921,  1.46935877,\n        0.15494743,  0.37816252, -0.88778575, -1.98079647, -0.34791215,\n        0.15634897,  1.23029068,  1.20237985, -0.38732682, -0.30230275,\n       -1.04855297, -1.42001794, -1.70627019,  1.9507754 , -0.50965218,\n       -0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028,\n       -0.89546656,  0.3869025 , -0.51080514, -1.18063218, -0.02818223,\n        0.42833187,  0.06651722,  0.3024719 , -0.63432209, -0.36274117,\n       -0.67246045, -0.35955316, -0.81314628, -1.7262826 ,  0.17742614,\n       -0.40178094, -1.63019835,  0.46278226, -0.90729836,  0.0519454 ,\n        0.72909056,  0.12898291,  1.13940068, -1.23482582,  0.40234164,\n       -0.68481009, -0.87079715, -0.57884966, -0.31155253,  0.05616534,\n       -1.16514984,  0.90082649,  0.46566244, -1.53624369,  1.48825219,\n        1.89588918,  1.17877957, -0.17992484, -1.07075262,  1.05445173,\n       -0.40317695,  1.22244507,  0.20827498,  0.97663904,  0.3563664 ,\n        0.70657317,  0.01050002,  1.78587049,  0.12691209,  0.40198936])\n\n\n\nX = np.vstack((x1, x2)).T\nX.shape\n\n(100, 2)\n\n\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3\ny_min, y_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel(), np.square(xx.ravel()), np.square(yy.ravel())])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nlatexify()\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\npink_patch = mpatches.Patch(color='darksalmon', label='Predict oranges')\nlblue_patch = mpatches.Patch(color='lightblue', label='Predict tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch, pink_patch, lblue_patch], loc='upper center',\n           bbox_to_anchor=(0.5, 1.25),\n          ncol=2, fancybox=True, shadow=True)\n\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.4)\nplt.gca().set_aspect('equal')\nplt.scatter(x1, x2, c=y,s=5)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.gca().set_aspect('equal')\nplt.savefig(\"../figures/logistic-regression/logisitic-circular-prediction.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nZ.shape\n\n(261, 272)\n\n\n\nnp.c_[xx.ravel(), yy.ravel(), np.square(xx.ravel()), np.square(yy.ravel())]\n\narray([[-2.85298982, -2.52340315,  8.13955089,  6.36756347],\n       [-2.83298982, -2.52340315,  8.0258313 ,  6.36756347],\n       [-2.81298982, -2.52340315,  7.9129117 ,  6.36756347],\n       ...,\n       [ 2.52701018,  2.67659685,  6.38578047,  7.16417069],\n       [ 2.54701018,  2.67659685,  6.48726088,  7.16417069],\n       [ 2.56701018,  2.67659685,  6.58954129,  7.16417069]])\n\n\n\nxx.ravel()\n\narray([-2.85298982, -2.83298982, -2.81298982, ...,  2.52701018,\n        2.54701018,  2.56701018])\n\n\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - h, X[:, 0].max() + h\ny_min, y_max = X[:, 1].min() - h, X[:, 1].max() + h\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf.predict_proba(np.c_[xx.ravel(), yy.ravel(), np.square(xx.ravel()), np.square(yy.ravel())])\n# Put the result into a color plot\nZ = Z[:, 0].reshape(xx.shape)\nlatexify()\nplt.contourf(xx, yy, Z,levels=np.linspace(0, 1.1, num=10),cmap='Blues')\nplt.gca().set_aspect('equal')\n#plt.scatter(x1, x2, c=y)\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\nplt.colorbar(label='P(Tomatoes)')\nplt.savefig(\"../figures/logistic-regression/logisitic-circular-probability.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nxx.shape\n\n(233, 244)\n\n\n\nZ.size\n\n56852\n\n\n\nnp.linspace(0, 1.1, num=50)\n\narray([0.        , 0.02244898, 0.04489796, 0.06734694, 0.08979592,\n       0.1122449 , 0.13469388, 0.15714286, 0.17959184, 0.20204082,\n       0.2244898 , 0.24693878, 0.26938776, 0.29183673, 0.31428571,\n       0.33673469, 0.35918367, 0.38163265, 0.40408163, 0.42653061,\n       0.44897959, 0.47142857, 0.49387755, 0.51632653, 0.53877551,\n       0.56122449, 0.58367347, 0.60612245, 0.62857143, 0.65102041,\n       0.67346939, 0.69591837, 0.71836735, 0.74081633, 0.76326531,\n       0.78571429, 0.80816327, 0.83061224, 0.85306122, 0.8755102 ,\n       0.89795918, 0.92040816, 0.94285714, 0.96530612, 0.9877551 ,\n       1.01020408, 1.03265306, 1.05510204, 1.07755102, 1.1       ])"
  },
  {
    "objectID": "notebooks/autoregressive_model.html",
    "href": "notebooks/autoregressive_model.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "%pip install numpy pandas matplotlib statsmodels\n\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\nRequirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.47.2)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\nRequirement already satisfied: scipy!=1.9.2,&gt;=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.4)\nRequirement already satisfied: patsy&gt;=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.6)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy&gt;=0.5.4-&gt;statsmodels) (1.16.0)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n\n# Interactive widget\nfrom ipywidgets import interact\n\n\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n# Download CO2 data from NOAA\nurl = 'https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv'\n\nnames = 'year,month,decimal date,average,deseasonalized,ndays,sdev,unc'.split(',')\n\n# no index\ndf = pd.read_csv(url, skiprows=72, names=names, index_col=False)\ndf.index = pd.to_datetime(df[['year', 'month']].apply(lambda x: '{}-{}'.format(x[0], x[1]), axis=1))\n# Create X and y\n\n# X = months since first measurement\n\nX = np.array(range(len(df)))\ny = df.average.values\nplt.plot(X, y)\nplt.xlabel('Months since first measurement')\nplt.ylabel('CO2 Level')\n\nText(0, 0.5, 'CO2 Level')"
  },
  {
    "objectID": "notebooks/autoregressive_model.html#autocorrelation-function-acf",
    "href": "notebooks/autoregressive_model.html#autocorrelation-function-acf",
    "title": "Machine Learning Resources",
    "section": "Autocorrelation Function (ACF):",
    "text": "Autocorrelation Function (ACF):\nThe Autocorrelation Function (ACF) measures the correlation between a time series and its lagged values. It quantifies how well the previous observations at different lags explain the current observation. ## Partial Autocorrelation Function (PACF):\nThe Partial Autocorrelation Function (PACF) measures the correlation between a time series and its lagged values, while controlling for the intermediate lags. It helps identify the direct relationship between two time points, excluding the influence of other time points in between.\n\ny_co = pd.Series(y)\n\n\n\n# Plot the autocorrelation function (ACF) using statsmodels\nplt.figure(figsize=(10, 6))\nplot_acf(y_co, lags=80, title='Autocorrelation Plot')\nplt.xlabel('Lag')\nplt.ylabel('Autocorrelation')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_pacf\nplot_pacf(y_co, lags=50)\nplt.show()\n\n\n\n\n\n\n\n\n\nAuto Regressive Model\n\nWe convert the CO2 level time sequence into an input feature matrix and the corresponding output vector by setting a lag value. A sliding window of lag length will be used across the sequence to create the feature matrix and the output.\n\n# making the features using previous values according to lag\ndef make_dataset(y, X, lag):\n    X_train = []\n    y_train = []\n    months = []\n    for i in range(len(y)-lag):\n        row = y[i:i+lag]\n        y_train.append(y[i+lag])\n        X_train.append(row)\n\n        month_value = X[i+lag]\n        months.append(month_value)\n\n    X_train = pd.DataFrame(X_train)\n    y_train = pd.Series(y_train)\n    return X_train, y_train, months\n\n\n# adding the month value as a feature\ndef make_dataset_with_month(y, X, lag):\n    X_train = []\n    y_train = []\n    months_train = []\n\n    for i in range(len(y)-lag):\n        row = y[i:i+lag]\n        y_train.append(y[i+lag])\n        X_train.append(row)\n\n        month_value = X[i+lag]\n        months_train.append(month_value)\n\n    X_train = pd.DataFrame(X_train)\n    X_train[10] = months_train\n    y_train = pd.Series(y_train)\n\n    return X_train, y_train, months_train\n\n\n# example run of the model using a lag of 10\nlag = 10\nX_train, y_train, months_train = make_dataset(train_data, X_train_data, lag)\nreg = LinearRegression().fit(X_train, y_train)\nX_test, y_test, months_test = make_dataset(test_data,X_test_data, lag)\ny_hat = reg.predict(X_test)\nmse = mean_squared_error(y_test, y_hat)\n\nprint(f\"Mean Squared Error (MSE): {mse}\")\n\nMean Squared Error (MSE): 0.2099165801507042\n\n\n\n# Ploting the predicted output for lag=10 for the test sequence\nplt.plot(months_test, y_test, label='Actual Values (y_test)', marker='o')\nplt.plot(months_test, y_hat, label='Predicted Values (y_hat)', marker='o')\nplt.xlabel('Months Since first measurement')\nplt.ylabel('CO2 levels')\nplt.title('Actual vs Predicted Values')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Creating a custom class for training and testing the AR model for different lag values\nclass AR_model:\n\n  def __init__(self, lag, add_month = False):\n    self.reg = LinearRegression()\n    self.lag = lag\n    self.add_month = add_month\n\n  def train_AR_model(self, train_data, X_train_data):\n\n    if(self.add_month):\n      X_train, y_train, month_train = make_dataset_with_month(train_data, X_train_data, self.lag)\n      self.reg.fit(X_train, y_train)\n\n    else:\n      X_train, y_train, months_train = make_dataset(train_data, X_train_data, self.lag)\n      self.reg.fit(X_train, y_train)\n\n    return self.reg\n\n  def test_AR_model(self, test_data,X_test_data):\n    if(self.add_month):\n      X_test, y_test, month_test = make_dataset_with_month(test_data,X_test_data, self.lag)\n      y_hat = self.reg.predict(X_test)\n      mse = mean_squared_error(y_test, y_hat)\n\n\n    else:\n      X_test, y_test, months_test = make_dataset(test_data,X_test_data, self.lag)\n      y_hat = self.reg.predict(X_test)\n      mse = mean_squared_error(y_test, y_hat)\n\n    return mse\n\n\n\n# Finding the variation in the mse values with increasing lag\n\nlag_values = [1, 5, 10, 15, 20, 30, 35, 40, 50, 55, 65]\n\n# Compairing the accuracy of the models with and without the month feature\nloss_table_1 = []\nloss_table_2 = []\n\nfor lag in lag_values:\n    model_1 = AR_model(lag)\n    model_2 = AR_model(lag, True)\n    reg1 = model_1.train_AR_model(train_data, X_train_data)\n    reg2 = model_2.train_AR_model(train_data, X_train_data)\n\n    mse1 = model_1.test_AR_model(test_data,X_test_data)\n    mse2 = model_2.test_AR_model(test_data,X_test_data)\n    loss_table_1.append((lag, mse1))\n    loss_table_2.append((lag, mse2))\n\n\n# Visualize the loss values\nlags, losses1 = zip(*loss_table_1)\nlags, losses2 = zip(*loss_table_2)\nplt.plot(lags, losses1,label='Without month as a feature', marker='o')\nplt.plot(lags, losses2,label='With month as a feature', marker='o')\nplt.xlabel('Lag')\nplt.ylabel('Mean Squared Error (MSE)')\nplt.title('Loss vs Lag')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nUsing a more complicated dataset : Air passengers dataset\n\n# downloading and plotting the dataset\nurl_ap = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'\ndf_ap = pd.read_csv(url_ap)\ndf_ap['Month'] = pd.to_datetime(df_ap['Month'])\ndf_ap.set_index('Month', inplace=True)\ndf = df_ap\nplt.figure(figsize=(10, 6))\nplt.plot(np.array(range(len(df_ap))), df_ap['Passengers'], label='Air Passengers')\nplt.title('Air Passengers Over Time')\nplt.xlabel('Months')\nplt.ylabel('Number of Passengers')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Extract features (X) and target variable (y)\nX_ap = np.array(range(len(df_ap)))\ny_ap= df_ap['Passengers'].values.flatten().tolist()\n\nsplit_index_ap = int(0.8 * len(y_ap))\ntrain_data_ap = y_ap[:split_index_ap]\nX_train_data_ap = X_ap[:split_index_ap]\ntest_data_ap = y_ap[split_index_ap:]\nX_test_data_ap = X_ap[split_index_ap:]\n\n\ny_co = pd.Series(y_ap)\n\n\n# Plot the autocorrelation function (ACF) using statsmodels\nplt.figure(figsize=(10, 6))\nplot_acf(y_co, lags=80, title='Autocorrelation Plot')\nplt.xlabel('Lag')\nplt.ylabel('Autocorrelation')\nplt.show()\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_pacf\nplot_pacf(y_co, lags=50)\nplt.show()\n\n\n\n\n\n\n\n\n\n# example run of the model using a lag of 20\nlag_ap = 20\nX_train_ap, y_train_ap, months_train_ap = make_dataset(train_data_ap, X_train_data_ap, lag_ap)\nreg_ap = LinearRegression().fit(X_train_ap, y_train_ap)\nX_test_ap, y_test_ap, months_test_ap = make_dataset(test_data_ap,X_test_data_ap, lag_ap)\ny_hat_ap = reg_ap.predict(X_test_ap)\nrmse_ap = np.sqrt(mean_squared_error(y_test_ap, y_hat_ap))\n\nprint(f\"Mean Squared Error (RMSE): {rmse_ap}\")\n\nMean Squared Error (RMSE): 18.798515889301697\n\n\n\n# Ploting the predicted output for lag=10 for the test sequence\nplt.plot(months_test_ap, y_test_ap, label='Actual Values (y_test)', marker='o')\nplt.plot(months_test_ap, y_hat_ap, label='Predicted Values (y_hat)', marker='o')\nplt.xlabel('Month')\nplt.ylabel('Number of passangers')\nplt.title('Actual vs Predicted Values')\n\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nComparison with another time series model : ConvLSTM\n\n# univariate convlstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import ConvLSTM2D\n\n\nlag = 20\n# split into samples\nX, y,_ = make_dataset(train_data_ap, X_train_data_ap, lag)\n# reshape from [samples, timesteps] into [samples, timesteps, rows, columns, features]\nn_features = 1\nn_seq = 4\nn_steps = 5\nX = array(X)\nX = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features))\n# define model\nmodel = Sequential()\nmodel.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\nmodel.add(Flatten())\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# fit model\nmodel.fit(X, y, epochs=500, verbose=0)\n# demonstrate prediction\nX_test, y_test, months_test = make_dataset(test_data_ap, X_test_data_ap, lag)\nX_test = array(X_test)\ny_test = array(y_test)\nX_test = X_test.reshape((X_test.shape[0], n_seq, 1, n_steps, n_features))\ny_hat_lstm = model.predict(X_test)\ny_hat_lstm = y_hat_lstm.flatten()\n\n1/1 [==============================] - 1s 589ms/step\n\n\n\n# Ploting the predicted output for lag=10 for the test sequence\nplt.plot(months_test_ap, y_test_ap, label='Actual Values (y_test)', marker='o')\nplt.plot(months_test_ap, y_hat_ap, label='Predicted Values : ARM', marker='o')\nplt.plot(months_test_ap, y_hat_lstm, label='Predicted Values : ConvLSTM', marker='o')\nplt.xlabel('Month')\nplt.ylabel('Number of passangers')\nplt.title('ARM vs ConvLSTM')\n\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Creating a class to train and test the ConvLSTM model\n\nclass custom_ConvLSTM:\n\n  def __init__(self, lag,n_seq, n_steps, n_features=1):\n    self.lag = lag\n    self.n_features = n_features\n    self.n_steps = n_steps\n    self.n_seq = n_seq\n    self.model = Sequential()\n    self.model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\n    self.model.add(Flatten())\n    self.model.add(Dense(1))\n    self.model.compile(optimizer='adam', loss='mse')\n\n  def train(self, train_data, X_train_data):\n    X_train, y_train, months_train = make_dataset(train_data, X_train_data, self.lag)\n    X_train = array(X_train)\n    y_train = array(y_train)\n    X_train = X_train.reshape((X_train.shape[0], self.n_seq, 1, self.n_steps, self.n_features))\n    self.model.fit(X_train, y_train, epochs=500, verbose=0)\n\n  def test(self, test_data, X_test_data):\n    X_test, y_test, months_test = make_dataset(test_data, X_test_data, self.lag)\n    X_test = array(X_test)\n    y_test = array(y_test)\n    X_test = X_test.reshape((X_test.shape[0], self.n_seq, 1, self.n_steps, self.n_features))\n    y_hat_lstm = self.model.predict(X_test)\n    y_hat_lstm = y_hat_lstm.flatten()\n\n    mse_lstm = mean_squared_error(y_test, y_hat_lstm)\n    return mse_lstm\n\n\n\nimport random\n\ndef random_split(original_number):\n    # Find factors of the original number\n    factors = [i for i in range(2, original_number) if original_number % i == 0]\n\n    # Select one factor randomly\n    factor = random.choice(factors)\n\n    # Calculate the other factor\n    other_factor = original_number // factor\n\n    return factor, other_factor"
  },
  {
    "objectID": "notebooks/logistic-apple-oranges.html",
    "href": "notebooks/logistic-apple-oranges.html",
    "title": "Logistic Regression - I",
    "section": "",
    "text": "import numpy as np\nimport sklearn \nimport matplotlib.pyplot as plt\nfrom latexify import *\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.patches as mpatches\n\n\nx = np.array([0, 0.1, 0.2, 0.3, 0.6, 0.7, 0.9])\n\n\ny = (x&gt;0.4).astype('int')\n\n\ny\n\narray([0, 0, 0, 0, 1, 1, 1])\n\n\n\n\nlatexify()\nplt.scatter(x, np.zeros_like(x), c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\n# legend outside the plot\nplt.legend(handles=[yellow_patch, blue_patch], bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n#plt.legend(handles=[yellow_patch, blue_patch])\n\n\nplt.xlabel('Radius')\nplt.gca().yaxis.set_visible(False) \nformat_axes(plt.gca())\nplt.savefig(\"../figures/logistic-regression/logistic-orange-tomatoes-original.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nplt.legend(handles=[yellow_patch, blue_patch],  bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/logistic-regression/logistic-orange-tomatoes.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nFitting linear model\n\nfrom sklearn.linear_model import LinearRegression\n\n\nlinr_reg = LinearRegression()\n\n\nlinr_reg.fit(x.reshape(-1, 1), y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\nplt.plot(np.linspace(0, 1, 50), linr_reg.predict(np.linspace(0, 1, 50).reshape(-1, 1)))\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\nplt.axhline(y=1, color='grey', label='P(y=1)')\nplt.axhline(y=0, color='grey')\nplt.legend(handles=[yellow_patch, blue_patch],  bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\nplt.savefig(\"../figures/logistic-regression/linear-orange-tomatoes.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\n (0.5-linr_reg.intercept_)/linr_reg.coef_\n\narray([0.44857143])\n\n\n\nplt.plot(np.linspace(0, 1, 50), linr_reg.predict(np.linspace(0, 1, 50).reshape(-1, 1)))\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\n#plt.axhline(y=1, color='grey', label='P(y=1)')\n#plt.axhline(y=0, color='grey')\nplt.axhline(y=0.5, color='grey')\nplt.axvline(x=((0.5-linr_reg.intercept_)/linr_reg.coef_)[0], color='grey')\n\nplt.legend(handles=[yellow_patch, blue_patch],  bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\nplt.savefig(\"../figures/logistic-regression/linear-orange-tomatoes-decision.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nx_dash = np.append(x, 2.5)\ny_dash = np.append(y, 1)\nlinr_reg.fit(x_dash.reshape(-1, 1), y_dash)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\nplt.plot(np.linspace(0, 2.5, 50), linr_reg.predict(np.linspace(0, 2.5, 50).reshape(-1, 1)))\nplt.scatter(x_dash, y_dash, c=y_dash)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\n\nplt.xlabel('Radius')\nformat_axes(plt.gca())\n#plt.axhline(y=1, color='grey', label='P(y=1)')\n#plt.axhline(y=0, color='grey')\nplt.axhline(y=0.5, color='grey')\nplt.axvline(x=((0.5-linr_reg.intercept_)/linr_reg.coef_)[0], color='grey')\n\nplt.legend(handles=[yellow_patch, blue_patch],  bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\nplt.savefig(\"../figures/logistic-regression/linear-orange-tomatoes-decision-modified.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nclf = LogisticRegression(penalty=None, solver='lbfgs')\n\n\nclf.fit(x.reshape(-1,1), y)\n\nLogisticRegression(penalty=None)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(penalty=None) \n\n\n\nclf.coef_\n\narray([[55.99493009]])\n\n\n\n-clf.intercept_[0]/clf.coef_[0]\n\narray([0.4484548])\n\n\n\nclf.intercept_\n\narray([-25.11119514])\n\n\n\ndef sigmoid(z):\n    return 1/(1+np.exp(-z))\n\n\nlatexify()\nplt.scatter(x, y, c=y)\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nblack_patch = mpatches.Patch(color='black', label='Decision Boundary')\nplt.axvline(x = -clf.intercept_[0]/clf.coef_[0],label='Decision Boundary',linestyle='--',color='k',lw=1)\nplt.xlabel('Radius')\nformat_axes(plt.gca())\nplt.legend(handles=[black_patch, yellow_patch, blue_patch],  bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.axhspan(0,1, xmin=0, xmax=0.49, linestyle='--',color='darkblue',lw=1, alpha=0.2)\nplt.axhspan(0,0.001, xmin=0, xmax=0.49, linestyle='--',color='k',lw=1, )\n\nplt.axhspan(0,1, xmax=1, xmin=0.49, linestyle='--',color='yellow',lw=1, alpha=0.2)\nplt.axhspan(1,1.001,  xmax=1, xmin=0.49, linestyle='--',color='k',lw=1, )\nplt.savefig(\"../figures/logistic-regression/linear-orange-tomatoes-decision-ideal.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nx_dum = np.linspace(-0.1, 1, 100)\nplt.plot(x_dum, sigmoid(x_dum*clf.coef_[0] + clf.intercept_[0]))\nplt.scatter(x, y, c=y)\nlatexify()\nplt.axvline(-clf.intercept_[0]/clf.coef_[0], lw=2, color='black')\nplt.axhline(0.5, linestyle='--',color='k',lw=3, label='P(y=1) = P(y=0)')\nplt.ylabel(\"P(y=1)\")\nplt.xlabel('Radius')\nyellow_patch = mpatches.Patch(color='yellow', label='Oranges')\nblue_patch = mpatches.Patch(color='darkblue', label='Tomatoes')\nblack_patch = mpatches.Patch(color='black', label='Decision Boundary')\nsigmoid_patch = mpatches.Patch(color='steelblue', label='Sigmoid')\nplt.legend(handles=[black_patch, yellow_patch, blue_patch, sigmoid_patch],  bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nformat_axes(plt.gca())\nplt.title(\"Logistic Regression\")\nplt.savefig(\"../figures/logistic-regression/logistic.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nz_lins = np.linspace(-10, 10, 500)\nplt.plot(z_lins, sigmoid(z_lins), label=r'$\\sigma(z) = \\frac{1}{1+e^{-z}}$')\nformat_axes(plt.gca())\nplt.xlabel('z')\nplt.ylabel(r'$\\sigma(z)$')\n# vline at 0\nplt.axvline(0, color='black', lw=1, linestyle='--')\n# hline at 0.5\nplt.axhline(0.5, color='black', lw=1, linestyle='--')\nplt.savefig(\"../figures/logistic-regression/logistic-function.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nx_dum = np.linspace(-0.1, 1, 100)\nplt.plot(x_dum, sigmoid(x_dum*clf.coef_[0] + clf.intercept_[0]))\n\n\nformat_axes(plt.gca())\nplt.xlabel(\"z\")\nplt.ylabel(r\"$\\sigma(z)$\")\n#plt.savefig(\"../figures/logistic-regression/logistic-function.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "notebooks/knn-variants.html",
    "href": "notebooks/knn-variants.html",
    "title": "KNN variants (tutorial)",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Retina \n%config InlineBackend.figure_format = 'retina'\n\n\nX = np.array([[1, 9], [2, 3], [4, 1], [3, 7], [5, 4], [6, 8], [7, 2], [8, 8], [7, 9], [9, 6]])\n\nquery_pt = np.array([7, 4])\n\ndef plot_dataset():\n    plt.scatter(X[:, 0], X[:, 1], s=100)\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.gca().set_aspect('equal', adjustable='box')\n    plt.grid(True, which='both', axis='both', linestyle='--', linewidth=1, alpha=0.5)\n    plt.xticks(np.arange(min(X[:, 0]), max(X[:, 0])+1, 1))\n    plt.yticks(np.arange(min(X[:, 1]), max(X[:, 1])+1, 1))\n    \n    plt.scatter(query_pt[0], query_pt[1], color='red', s=100)\n    \n\nplot_dataset()\n\n\n\n\n\n\n\n\n\n\n\n# Exact 1NN from sklearn\nfrom sklearn.neighbors import NearestNeighbors\n\nk = 2\nnbrs = NearestNeighbors(n_neighbors=k, algorithm='brute').fit(X)\ndistances, indices = nbrs.kneighbors([query_pt])\n\n\n\nX[indices], distances\n\n(array([[[5, 4],\n         [7, 2]]]),\n array([[2., 2.]]))\n\n\n\ndef pairwise_dist_naive(x: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"\n    x: numpy array of shape (d,)\n    y: numpy array of shape (d,)\n    \n    Returns the Euclidean distance between x and y\n    \"\"\"\n    d = len(x)\n    assert d == len(y)\n    sqrd_distance = 0.0\n    for i in range(d):\n        sqrd_distance += (x[i] - y[i])**2\n    return np.sqrt(sqrd_distance)\n\n\ndef pairwise_dist_numpy(x: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"\n    x: numpy array of shape (d,)\n    y: numpy array of shape (d,)\n    \n    Returns the Euclidean distance between x and y\n    \"\"\"\n    return np.sqrt(np.sum((x - y)**2))\n\n\ndef pairwise_dist_numpy_norm(x: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"\n    x: numpy array of shape (d,)\n    y: numpy array of shape (d,)\n    \n    Returns the Euclidean distance between x and y\n    \"\"\"\n    return np.linalg.norm(x - y)\n\n\npairwise_dist_naive(X[0], X[1]), pairwise_dist_numpy(X[0], X[1]), pairwise_dist_numpy_norm(X[0], X[1])\n\n(6.082762530298219, 6.082762530298219, 6.082762530298219)\n\n\n\ndef distance_vector(X: np.ndarray, query_pt: np.ndarray) -&gt; np.ndarray:\n    \"\"\"\n    X: numpy array of shape (n, d)\n    query_pt: numpy array of shape (d,)\n    \n    Returns the Euclidean distance between query_pt and each point in X\n    \"\"\"\n    n, d = X.shape\n    distances = np.zeros(n)\n    \"\"\"Write logic here\"\"\"\n    \n    return distances\n\n\n# Test that the function is correct by comparing to sklearn\n\n\n# Find all distances from query_pt to all points in X using sklearn\nnbrs = NearestNeighbors(n_neighbors=len(X), algorithm='brute').fit(X)\ndistances_sklearn, idxs_sklearn = nbrs.kneighbors([query_pt])\nprint(distances_sklearn)\n\n[[2.         2.         2.82842712 4.12310563 4.12310563 4.24264069\n  5.         5.         5.09901951 7.81024968]]\n\n\n\ndistances_sklearn[0, idxs_sklearn[0]]\n\narray([4.12310563, 5.        , 7.81024968, 4.24264069, 5.        ,\n       2.82842712, 4.12310563, 5.09901951, 2.        , 2.        ])\n\n\n\nimport pandas as pd\n\ndf = pd.DataFrame(X, columns=['X1', 'X2'])\ndf[\"query_distance\"] = distances_sklearn[0, idxs_sklearn[0]]\ndf\n\n\n\n\n\n\n\n\nX1\nX2\nquery_distance\n\n\n\n\n0\n1\n9\n4.123106\n\n\n1\n2\n3\n5.000000\n\n\n2\n4\n1\n7.810250\n\n\n3\n3\n7\n4.242641\n\n\n4\n5\n4\n5.000000\n\n\n5\n6\n8\n2.828427\n\n\n6\n7\n2\n4.123106\n\n\n7\n8\n8\n5.099020\n\n\n8\n7\n9\n2.000000\n\n\n9\n9\n6\n2.000000\n\n\n\n\n\n\n\n\n### LSH with Random Projections\n\n\n\n### Random Projections\n\nP = 3\nnp.random.seed(35)\nR = np.random.randn(X.shape[1] + 1, P)  # why +1?\n\n\nR\n\narray([[-1.88973671, -0.41359218, -0.76602601],\n       [-0.92412667, -1.42159783,  0.80525599],\n       [ 1.14886176,  1.1694284 , -0.80200928]])\n\n\n\n# For now, make R[:, 2] =1 to make it easier to plot\nR[:, 2] = 1\nR\n\narray([[-1.88973671, -0.41359218,  1.        ],\n       [-0.92412667, -1.42159783,  1.        ],\n       [ 1.14886176,  1.1694284 ,  1.        ]])\n\n\n\nplot_dataset()\n\n# Plot hyperplanes\nfor i in range(P):\n    x1 = np.array([min(X[:, 0]), max(X[:, 0])])\n    x2 = (-R[0, i] - R[1, i]*x1) / R[2, i]\n    plt.plot(x1, x2, label=f'Hyperplane {i+1}')\nplt.legend()\n\n\n\n\n\n\n\n\n\nX_aug = np.hstack([ np.ones((X.shape[0], 1)), X])\nX_aug\n\narray([[1., 1., 9.],\n       [1., 2., 3.],\n       [1., 4., 1.],\n       [1., 3., 7.],\n       [1., 5., 4.],\n       [1., 6., 8.],\n       [1., 7., 2.],\n       [1., 8., 8.],\n       [1., 7., 9.],\n       [1., 9., 6.]])\n\n\n\nX_aug @ R\n\narray([[ 7.52589247,  8.68966556, 11.        ],\n       [-0.29140476,  0.25149734,  6.        ],\n       [-4.43738162, -4.93055512,  6.        ],\n       [ 3.37991562,  3.5076131 , 11.        ],\n       [-1.914923  , -2.84386777, 10.        ],\n       [ 1.75639738,  0.41224799, 15.        ],\n       [-6.06089985, -8.02592023, 10.        ],\n       [-0.09185596, -2.43094768, 17.        ],\n       [ 1.98113247,  0.16007855, 17.        ],\n       [-3.31370614, -6.19140231, 16.        ]])\n\n\n\nnp.sign(X_aug @ R)\n\narray([[ 1.,  1.,  1.],\n       [-1.,  1.,  1.],\n       [-1., -1.,  1.],\n       [ 1.,  1.,  1.],\n       [-1., -1.,  1.],\n       [ 1.,  1.,  1.],\n       [-1., -1.,  1.],\n       [-1., -1.,  1.],\n       [ 1.,  1.,  1.],\n       [-1., -1.,  1.]])"
  },
  {
    "objectID": "notebooks/numpy-pandas-basics.html",
    "href": "notebooks/numpy-pandas-basics.html",
    "title": "Numpy Pandas Basics",
    "section": "",
    "text": "# importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport time\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nLists\n\n# Creating lists\nlist_a = [1, 2, 3, 4, 5]\nlist_b = [6, 7, 8, 9, 10]\n\n\n# Operations on lists\n# Adding lists\nlist_sum = [a + b for a, b in zip(list_a, list_b)]\nprint(\"List Sum:\", list_sum)\n\n# Vector product using lists    \nvector_product = [a * b for a, b in zip(list_a, list_b)]\nprint(\"Vector Product:\", vector_product)\n\nList Sum: [7, 9, 11, 13, 15]\nVector Product: [6, 14, 24, 36, 50]\n\n\n\n\nNumpy Array\n\n# Creating numpy arrays\nnumpy_array_a = np.array(list_a)\nnumpy_array_b = np.array(list_b)\n\n\n# Operations on numpy arrays\n# Adding numpy arrays\nnumpy_sum = numpy_array_a + numpy_array_b\nprint(\"Numpy Sum:\", numpy_sum)\n\n# Vector product using numpy arrays\nnumpy_vector_product = np.multiply(numpy_array_a, numpy_array_b)\nprint(\"Numpy Vector Product:\", numpy_vector_product)\n\nNumpy Sum: [ 7  9 11 13 15]\nNumpy Vector Product: [ 6 14 24 36 50]\n\n\n\nnp.allclose(list_sum, numpy_sum), np.allclose(vector_product, numpy_vector_product)\n\n(True, True)\n\n\n\n\nTime comparison between list and numpy array\n\n# Creating large arrays and lists for time comparison\nnumpy_array_a = np.random.randint(0, 100, size=10000)\nnumpy_array_b = np.random.randint(0, 100, size=10000)\n\nlist_a = list(numpy_array_a)\nlist_b = list(numpy_array_b)\n\n\n# Time for list addition\nstart_time = time.time()\nfor _ in range(1000):\n    list_sum = [a + b for a, b in zip(list_a, list_b)]\nend_time = time.time()\nprint(\"Time taken for lists addition:\", end_time - start_time)\n\n# Time for numpy addition\nstart_time = time.time()\nfor _ in range(1000):\n    numpy_sum = numpy_array_a + numpy_array_b\nend_time = time.time()\nprint(\"Time taken for numpy addition:\", end_time - start_time)\n\nTime taken for lists addition: 0.5500102043151855\nTime taken for numpy addition: 0.0038487911224365234\n\n\n\n# Time for list vector product\nstart_time = time.time()\nfor _ in range(10000):\n    list_product = [a * b for a, b in zip(list_a, list_b)]\n\nend_time = time.time()\nprint(\"Time taken for list vector product:\", end_time - start_time)\n\n# Time for numpy vector product \nstart_time = time.time()\nfor _ in range(10000):\n    numpy_product = np.multiply(numpy_array_a, numpy_array_b)\n\nend_time = time.time()\nprint(\"Time taken for numpy vector product:\", end_time - start_time)\n\nTime taken for list vector product: 5.371699571609497\nTime taken for numpy vector product: 0.047417640686035156\n\n\n\nnp.allclose(list_sum, numpy_sum), np.allclose(vector_product, numpy_vector_product)\n\n(True, True)\n\n\n\ntimeit_add_list = %timeit -o [a + b for a, b in zip(list_a, list_b)]\n\n542 µs ± 593 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\ntimeit_add_numpy = %timeit -o numpy_array_a + numpy_array_b\n\n3.5 µs ± 6.1 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\nCode clarity\n\n# Numpy code is often more concise and readable than list comprehensions\n# Example: Calculate the element-wise product of two lists\nlist_product = [a * b for a, b in zip(list_a, list_b)]\nnumpy_product = np.multiply(numpy_array_a, numpy_array_b)\n\n\nnumpy_product\n\narray([5950, 1995,  264, ..., 2436,  928,  665])\n\n\n\nnumpy_array_a@numpy_array_b\n\n24470992\n\n\n\n\nReading CSV file using Numpy\n\n!head ../datasets/tennis-discrete-output.csv\n\nDay,Outlook,Temp,Humidity,Windy,Play\nD1,Sunny,Hot,High,Weak,No\nD2,Sunny,Hot,High,Strong,No\nD3,Overcast,Hot,High,Weak,Yes\nD4,Rain,Mild,High,Weak,Yes\nD5,Rain,Cool,Normal,Weak,Yes\nD6,Rain,Cool,Normal,Strong,No\nD7,Overcast,Cool,Normal,Strong,Yes\nD8,Sunny,Mild,High,Weak,No\nD9,Sunny,Cool,Normal,Weak,Yes\n\n\n\nnp.genfromtxt?\n\nSignature:\nnp.genfromtxt(\n    fname,\n    dtype=&lt;class 'float'&gt;,\n    comments='#',\n    delimiter=None,\n    skip_header=0,\n    skip_footer=0,\n    converters=None,\n    missing_values=None,\n    filling_values=None,\n    usecols=None,\n    names=None,\n    excludelist=None,\n    deletechars=\" !#$%&'()*+,-./:;&lt;=&gt;?@[\\\\]^{|}~\",\n    replace_space='_',\n    autostrip=False,\n    case_sensitive=True,\n    defaultfmt='f%i',\n    unpack=None,\n    usemask=False,\n    loose=True,\n    invalid_raise=True,\n    max_rows=None,\n    encoding='bytes',\n    *,\n    ndmin=0,\n    like=None,\n)\nDocstring:\nLoad data from a text file, with missing values handled as specified.\n\nEach line past the first `skip_header` lines is split at the `delimiter`\ncharacter, and characters following the `comments` character are discarded.\n\nParameters\n----------\nfname : file, str, pathlib.Path, list of str, generator\n    File, filename, list, or generator to read.  If the filename\n    extension is ``.gz`` or ``.bz2``, the file is first decompressed. Note\n    that generators must return bytes or strings. The strings\n    in a list or produced by a generator are treated as lines.\ndtype : dtype, optional\n    Data type of the resulting array.\n    If None, the dtypes will be determined by the contents of each\n    column, individually.\ncomments : str, optional\n    The character used to indicate the start of a comment.\n    All the characters occurring on a line after a comment are discarded.\ndelimiter : str, int, or sequence, optional\n    The string used to separate values.  By default, any consecutive\n    whitespaces act as delimiter.  An integer or sequence of integers\n    can also be provided as width(s) of each field.\nskiprows : int, optional\n    `skiprows` was removed in numpy 1.10. Please use `skip_header` instead.\nskip_header : int, optional\n    The number of lines to skip at the beginning of the file.\nskip_footer : int, optional\n    The number of lines to skip at the end of the file.\nconverters : variable, optional\n    The set of functions that convert the data of a column to a value.\n    The converters can also be used to provide a default value\n    for missing data: ``converters = {3: lambda s: float(s or 0)}``.\nmissing : variable, optional\n    `missing` was removed in numpy 1.10. Please use `missing_values`\n    instead.\nmissing_values : variable, optional\n    The set of strings corresponding to missing data.\nfilling_values : variable, optional\n    The set of values to be used as default when the data are missing.\nusecols : sequence, optional\n    Which columns to read, with 0 being the first.  For example,\n    ``usecols = (1, 4, 5)`` will extract the 2nd, 5th and 6th columns.\nnames : {None, True, str, sequence}, optional\n    If `names` is True, the field names are read from the first line after\n    the first `skip_header` lines. This line can optionally be preceded\n    by a comment delimiter. If `names` is a sequence or a single-string of\n    comma-separated names, the names will be used to define the field names\n    in a structured dtype. If `names` is None, the names of the dtype\n    fields will be used, if any.\nexcludelist : sequence, optional\n    A list of names to exclude. This list is appended to the default list\n    ['return','file','print']. Excluded names are appended with an\n    underscore: for example, `file` would become `file_`.\ndeletechars : str, optional\n    A string combining invalid characters that must be deleted from the\n    names.\ndefaultfmt : str, optional\n    A format used to define default field names, such as \"f%i\" or \"f_%02i\".\nautostrip : bool, optional\n    Whether to automatically strip white spaces from the variables.\nreplace_space : char, optional\n    Character(s) used in replacement of white spaces in the variable\n    names. By default, use a '_'.\ncase_sensitive : {True, False, 'upper', 'lower'}, optional\n    If True, field names are case sensitive.\n    If False or 'upper', field names are converted to upper case.\n    If 'lower', field names are converted to lower case.\nunpack : bool, optional\n    If True, the returned array is transposed, so that arguments may be\n    unpacked using ``x, y, z = genfromtxt(...)``.  When used with a\n    structured data-type, arrays are returned for each field.\n    Default is False.\nusemask : bool, optional\n    If True, return a masked array.\n    If False, return a regular array.\nloose : bool, optional\n    If True, do not raise errors for invalid values.\ninvalid_raise : bool, optional\n    If True, an exception is raised if an inconsistency is detected in the\n    number of columns.\n    If False, a warning is emitted and the offending lines are skipped.\nmax_rows : int,  optional\n    The maximum number of rows to read. Must not be used with skip_footer\n    at the same time.  If given, the value must be at least 1. Default is\n    to read the entire file.\n\n    .. versionadded:: 1.10.0\nencoding : str, optional\n    Encoding used to decode the inputfile. Does not apply when `fname` is\n    a file object.  The special value 'bytes' enables backward compatibility\n    workarounds that ensure that you receive byte arrays when possible\n    and passes latin1 encoded strings to converters. Override this value to\n    receive unicode arrays and pass strings as input to converters.  If set\n    to None the system default is used. The default value is 'bytes'.\n\n    .. versionadded:: 1.14.0\nndmin : int, optional\n    Same parameter as `loadtxt`\n\n    .. versionadded:: 1.23.0\nlike : array_like, optional\n    Reference object to allow the creation of arrays which are not\n    NumPy arrays. If an array-like passed in as ``like`` supports\n    the ``__array_function__`` protocol, the result will be defined\n    by it. In this case, it ensures the creation of an array object\n    compatible with that passed in via this argument.\n\n    .. versionadded:: 1.20.0\n\nReturns\n-------\nout : ndarray\n    Data read from the text file. If `usemask` is True, this is a\n    masked array.\n\nSee Also\n--------\nnumpy.loadtxt : equivalent function when no data is missing.\n\nNotes\n-----\n* When spaces are used as delimiters, or when no delimiter has been given\n  as input, there should not be any missing data between two fields.\n* When the variables are named (either by a flexible dtype or with `names`),\n  there must not be any header in the file (else a ValueError\n  exception is raised).\n* Individual values are not stripped of spaces by default.\n  When using a custom converter, make sure the function does remove spaces.\n\nReferences\n----------\n.. [1] NumPy User Guide, section `I/O with NumPy\n       &lt;https://docs.scipy.org/doc/numpy/user/basics.io.genfromtxt.html&gt;`_.\n\nExamples\n--------\n&gt;&gt;&gt; from io import StringIO\n&gt;&gt;&gt; import numpy as np\n\nComma delimited file with mixed dtype\n\n&gt;&gt;&gt; s = StringIO(u\"1,1.3,abcde\")\n&gt;&gt;&gt; data = np.genfromtxt(s, dtype=[('myint','i8'),('myfloat','f8'),\n... ('mystring','S5')], delimiter=\",\")\n&gt;&gt;&gt; data\narray((1, 1.3, b'abcde'),\n      dtype=[('myint', '&lt;i8'), ('myfloat', '&lt;f8'), ('mystring', 'S5')])\n\nUsing dtype = None\n\n&gt;&gt;&gt; _ = s.seek(0) # needed for StringIO example only\n&gt;&gt;&gt; data = np.genfromtxt(s, dtype=None,\n... names = ['myint','myfloat','mystring'], delimiter=\",\")\n&gt;&gt;&gt; data\narray((1, 1.3, b'abcde'),\n      dtype=[('myint', '&lt;i8'), ('myfloat', '&lt;f8'), ('mystring', 'S5')])\n\nSpecifying dtype and names\n\n&gt;&gt;&gt; _ = s.seek(0)\n&gt;&gt;&gt; data = np.genfromtxt(s, dtype=\"i8,f8,S5\",\n... names=['myint','myfloat','mystring'], delimiter=\",\")\n&gt;&gt;&gt; data\narray((1, 1.3, b'abcde'),\n      dtype=[('myint', '&lt;i8'), ('myfloat', '&lt;f8'), ('mystring', 'S5')])\n\nAn example with fixed-width columns\n\n&gt;&gt;&gt; s = StringIO(u\"11.3abcde\")\n&gt;&gt;&gt; data = np.genfromtxt(s, dtype=None, names=['intvar','fltvar','strvar'],\n...     delimiter=[1,3,5])\n&gt;&gt;&gt; data\narray((1, 1.3, b'abcde'),\n      dtype=[('intvar', '&lt;i8'), ('fltvar', '&lt;f8'), ('strvar', 'S5')])\n\nAn example to show comments\n\n&gt;&gt;&gt; f = StringIO('''\n... text,# of chars\n... hello world,11\n... numpy,5''')\n&gt;&gt;&gt; np.genfromtxt(f, dtype='S12,S12', delimiter=',')\narray([(b'text', b''), (b'hello world', b'11'), (b'numpy', b'5')],\n  dtype=[('f0', 'S12'), ('f1', 'S12')])\nFile:      ~/miniforge3/lib/python3.9/site-packages/numpy/lib/npyio.py\nType:      function\n\n\n\ndata = np.genfromtxt('../datasets/tennis-discrete-output.csv', delimiter=',')\ndata\n\narray([[nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan],\n       [nan, nan, nan, nan, nan, nan]])\n\n\nWait! What happened?\n\ndata = np.genfromtxt('../datasets/tennis-discrete-output.csv', delimiter=',', dtype=str)\ndata\n\narray([['Day', 'Outlook', 'Temp', 'Humidity', 'Windy', 'Play'],\n       ['D1', 'Sunny', 'Hot', 'High', 'Weak', 'No'],\n       ['D2', 'Sunny', 'Hot', 'High', 'Strong', 'No'],\n       ['D3', 'Overcast', 'Hot', 'High', 'Weak', 'Yes'],\n       ['D4', 'Rain', 'Mild', 'High', 'Weak', 'Yes'],\n       ['D5', 'Rain', 'Cool', 'Normal', 'Weak', 'Yes'],\n       ['D6', 'Rain', 'Cool', 'Normal', 'Strong', 'No'],\n       ['D7', 'Overcast', 'Cool', 'Normal', 'Strong', 'Yes'],\n       ['D8', 'Sunny', 'Mild', 'High', 'Weak', 'No'],\n       ['D9', 'Sunny', 'Cool', 'Normal', 'Weak', 'Yes'],\n       ['D10', 'Rain', 'Mild', 'Normal', 'Weak', 'Yes'],\n       ['D11', 'Sunny', 'Mild', 'Normal', 'Strong', 'Yes'],\n       ['D12', 'Overcast', 'Mild', 'High', 'Strong', 'Yes'],\n       ['D13', 'Overcast', 'Hot', 'Normal', 'Weak', 'Yes'],\n       ['D14', 'Rain', 'Mild', 'High', 'Strong', 'No']], dtype='&lt;U8')\n\n\n\ndata.shape\n\n(15, 6)\n\n\nQuestion: Find the outlook on D11\n\nidx = np.argwhere(data[:, 0] == 'D11')[0, 0]\nidx\n\n11\n\n\n\ndata[idx]\n\narray(['D11', 'Sunny', 'Mild', 'Normal', 'Strong', 'Yes'], dtype='&lt;U8')\n\n\n\ndata[idx][1]\n\n'Sunny'\n\n\n\n\nReading CSV file using Pandas\n\ndf = pd.read_csv('../datasets/tennis-discrete-output.csv')\n\n\ndf\n\n\n\n\n\n\n\n\nDay\nOutlook\nTemp\nHumidity\nWindy\nPlay\n\n\n\n\n0\nD1\nSunny\nHot\nHigh\nWeak\nNo\n\n\n1\nD2\nSunny\nHot\nHigh\nStrong\nNo\n\n\n2\nD3\nOvercast\nHot\nHigh\nWeak\nYes\n\n\n3\nD4\nRain\nMild\nHigh\nWeak\nYes\n\n\n4\nD5\nRain\nCool\nNormal\nWeak\nYes\n\n\n5\nD6\nRain\nCool\nNormal\nStrong\nNo\n\n\n6\nD7\nOvercast\nCool\nNormal\nStrong\nYes\n\n\n7\nD8\nSunny\nMild\nHigh\nWeak\nNo\n\n\n8\nD9\nSunny\nCool\nNormal\nWeak\nYes\n\n\n9\nD10\nRain\nMild\nNormal\nWeak\nYes\n\n\n10\nD11\nSunny\nMild\nNormal\nStrong\nYes\n\n\n11\nD12\nOvercast\nMild\nHigh\nStrong\nYes\n\n\n12\nD13\nOvercast\nHot\nNormal\nWeak\nYes\n\n\n13\nD14\nRain\nMild\nHigh\nStrong\nNo\n\n\n\n\n\n\n\n\ndf['Day'] == 'D11'\n\n0     False\n1     False\n2     False\n3     False\n4     False\n5     False\n6     False\n7     False\n8     False\n9     False\n10     True\n11    False\n12    False\n13    False\nName: Day, dtype: bool\n\n\n\ndf[df['Day'] == 'D11']\n\n\n\n\n\n\n\n\nDay\nOutlook\nTemp\nHumidity\nWindy\nPlay\n\n\n\n\n10\nD11\nSunny\nMild\nNormal\nStrong\nYes\n\n\n\n\n\n\n\n\ndf[df['Day'] == 'D11']['Outlook']\n\n10    Sunny\nName: Outlook, dtype: object\n\n\n\ndf.query('Day == \"D11\"')['Outlook']\n\n10    Sunny\nName: Outlook, dtype: object\n\n\n\ndf.shape\n\n(14, 6)\n\n\nQuestion. How many times do we play v/s not play tennis\n\nser = df['Play']\nser\n\n0      No\n1      No\n2     Yes\n3     Yes\n4     Yes\n5      No\n6     Yes\n7      No\n8     Yes\n9     Yes\n10    Yes\n11    Yes\n12    Yes\n13     No\nName: Play, dtype: object\n\n\n\nunique_play_options = df['Play'].unique()\nunique_play_options\n\narray(['No', 'Yes'], dtype=object)\n\n\n\nfor option in unique_play_options:\n    print(option, (df['Play'] == option).sum())\n\nNo 5\nYes 9\n\n\n\ndf['Play'].value_counts()\n\nPlay\nYes    9\nNo     5\nName: count, dtype: int64\n\n\n\ndf.groupby('Play').size()\n\nPlay\nNo     5\nYes    9\ndtype: int64\n\n\n\ngby = df.groupby('Play')\n\n\n{k: len(v) for k, v in gby.groups.items()}\n\n{'No': 5, 'Yes': 9}\n\n\n\npd.crosstab(index=df['Play'], columns='count')\n\n\n\n\n\n\n\ncol_0\ncount\n\n\nPlay\n\n\n\n\n\nNo\n5\n\n\nYes\n9\n\n\n\n\n\n\n\nWhat is the distribution of any given attribute?\n\ndef distribution(df, attribute):\n    return df[attribute].value_counts()\n\n\nser = distribution(df, 'Outlook')\n\n\nser\n\nOutlook\nSunny       5\nRain        5\nOvercast    4\nName: count, dtype: int64\n\n\n\ntype(ser)\n\npandas.core.series.Series\n\n\n\nser.values\n\narray([5, 5, 4])\n\n\n\nser.index\n\nIndex(['Sunny', 'Rain', 'Overcast'], dtype='object', name='Outlook')\n\n\n\ndistribution(df, 'Temp')\n\nTemp\nMild    6\nHot     4\nCool    4\nName: count, dtype: int64\n\n\nFinding entropy for target variable\n\ntarget_attribute = 'Play'\ndist_target = distribution(df, target_attribute)\n\n\ndist_target\n\nPlay\nYes    9\nNo     5\nName: count, dtype: int64\n\n\nNormalize distribution\n\ndist_target/dist_target.sum()\n\nPlay\nYes    0.642857\nNo     0.357143\nName: count, dtype: float64\n\n\n\ndf['Play'].value_counts(normalize=True)\n\nPlay\nYes    0.642857\nNo     0.357143\nName: proportion, dtype: float64\n\n\n\nnormalized_dist_target = dist_target/dist_target.sum()\n\nFor loop way of calculating entropy\n\ne = 0.0\nfor value, p in normalized_dist_target.items():\n    e = e - p * np.log2(p + 1e-6) # 1e-6 is added to avoid log(0)\nprint(e)\n\n0.9402830732836911\n\n\n\nnormalized_dist_target.apply(lambda x: -x * np.log2(x + 1e-6))\n\nPlay\nYes    0.409775\nNo     0.530508\nName: count, dtype: float64\n\n\n\nnormalized_dist_target.apply(lambda x: -x * np.log2(x + 1e-6)).sum()\n\n0.9402830732836911\n\n\nMore on crosstab\n\npd.crosstab(index=df['Outlook'], columns=df['Play'])\n\n\n\n\n\n\n\nPlay\nNo\nYes\n\n\nOutlook\n\n\n\n\n\n\nOvercast\n0\n4\n\n\nRain\n2\n3\n\n\nSunny\n3\n2\n\n\n\n\n\n\n\n\npd.crosstab(index=df['Outlook'], columns=df['Play']).T\n\n\n\n\n\n\n\nOutlook\nOvercast\nRain\nSunny\n\n\nPlay\n\n\n\n\n\n\n\nNo\n0\n2\n3\n\n\nYes\n4\n3\n2\n\n\n\n\n\n\n\n\ndf_attr = pd.crosstab(index=df['Play'], columns=df['Outlook'], normalize='columns')\ndf_attr\n\n\n\n\n\n\n\nOutlook\nOvercast\nRain\nSunny\n\n\nPlay\n\n\n\n\n\n\n\nNo\n0.0\n0.4\n0.6\n\n\nYes\n1.0\n0.6\n0.4\n\n\n\n\n\n\n\nUsing groupby\n\ndf.groupby(['Play', 'Outlook']).size()\n\nPlay  Outlook \nNo    Rain        2\n      Sunny       3\nYes   Overcast    4\n      Rain        3\n      Sunny       2\ndtype: int64\n\n\n\ndf.groupby(['Play', 'Outlook']).size().index\n\nMultiIndex([( 'No',     'Rain'),\n            ( 'No',    'Sunny'),\n            ('Yes', 'Overcast'),\n            ('Yes',     'Rain'),\n            ('Yes',    'Sunny')],\n           names=['Play', 'Outlook'])\n\n\n\ndf.groupby(['Play', 'Outlook']).size().unstack('Outlook')\n\n\n\n\n\n\n\nOutlook\nOvercast\nRain\nSunny\n\n\nPlay\n\n\n\n\n\n\n\nNo\nNaN\n2.0\n3.0\n\n\nYes\n4.0\n3.0\n2.0\n\n\n\n\n\n\n\n\ndf_attr_groupby = df.groupby(['Play', 'Outlook']).size().unstack('Outlook').fillna(0)\ndf_attr_groupby\n\n\n\n\n\n\n\nOutlook\nOvercast\nRain\nSunny\n\n\nPlay\n\n\n\n\n\n\n\nNo\n0.0\n2.0\n3.0\n\n\nYes\n4.0\n3.0\n2.0\n\n\n\n\n\n\n\nApply\n\nneg_plogp = df_attr.apply(lambda x: -x * np.log2(x + 1e-6), axis=0)\nneg_plogp\n\n\n\n\n\n\n\nOutlook\nOvercast\nRain\nSunny\n\n\nPlay\n\n\n\n\n\n\n\nNo\n0.000000\n0.528770\n0.442178\n\n\nYes\n-0.000001\n0.442178\n0.528770\n\n\n\n\n\n\n\n\nneg_plogp.sum(axis=0).sort_index()\n\nOutlook\nOvercast   -0.000001\nRain        0.970948\nSunny       0.970948\ndtype: float64\n\n\n\ndf_attr_dist = distribution(df, 'Outlook')\nnorm_attr_dist = df_attr_dist/df_attr_dist.sum()\nnorm_attr_dist\n\nOutlook\nSunny       0.357143\nRain        0.357143\nOvercast    0.285714\nName: count, dtype: float64\n\n\n\n(norm_attr_dist*neg_plogp.sum(axis=0).sort_index()).sum()\n\n0.6935336657070463"
  },
  {
    "objectID": "notebooks/dummy-variables-multi-colinearity.html",
    "href": "notebooks/dummy-variables-multi-colinearity.html",
    "title": "Dummy Variables and Multi-collinearity",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport numpy as np\n\n\nx1 = np.array([1, 2, 3])\nx2 = 2*x1\n\ny = np.array([4, 6, 8])\n\n\nall_ones = np.ones(x1.shape[0])\nX = np.array([all_ones, x1, x2]).T\n\n\nX.shape\n\n(3, 3)\n\n\n\nX\n\narray([[1., 1., 2.],\n       [1., 2., 4.],\n       [1., 3., 6.]])\n\n\n\ndef solve_normal_equation(X, y):\n    try:\n        theta = np.linalg.inv(X.T @ X) @ X.T @ y\n        return theta\n    except np.linalg.LinAlgError:\n        print('The matrix is singular')\n        print(\"X.T @ X = \\n\", X.T @ X)\n        return None\n    \n### Assignment question: Use np.linalg.solve instead of inv. Why is this better?\n\n\nsolve_normal_equation(X, y)\n\nThe matrix is singular\nX.T @ X = \n [[ 3.  6. 12.]\n [ 6. 14. 28.]\n [12. 28. 56.]]\n\n\n\nnp.linalg.matrix_rank(X), np.linalg.matrix_rank(X.T @ X)\n\n(2, 2)\n\n\n\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\ndata = np.array([x1, x2]).T\n\nlr.fit(data, y)\nlr.coef_, lr.intercept_\n\n\n# Assignment question: figure why sklearn is able to solve the problem\n\n(array([0.4, 0.8]), 2.0)\n\n\n\n# Regularization\n\neps = 1e-5\nX = np.array([all_ones, x1, x2]).T\nX = np.eye(3)*eps + X\nX\n\narray([[1.00001, 1.     , 2.     ],\n       [1.     , 2.00001, 4.     ],\n       [1.     , 3.     , 6.00001]])\n\n\n\nnp.linalg.matrix_rank(X)\n\n3\n\n\n\nsolve_normal_equation(X, y)\n\narray([2.00023248, 1.19987743, 0.40001887])\n\n\n\n# Drop variables\nX = np.array([all_ones, x1]).T\nprint(X)\n\n[[1. 1.]\n [1. 2.]\n [1. 3.]]\n\n\n\nsolve_normal_equation(X, y)\n\narray([2., 2.])\n\n\n\n# Dummy variables\n\n## dataset\nnum_records = 12\nwindspeed = np.random.randint(0, 10, num_records)\nvehicles = np.random.randint(100, 500, num_records)\ndirection = np.random.choice(['N', 'S', 'E', 'W'], num_records)\npollution = np.random.randint(0, 100, num_records)\n\ndf = pd.DataFrame({'windspeed': windspeed, 'vehicles': vehicles, 'direction': direction, 'pollution': pollution})\ndf\n\n\n\n\n\n\n\n\nwindspeed\nvehicles\ndirection\npollution\n\n\n\n\n0\n0\n355\nW\n30\n\n\n1\n2\n367\nS\n30\n\n\n2\n2\n447\nS\n78\n\n\n3\n1\n223\nE\n32\n\n\n4\n1\n272\nS\n16\n\n\n5\n9\n394\nS\n36\n\n\n6\n0\n333\nN\n45\n\n\n7\n3\n308\nW\n52\n\n\n8\n7\n480\nN\n24\n\n\n9\n9\n360\nN\n74\n\n\n10\n0\n125\nS\n36\n\n\n11\n9\n401\nS\n62\n\n\n\n\n\n\n\n\ndef fit_data(df, X, y):\n    try:\n        lr = LinearRegression()\n        lr.fit(X, y)\n        rep = f\"y = {lr.intercept_:0.2f}\"\n        for i, coef in enumerate(lr.coef_):\n            rep += f\" + {coef:0.2f}*{df.columns[i]}\"\n        return rep\n    except Exception as e:\n        print(e)\n        return None\n        \n\n\nfit_data(df, df[df.columns[:-1]], df['pollution'])\n\ncould not convert string to float: 'W'\n\n\n\n# Ordinal encoding\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\nenc = OrdinalEncoder()\n\n\ndf2 = df.copy()\ndf2['direction'] = enc.fit_transform(df[['direction']]).flatten()\ndf2\n\n\n\n\n\n\n\n\nwindspeed\nvehicles\ndirection\npollution\n\n\n\n\n0\n0\n355\n3.0\n30\n\n\n1\n2\n367\n2.0\n30\n\n\n2\n2\n447\n2.0\n78\n\n\n3\n1\n223\n0.0\n32\n\n\n4\n1\n272\n2.0\n16\n\n\n5\n9\n394\n2.0\n36\n\n\n6\n0\n333\n1.0\n45\n\n\n7\n3\n308\n3.0\n52\n\n\n8\n7\n480\n1.0\n24\n\n\n9\n9\n360\n1.0\n74\n\n\n10\n0\n125\n2.0\n36\n\n\n11\n9\n401\n2.0\n62\n\n\n\n\n\n\n\n\nfit_data(df2, df2[df2.columns[:-1]], df2['pollution'])\n\n'y = 26.49 + 1.49*windspeed + 0.03*vehicles + 1.02*direction'\n\n\n\npd.Series({x: i for i, x in enumerate(enc.categories_[0])})\n\nE    0\nN    1\nS    2\nW    3\ndtype: int64\n\n\n\n# One-hot encoding\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder(sparse_output=False)\n\n\ndirection_ohe = ohe.fit_transform(df[['direction']])\ndirection_ohe\n\narray([[0., 0., 0., 1.],\n       [0., 0., 1., 0.],\n       [0., 0., 1., 0.],\n       [1., 0., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 1., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 0., 1.],\n       [0., 1., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 1., 0.],\n       [0., 0., 1., 0.]])\n\n\n\ncol_names_ohe = [f\"Is it {x}?\" for x in enc.categories_[0]]\n\n\ndirection_ohe_df = pd.DataFrame(direction_ohe, columns=col_names_ohe)\ndirection_ohe_df\n\n\n\n\n\n\n\n\nIs it E?\nIs it N?\nIs it S?\nIs it W?\n\n\n\n\n0\n0.0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n0.0\n1.0\n0.0\n\n\n2\n0.0\n0.0\n1.0\n0.0\n\n\n3\n1.0\n0.0\n0.0\n0.0\n\n\n4\n0.0\n0.0\n1.0\n0.0\n\n\n5\n0.0\n0.0\n1.0\n0.0\n\n\n6\n0.0\n1.0\n0.0\n0.0\n\n\n7\n0.0\n0.0\n0.0\n1.0\n\n\n8\n0.0\n1.0\n0.0\n0.0\n\n\n9\n0.0\n1.0\n0.0\n0.0\n\n\n10\n0.0\n0.0\n1.0\n0.0\n\n\n11\n0.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\n# Confirm that we can write Is it W? as a linear combination of the other columns\n1-direction_ohe_df[[\"Is it N?\", \"Is it S?\", \"Is it E?\"]].sum(axis=1) - direction_ohe_df[\"Is it W?\"]\n\n0     0.0\n1     0.0\n2     0.0\n3     0.0\n4     0.0\n5     0.0\n6     0.0\n7     0.0\n8     0.0\n9     0.0\n10    0.0\n11    0.0\ndtype: float64\n\n\n\nX = np.hstack([df[['windspeed', 'vehicles']].values, direction_ohe])\n\n\nX\n\narray([[  0., 355.,   0.,   0.,   0.,   1.],\n       [  2., 367.,   0.,   0.,   1.,   0.],\n       [  2., 447.,   0.,   0.,   1.,   0.],\n       [  1., 223.,   1.,   0.,   0.,   0.],\n       [  1., 272.,   0.,   0.,   1.,   0.],\n       [  9., 394.,   0.,   0.,   1.,   0.],\n       [  0., 333.,   0.,   1.,   0.,   0.],\n       [  3., 308.,   0.,   0.,   0.,   1.],\n       [  7., 480.,   0.,   1.,   0.,   0.],\n       [  9., 360.,   0.,   1.,   0.,   0.],\n       [  0., 125.,   0.,   0.,   1.,   0.],\n       [  9., 401.,   0.,   0.,   1.,   0.]])\n\n\n\nX_aug = np.hstack([np.ones((X.shape[0], 1)), X])\n\n\nX_aug\n\narray([[  1.,   0., 355.,   0.,   0.,   0.,   1.],\n       [  1.,   2., 367.,   0.,   0.,   1.,   0.],\n       [  1.,   2., 447.,   0.,   0.,   1.,   0.],\n       [  1.,   1., 223.,   1.,   0.,   0.,   0.],\n       [  1.,   1., 272.,   0.,   0.,   1.,   0.],\n       [  1.,   9., 394.,   0.,   0.,   1.,   0.],\n       [  1.,   0., 333.,   0.,   1.,   0.,   0.],\n       [  1.,   3., 308.,   0.,   0.,   0.,   1.],\n       [  1.,   7., 480.,   0.,   1.,   0.,   0.],\n       [  1.,   9., 360.,   0.,   1.,   0.,   0.],\n       [  1.,   0., 125.,   0.,   0.,   1.,   0.],\n       [  1.,   9., 401.,   0.,   0.,   1.,   0.]])\n\n\n\nX_aug.shape\n\n(12, 7)\n\n\n\nnp.linalg.matrix_rank(X_aug), np.linalg.matrix_rank(X_aug.T @ X_aug), (X_aug.T @ X_aug).shape\n\n(6, 6, (7, 7))\n\n\n\npd.DataFrame(X_aug.T @ X_aug)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\n\n\n0\n12.0\n43.0\n4065.0\n1.0\n3.0\n6.0\n2.0\n\n\n1\n43.0\n311.0\n16802.0\n1.0\n16.0\n23.0\n3.0\n\n\n2\n4065.0\n16802.0\n1481651.0\n223.0\n1173.0\n2006.0\n663.0\n\n\n3\n1.0\n1.0\n223.0\n1.0\n0.0\n0.0\n0.0\n\n\n4\n3.0\n16.0\n1173.0\n0.0\n3.0\n0.0\n0.0\n\n\n5\n6.0\n23.0\n2006.0\n0.0\n0.0\n6.0\n0.0\n\n\n6\n2.0\n3.0\n663.0\n0.0\n0.0\n0.0\n2.0\n\n\n\n\n\n\n\n\nohe = OneHotEncoder(sparse_output=False, drop='first')\nohe.fit_transform(df[['direction']])\n\narray([[0., 0., 1.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [1., 0., 0.],\n       [0., 0., 1.],\n       [1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.]])\n\n\n\ndirection_ohe_n_1 = ohe.fit_transform(df[['direction']])\ncol_names_ohe_n_1 = [f\"Is it {x}?\" for x in enc.categories_[0][1:]]\ndf_ohe_n_1 = pd.DataFrame(direction_ohe_n_1, columns=col_names_ohe_n_1)\ndf_ohe_n_1\n\n\n\n\n\n\n\n\nIs it N?\nIs it S?\nIs it W?\n\n\n\n\n0\n0.0\n0.0\n1.0\n\n\n1\n0.0\n1.0\n0.0\n\n\n2\n0.0\n1.0\n0.0\n\n\n3\n0.0\n0.0\n0.0\n\n\n4\n0.0\n1.0\n0.0\n\n\n5\n0.0\n1.0\n0.0\n\n\n6\n1.0\n0.0\n0.0\n\n\n7\n0.0\n0.0\n1.0\n\n\n8\n1.0\n0.0\n0.0\n\n\n9\n1.0\n0.0\n0.0\n\n\n10\n0.0\n1.0\n0.0\n\n\n11\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\nX = np.hstack([df[['windspeed', 'vehicles']].values, df_ohe_n_1.values])\nX_aug = np.hstack([np.ones((X.shape[0], 1)), X])\n\nX_aug\n\narray([[  1.,   0., 355.,   0.,   0.,   1.],\n       [  1.,   2., 367.,   0.,   1.,   0.],\n       [  1.,   2., 447.,   0.,   1.,   0.],\n       [  1.,   1., 223.,   0.,   0.,   0.],\n       [  1.,   1., 272.,   0.,   1.,   0.],\n       [  1.,   9., 394.,   0.,   1.,   0.],\n       [  1.,   0., 333.,   1.,   0.,   0.],\n       [  1.,   3., 308.,   0.,   0.,   1.],\n       [  1.,   7., 480.,   1.,   0.,   0.],\n       [  1.,   9., 360.,   1.,   0.,   0.],\n       [  1.,   0., 125.,   0.,   1.,   0.],\n       [  1.,   9., 401.,   0.,   1.,   0.]])\n\n\n\nnp.linalg.matrix_rank(X_aug), np.linalg.matrix_rank(X_aug.T @ X_aug), (X_aug.T @ X_aug).shape\n\n(6, 6, (6, 6))\n\n\n\n# Interepeting dummy variables\n\n## dataset\n\nX = np.array(['F', 'F', 'F', 'M', 'M'])\ny = np.array([5, 5.2, 5.4, 5.8, 6])\n\n\nfrom sklearn.preprocessing import LabelBinarizer\nl = LabelBinarizer()\nl.fit_transform(X)\n\narray([[0],\n       [0],\n       [0],\n       [1],\n       [1]])\n\n\n\nX_binary = 1 - l.fit_transform(X)\n\n\nX_binary    \n\narray([[1],\n       [1],\n       [1],\n       [0],\n       [0]])\n\n\n\nlr = LinearRegression()\nlr.fit(X_binary, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nlr.coef_, lr.intercept_\n\n(array([-0.7]), 5.8999999999999995)\n\n\n\ny[(X_binary==0).flatten()].mean()\n\n5.9\n\n\n\ny[(X_binary==1).flatten()].mean()\n\n5.2"
  },
  {
    "objectID": "linear-reg/Linear Regression Notebook.html",
    "href": "linear-reg/Linear Regression Notebook.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(5,5))\nx = [3,4,5,2,6]\ny = [25,35,39,20,41]\nplt.scatter(x,y)\nplt.xlabel(\"Height in feet\")\nplt.ylabel(\"Weight in KG\")\nplt.savefig(\"height-weight-scatterplot.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.savefig(\"scatterplot-2.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\n# plt.figure(fig)\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y,label=\"Ordinary data\")\nplt.scatter([4],[0],label=\"Outlier\")\nplt.xlabel('x')\nplt.ylabel('y')\n# plt.legend(loc=(1.04,0)\nplt.legend()\nplt.savefig(\"scatterplot-3.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\n\nx = np.array(x).reshape((-1,1))\ny = np.array(y).reshape((-1,1))\nmodel = LinearRegression()\nmodel.fit(x,y)\nprediction = model.predict(x)\n\n\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.plot(x,prediction,label=\"Learnt Model\")\nfor i in range(len(x)):\n  plt.plot([x[i],x[i]],[prediction[i],y[i]],'r')\nplt.legend()\nplt.savefig(\"linear-fit.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n\nx\n\narray([-10.        ,  -9.7979798 ,  -9.5959596 ,  -9.39393939,\n        -9.19191919,  -8.98989899,  -8.78787879,  -8.58585859,\n        -8.38383838,  -8.18181818,  -7.97979798,  -7.77777778,\n        -7.57575758,  -7.37373737,  -7.17171717,  -6.96969697,\n        -6.76767677,  -6.56565657,  -6.36363636,  -6.16161616,\n        -5.95959596,  -5.75757576,  -5.55555556,  -5.35353535,\n        -5.15151515,  -4.94949495,  -4.74747475,  -4.54545455,\n        -4.34343434,  -4.14141414,  -3.93939394,  -3.73737374,\n        -3.53535354,  -3.33333333,  -3.13131313,  -2.92929293,\n        -2.72727273,  -2.52525253,  -2.32323232,  -2.12121212,\n        -1.91919192,  -1.71717172,  -1.51515152,  -1.31313131,\n        -1.11111111,  -0.90909091,  -0.70707071,  -0.50505051,\n        -0.3030303 ,  -0.1010101 ,   0.1010101 ,   0.3030303 ,\n         0.50505051,   0.70707071,   0.90909091,   1.11111111,\n         1.31313131,   1.51515152,   1.71717172,   1.91919192,\n         2.12121212,   2.32323232,   2.52525253,   2.72727273,\n         2.92929293,   3.13131313,   3.33333333,   3.53535354,\n         3.73737374,   3.93939394,   4.14141414,   4.34343434,\n         4.54545455,   4.74747475,   4.94949495,   5.15151515,\n         5.35353535,   5.55555556,   5.75757576,   5.95959596,\n         6.16161616,   6.36363636,   6.56565657,   6.76767677,\n         6.96969697,   7.17171717,   7.37373737,   7.57575758,\n         7.77777778,   7.97979798,   8.18181818,   8.38383838,\n         8.58585859,   8.78787879,   8.98989899,   9.19191919,\n         9.39393939,   9.5959596 ,   9.7979798 ,  10.        ])\n\n\n\nx[y==val]\n\narray([-2.12121212])\n\n\n\nval\n\n-2.3745682396702437\n\n\n\ny[x&lt;25]\n\narray([ 1.69351335,  1.47131924,  1.22371576,  0.9587681 ,  0.684928  ,\n        0.41069988,  0.14430802, -0.10662173, -0.33535303, -0.53629097,\n       -0.70518496, -0.83927443, -0.93737161, -0.99987837, -1.02873658,\n       -1.02731441, -1.00023337, -0.9531436 , -0.89245674, -0.82504765,\n       -0.7579375 , -0.69797133, -0.65150368, -0.62410537, -0.62030365,\n       -0.64336659, -0.69514097, -0.77595027, -0.88455735, -1.01819364,\n       -1.17265367, -1.34245142, -1.5210323 , -1.7010322 , -1.8745732 ,\n       -2.033584  , -2.17013196, -2.2767534 , -2.34676854, -2.37456824,\n       -2.35586079, -2.28786853, -2.16946611, -2.00125462, -1.7855683 ,\n       -1.52641324, -1.22934038, -0.90125763, -0.55018832, -0.18498567,\n        0.18498567,  0.55018832,  0.90125763,  1.22934038,  1.52641324,\n        1.7855683 ,  2.00125462,  2.16946611,  2.28786853,  2.35586079,\n        2.37456824,  2.34676854,  2.2767534 ,  2.17013196,  2.033584  ,\n        1.8745732 ,  1.7010322 ,  1.5210323 ,  1.34245142,  1.17265367,\n        1.01819364,  0.88455735,  0.77595027,  0.69514097,  0.64336659,\n        0.62030365,  0.62410537,  0.65150368,  0.69797133,  0.7579375 ,\n        0.82504765,  0.89245674,  0.9531436 ,  1.00023337,  1.02731441,\n        1.02873658,  0.99987837,  0.93737161,  0.83927443,  0.70518496,\n        0.53629097,  0.33535303,  0.10662173, -0.14430802, -0.41069988,\n       -0.684928  , -0.9587681 , -1.22371576, -1.47131924, -1.69351335])\n\n\n\nfunc([1.4])\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-69-a7647ebf0a8e&gt; in &lt;module&gt;\n----&gt; 1 func([1.4])\n\n&lt;ipython-input-66-27f8a49456fd&gt; in func(x)\n      1 def func(x):\n----&gt; 2     return np.sin(x) + np.sin(x/2) + np.sin(x/3)\n\nTypeError: unsupported operand type(s) for /: 'list' and 'int'"
  },
  {
    "objectID": "notebooks.html",
    "href": "notebooks.html",
    "title": "Notebooks",
    "section": "",
    "text": "Question\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotebook to demonstrate Zero shot and Few shot Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAutocorrelation Function (ACF):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Neural Networks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeshgrid\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nAug 20, 2024\n\n\nNipun Batra, Guntas Singh Saran\n\n\n\n\n\n\n\n\n\n\n\n\nSVM implementation using CVXOpt\n\n\n\n\n\nSVM using CVXOpt\n\n\n\n\n\nApr 12, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nSVM Kernels Understanding\n\n\n\n\n\nSVM Kernels Understanding\n\n\n\n\n\nApr 12, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nPCA\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nApr 6, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nKNN variants (tutorial)\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nApr 5, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nReinforcement Learning 1 Gym Environments\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nReinforcement Learning 3 Deep Q Learning\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nReinforcement Learning 2 Q Learning\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nApr 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating names using MLPs\n\n\n\n\n\nGenerating names\n\n\n\n\n\nMar 31, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression Torch\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nWhy use logits\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nSome practice problems\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nFeb 13, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nSklearn on GPU\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nKalash Kankaria\n\n\n\n\n\n\n\n\n\n\n\n\nMovie Recommendation using Matrix Factorization\n\n\n\n\n\nMovie Recommendation using Matrix Factorization\n\n\n\n\n\nFeb 6, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nPolynomial Regression with Basis Functions\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 31, 2024\n\n\nR Yeeshu Dhurandhar, Nipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nDummy Variables and Multi-collinearity\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nTaylor’s Series\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nGeometric interpretation of Linear Regression\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nBasis Expansion in Linear Regression\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nNipun Batra, R Yeeshu Dhurandhar, Kalash Kankaria, Inderjeet Singh Bhullar\n\n\n\n\n\n\n\n\n\n\n\n\nBasis Expansion in Linear Regression\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nNipun Batra, R Yeeshu Dhurandhar\n\n\n\n\n\n\n\n\n\n\n\n\nContour and Surface Plots\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees Discrete Input and Discrete Output\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nSirens v/s Linear Regression\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest Feature Importance\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nNipun Batra, R Yeeshu Dhurandhar\n\n\n\n\n\n\n\n\n\n\n\n\nRepresentation of Ensemble Models\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparams Tuning Strategies Experimentation\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 11, 2024\n\n\nRahul Chembakasseril\n\n\n\n\n\n\n\n\n\n\n\n\nWeighted Decision Trees\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\nChirag Sarda\n\n\n\n\n\n\n\n\n\n\n\n\nBias Variance Tradeoff\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nNotion of Confusion in ML\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nR Yeeshu Dhurandhar\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of Sophisticated vs Dummy Baseline ML Algorithms for Imbalanced Datasets\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nRahul Chembakasseril\n\n\n\n\n\n\n\n\n\n\n\n\nTraditional Programming vs Machine Learning\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nR Yeeshu Dhurandhar, Rahul Chembakasseril\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees Real Input and Discrete Output\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees Real Output\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees Entropy\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Diffusion to Generate Images from Text\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression - I\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression - Basis\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression - Iris dataset\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression - Cost Function\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nObject detection using YOLO and segmentation using Segment Anything\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron learning algorithm\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nJan 1, 2024\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Trees [Real I/P Real O/P, Bias vs Variance]\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nNipun Batra, Rahul Chembakasseril\n\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Tuning\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nRahul Chembakasseril\n\n\n\n\n\n\n\n\n\n\n\n\nNumpy Pandas Basics\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nR Yeeshu Dhurandhar, Nipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nBasics of Classes and Plotting Trees\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 28, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nAnscombe’s Quartet\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nCross Validation Diagrams\n\n\n\n\n\n\nML\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nDec 27, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nYouTube video to transcript using openAI whisper and summary using OLLama\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\nDec 18, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nSVM with RBF kernel\n\n\n\n\n\nSVM with RBF kernel\n\n\n\n\n\nApr 25, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nSVM Soft Margin\n\n\n\n\n\nSVM Soft Margin\n\n\n\n\n\nApr 25, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nSVM Kernel\n\n\n\n\n\nSVM Kernel\n\n\n\n\n\nApr 20, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nSVM\n\n\n\n\n\nSVM\n\n\n\n\n\nApr 20, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nCurse of Dimensionality\n\n\n\n\n\nCurse of Dimensionality\n\n\n\n\n\nApr 16, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\n1d CNN\n\n\n\n\n\n1d CNN\n\n\n\n\n\nApr 4, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nAutodiff\n\n\n\n\n\nAutodiff Helper\n\n\n\n\n\nApr 4, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nCNN\n\n\n\n\n\nCNN\n\n\n\n\n\nApr 3, 2023\n\n\nNipun Batra\n\n\n\n\n\n\n\n\n\n\n\n\nCNN Edge 2d\n\n\n\n\n\nCNNs\n\n\n\n\n\nJan 31, 2023\n\n\nNipun Batra\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "knn/knn/knn.html",
    "href": "knn/knn/knn.html",
    "title": "IRIS Dataset",
    "section": "",
    "text": "# https://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/tutorial/plot_knn_iris.html\n\nimport numpy as np\nimport pylab as plt\nfrom sklearn import neighbors, datasets\nfrom scipy.spatial.distance import cdist\nfrom sklearn.model_selection import train_test_split\niris = datasets.load_iris()\nX = iris.data[:, :2] # we only take the first two features. \nY = iris.target\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.savefig('iris.pdf', transparent=True, bbox_inches=\"tight\")"
  },
  {
    "objectID": "knn/knn/knn.html#random-blobs",
    "href": "knn/knn/knn.html#random-blobs",
    "title": "IRIS Dataset",
    "section": "Random Blobs",
    "text": "Random Blobs\n\nX, Y = datasets.make_blobs(n_samples=500, centers=3, n_features=2, random_state=0, cluster_std=1)\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.savefig('big.pdf'.format(K, B), transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .02 # step size in the mesh\nknn=neighbors.KNeighborsClassifier()\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.show()\n\n\n\n\n\n\n\n\n\nK = 1000\nB = 2\n\nh = .1 # step size in the mesh\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = np.array([probDist(temp, K, B) for temp in np.c_[xx.ravel(), yy.ravel()]])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.contourf(xx, yy, Z, 20, cmap='viridis')\nplt.colorbar();\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xticks(())\nplt.yticks(())\n\nplt.show()\n\n\n\n\n\n\n\n\n\nExample 1\n\nX = np.array([[1,5],[2,5],[3,5],[4,5],[1,3],[2,3],[3,3],[4,3],[2.5,1]])\nY = np.array([0,0,0,0,1,1,1,1,0])\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.savefig('exp.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nh = .1 # step size in the mesh\nfor K in [ 1, 3, 9]:\n    knn=neighbors.KNeighborsClassifier(n_neighbors=K)\n    knn.fit(X, Y)\n    x_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\n    y_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.set_cmap(plt.cm.Paired)\n    plt.pcolormesh(xx, yy, Z)\n\n    # Plot also the training points\n    plt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\n    plt.xlabel('X')\n    plt.ylabel('Y')\n\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n\n    plt.savefig('exp_knn_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\n    plt.clf()\n    #plt.show()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\nExample 2\n\n# X, Y = datasets.make_blobs(n_samples=50, centers=3, n_features=2, random_state=12, cluster_std=2)\n\n# plt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\n# plt.xlabel('X')\n# plt.ylabel('Y')\n\n\niris = datasets.load_iris()\nX = iris.data[:, :2] # we only take the first two features. \nY = iris.target\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.savefig('iris.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n\ntrain_err = []\ntest_err = []\nk_list = [t for t in range(1,100)]\nfor k in k_list:\n    knn=neighbors.KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, Y_train)\n    Z_train = knn.predict(X_train)\n    Z_test = knn.predict(X_test)\n    \n    train_err.append((Z_train != Y_train).sum()/Y_train.size)\n    test_err.append((Z_test != Y_test).sum()/Y_test.size)\n    #print(k,(Z_train != Y_train).sum()/Y_train.size, (Z_test != Y_test).sum()/Y_test.size)\n\n\nplt.plot(k_list, train_err)\nplt.plot(k_list, test_err)"
  },
  {
    "objectID": "knn/knn/knn.html#other-distance-metrics",
    "href": "knn/knn/knn.html#other-distance-metrics",
    "title": "IRIS Dataset",
    "section": "Other Distance Metrics",
    "text": "Other Distance Metrics\n\nX, Y = datasets.make_blobs(n_samples=100, centers=3, n_features=2, random_state=0, cluster_std=1)\n\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('X')\nplt.ylabel('Y')\n\nplt.savefig('iris_knn_data.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .1 # step size in the mesh\nK = 1\nknn=neighbors.KNeighborsClassifier(n_neighbors=K)\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.savefig('iris_knn_def_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .1 # step size in the mesh\nK = 1\nknn=neighbors.KNeighborsClassifier(n_neighbors=K, metric=\"manhattan\")\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.savefig('iris_knn_man_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot the decision boundary. For that, we will asign a color to each\n# point in the mesh [x_min, m_max]x[y_min, y_max].\nh = .1 # step size in the mesh\nK = 1\nknn=neighbors.KNeighborsClassifier(n_neighbors=K, metric=\"chebyshev\")\nknn.fit(X, Y)\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.set_cmap(plt.cm.Paired)\nplt.pcolormesh(xx, yy, Z)\n\n# Plot also the training points\nplt.scatter(X[:,0], X[:,1],c=Y, edgecolors='k')\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\nplt.savefig('iris_knn_che_{}.pdf'.format(K), transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\nCurse of Dimensionality"
  },
  {
    "objectID": "Lasso/Lasso_Regression.html",
    "href": "Lasso/Lasso_Regression.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nfrom mpl_toolkits import mplot3d\n\nfrom math import sqrt\nSPINE_COLOR = 'gray'\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.get_cmap('gnuplot2')\n\n%matplotlib inline\n# Based on: https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n\n\n#Define input array with angles from 60deg to 300deg converted to radians\nx = np.array([i*np.pi/180 for i in range(60,300,4)])\nnp.random.seed(10)  #Setting seed for reproducability\ny = 4*x + 7 + np.random.normal(0,3,len(x))\n\ny_true = 4*x + 7\nmax_deg = 20\n\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\ndata[\"ones\"] = 1\nplt.plot(data['x'],data['y'],'.', label='Data Points')\nplt.plot(data['x'], y_true,'g', label='True Function')\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.savefig('true_function.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\ndef cost(theta_0, theta_1, x, y):\n    s = 0\n    for i in range(len(x)):\n        y_i_hat = x[i]*theta_1 + theta_0\n        s += (y[i]-y_i_hat)**2\n    return s/len(x)\n\nx_grid, y_grid = np.mgrid[-4:15:.2, -4:15:.2]\n\ncost_matrix = np.zeros_like(x_grid)\nfor i in range(x_grid.shape[0]):\n    for j in range(x_grid.shape[1]):\n        cost_matrix[i, j] = cost(x_grid[i, j], y_grid[i, j], data['x'], data['y'])\n\n\ndef cost_lasso(theta_0, theta_1, x, y, lamb):\n    s = 0\n    for i in range(len(x)):\n        y_i_hat = x[i]*theta_1 + theta_0\n        s += (y[i]-y_i_hat)**2 + lamb*(abs(theta_0) + abs(theta_1))\n    return s/len(x)\n\nx_grid, y_grid = np.mgrid[-4:15:.2, -4:15:.2]\n\n\n\n#lambda  = 1000 cost curve tends to lasso objective. \nfig = plt.figure(figsize=(7,7))\nlamb_list = [10,100,1000]\nfor lamb in lamb_list:\n    lasso_cost_matrix = np.zeros_like(x_grid)\n    for i in range(x_grid.shape[0]):\n        for j in range(x_grid.shape[1]):\n            lasso_cost_matrix[i, j] = cost_lasso(x_grid[i, j], y_grid[i, j], data['x'], data['y'],lamb)\n\n\n    ax = plt.axes(projection='3d')\n    ax.plot_surface(x_grid, y_grid, lasso_cost_matrix,cmap='viridis', edgecolor='none')\n\n    ax.set_title('Least squares objective function');\n    ax.set_xlabel(r\"$\\theta_0$\")\n    ax.set_ylabel(r\"$\\theta_1$\")\n    ax.set_xlim([-4,15])\n    ax.set_ylim([-4,15])\n\n    u = np.linspace(0, np.pi, 30)\n    v = np.linspace(0, 2 * np.pi, 30)\n\n    # x = np.outer(500*np.sin(u), np.sin(v))\n    # y = np.outer(500*np.sin(u), np.cos(v))\n    # z = np.outer(500*np.cos(u), np.ones_like(v))\n    # ax.plot_wireframe(x, y, z)\n\n    ax.view_init(45, 120)\n    plt.savefig('lasso_lamb_{}_surface.pdf'.format(lamb), transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\ndef yy(p,soln):\n    xx = np.linspace(-soln,soln,100)\n    xx_final = []\n    yy_final = []\n    for x in xx:\n        if(x&gt;0):\n            xx_final.append(x)\n            xx_final.append(x)\n            y = (soln**p - x**p)**(1.0/p)\n            yy_final.append(y)\n            yy_final.append(-y)\n            \n        else:\n            xx_final.append(x)\n            xx_final.append(x)\n            y = (soln**p - (-x)**p)**(1.0/p)\n            yy_final.append(y)\n            yy_final.append(-y)\n    return xx_final, yy_final\n\n\nfig,ax = plt.subplots()\nax.fill(xx_final,yy_final)\n\n\n\n\n\n\n\n\n\nfrom matplotlib.patches import Rectangle\n\nsoln = 3\np  = 0.5\nlevels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\nfig,ax = plt.subplots()\nplt.contourf(x_grid, y_grid, cost_matrix, levels,alpha=.7)\nplt.colorbar()\nplt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\nplt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\nxx = np.linspace(-soln,soln,100)\ny1  = yy1(xx,soln,p)\ny2  = yy2(xx,soln,p)\n\nx_final = np.hstack((xx,xx))\ny_final = np.hstack((y1,y2))\n\nxx_final, yy_final = yy(p,soln)\n\nplt.fill(xx_final,yy_final)\n\n\n\nCS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\nplt.clabel(CS, inline=1, fontsize=8)\nplt.title(\"Least squares objective function\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\np1 = Rectangle((-soln, 0), np.sqrt(2)*soln,np.sqrt(2)*soln, angle = '-45', color='g', label=r'$|\\theta_0|+|\\theta_1|=3$')\nplt.scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\nplt.gca().add_patch(p1)\nplt.legend()\nplt.gca().set_aspect('equal')\n\nplt.savefig('lasso_base_contour.pdf', transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in double_scalars\n  import sys\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in double_scalars\n  \"\"\"\n\n\n\n\n\n\n\n\n\n\nfrom matplotlib.patches import Rectangle\n\n\nlevels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\nfig,ax = plt.subplots()\nplt.contourf(x_grid, y_grid, cost_matrix, levels,alpha=.7)\nplt.colorbar()\nplt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\nplt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\nplt.scatter(xx, y1, s=0.1,color='k',)\nplt.scatter(xx, y2, s=0.1,color='k',)\n\nCS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\nplt.clabel(CS, inline=1, fontsize=8)\nplt.title(\"Least squares objective function\")\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\np1 = Rectangle((-soln, 0), np.sqrt(2)*soln,np.sqrt(2)*soln, angle = '-45', color='g', label=r'$|\\theta_0|+|\\theta_1|=3$')\nplt.scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\nplt.gca().add_patch(p1)\nplt.legend()\nplt.gca().set_aspect('equal')\n\nplt.savefig('lasso_base_contour.pdf', transparent=True, bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nregressor.coef_[0] + regressor.coef_[1]\n\n5.74196012460497\n\n\n\n## Function generator. \niterations = 60\np = 4\nq = 4\nalpha  = 0.1\n\nx = np.linspace(-5,5,1000)\ny1 = x**2\ny2 = abs(x)\n\n\nfor i in range(iterations):\n    fig,ax = plt.subplots(1,2)\n    ax[0].plot(x,y1)\n    ax[1].plot(x,y2)\n    prev = p\n    qrev = q\n    p = p - 2*alpha*p\n    q = q - alpha\n    val = p\n    \n    ax[0].arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    ax[1].arrow(qrev,abs(qrev),q - qrev ,abs(q) - abs(qrev),head_width = 0.5)\n    ax[0].scatter([prev],[prev**2],s=100)\n    ax[1].scatter([qrev],abs(qrev),s=100)\n    ax[0].set_xlabel(\"x\")\n    ax[1].set_xlabel(\"x\")\n    ax[1].set_ylabel(\"Cost\")\n    ax[0].set_ylabel(\"Cost\")\n    ax[1].set_xlim(-5,5)\n    ax[1].set_ylim(0,5)\n    ax[1].set_title(\"Iteration\"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    ax[0].set_title(\"Iteration\"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    if(i==0):\n        plt.savefig(\"GD_iteration_\"+str((i+1)//10)+\".pdf\", format='pdf',transparent=True)\n    if(i%10==9):\n        plt.savefig(\"GD_iteration_\"+str((i+1)//10)+\".pdf\", format='pdf',transparent=True)\n    \n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx1,y1= 5**0.5,5\ny2,x2 = 5,5\nx_shift = 0\ny_shift = -0.5\niterations = 11\nfor i in range(iterations):\n    fig, ax = plt.subplots(nrows=1, ncols=2)\n   \n    ax[0].set_ylim(-1,10)\n    ax[0].set_xlim(-5,5)\n    ax[1].set_xlim(-5,5)\n    ax[1].set_ylim(-1,10)\n    ax[0].plot(x,x**2, color = 'blue')\n    ax[1].plot(x,abs(x),color = 'red')\n    ax[0].scatter(x1,y1,color = 'black')\n    \n    ax[0].annotate(str(round(y1,3)), (x1 + x_shift, y1+y_shift))\n    ax[1].annotate(str(y2), (x2 + x_shift, y2 + y_shift))\n    ax[1].scatter(x2,y2,color = 'black')\n    fig.suptitle('Iteration {}'.format(i))\n    if(iteratio)\n    plt.savefig('GD_Iteration_{}.pdf'.format(i))\n    \n    y1 = y1 - alpha*y1\n    y2 = y2 - 0.5\n    x2 = y2\n    x1 = y1**0.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import Lasso\nfrom matplotlib.patches import Rectangle\n\n\nfor alpha in np.linspace(1,2,5):\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n    \n    deg = 1\n    predictors = ['ones','x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = Lasso(alpha=alpha,normalize=True, fit_intercept=False)\n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n    # Plot\n    ax[0].scatter(data['x'],data['y'], label='Train')\n    ax[0].plot(data['x'], y_pred,'k', label='Prediction')\n    ax[0].plot(data['x'], y_true,'g.', label='True Function')\n    ax[0].legend() \n    ax[0].set_title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coef: {max(regressor.coef_, key=abs):.2f}\")\n\n    # Circle\n    total = abs(regressor.coef_[0]) + abs(regressor.coef_[1])\n    p1 = Rectangle((-total, 0), np.sqrt(2)*total, np.sqrt(2)*total, angle = -45, alpha=0.6, color='g', label=r'$|\\theta_0|+|\\theta_1|={:.2f}$'.format(total))\n    ax[1].add_patch(p1)\n\n    # Contour\n    levels = np.sort(np.array([2,10,20,50,100,150,200,400,600,800,1000]))\n    ax[1].contourf(x_grid, y_grid, cost_matrix, levels,alpha=.7)\n    #ax[1].colorbar()\n    ax[1].axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)\n    ax[1].axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)\n\n    CS = plt.contour(x_grid, y_grid, cost_matrix, levels, linewidths=1,colors='black')\n    ax[1].clabel(CS, inline=1, fontsize=8)\n    ax[1].set_title(\"Least squares objective function\")\n    ax[1].set_xlabel(r\"$\\theta_0$\")\n    ax[1].set_ylabel(r\"$\\theta_1$\")\n    ax[1].scatter(regressor.coef_[0],regressor.coef_[1] ,marker='x', color='r',s=25,label='Lasso Solution')\n    ax[1].scatter([7], [4],marker='*', color='r',s=25,label='Actual Solution')\n    ax[1].set_xlim([-4,15])\n    ax[1].set_ylim([-4,15])\n    ax[1].legend()\n\n    plt.savefig('lasso_{}.pdf'.format(alpha), transparent=True, bbox_inches=\"tight\")\n    plt.show()\n    plt.clf()\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\nfrom sklearn.linear_model import Lasso\n\nfor i,deg in enumerate([19]):\n    predictors = ['ones', 'x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for i,alpha in enumerate([1, 1e10]):\n        regressor = Lasso(alpha=alpha,normalize=False, fit_intercept=False)\n        regressor.fit(data[predictors],data['y'])\n        y_pred = regressor.predict(data[predictors])\n        plt.scatter(data['x'],data['y'], label='Train')\n        plt.plot(data['x'], y_pred,'k', label='Prediction')\n        plt.plot(data['x'], y_true,'g.', label='True Function')\n        plt.legend() \n        plt.title(f\"Degree: {deg} | $\\mu$: {alpha} | Max Coeff: {max(regressor.coef_, key=abs):.2f}\")\n        plt.savefig('lasso_{}_{}.pdf'.format(alpha, deg), transparent=True, bbox_inches=\"tight\")\n        plt.show()\n        plt.clf()\n\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 659.2329891662157, tolerance: 2.566256097809531\n  positive)\n\n\n\n\n\n\n\n\n\n/home/btech2/miniconda3/envs/latex_slides/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5017.444529811921, tolerance: 2.566256097809531\n  positive)\n\n\n\n\n\n\n\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\nimport pandas as pd\n\ndata = pd.read_excel(\"dataset.xlsx\")\ncols = data.columns\nalph_list = np.logspace(-5,1,num=20, endpoint=False)\ncoef_list = []\n\nfor i,alpha in enumerate(alph_list):\n    regressor = Lasso(alpha=alpha,normalize=True)\n    regressor.fit(data[cols[1:-1]],data[cols[-1]])\n    coef_list.append(regressor.coef_)\n\ncoef_list = np.abs(np.array(coef_list).T)\nfor i in range(len(cols[1:-1])):\n    plt.loglog(alph_list, coef_list[i] , label=r\"$\\theta_{}$\".format(i))\nplt.xlabel('$\\mu$ value')\nplt.ylabel('Coefficient Value')\nplt.legend() \nplt.savefig('lasso_reg.pdf', transparent=True, bbox_inches=\"tight\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This webpage contains the course materials for the course “Machine Learning” that I (Nipun Batra) teaches at Indian Institute of Technology, Gandhinagar. These materials have been developed over several years by me and excellent teaching assistants who have helped me in teaching this course."
  },
  {
    "objectID": "knn/knn/curse_dimensionality.html",
    "href": "knn/knn/curse_dimensionality.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import euclidean_distances\n\n%matplotlib inline\n\n\nnp.random.seed(0)\n\n\nnum_points = 10\n\n\ndata = {}\nfor num_dimensions in range(1, 20):\n    data[num_dimensions] = np.random.uniform(low=0.0, high=1.0, size=(num_points, num_dimensions))\n\n\nplt.scatter(data[1], np.zeros_like(data[1]))\n\n\n\n\n\n\n\n\n\nplt.scatter(data[2][:, 0], data[2][:, 1])\n\n\n\n\n\n\n\n\n\ndist = {}\nfor num_dimensions in range(1, 20):\n    dist[num_dimensions] = pd.DataFrame(euclidean_distances(data[num_dimensions], data[num_dimensions]))\n\n\nmean_distances = pd.Series({num_dimensions: dist[num_dimensions].mean().mean() for num_dimensions in range(1, 20)})\n\n\nmean_distances.plot(style='ko-')\nplt.xlabel(\"Number of dimensions (d)\")\nplt.ylabel(\"Mean distance between two points\")\nplt.savefig('curse_dist.pdf', transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nratio_max_min = pd.Series({num_dimensions:(dist[num_dimensions].replace({0:np.NAN}).max()/dist[num_dimensions].replace({0:np.NAN}).min()).mean() \n                           for num_dimensions in range(1, 20) })\n\n\nratio_max_min.plot(logy=True, style='ko-')\nplt.xlabel(\"Number of dimensions (d)\")\nplt.ylabel(\"Ratio of max to min distances\")\nplt.ylim((-1, 180))\nplt.savefig('curse_spread.pdf', transparent=True, bbox_inches=\"tight\")\n\n/home/prof/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Attempted to set non-positive bottom ylim on a log-scaled axis.\nInvalid limit will be ignored.\n  after removing the cwd from sys.path."
  },
  {
    "objectID": "ensemble/binomial.html",
    "href": "ensemble/binomial.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nSPINE_COLOR = 'gray'\n\ndef format_axes(ax):\n\n    for spine in ['top', 'right']:\n        ax.spines[spine].set_visible(False)\n\n    for spine in ['left', 'bottom']:\n        ax.spines[spine].set_color(SPINE_COLOR)\n        ax.spines[spine].set_linewidth(0.5)\n\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n    for axis in [ax.xaxis, ax.yaxis]:\n        axis.set_tick_params(direction='out', color=SPINE_COLOR)\n\n    return ax\n\n\ndef bino(n, p, k)\n\n\n choose(n, k) * p**k * (1-p)**(n-k)\n\n\nstats.binom.pmf(n=21, p=0.3, k=4)\n\n0.11277578372328753\n\n\n\nfrom scipy import stats\na=range(21)\nfor error in [0.3, 0.6]:\n    fig, ax = plt.subplots(figsize=(4,3))\n    ax.bar(a,stats.binom.pmf(n=21, p=error, k=a), color='grey', alpha=0.3)\n    ax.bar(a[11:],stats.binom.pmf(n=21, p=error, k=a[11:]), color='grey', alpha=0.9)\n\n\n    ax.set_ylabel(r'$P(X=k)$')\n    ax.set_xlabel(r'$k$')\n    #ax.set_ylim((0,0.4))\n    #ax.legend()\n    format_axes(ax)\n    #plt.fill_betweenx(3, 11, 20)\n    plt.axvline(10.5, color='k',lw=2, label=r'$k=11, \\epsilon={}$'.format(error))\n    plt.title(\"Probability that majority vote \\n (11 out of 21) is wrong = {}\".format(sum(stats.binom.pmf(n=21, p=error, k=a[11:])).round(3)))\n    plt.legend()\n    import tikzplotlib\n\n    tikzplotlib.save(\"test-{}.tex\".format(error), )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n!cat test.tex\n\n% This file was created by tikzplotlib v0.9.0.\n\\begin{tikzpicture}\n\n\\definecolor{color0}{rgb}{0.12156862745098,0.466666666666667,0.705882352941177}\n\n\\begin{axis}[\ntick align=outside,\ntick pos=left,\ntitle={Simple plot \\(\\displaystyle \\frac{\\alpha}{2}\\)},\nx grid style={white!69.0196078431373!black},\nxlabel={time (s)},\nxmin=-0.75, xmax=15.75,\nxtick style={color=black},\ny grid style={white!69.0196078431373!black},\nylabel={Voltage (mV)},\nymin=0, ymax=0.24892,\nytick style={color=black}\n]\n\\draw[draw=none,fill=color0] (axis cs:0,0) rectangle (axis cs:1.5,0.00293333333333333);\n\\draw[draw=none,fill=color0] (axis cs:1.5,0) rectangle (axis cs:3,0.0157333333333333);\n\\draw[draw=none,fill=color0] (axis cs:3,0) rectangle (axis cs:4.5,0.110866666666667);\n\\draw[draw=none,fill=color0] (axis cs:4.5,0) rectangle (axis cs:6,0.116866666666667);\n\\draw[draw=none,fill=color0] (axis cs:6,0) rectangle (axis cs:7.5,0.237066666666667);\n\\draw[draw=none,fill=color0] (axis cs:7.5,0) rectangle (axis cs:9,0.0889333333333333);\n\\draw[draw=none,fill=color0] (axis cs:9,0) rectangle (axis cs:10.5,0.0760666666666667);\n\\draw[draw=none,fill=color0] (axis cs:10.5,0) rectangle (axis cs:12,0.012);\n\\draw[draw=none,fill=color0] (axis cs:12,0) rectangle (axis cs:13.5,0.00593333333333333);\n\\draw[draw=none,fill=color0] (axis cs:13.5,0) rectangle (axis cs:15,0.000266666666666667);\n\\end{axis}\n\n\\end{tikzpicture}\n\n\n\nsum(stats.binom.pmf(n=21, p=error, k=a[11:])).round(2)\n\n0.83"
  },
  {
    "objectID": "bias-variance/Charts.html",
    "href": "bias-variance/Charts.html",
    "title": "Bias New",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nplt.style.use('seaborn-whitegrid')\n%matplotlib inline\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nplt.legend()\nplt.savefig('images/true.pdf', transparent=True)\n\n\n\n\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.legend()\nplt.savefig('images/data.pdf', transparent=True)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n# plt.fill_between(data['x'], y_true-dy, y_true+dy, color='green',alpha=0.2, label='Variance')\nplt.errorbar(data['x'][15], y_true[15], yerr=dy, fmt='k', capsize=5, label='Variance')\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\n\nplt.legend()\n\n\nplt.savefig('images/data_var.pdf', transparent=True)\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 16\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.legend()\n\nplt.savefig('images/biasn_1.pdf', transparent=True)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.plot(data['x'], [data['y'].mean() for _ in data['x']], ':r', label='Prediction')\nplt.legend()\n\nplt.savefig('images/biasn_2.pdf', transparent=True)\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\nplt.plot(data['x'], data['y'], '.', label='Actual Prices')\nplt.plot(data['x'], [data['y'].mean() for _ in data['x']], ':r', label='Prediction')\nplt.fill_between(x, y_true, [data['y'].mean() for _ in data['x']], color='green',alpha=0.2, label='Bias')\nplt.legend()\n\nplt.savefig('images/biasn_3.pdf', transparent=True)"
  },
  {
    "objectID": "bias-variance/Charts.html#bias-old",
    "href": "bias-variance/Charts.html#bias-old",
    "title": "Bias New",
    "section": "Bias Old",
    "text": "Bias Old\n\nx1 = np.array([i*np.pi/180 for i in range(0,70,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny1 = np.sin(x1) + 0.5 + np.random.normal(0,var,len(x1))\ny_true = np.sin(x) + 0.5\n\n\nx2 = np.array([i*np.pi/180 for i in range(20,90,2)])\nnp.random.seed(40) \ny2 = np.sin(x2) + 0.5 + np.random.normal(0,var,len(x2))\ny_true = np.sin(x) + 0.5\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\n\nax[0].plot(x, y_true, 'g', label='True Function')\nax[0].set_xlabel('Size (sq.ft)')\nax[0].set_ylabel('Price (\\$)')\nax[0].plot(x1, y1, '.', label='Actual Prices')\n\nax[1].plot(x, y_true, 'g', label='True Function')\nax[1].set_xlabel('Size (sq.ft)')\nax[1].set_ylabel('Price (\\$)')\nax[1].plot(x2, y2, '.', label='Actual Prices')\n\nplt.savefig('images/bias1.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\n\nax[0].plot(x, y_true, 'g', label='True Function')\nax[0].set_xlabel('Size (sq.ft)')\nax[0].set_ylabel('Price (\\$)')\nax[0].plot(x1, y1, '.', label='Actual Prices')\nax[0].plot(x, [y1.mean() for _ in x], 'r:', label='Prediction')\n\nax[1].plot(x, y_true, 'g', label='True Function')\nax[1].set_xlabel('Size (sq.ft)')\nax[1].set_ylabel('Price (\\$)')\nax[1].plot(x2, y2, '.', label='Actual Prices')\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=3)\nplt.savefig('images/bias2.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\n\nax[0].plot(x, y_true, 'g', label='True Function')\nax[0].set_xlabel('Size (sq.ft)')\nax[0].set_ylabel('Price (\\$)')\nax[0].plot(x1, y1, '.', label='Actual Prices')\nax[0].plot(x, [y1.mean() for _ in x], 'r:', label='Prediction')\n\nax[1].plot(x, y_true, 'g', label='True Function')\nax[1].set_xlabel('Size (sq.ft)')\nax[1].set_ylabel('Price (\\$)')\nax[1].plot(x2, y2, '.', label='Actual Prices')\nax[1].plot(x, [y2.mean() for _ in x], 'r:', label='Prediction')\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=3)\nplt.savefig('images/bias3.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nplt.plot(x, [y2.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train1)}$')\nplt.plot(x, [y1.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train2)}$')\nplt.plot(x, [y2.mean()-0.3 for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train3)}$')\nplt.plot(x, [y1.mean()+0.1 for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train4)}$')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\n\n\nplt.savefig('images/bias4.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 3))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nplt.plot(x, [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x], 'r-.', label=r'$f_\\bar{\\theta}$')\n\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\n\n\nplt.savefig('images/bias5.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nplt.plot(x, y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\nfit = np.array([(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x])\nplt.plot(x, [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x], 'r-.', label=r'$f_\\bar{\\theta}$')\nplt.fill_between(x, y_true, fit, color='green',alpha=0.2, label='Bias')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\n\n\nplt.savefig('images/bias6.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nVarying Degree on Bias\n\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 16\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\n\n\nfrom sklearn.linear_model import LinearRegression\n\nseed=10\nfig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(10, 4))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nax[0].plot(x, [y.mean() for _ in x], 'r-.', label=r'$f_\\bar{\\theta}$')\nax[0].plot(data['x'], data['y'], '.b', label='Actual Prices')\nax[0].plot(data['x'], y_true,'g', label='True Function')\nax[0].fill_between(x, y_true, [y.mean() for _ in x], color='green',alpha=0.2, label='Bias')\nax[0].set_title(f\"Degree = 0\")\nfor i,deg in enumerate([1]):\n    i=i+1\n    predictors = ['x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n    ax[i].plot(data['x'],data['y'], '.b', label='Actual Prices')\n    ax[i].plot(data['x'], y_pred,'-.r', label=r'$f_\\bar{\\theta}$')\n    ax[i].plot(data['x'], y_true,'g', label='True Function')\n    ax[i].fill_between(x, y_true, y_pred, color='green',alpha=0.2, label='Bias')\n    ax[i].set_title(f\"Degree = {deg}\")\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='lower center', frameon=True, fancybox=True, framealpha=1, ncol=4)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/bias7.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nseed=10\nfig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(10, 4))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nfor i,deg in enumerate([2,3]):\n    predictors = ['x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n    ax[i].plot(data['x'],data['y'], '.b', label='Actual Prices')\n    ax[i].plot(data['x'], y_pred,'-.r', label=r'$f_\\bar{\\theta}$')\n    ax[i].plot(data['x'], y_true,'g', label='True Function')\n    ax[i].fill_between(x, y_true, y_pred, color='green',alpha=0.2, label='Bias')\n    ax[i].set_title(f\"Degree = {deg}\")\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='lower center', frameon=True, fancybox=True, framealpha=1, ncol=4)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/bias8.pdf', transparent=True, bbox_inches='tight')"
  },
  {
    "objectID": "bias-variance/Charts.html#variance",
    "href": "bias-variance/Charts.html#variance",
    "title": "Bias New",
    "section": "Variance",
    "text": "Variance\n\nx = np.array([i*np.pi/180 for i in range(0,90,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5\nmax_deg = 25\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\n\n\nx1 = np.array([i*np.pi/180 for i in range(0,70,2)])\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 0.25\ndy = 2*0.25\ny1 = np.sin(x1) + 0.5 + np.random.normal(0,var,len(x1))\ny_true = np.sin(x) + 0.5\nx2 = np.array([i*np.pi/180 for i in range(20,90,2)])\nnp.random.seed(40) \ny2 = np.sin(x2) + 0.5 + np.random.normal(0,var,len(x2))\ny_true = np.sin(x) + 0.5\n\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\ndy = y2.mean()-(2*y2.mean()+2*y1.mean()-0.2)/4\nplt.plot(x, [y2.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train1)}$')\nplt.plot(x, [y1.mean() for _ in x], 'b-.', label=r'$f_{\\hat\\theta(train2)}$')\nplt.plot(x, [y2.mean()-0.3 for _ in x], 'c-.', label=r'$f_{\\hat\\theta(train3)}$')\nplt.plot(x, [y1.mean()+0.1 for _ in x], 'y-.', label=r'$f_{\\hat\\theta(train4)}$')\n# plt.errorbar(x[::3], [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x][::3], yerr=dy, fmt='k', capsize=5, label='Variance')\nplt.fill_between(x, [y2.mean() for _ in x], [y2.mean()-0.3 for _ in x], color='green',alpha=0.2, label='Variance')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=4)\n\n\nplt.savefig('images/var1.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\nplt.xlabel('Size (sq.ft)')\nplt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\nplt.ylim(0,2)\nplt.xlim(0,1.6)\n\ndy = y2.mean()-(2*y2.mean()+2*y1.mean()-0.2)/4\nplt.plot(x, [y2.mean() for _ in x], 'r-.', label=r'$f_{\\hat\\theta(train1)}$')\nplt.plot(x, [y1.mean() for _ in x], 'b-.', label=r'$f_{\\hat\\theta(train2)}$')\nplt.plot(x, [y2.mean()-0.3 for _ in x], 'c-.', label=r'$f_{\\hat\\theta(train3)}$')\nplt.plot(x, [y1.mean()+0.1 for _ in x], 'y-.', label=r'$f_{\\hat\\theta(train4)}$')\nplt.errorbar(x[::4], [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x][::4], yerr=dy, fmt='k', capsize=3, label='Variance')\n# plt.fill_between(x, [y2.mean() for _ in x], [y2.mean()-0.3 for _ in x], color='green',alpha=0.2, label='Variance')\n\nplt.legend(loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=4)\n\n\nplt.savefig('images/var2.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nVaraince Variation\n\nfrom sklearn.linear_model import LinearRegression\n\nfig, ax = plt.subplots(nrows=2, ncols=2, sharey=True, figsize=(10, 8))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nmodles = []\n\nfor i,seed in enumerate([2,4,8,16]):\n    np.random.seed(seed)\n    y_random = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\n    data_x_s = [x**(i+1) for i in range(max_deg)] + [y_random]\n    data_c_s = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\n    data_s = pd.DataFrame(np.column_stack(data_x_s),columns=data_c_s)\n      \n    deg = 25\n    predictors = ['x']\n    if deg &gt; 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data_s[predictors],data_s['y'])\n    y_pred = regressor.predict(data_s[predictors])\n    \n    modles.append(y_pred)\n    \n    ax[int(i/2)][i%2].plot(data_s['x'],data_s['y'], '.b', label='Data Point')\n#     ax[i].plot(data_n['x'],data_n['y'], 'ok', label='UnSelected Points')\n    ax[int(i/2)][i%2].plot(data_s['x'], y_pred,'r-.', label='Prediction')\n    ax[int(i/2)][i%2].plot(data['x'], y_true,'g-', label='True Function')\n#     ax[i].set_title(f\"{deg} : {max(regressor.coef_, key=abs):.2f}\")\n    \n\nhandles, labels = ax[0][0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='center', frameon=True, fancybox=True, framealpha=1, ncol=4)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/var3.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nmodles = []\n\nfor i,seed in enumerate(range(1,50)):\n    np.random.seed(seed)\n    y_random = np.sin(x) + 0.5 + np.random.normal(0,var,len(x))\n    data_x_s = [x**(i+1) for i in range(max_deg)] + [y_random]\n    data_c_s = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\n    data_s = pd.DataFrame(np.column_stack(data_x_s),columns=data_c_s)\n      \n    deg = 25\n    predictors = ['x']\n    if deg &gt; 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data_s[predictors],data_s['y'])\n    y_pred = regressor.predict(data_s[predictors])\n    \n    modles.append(y_pred)\n\n\nfig, ax = plt.subplots(nrows=1, ncols=1, sharey=True, figsize=(8, 4))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,1.6), ylim=(0,2))\n\nmodles=np.array(modles)\n\n# ax[0].plot(x, modles.mean(axis=0), 'r-.', label='Average Fit')\n# ax[0].plot(data['x'], y_true,'g-', label='True Function')\n# ax[0].set_xlabel('Size (sq.ft)')\n# ax[0].set_ylabel('Price (\\$)')\n\n# ax[1].errorbar(x[::4], modles.mean(axis=0)[::4], yerr=2*modles.std(axis=0)[::4], fmt=':k', capsize=3, label='Variance')\n# ax[1].plot(x, modles[1], 'c-.', label=r'$f_{\\hat\\theta(train1)}$')\n# ax[1].plot(x, modles[2], 'y-.', label=r'$f_{\\hat\\theta(train2)}$')\n# ax[1].plot(x, modles[3], 'm-.', label=r'$f_{\\hat\\theta(train3)}$')\n# ax[1].plot(data['x'], y_true,'g-', label='True Function')\n# ax[1].set_xlabel('Size (sq.ft)')\n\nax.errorbar(x[::4], modles.mean(axis=0)[::4], yerr=2*modles.std(axis=0)[::4], fmt=':k', capsize=3, label='Variance')\nax.plot(x, modles[1], 'c-.', label=r'$f_{\\hat\\theta(train1)}$')\nax.plot(x, modles[2], 'y-.', label=r'$f_{\\hat\\theta(train2)}$')\nax.plot(x, modles[3], 'm-.', label=r'$f_{\\hat\\theta(train3)}$')\nax.plot(data['x'], y_true,'g-', label='True Function')\nax.set_xlabel('Size (sq.ft)')\n\n# plt.plot(x, modles.mean(axis=0), 'k.-', label=r'Average Fit')\n# plt.plot(x, modles[2], 'y-.', label=r'$f_{\\hat\\theta(train3)}$')\n# plt.errorbar(x[::4], [(2*y2.mean()+2*y1.mean()-0.2)/4 for _ in x][::4], yerr=dy, fmt='k', capsize=3, label='Variance')\n# plt.fill_between(x, [y2.mean() for _ in x], [y2.mean()-0.3 for _ in x], color='green',alpha=0.2, label='Variance')\n\n# handles, labels = [(a + b) for a, b in zip(ax[0].get_legend_handles_labels(), ax[1].get_legend_handles_labels())]\nhandles, labels = ax.get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper center', frameon=True, fancybox=True, framealpha=1, ncol=5)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/var4.pdf', transparent=True, bbox_inches='tight')"
  },
  {
    "objectID": "bias-variance/Charts.html#bias-variance-tradeoff",
    "href": "bias-variance/Charts.html#bias-variance-tradeoff",
    "title": "Bias New",
    "section": "Bias-Variance Tradeoff",
    "text": "Bias-Variance Tradeoff\n\nx = x = np.linspace(0, 4*np.pi, 201)\nnp.random.seed(10)  #Setting seed for reproducability\nvar = 1\np = np.poly1d([1, 2, 3])\ny = np.sin(x) + 0.5*x -  0.05*x**2 + np.random.normal(0,var,len(x))\ny_true = np.sin(x) + 0.5*x - 0.05*x**2\nmax_deg = 20\ndata_x = [x**(i+1) for i in range(max_deg)] + [y]\ndata_c = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\ndata = pd.DataFrame(np.column_stack(data_x),columns=data_c)\n\nplt.plot(data['x'], y_true, 'g', label='True Function')\n# plt.xlabel('Size (sq.ft)')\n# plt.ylabel('Price (\\$)')\nplt.xticks([],[])\nplt.yticks([],[])\n# plt.ylim(-2,2)\n# plt.xlim(0,4*np.pi)\nplt.plot(data['x'], data['y'], '.', label='Data Points')\nplt.legend()\nplt.savefig('images/bv-1.pdf', transparent=True, bbox_inches='tight')\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\nseed=10\nfig, ax = plt.subplots(nrows=2, ncols=3, sharey=True, figsize=(10, 5))\nplt.setp(ax, xticks=[], xticklabels=[], yticks=[], yticklabels=[], xlim=(0,4*np.pi))\n\ndegs = [1,3,7]\nfor i,deg in enumerate(degs):\n    predictors = ['x']\n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n    \n#     print(predictors)\n    regressor = LinearRegression(normalize=True)  \n    regressor.fit(data[predictors],data['y'])\n    y_pred = regressor.predict(data[predictors])\n\n#     ax[0][i].plot(data['x'],data['y'], '.b', label='Actual Prices')\n    ax[0][i].plot(data['x'], y_pred,'-.r', label='Prediction')\n    ax[0][i].plot(data['x'], y_true,'g', label='True Function')\n    ax[0][i].fill_between(x, y_true, y_pred, color='green',alpha=0.2, label='Bias')\n    ax[0][i].set_title(f\"Degree = {deg}\")\n\nfor i,deg in enumerate(degs):    \n    predictors = ['x']\n    models=[]\n    \n    if deg &gt;= 2:\n        predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n    for t,seed in enumerate(range(1,50)):\n        np.random.seed(seed)\n        y_random = np.sin(x) + 0.5*x - 0.05*x**2 + np.random.normal(0,var,len(x))\n        data_x_s = [x**(i+1) for i in range(max_deg)] + [y_random]\n        data_c_s = ['x'] + ['x_{}'.format(i+1) for i in range(1,max_deg)] + ['y']\n        data_s = pd.DataFrame(np.column_stack(data_x_s),columns=data_c_s)\n\n        predictors = ['x']\n        if deg &gt;= 2:\n            predictors.extend(['x_%d'%i for i in range(2,deg+1)])  \n\n        regressor = LinearRegression(normalize=True)  \n        regressor.fit(data_s[predictors],data_s['y'])\n        y_pred = regressor.predict(data_s[predictors])\n\n        models.append(y_pred)\n    \n    models=np.array(models)\n    ax[1][i].errorbar(x[::7], models.mean(axis=0)[::7], yerr=2*models.std(axis=0)[::7], fmt=':k', capsize=3, label='Variance')\n    ax[1][i].plot(data['x'], y_true,'g-', label='True Function')\n\nhandles, labels = [(a + b) for a, b in zip(ax[0][0].get_legend_handles_labels(), ax[1][0].get_legend_handles_labels())]\nfig.legend(handles, labels, loc='center', frameon=True, fancybox=True, framealpha=1, ncol=5)\nplt.subplots_adjust(wspace=0.01, hspace=0)\nplt.savefig('images/bv-2.pdf', transparent=True, bbox_inches='tight')"
  },
  {
    "objectID": "notebooks/svm.html",
    "href": "notebooks/svm.html",
    "title": "SVM",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n%matplotlib inline\n\n\n# Linearly separable data in 2d\n\n# Generate data\nnp.random.seed(0)\nX = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\ny = [0] * 20 + [1] * 20\n\n# Plot data\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm.Paired)\n\n\n\n\n\n\n\n\n\n# Fit SVM\nfrom sklearn.svm import SVC\nclf = SVC(kernel='linear')\nclf.fit(X, y)\n\n# Plot decision boundary\nw = clf.coef_[0]\na = -w[0] / w[1]\nxx = np.linspace(-5, 5)\nyy = a * xx - (clf.intercept_[0]) / w[1]\nplt.plot(xx, yy, 'k-')\n\n# Plot support vectors\nplt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80, facecolors='none')\n\n# Plot margins\nb = clf.support_vectors_[0]\nyy_down = a * xx + (b[1] - a * b[0])\nb = clf.support_vectors_[-1]\nyy_up = a * xx + (b[1] - a * b[0])\nplt.plot(xx, yy_down, 'k--')\nplt.plot(xx, yy_up, 'k--')\n\n# Plot data\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm.Paired)\n\n\n\n\n\n\n\n\n\n# Magnitude of w\nnp.linalg.norm(w)\n\n1.111010607589106"
  },
  {
    "objectID": "notebooks/dummy-baselines.html",
    "href": "notebooks/dummy-baselines.html",
    "title": "Comparison of Sophisticated vs Dummy Baseline ML Algorithms for Imbalanced Datasets",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nfrom latexify import latexify\n# retina\n%config InlineBackend.figure_format = 'retina'\n\nClassification\n\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Create an imbalanced dataset with two features\nX, y = make_blobs(\n    n_samples=[4500,500],\n    n_features=2,  # Use only two features\n    cluster_std=[4.0,4.0],\n    random_state=42\n)\n\nprint(len(y))\nprint(len(y[y==1]))\nprint(len(y[y==0]))\n\n5000\n500\n4500\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Print the shapes of the train and test sets\nprint(\"Train set shape:\", X_train.shape, y_train.shape)\nprint(\"Test set shape:\", X_test.shape, y_test.shape)\n\nTrain set shape: (4000, 2) (4000,)\nTest set shape: (1000, 2) (1000,)\n\n\n\ncolors = ['blue' if label == 0 else 'red' for label in y_train]\nlatexify(fig_width=7, fig_height=5)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=colors, alpha=0.8)\nplt.xlabel('$x_1$')\nplt.ylabel('$x_2$')\nplt.title('Training Data')\n\nText(0.5, 1.0, 'Training Data')\n\n\n\n\n\n\n\n\n\n\nprint(len(y_train[y_train==1]))\nprint(len(y_train[y_train==0]))\n\n393\n3607\n\n\n\nprint(len(y_test[y_test==1]))\nprint(len(y_test[y_test==0]))\n\n107\n893\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\n\nrf_classifier = RandomForestClassifier()\nrf_classifier.fit(X_train, y_train)\n\ny_pred = rf_classifier.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\n# Print the accuracy and F1 score\nprint(\"Accuracy:\", accuracy)\nprint(\"F1 Score:\", f1)\n\nAccuracy: 0.945\nF1 Score: 0.7208121827411167\n\n\n\nfrom sklearn.dummy import DummyClassifier\n\ndummy_classifier = DummyClassifier(strategy='stratified')\ndummy_classifier.fit(X_train, y_train)\n\ny_pred_dummy = dummy_classifier.predict(X_test)\naccuracy_dummy = accuracy_score(y_test, y_pred_dummy)\nf1_dummy = f1_score(y_test, y_pred_dummy)\n\n# Print the accuracy and F1 score for the dummy classifier\nprint(\"Dummy Classifier Accuracy:\", accuracy_dummy)\nprint(\"Dummy Classifier F1 Score:\", f1_dummy)\n\nDummy Classifier Accuracy: 0.81\nDummy Classifier F1 Score: 0.07766990291262137\n\n\nRegression\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.dummy import DummyRegressor\n\n# Generate synthetic regression dataset with noise\nnp.random.seed(42)\nX = np.linspace(0, 1, 500).reshape(-1, 1)\nslope = 1.2\ny_true = slope * X.squeeze()\ny = y_true + np.random.normal(scale=0.5, size=len(X))\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Print the shapes of the train and test sets\nprint(\"Train set shape:\", X_train.shape, y_train.shape)\nprint(\"Test set shape:\", X_test.shape, y_test.shape)\n\nTrain set shape: (400, 1) (400,)\nTest set shape: (100, 1) (100,)\n\n\n\n# Scatter plot of the training data\nplt.scatter(X_train, y_train, color='red', alpha=0.8, label='Actual')\nplt.plot(X, y_true, color='blue', label='True')\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.legend()\nplt.title('Training Data')\n\nText(0.5, 1.0, 'Training Data')\n\n\n\n\n\n\n\n\n\n\n# RandomForestRegressor\nrf_regressor = RandomForestRegressor(n_estimators=10, random_state=42)\nrf_regressor.fit(X_train, y_train)\n\ny_pred_rf = rf_regressor.predict(X_test)\nmse_rf = mean_squared_error(y_test, y_pred_rf)\n\n# Print the Mean Squared Error for the RandomForestRegressor\nprint(\"Random Forest Regressor MSE:\", mse_rf)\n\nRandom Forest Regressor MSE: 0.3446175878909235\n\n\n\nplt.scatter(X_test, y_test, color='red', alpha=0.8, label='Actual')\nplt.scatter(X_test, y_pred_rf, color='black', alpha=0.8, label='Predicted')\nplt.plot(X_test, slope*X_test.squeeze(), color='blue', label='True')\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title('Predictions')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# DummyRegressor\ndummy_regressor = DummyRegressor(strategy='mean')\ndummy_regressor.fit(X_train, y_train)\n\ny_pred_dummy = dummy_regressor.predict(X_test)\nmse_dummy = mean_squared_error(y_test, y_pred_dummy)\n\n# Print the Mean Squared Error for the DummyRegressor\nprint(\"Dummy Regressor MSE:\", mse_dummy)\n\nDummy Regressor MSE: 0.39550009552721027\n\n\n\nplt.scatter(X_test, y_test, color='red', alpha=0.8, label='Actual')\nplt.scatter(X_test, y_pred_dummy, color='black', alpha=0.8, label='Predicted')\nplt.plot(X_test, slope*X_test.squeeze(), color='blue', label='True')\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.title('Predictions')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "notebooks/movie-recommendation-knn-mf.html",
    "href": "notebooks/movie-recommendation-knn-mf.html",
    "title": "Movie Recommendation using Matrix Factorization",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# Now working with real data\n\n# Load the data\n\ndf = pd.read_excel(\"Movie-Recommendation-2024-Fall.xlsx\")\ndf.head(10)\n\n\n\n\n\n\n\n\nTimestamp\nYour name\nSholay\nSwades (We The People)\nThe Matrix (I)\nInterstellar\nDangal\nTaare Zameen Par\nShawshank Redemption\nThe Dark Knight\nNotting Hill\nUri: The Surgical Strike\n\n\n\n\n0\n2024-09-09 18:12:41.227\nP.Sai Keerthana\n3.0\n5.0\n5.0\n4.0\n5.0\n5.0\n3.0\n3.0\n4.0\n5.0\n\n\n1\n2024-09-09 18:12:47.984\nPradeep\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n4.0\n5.0\n3.0\n5.0\n\n\n2\n2024-09-09 18:12:54.979\nUday Sankar Gottipalli\n3.0\nNaN\n5.0\n5.0\n5.0\nNaN\nNaN\nNaN\nNaN\n4.0\n\n\n3\n2024-09-09 18:12:58.795\nAashmun Gupta\n4.0\n5.0\n4.0\n5.0\n3.0\n5.0\n1.0\n3.0\n4.0\n4.0\n\n\n4\n2024-09-09 18:13:01.629\nSiddharth Mohanty\n4.0\n3.0\n5.0\n5.0\n5.0\n5.0\n3.0\n5.0\n2.0\n4.0\n\n\n5\n2024-09-09 18:13:17.288\nAyush Shrivastava\n4.0\n4.0\n5.0\n5.0\n3.0\n4.0\nNaN\nNaN\nNaN\n4.0\n\n\n6\n2024-09-09 18:15:18.092\nAbhay Pisharodi\n3.0\nNaN\nNaN\n3.0\n4.0\n4.0\nNaN\n4.0\nNaN\n5.0\n\n\n7\n2024-09-09 18:16:12.892\nShreya Mali\nNaN\nNaN\nNaN\nNaN\n3.0\n4.0\nNaN\nNaN\nNaN\nNaN\n\n\n8\n2024-09-09 18:17:39.678\nShounak Ranade\n3.0\nNaN\n5.0\n5.0\n4.0\n5.0\n5.0\n3.0\nNaN\n4.0\n\n\n9\n2024-09-09 18:17:58.374\nPrathamesh P. Shanbhag\n3.0\n4.0\n5.0\n4.0\n4.0\n4.0\nNaN\nNaN\nNaN\n5.0\n\n\n\n\n\n\n\n\n# Discard the timestamp column\n\ndf = df.drop('Timestamp', axis=1)\n\n# Make the \"Your Name\" column the index\n\ndf = df.set_index('Your name')\ndf\n\n\n\n\n\n\n\n\nSholay\nSwades (We The People)\nThe Matrix (I)\nInterstellar\nDangal\nTaare Zameen Par\nShawshank Redemption\nThe Dark Knight\nNotting Hill\nUri: The Surgical Strike\n\n\nYour name\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nP.Sai Keerthana\n3.0\n5.0\n5.0\n4.0\n5.0\n5.0\n3.0\n3.0\n4.0\n5.0\n\n\nPradeep\n5.0\n5.0\n5.0\n5.0\n5.0\n5.0\n4.0\n5.0\n3.0\n5.0\n\n\nUday Sankar Gottipalli\n3.0\nNaN\n5.0\n5.0\n5.0\nNaN\nNaN\nNaN\nNaN\n4.0\n\n\nAashmun Gupta\n4.0\n5.0\n4.0\n5.0\n3.0\n5.0\n1.0\n3.0\n4.0\n4.0\n\n\nSiddharth Mohanty\n4.0\n3.0\n5.0\n5.0\n5.0\n5.0\n3.0\n5.0\n2.0\n4.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nVadithe Venkat Akhilesh Naik\n4.0\n3.0\n4.0\n4.0\n5.0\n5.0\n3.0\n5.0\n3.0\n5.0\n\n\nRudra\n3.0\nNaN\n5.0\n5.0\n3.0\n4.0\n5.0\n5.0\n3.0\n1.0\n\n\nMukesh\n3.0\n4.0\n5.0\n5.0\n3.0\n3.0\n5.0\n5.0\n4.0\n4.0\n\n\nNiyati siju\n5.0\n5.0\n5.0\n5.0\n4.0\n5.0\n2.0\n2.0\n2.0\n4.0\n\n\nRahul Singal\n2.0\nNaN\n1.0\nNaN\n3.0\n1.0\nNaN\nNaN\nNaN\n1.0\n\n\n\n\n83 rows × 10 columns\n\n\n\n\ndf.index\n\nIndex(['P.Sai Keerthana ', 'Pradeep ', 'Uday Sankar Gottipalli',\n       'Aashmun Gupta', 'Siddharth Mohanty', 'Ayush Shrivastava',\n       'Abhay Pisharodi', 'Shreya Mali', 'Shounak Ranade',\n       'Prathamesh P. Shanbhag', 'Suruchi Hardaha', 'Balkrishna Sehra',\n       'Het Shukla', 'N. Eshwar karthikeya ', 'Dhruv Patel',\n       'Shreyas Dharmatti ', 'Mohit', 'Praveen Rathod ', 'Dinesh', 'Rahul',\n       'Devansh Lodha', 'Abhinav ', 'Tejas Lohia', 'Laksh Jain ',\n       'Burra Saharsh', 'Vanshri', 'Karan Gandhi', 'Romit Mohane ',\n       'Vedant Acharya', 'Abhiroop Chintalapudi', 'Yash sahu', 'Dinesh',\n       'Manish Prasad', 'Parthiv', 'Shivansh Soni',\n       'Chepuri Venkata Naga Thrisha', 'Abhinav Kumar', 'Anurag',\n       'Bhavya Parmar ', 'Soham Shrivastava ', 'Aditya Borate', 'Hemant ',\n       'Abhinav Singh Yadav ', 'Arul Singh', 'Harinarayan J ',\n       'uday kumar jarapala', 'Sailesh Panda', 'Yash Sonone ', 'Sayak Dutta',\n       'Biswajit Rakshit', 'Praanshu ', 'Samarth Sonawane ',\n       'Shah Harshil Hardik ', 'Paras Prashant Shirvale ', 'Venkatakrishnan E',\n       'Patel Ridham Vijaykumar', 'Kaushal', 'Suriya', 'Nishchay Bhutoria',\n       'Darpana', 'Umang Shikarvar', 'Rishabh Jogani', 'Shriniket Behera',\n       'Srivaths P', 'M. Lakshmi Manasa ', 'Afraz', 'Arpita Kumawat ',\n       'Naveen pal', 'Naveen pal', 'Vedant', 'Tanishq Bhushan Chaudhari',\n       'Shardul Junagade', 'Soham Gaonkar', 'Xyz', 'Akash Gupta ',\n       'Pranav Somase', 'Maharshi Patel', 'Susmita R',\n       'Vadithe Venkat Akhilesh Naik', 'Rudra', 'Mukesh ', 'Niyati siju ',\n       'Rahul Singal'],\n      dtype='object', name='Your name')\n\n\n\n# Get index for user and movie\nuser = 'Ayush Shrivastava'\n\nprint(user in df.index)\n\n# Get the movie ratings for user\nuser_ratings = df.loc[user]\nuser_ratings\n\nTrue\n\n\nSholay                      4.0\nSwades (We The People)      4.0\nThe Matrix (I)              5.0\nInterstellar                5.0\nDangal                      3.0\nTaare Zameen Par            4.0\nShawshank Redemption        NaN\nThe Dark Knight             NaN\nNotting Hill                NaN\nUri: The Surgical Strike    4.0\nName: Ayush Shrivastava, dtype: float64\n\n\n\n# Number of missing values\ndf.isnull().sum()\n\nSholay                      20\nSwades (We The People)      35\nThe Matrix (I)               6\nInterstellar                14\nDangal                       4\nTaare Zameen Par             8\nShawshank Redemption        35\nThe Dark Knight             25\nNotting Hill                45\nUri: The Surgical Strike    12\ndtype: int64\n\n\n\n# Generic Matrix Factorization (without missing values)\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nn_users, n_movies = 20, 10\n\n# A is a matrix of size (n_users, n_movies) randomly generated values between 1 and 5\nA = torch.randint(1, 6, (n_users, n_movies), dtype=torch.float)\nA\n\ntensor([[3., 5., 1., 2., 2., 5., 1., 5., 5., 3.],\n        [1., 5., 4., 4., 5., 5., 5., 1., 4., 2.],\n        [3., 3., 5., 1., 1., 5., 5., 1., 1., 2.],\n        [4., 3., 2., 2., 5., 4., 4., 5., 2., 1.],\n        [4., 1., 1., 5., 4., 2., 2., 4., 3., 1.],\n        [4., 2., 5., 4., 3., 5., 5., 3., 4., 3.],\n        [1., 3., 1., 3., 1., 3., 5., 4., 1., 2.],\n        [1., 1., 2., 4., 3., 3., 2., 1., 5., 1.],\n        [3., 1., 1., 1., 5., 4., 2., 5., 5., 2.],\n        [2., 2., 1., 5., 3., 5., 3., 2., 1., 4.],\n        [3., 3., 3., 4., 4., 1., 5., 3., 4., 4.],\n        [5., 1., 1., 3., 2., 2., 4., 5., 2., 3.],\n        [3., 1., 4., 1., 1., 3., 3., 3., 1., 2.],\n        [1., 5., 1., 3., 3., 4., 1., 4., 2., 3.],\n        [1., 2., 1., 3., 5., 4., 4., 1., 3., 3.],\n        [3., 4., 3., 4., 4., 2., 2., 5., 4., 2.],\n        [1., 4., 4., 4., 4., 1., 3., 5., 2., 3.],\n        [1., 5., 4., 3., 3., 2., 3., 2., 3., 4.],\n        [5., 4., 2., 3., 2., 4., 5., 2., 1., 1.],\n        [4., 4., 2., 1., 2., 3., 4., 1., 4., 4.]])\n\n\n\nA.shape\n\ntorch.Size([20, 10])\n\n\nLet us decompose A as WH. W is of shape (n, k) and H is of shape (k, n). We can write the above equation as: A = WH\n\n# Randomly initialize A and B\n\n# Set device to cuda:0 if available and on Linux\n# If on mac try mps\nif torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\nprint(\"Working on device: \", device)\nr = 3\nW = torch.randn(n_users, r, requires_grad=True, device=device)\nH = torch.randn(r, n_movies, requires_grad=True, device=device)\n\nA = A.to(device)\n\n# Compute the loss\nwith torch.no_grad():\n    loss = torch.norm(torch.mm(W, H) - A)\n    print(loss)\n\nWorking on device:  mps\ntensor(50.2602, device='mps:0')\n\n\n\nwith torch.no_grad():\n    temp_pred = torch.mm(W, H)\n    bw_0_1 = torch.nn.Sigmoid()(temp_pred)\n    bw_0_5 = 5*bw_0_1\nprint(temp_pred.max(), temp_pred.min())\nprint(bw_0_1.max(), bw_0_1.min())\nprint(bw_0_5.max(), bw_0_5.min())\n\ntensor(4.2534, device='mps:0') tensor(-6.2291, device='mps:0')\ntensor(0.9860, device='mps:0') tensor(0.0020, device='mps:0')\ntensor(4.9299, device='mps:0') tensor(0.0098, device='mps:0')\n\n\n\npd.DataFrame(torch.mm(W, H).cpu().detach().numpy()).head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n-0.891001\n-0.634515\n-3.235490\n-1.123262\n-0.822733\n0.247268\n-0.177382\n-0.227028\n-1.626003\n1.122110\n\n\n1\n-0.877492\n-0.278806\n-1.831263\n-0.834423\n-0.545472\n0.780769\n-0.330289\n-0.112227\n-0.676177\n0.701097\n\n\n2\n-0.264974\n-2.991497\n-2.802118\n0.335193\n-0.957647\n0.119414\n-1.969304\n2.189706\n-1.220401\n3.390876\n\n\n3\n-2.748779\n-0.559342\n-5.203822\n-2.586238\n-1.577665\n2.597816\n-0.933461\n-0.491536\n-1.794108\n1.846012\n\n\n4\n1.951718\n0.157127\n2.712191\n1.634342\n0.929842\n-2.237743\n0.785591\n0.256963\n0.600561\n-1.029533\n\n\n\n\n\n\n\n\npd.DataFrame(A.cpu().detach().numpy()).head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n3.0\n4.0\n2.0\n5.0\n5.0\n2.0\n2.0\n1.0\n5.0\n1.0\n\n\n1\n5.0\n2.0\n5.0\n1.0\n4.0\n5.0\n4.0\n4.0\n5.0\n5.0\n\n\n2\n5.0\n5.0\n5.0\n2.0\n2.0\n3.0\n1.0\n2.0\n5.0\n1.0\n\n\n3\n3.0\n5.0\n3.0\n5.0\n2.0\n5.0\n2.0\n4.0\n1.0\n5.0\n\n\n4\n3.0\n4.0\n4.0\n1.0\n1.0\n5.0\n3.0\n4.0\n5.0\n1.0\n\n\n\n\n\n\n\n\n# Optimizer\noptimizer = optim.Adam([W, H], lr=0.01)\n\n# Train the model\n\nfor i in range(600):\n    # Compute the loss\n    loss = torch.norm(torch.mm(W, H) - A)\n    \n    # Zero the gradients\n    optimizer.zero_grad()\n    \n    # Backpropagate\n    loss.backward()\n    \n    # Update the parameters\n    optimizer.step()\n    \n    # Print the loss\n    if i % 10 == 0:\n        print(i, loss.item())\n\n0 55.518882751464844\n10 52.659767150878906\n20 50.37566375732422\n30 48.51186752319336\n40 46.83131790161133\n50 45.094539642333984\n60 43.114288330078125\n70 40.78749084472656\n80 38.10878372192383\n90 35.157928466796875\n100 32.074623107910156\n110 29.021379470825195\n120 26.15049934387207\n130 23.621992111206055\n140 21.566579818725586\n150 20.013381958007812\n160 18.89710235595703\n170 18.127681732177734\n180 17.621126174926758\n190 17.2957820892334\n200 17.079294204711914\n210 16.919897079467773\n220 16.787397384643555\n230 16.666202545166016\n240 16.548551559448242\n250 16.430585861206055\n260 16.310630798339844\n270 16.188627243041992\n280 16.06583595275879\n290 15.944452285766602\n300 15.82705020904541\n310 15.71599006652832\n320 15.612943649291992\n330 15.518715858459473\n340 15.433331489562988\n350 15.356307983398438\n360 15.286922454833984\n370 15.224400520324707\n380 15.168001174926758\n390 15.117039680480957\n400 15.070890426635742\n410 15.028966903686523\n420 14.990720748901367\n430 14.955638885498047\n440 14.923242568969727\n450 14.893086433410645\n460 14.864763259887695\n470 14.837905883789062\n480 14.812182426452637\n490 14.78730297088623\n500 14.76301097869873\n510 14.739081382751465\n520 14.715320587158203\n530 14.691561698913574\n540 14.667658805847168\n550 14.643492698669434\n560 14.618959426879883\n570 14.593977928161621\n580 14.568486213684082\n590 14.542439460754395\n\n\n\npd.DataFrame(torch.mm(W, H).cpu().detach().numpy()).head(2)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n2.507077\n2.882409\n1.014812\n3.618050\n4.393669\n4.244705\n2.526482\n3.759181\n4.175953\n2.472470\n\n\n1\n2.096234\n4.719135\n3.403720\n3.538269\n3.727524\n5.825880\n4.797643\n1.286691\n3.555986\n3.319029\n\n\n\n\n\n\n\n\nwith torch.no_grad():\n    pred = torch.mm(W, H)\nprint(pred)\n\ntensor([[2.5071, 2.8824, 1.0148, 3.6180, 4.3937, 4.2447, 2.5265, 3.7592, 4.1760,\n         2.4725],\n        [2.0962, 4.7191, 3.4037, 3.5383, 3.7275, 5.8259, 4.7976, 1.2867, 3.5560,\n         3.3190],\n        [1.9424, 3.8889, 4.4717, 2.1537, 1.5326, 3.5554, 4.9624, 0.3696, 1.3432,\n         2.7223],\n        [3.2004, 2.8246, 2.0544, 3.4926, 3.8315, 3.3191, 3.2242, 4.3713, 3.5237,\n         2.6092],\n        [3.1014, 1.7170, 0.8792, 3.1483, 3.6712, 2.1976, 1.9481, 5.0449, 3.3680,\n         1.9746],\n        [3.4649, 4.0731, 3.9972, 3.6500, 3.5021, 4.1153, 5.0495, 3.4916, 3.1635,\n         3.3433],\n        [2.4187, 2.5760, 2.6503, 2.4030, 2.2778, 2.4943, 3.3048, 2.5503, 2.0379,\n         2.1813],\n        [1.4519, 2.4073, 0.8205, 2.5799, 3.1635, 3.6162, 1.9812, 1.9655, 3.0497,\n         1.8606],\n        [2.9072, 2.0137, 0.5374, 3.4440, 4.2075, 2.9997, 1.8686, 4.8841, 3.9363,\n         2.1061],\n        [1.8149, 3.1825, 1.7480, 2.9721, 3.4117, 4.3003, 2.9872, 1.9706, 3.2621,\n         2.3937],\n        [3.6335, 3.1765, 3.4501, 3.3264, 3.1627, 2.8898, 4.2959, 4.2345, 2.7955,\n         2.8895],\n        [3.6077, 1.9201, 2.1290, 2.9908, 3.0406, 1.6020, 2.8706, 5.1514, 2.6693,\n         2.1967],\n        [2.5846, 2.2309, 3.2388, 1.9116, 1.4077, 1.4257, 3.4981, 2.5014, 1.1339,\n         1.9999],\n        [1.9620, 2.6912, 1.1617, 2.9910, 3.5664, 3.8354, 2.4170, 2.6666, 3.3997,\n         2.1766],\n        [1.4638, 3.2153, 1.3959, 2.9428, 3.5207, 4.6469, 2.7373, 1.5462, 3.4142,\n         2.3180],\n        [3.6219, 2.5907, 2.2168, 3.5091, 3.7423, 2.7442, 3.2718, 5.0664, 3.3826,\n         2.5968],\n        [3.6559, 2.6337, 3.2100, 3.0238, 2.7970, 2.0870, 3.8787, 4.4767, 2.4195,\n         2.5847],\n        [2.5847, 3.4608, 3.6446, 2.7093, 2.4208, 3.3422, 4.3718, 2.1372, 2.1672,\n         2.7134],\n        [2.5238, 3.3669, 3.3085, 2.7727, 2.6074, 3.4258, 4.1167, 2.2515, 2.3633,\n         2.6544],\n        [2.2396, 3.3835, 3.0556, 2.7231, 2.6510, 3.6647, 3.9279, 1.8880, 2.4411,\n         2.5856]], device='mps:0')\n\n\n\npred.max(), pred.min()\n\n(tensor(5.8259, device='mps:0'), tensor(0.3696, device='mps:0'))\n\n\n\npd.DataFrame(A.cpu()).head(2)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n3.0\n5.0\n1.0\n2.0\n2.0\n5.0\n1.0\n5.0\n5.0\n3.0\n\n\n1\n1.0\n5.0\n4.0\n4.0\n5.0\n5.0\n5.0\n1.0\n4.0\n2.0\n\n\n\n\n\n\n\n\ndef factorize(A, k, device=torch.device(\"cpu\")):\n    \"\"\"Factorize the matrix A into W and H\n    A: input matrix of size (n_users, n_movies)\n    k: number of latent features\n    \n    Returns W and H\n    W: matrix of size (n_users, k)\n    H: matrix of size (k, n_movies)\n    \"\"\"\n    A = A.to(device)\n    # Randomly initialize W and H\n    W = torch.randn(A.shape[0], k, requires_grad=True, device=device)\n    H = torch.randn(k, A.shape[1], requires_grad=True, device=device)\n    \n    # Optimizer\n    optimizer = optim.Adam([W, H], lr=0.01)\n    \n    # Train the model\n    for i in range(1000):\n        # Compute the loss\n        loss = torch.norm(torch.mm(W, H) - A)\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Backpropagate\n        loss.backward()\n        \n        # Update the parameters\n        optimizer.step()\n        \n    return W, H, loss\n\n\nfor k in [1, 2, 3, 4, 5, 6, 9]:\n    W, H, loss = factorize(A, k, device=device)\n    print(k, loss.item())\n\n1 18.47580909729004\n2 16.0474853515625\n3 13.883170127868652\n4 12.110709190368652\n5 10.14443302154541\n6 8.560541152954102\n9 3.077495813369751\n\n\n\npd.DataFrame(torch.mm(W,H).cpu().detach().numpy()).head(2)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n3.221090\n5.013057\n1.102975\n1.975984\n2.243918\n5.007381\n0.727418\n4.854879\n4.776544\n3.170928\n\n\n1\n1.150956\n5.008823\n4.070340\n3.983593\n5.166485\n5.005116\n4.813902\n0.900851\n3.847399\n2.116858\n\n\n\n\n\n\n\n\npd.DataFrame(A.cpu()).head(2)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n3.0\n5.0\n1.0\n2.0\n2.0\n5.0\n1.0\n5.0\n5.0\n3.0\n\n\n1\n1.0\n5.0\n4.0\n4.0\n5.0\n5.0\n5.0\n1.0\n4.0\n2.0\n\n\n\n\n\n\n\n\nx_uniform2d = torch.rand(1000, 2)\n# satter plot\nplt.scatter(x_uniform2d[:, 0], x_uniform2d[:, 1])\n\n\n\n\n\n\n\n\n\n# Check values of x_uniform2d &lt; 0.5 and color them red\nmask = x_uniform2d &lt; 0.5\nprint(mask)\n\ntensor([[ True,  True],\n        [ True,  True],\n        [ True,  True],\n        ...,\n        [ True,  True],\n        [False,  True],\n        [False,  True]])\n\n\n\n# With missing values\n\n# Randomly replace some entries with NaN\n\nA = torch.randint(1, 6, (n_users, n_movies), dtype=torch.float)\nA[torch.rand(A.shape) &lt; 0.5] = float('nan')\nA\n\ntensor([[5., nan, nan, nan, 3., nan, nan, nan, 2., nan],\n        [nan, nan, 1., nan, 1., nan, nan, 3., 5., 1.],\n        [3., nan, nan, nan, 4., 1., nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, 5., 5., nan, 3., nan],\n        [2., 3., nan, 2., 1., nan, nan, 1., 1., nan],\n        [nan, 4., nan, 3., 5., nan, 3., 4., 2., 2.],\n        [nan, nan, nan, nan, nan, 2., 1., 5., 2., nan],\n        [5., 5., nan, nan, 1., nan, nan, nan, 4., 4.],\n        [nan, nan, nan, nan, 3., nan, nan, nan, 2., 4.],\n        [nan, nan, nan, nan, nan, 3., 2., 3., 5., 5.],\n        [nan, 4., nan, nan, 2., nan, 2., nan, nan, nan],\n        [nan, 2., 3., nan, 4., nan, nan, 4., nan, 2.],\n        [nan, nan, 2., 2., 4., 2., 2., 3., nan, nan],\n        [3., nan, 1., nan, nan, 3., nan, nan, 1., nan],\n        [nan, 2., nan, nan, 5., nan, nan, 2., 1., nan],\n        [nan, 2., nan, nan, 5., nan, 4., nan, nan, 1.],\n        [nan, nan, nan, nan, 1., 3., 2., 3., 1., nan],\n        [nan, nan, nan, 3., nan, nan, nan, 3., 4., 1.],\n        [nan, 1., 5., 1., nan, nan, 2., nan, 2., nan],\n        [2., 1., nan, nan, nan, nan, nan, nan, nan, 5.]])\n\n\n\nW, H, loss = factorize(A, 2, device=device)\nloss\n\ntensor(nan, device='mps:0', grad_fn=&lt;NormBackward1&gt;)\n\n\nAs expected, the above function does not work. Our current loss function does not handle missing values.\n\nA.shape\n\ntorch.Size([20, 10])\n\n\n\nmask = ~torch.isnan(A)\nmask\n\ntensor([[ True, False, False, False,  True, False, False, False,  True, False],\n        [False, False,  True, False,  True, False, False,  True,  True,  True],\n        [ True, False, False, False,  True,  True, False, False, False, False],\n        [False, False, False, False, False,  True,  True, False,  True, False],\n        [ True,  True, False,  True,  True, False, False,  True,  True, False],\n        [False,  True, False,  True,  True, False,  True,  True,  True,  True],\n        [False, False, False, False, False,  True,  True,  True,  True, False],\n        [ True,  True, False, False,  True, False, False, False,  True,  True],\n        [False, False, False, False,  True, False, False, False,  True,  True],\n        [False, False, False, False, False,  True,  True,  True,  True,  True],\n        [False,  True, False, False,  True, False,  True, False, False, False],\n        [False,  True,  True, False,  True, False, False,  True, False,  True],\n        [False, False,  True,  True,  True,  True,  True,  True, False, False],\n        [ True, False,  True, False, False,  True, False, False,  True, False],\n        [False,  True, False, False,  True, False, False,  True,  True, False],\n        [False,  True, False, False,  True, False,  True, False, False,  True],\n        [False, False, False, False,  True,  True,  True,  True,  True, False],\n        [False, False, False,  True, False, False, False,  True,  True,  True],\n        [False,  True,  True,  True, False, False,  True, False,  True, False],\n        [ True,  True, False, False, False, False, False, False, False,  True]])\n\n\n\nmask.sum()\n\ntensor(87)\n\n\n\nW = torch.randn(A.shape[0], k, requires_grad=True, device=device)\nH = torch.randn(k, A.shape[1],  requires_grad=True, device=device)\n\ndiff_matrix = torch.mm(W, H)-A.to(device)\ndiff_matrix.shape\n\ntorch.Size([20, 10])\n\n\n\ndiff_matrix\n\ntensor([[-6.9854,     nan,     nan,     nan,  2.5382,     nan,     nan,     nan,\n         -3.3296,     nan],\n        [    nan,     nan, -4.0131,     nan, -6.2010,     nan,     nan, -3.9650,\n         -6.5180,  0.9829],\n        [-1.0404,     nan,     nan,     nan, -4.7079, -0.7452,     nan,     nan,\n             nan,     nan],\n        [    nan,     nan,     nan,     nan,     nan, -6.6035, -6.2113,     nan,\n         -5.2806,     nan],\n        [ 2.2240, -0.5113,     nan, -1.0281, -0.3319,     nan,     nan, -1.0290,\n         -8.5193,     nan],\n        [    nan, -1.5060,     nan, -1.0225, -5.9332,     nan,  3.1254, -1.2558,\n         -7.6531, -1.3446],\n        [    nan,     nan,     nan,     nan,     nan, -1.1617,  1.4732, -4.9664,\n         -6.6226,     nan],\n        [-2.9062, -4.3373,     nan,     nan, -3.4020,     nan,     nan,     nan,\n         -7.5088, -4.6871],\n        [    nan,     nan,     nan,     nan, -6.3027,     nan,     nan,     nan,\n         -6.2829, -4.3568],\n        [    nan,     nan,     nan,     nan,     nan, -0.5816, -1.3435, -6.3993,\n         -5.6600, -4.1959],\n        [    nan, -7.8034,     nan,     nan,  0.4608,     nan, -5.6186,     nan,\n             nan,     nan],\n        [    nan, -6.6628,  1.1697,     nan, -3.5552,     nan,     nan, -8.1775,\n             nan,  2.9435],\n        [    nan,     nan, -1.3904, -2.1634,  6.3728, -1.6834,  0.0504, -3.1274,\n             nan,     nan],\n        [-1.1221,     nan,  1.2635,     nan,     nan, -3.3154,     nan,     nan,\n         -5.8986,     nan],\n        [    nan, -2.8094,     nan,     nan, -5.0974,     nan,     nan, -2.7950,\n         -6.6121,     nan],\n        [    nan, -3.9005,     nan,     nan, -7.2914,     nan, -5.1783,     nan,\n             nan, -0.4819],\n        [    nan,     nan,     nan,     nan, -4.6136,  0.8440,  0.9016,  1.4004,\n         -7.1846,     nan],\n        [    nan,     nan,     nan, -2.3378,     nan,     nan,     nan,  0.4268,\n         -9.2480, -0.9357],\n        [    nan, -5.5819,  2.4075, -3.1272,     nan,     nan,  3.5693,     nan,\n         -0.1390,     nan],\n        [ 4.1499, -2.5375,     nan,     nan,     nan,     nan,     nan,     nan,\n             nan, -4.0199]], device='mps:0', grad_fn=&lt;SubBackward0&gt;)\n\n\n\n# Mask the matrix\ndiff_matrix[mask].shape\n\ntorch.Size([87])\n\n\n\ndiff_matrix[mask]\n\ntensor([-6.9854,  2.5382, -3.3296, -4.0131, -6.2010, -3.9650, -6.5180,  0.9829,\n        -1.0404, -4.7079, -0.7452, -6.6035, -6.2113, -5.2806,  2.2240, -0.5113,\n        -1.0281, -0.3319, -1.0290, -8.5193, -1.5060, -1.0225, -5.9332,  3.1254,\n        -1.2558, -7.6531, -1.3446, -1.1617,  1.4732, -4.9664, -6.6226, -2.9062,\n        -4.3373, -3.4020, -7.5088, -4.6871, -6.3027, -6.2829, -4.3568, -0.5816,\n        -1.3435, -6.3993, -5.6600, -4.1959, -7.8034,  0.4608, -5.6186, -6.6628,\n         1.1697, -3.5552, -8.1775,  2.9435, -1.3904, -2.1634,  6.3728, -1.6834,\n         0.0504, -3.1274, -1.1221,  1.2635, -3.3154, -5.8986, -2.8094, -5.0974,\n        -2.7950, -6.6121, -3.9005, -7.2914, -5.1783, -0.4819, -4.6136,  0.8440,\n         0.9016,  1.4004, -7.1846, -2.3378,  0.4268, -9.2480, -0.9357, -5.5819,\n         2.4075, -3.1272,  3.5693, -0.1390,  4.1499, -2.5375, -4.0199],\n       device='mps:0', grad_fn=&lt;IndexBackward0&gt;)\n\n\n\n# Modify the loss function to ignore NaN values\n\ndef factorize(A, k, device=torch.device(\"cpu\")):\n    \"\"\"Factorize the matrix D into A and B\"\"\"\n    A = A.to(device)\n    # Randomly initialize A and B\n    W = torch.randn(A.shape[0], k, requires_grad=True, device=device)\n    H = torch.randn(k, A.shape[1], requires_grad=True, device=device)\n    # Optimizer\n    optimizer = optim.Adam([W, H], lr=0.01)\n    mask = ~torch.isnan(A)\n    # Train the model\n    for i in range(1000):\n        # Compute the loss\n        #diff_matrix = 5*torch.nn.Sigmoid()(torch.mm(W, H)) - A\n        diff_matrix = torch.mm(W, H) - A\n        diff_vector = diff_matrix[mask]\n        loss = torch.norm(diff_vector)\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Backpropagate\n        loss.backward()\n        \n        # Update the parameters\n        optimizer.step()\n        \n    return W, H, loss\n\n\nW, H, loss = factorize(A, 5, device=device)\nloss\n\ntensor(8.0831, device='mps:0', grad_fn=&lt;NormBackward1&gt;)\n\n\n\ntorch.mm(W, H)\n\ntensor([[ 3.7077e-01,  6.6310e+00,  1.5317e+01,  1.5463e+00,  2.3812e+00,\n          1.0566e+01,  4.1666e-01,  3.7488e-01,  1.9886e+01,  9.1443e+00],\n        [ 6.0725e+00,  4.9374e+00,  1.1223e+01,  1.6064e+01,  5.0131e+00,\n          7.6977e+00,  1.3872e+00,  8.5424e+00,  4.0570e-01,  8.0806e+00],\n        [ 4.0886e-01,  4.3280e-01,  3.1149e+01,  1.4252e+01,  3.7623e+00,\n          6.6555e+00,  2.3225e+01,  2.7410e+01,  5.6219e+00,  1.3889e+00],\n        [ 1.4532e+00,  1.7891e+00,  1.3172e+00,  3.0270e+00,  1.1199e+00,\n          2.3014e+00, -1.2625e+00,  3.5530e-01,  1.5922e+00,  2.7375e+00],\n        [ 1.6632e+00,  2.6585e-01,  2.1361e+01,  7.0106e+00,  2.8000e+00,\n          3.5102e+00,  4.2856e-01,  3.3931e+00, -3.8024e-01,  1.5930e+00],\n        [ 1.2141e+00,  1.0140e+00,  3.0760e+00,  2.6695e+00,  1.0877e+00,\n          1.3464e+00, -8.1209e+00, -2.7409e+00,  9.0885e-01,  2.8838e+00],\n        [ 4.0901e-01,  3.9038e+00,  8.7977e+00,  4.1466e-01,  1.3196e+00,\n          6.3608e+00,  6.7289e+00,  1.3803e+00,  1.0174e+01,  3.9643e+00],\n        [-9.8752e-01,  3.6740e-01,  2.6969e+00,  4.0674e+00,  4.0672e-01,\n          1.3851e+00,  2.4825e+00,  1.0826e+01,  5.9494e+00,  2.2289e+00],\n        [ 5.5207e-01,  1.1081e+00,  1.9142e+01,  2.3691e+00,  1.9192e+00,\n          4.2797e+00,  3.2013e+00,  4.6711e-01,  3.6369e+00,  1.3412e+00],\n        [ 4.6630e-01,  1.2243e+00,  5.9625e+00,  1.4046e+00,  1.0984e+00,\n          1.6349e+00, -2.0338e+01, -7.9498e+00,  5.1443e+00,  5.2937e+00],\n        [ 1.2857e+00,  1.5616e+00,  6.7867e+00,  4.8829e+00,  1.6570e+00,\n          2.8631e+00, -3.4178e+00,  2.2360e+00,  2.9275e+00,  3.5634e+00],\n        [ 6.3556e+00,  3.8148e-01, -4.0798e-01,  1.0863e+01,  2.8454e+00,\n          4.3686e-01,  1.5168e+01,  5.1601e+00, -1.8209e+01, -2.4838e+00],\n        [ 6.5525e+00,  8.4806e+00,  1.3856e+00,  8.3839e+00,  4.1564e+00,\n          1.0084e+01, -1.1960e+00, -4.9075e+00,  5.5259e+00,  1.0234e+01],\n        [ 6.5245e+00,  5.3424e-02,  9.7484e+00,  1.7353e+01,  4.4118e+00,\n          1.3884e+00,  2.8198e+00,  9.1513e+00, -1.5093e+01,  1.3853e+00],\n        [ 1.2177e+00,  9.9223e-01,  6.9760e+00,  3.4557e+00,  1.2884e+00,\n          2.5346e+00,  8.4102e+00,  4.5198e+00, -2.3623e-01,  2.9567e-01],\n        [ 1.7454e+00,  1.1318e+00,  4.1938e-01,  3.5559e+00,  1.0687e+00,\n          1.4206e+00,  1.3669e+00,  1.4285e+00, -1.4711e+00,  1.3943e+00],\n        [ 1.7910e+00,  1.2199e+00,  4.0360e-01,  4.2808e+00,  1.1723e+00,\n          1.5749e+00,  1.6105e+00,  2.6729e+00, -1.1288e+00,  1.7057e+00],\n        [-1.3556e+00, -1.4275e+00,  3.4785e+01,  1.2851e+01,  3.0411e+00,\n          5.2795e+00,  2.8474e+01,  3.1569e+01,  5.5362e+00, -1.3419e+00],\n        [-1.8762e-01, -2.3404e-01, -4.5376e-01, -7.7366e-01, -2.1038e-01,\n         -3.5672e-01,  3.9868e-01, -5.7151e-01, -4.0787e-01, -5.3638e-01],\n        [ 1.3838e+00, -2.2733e+00,  6.1621e+00,  8.6734e+00,  1.3886e+00,\n         -1.5119e+00,  5.2354e+00,  1.1302e+01, -8.0632e+00, -1.7889e+00],\n        [ 3.6552e+00,  2.5212e+00,  1.3857e+00,  6.9004e+00,  2.2509e+00,\n          3.1481e+00,  1.3866e+00,  1.3865e+00, -2.6466e+00,  3.1740e+00],\n        [ 1.0229e+00,  1.1825e+00,  4.2758e-01,  6.1655e+00,  1.0213e+00,\n          2.3481e+00,  1.6458e+01,  1.4319e+01,  5.4860e-01,  3.3834e-01],\n        [ 6.0379e+00,  1.4185e+00,  1.3875e+00,  1.3263e+01,  3.5322e+00,\n          1.3875e+00, -7.0567e+00,  1.3884e+00, -1.0741e+01,  3.8053e+00],\n        [-9.8381e-01, -1.3306e+00, -1.3938e+00, -2.1299e+00, -7.9858e-01,\n         -1.8705e+00, -1.4587e+00, -1.2413e+00, -1.2592e+00, -1.6469e+00],\n        [ 1.3220e+00, -4.9020e-01,  3.9410e+00,  7.6600e+00,  1.5701e+00,\n         -3.7274e-01, -1.4673e+01,  2.5561e+00, -1.1948e+00,  3.7111e+00],\n        [ 6.1220e+00,  7.4328e+00,  4.0750e-01,  3.4254e+00,  3.2144e+00,\n          8.5422e+00,  2.7018e+00, -1.0723e+01,  1.6187e+00,  6.6366e+00],\n        [-3.5685e+00, -1.7563e+00,  4.2421e+00,  5.5724e+00, -3.7115e-01,\n         -4.0291e-01,  4.3805e+00,  2.1632e+01,  9.1339e+00,  1.3505e+00],\n        [ 4.4832e-01, -1.5860e-02,  1.4055e+00,  1.0218e+01,  1.1916e+00,\n          1.3818e+00,  1.7442e+01,  2.3185e+01,  5.4138e-01,  4.2002e-01],\n        [ 4.1024e-01,  9.5018e-01,  1.3887e+00,  4.0686e-01,  3.8984e-01,\n          1.3909e+00,  1.3861e+00, -7.4408e-02,  1.4843e+00,  9.0930e-01],\n        [ 3.7110e+00,  7.8304e+00,  9.7398e+00,  9.2027e+00,  3.8842e+00,\n          1.1342e+01,  6.5249e+00,  6.9368e+00,  1.3636e+01,  1.0251e+01],\n        [ 2.8386e+00,  3.7927e+00, -2.7092e+00,  4.9386e+00,  1.6519e+00,\n          4.4198e+00,  7.3292e+00,  3.9203e+00,  2.1021e+00,  3.7490e+00],\n        [ 1.3547e+00,  1.0248e+00,  5.2940e-01,  5.9261e+00,  1.2267e+00,\n          1.4913e+00,  4.2189e-01,  6.3919e+00,  5.4764e-01,  2.5465e+00],\n        [ 2.9390e+00,  3.6583e+00,  1.4742e+01,  5.8615e+00,  2.8172e+00,\n          7.4801e+00,  2.6432e+01,  9.6709e+00,  1.3329e+00,  4.8200e-01],\n        [-5.5568e-01, -8.6809e-01,  1.3172e+01,  5.1727e+00,  1.1844e+00,\n          1.3867e+00,  4.6236e+00,  1.0093e+01,  2.0485e+00,  2.4118e-01],\n        [ 3.8864e+00,  2.3984e+00, -9.6685e+00,  6.0240e+00,  1.3454e+00,\n          1.5644e+00,  7.9850e+00,  3.7270e+00, -5.8279e+00,  1.4577e+00],\n        [ 1.1099e+00,  1.1423e+00,  3.8295e+00,  4.0440e+00,  1.1922e+00,\n          2.0748e+00,  1.4193e+00,  3.6980e+00,  1.2248e+00,  1.9989e+00],\n        [ 5.2677e+00,  3.8336e+00,  8.5316e+00,  8.7588e+00,  3.4359e+00,\n          6.2897e+00,  1.9443e+01,  5.7548e+00, -4.8913e+00,  1.3985e+00],\n        [ 1.3715e+00, -5.1365e-01,  6.1238e+00,  6.2748e+00,  1.4240e+00,\n          4.0118e-01,  1.2560e+00,  5.9392e+00, -3.4068e+00,  4.0654e-01],\n        [ 2.5741e+00,  1.6649e+00,  4.4096e+00,  7.5072e+00,  2.1164e+00,\n          2.7040e+00,  1.8751e-01,  4.6533e+00, -7.9788e-01,  3.1709e+00],\n        [ 1.9012e+00,  2.2319e+00,  1.4530e+00,  3.2700e+00,  1.3231e+00,\n          2.9013e+00,  1.2974e+00,  4.6004e-01,  1.1785e+00,  2.6793e+00],\n        [-1.3671e+00, -1.0607e+00,  4.9678e+00,  5.4084e+00,  4.1399e-01,\n         -4.9961e-03,  4.7888e-01,  1.2852e+01,  4.0295e+00,  1.3768e+00],\n        [ 1.7139e+00,  2.1075e+00,  1.2970e+00,  3.0185e+00,  1.1341e+00,\n          3.0403e+00,  9.3333e+00,  3.8229e+00,  5.1340e-01,  1.1762e+00],\n        [ 2.3762e+00,  5.2240e-01,  4.1048e-01,  1.8715e+01,  2.8521e+00,\n          1.4484e+00,  3.9806e-01,  2.5461e+01,  3.5661e-01,  5.8433e+00],\n        [ 1.3917e+00,  1.5611e+00,  4.2009e-01,  6.9560e+00,  1.2425e+00,\n          2.9011e+00,  1.9141e+01,  1.5693e+01,  3.9786e-01,  4.2775e-01],\n        [ 2.9598e+00,  4.8901e+00,  7.5000e+00,  1.0169e+01,  3.1609e+00,\n          7.5781e+00,  7.8298e+00,  1.1290e+01,  7.7062e+00,  6.9272e+00],\n        [ 9.3588e-01,  6.2922e-01,  1.4434e+00,  6.8615e+00,  1.2103e+00,\n          1.3005e+00,  1.4205e+00,  9.5011e+00,  1.1638e+00,  2.4664e+00],\n        [-5.1268e-01,  5.4848e-01,  4.0419e-01,  5.7054e+00,  6.1354e-01,\n          1.1910e+00, -4.0399e-01,  1.1494e+01,  5.5492e+00,  3.2506e+00],\n        [ 1.4683e+00,  1.5662e+00,  1.3074e+00,  2.7227e+00,  1.0122e+00,\n          2.1590e+00,  3.1041e+00,  1.4728e+00,  3.2532e-01,  1.5679e+00],\n        [ 7.6849e-01, -7.6948e-01,  1.6944e+01,  6.5754e+00,  1.9672e+00,\n          1.9643e+00,  6.9138e+00,  8.6214e+00, -1.5640e+00, -4.3651e-01],\n        [-2.1299e-01, -6.9929e-01, -4.1978e-01,  4.8846e+00,  2.6976e-01,\n         -2.8975e-01,  8.1442e+00,  1.2403e+01, -3.6345e-01, -4.0285e-01],\n        [ 2.5861e+00,  1.0537e+00,  4.0548e-01,  4.7218e+00,  1.3756e+00,\n          1.3889e+00,  5.6365e+00,  2.5055e+00, -4.5856e+00,  4.0554e-01],\n        [ 1.4566e+00,  1.3237e+00,  4.0939e-01,  4.5782e+00,  1.2249e+00,\n          1.4320e+00, -8.0603e+00,  4.0803e-01,  1.4356e+00,  3.8377e+00],\n        [ 8.1262e-01,  2.3074e-01,  1.7395e+01,  8.2779e+00,  2.3413e+00,\n          3.4393e+00,  8.3602e+00,  1.2176e+01,  1.7080e+00,  1.2617e+00],\n        [ 1.4001e+00,  1.5439e+00,  2.8954e+01,  9.4660e+00,  3.5679e+00,\n          7.0490e+00,  1.7842e+01,  1.4387e+01,  4.0233e+00,  1.3908e+00],\n        [ 9.1116e+00,  8.9888e+00, -1.3918e+00,  1.5321e+01,  5.5012e+00,\n          1.0329e+01,  4.0605e-01,  4.0663e-01,  4.0540e-01,  1.1435e+01],\n        [ 8.1939e+00,  1.3806e+00,  4.5657e+00,  5.2186e+00,  3.5657e+00,\n          4.2395e-01, -1.9076e+01, -2.4892e+01, -1.9680e+01,  1.3603e+00],\n        [ 1.0022e+00,  9.8732e-01, -3.5882e-01,  3.3790e+00,  7.4693e-01,\n          1.3344e+00,  3.3607e+00,  4.3721e+00,  3.0749e-01,  1.3101e+00],\n        [ 1.2174e-01, -1.0305e+00, -4.3249e-01, -3.0589e-01, -1.7696e-01,\n         -1.4498e+00, -1.3131e+00, -1.5901e+00, -3.4188e+00, -1.3657e+00],\n        [ 1.2888e+00,  8.2891e-01,  2.5650e+01,  3.5316e+00,  2.6780e+00,\n          4.7654e+00,  2.6639e+00, -1.2750e+00,  1.3973e+00,  9.3778e-01],\n        [ 1.3977e+00,  1.3825e+00,  2.3757e+01,  1.3840e+01,  3.5801e+00,\n          6.6870e+00,  2.6083e+01,  2.6249e+01,  3.8568e+00,  1.3932e+00],\n        [ 4.9924e+00,  8.0001e+00,  1.0591e+01,  6.0741e+00,  3.9055e+00,\n          1.1491e+01,  1.3512e+01,  8.5781e-02,  7.9272e+00,  7.2543e+00],\n        [ 1.8955e+00,  1.8061e+00,  1.3669e+00,  1.5413e+00,  1.0386e+00,\n          2.3137e+00,  3.1405e+00, -1.8895e+00, -8.7997e-01,  1.1596e+00],\n        [ 1.1633e+00,  5.6876e-02,  2.4588e+01,  5.2205e+00,  2.6591e+00,\n          3.6934e+00,  5.0582e-01,  1.2408e+00,  3.4684e-01,  9.7013e-01],\n        [ 1.8890e+00,  4.2230e+00,  1.3841e+00,  6.5417e+00,  1.9563e+00,\n          5.7192e+00,  4.3756e+00,  7.9008e+00,  7.8805e+00,  6.0569e+00],\n        [ 9.6615e-01,  1.0195e-01,  4.0618e-01,  1.3988e+00,  3.8151e-01,\n          4.0914e-01,  8.7006e+00,  2.8220e+00, -3.3580e+00, -1.5051e+00],\n        [ 4.2723e+00,  8.9018e+00,  1.0662e+01,  9.1192e+00,  4.2575e+00,\n          1.2717e+01,  6.7680e+00,  5.1806e+00,  1.4848e+01,  1.1238e+01],\n        [ 1.3898e+00,  3.1529e+00,  4.8560e+00,  1.3955e+00,  1.3844e+00,\n          4.4214e+00, -1.5194e+00, -2.7469e+00,  5.5049e+00,  4.0580e+00],\n        [ 5.4742e+00, -1.2980e+00,  1.1575e+01,  1.0763e+01,  3.3498e+00,\n         -4.1958e-01, -1.3949e+00, -1.3753e+00, -1.7263e+01, -1.4210e+00],\n        [ 1.3967e+00, -1.0635e+00,  1.3970e+01,  1.3876e+00,  1.3699e+00,\n          4.7296e-01,  1.5700e-01, -4.6728e+00, -6.0694e+00, -1.9964e+00],\n        [ 7.5026e-01, -1.4161e+00,  1.1501e+01,  5.9301e+00,  1.3854e+00,\n          4.0497e-01,  8.7014e+00,  9.1058e+00, -4.3039e+00, -1.8133e+00],\n        [ 1.2757e+00,  1.0440e+00,  5.8075e+00,  3.4733e+00,  1.3024e+00,\n          2.1550e+00,  1.1594e+00,  1.6926e+00,  3.6945e-01,  1.5767e+00],\n        [-4.1841e+00, -8.6575e-01, -5.6859e+00,  4.2037e+00, -1.3299e+00,\n         -4.1460e-01,  1.1076e+01,  2.5841e+01,  1.1696e+01,  1.3891e+00],\n        [ 5.0312e-01,  1.8976e+00,  5.2237e+00,  5.4767e+00,  1.4032e+00,\n          3.4213e+00,  1.1484e+00,  7.9271e+00,  6.1797e+00,  4.0041e+00],\n        [ 1.8295e+00,  1.5299e+00,  4.1042e-01,  7.8257e+00,  1.5812e+00,\n          2.3550e+00,  6.2944e+00,  1.0695e+01,  4.7176e-01,  2.5958e+00],\n        [ 1.3757e+00,  1.3617e+00,  1.4284e+00,  2.6318e+00,  9.8040e-01,\n          1.8306e+00,  3.8919e-01,  4.5521e-01,  3.0986e-01,  1.7845e+00],\n        [ 8.9114e-01,  1.9898e+00,  3.9593e-01,  3.5058e+00,  7.0391e-01,\n          3.6148e+00,  2.7193e+01,  1.4547e+01,  1.2249e+00, -1.2830e+00],\n        [ 1.1446e+00,  9.2672e-02,  1.2611e+00,  1.1337e+01,  1.6551e+00,\n          9.0619e-01,  3.1936e+00,  1.7302e+01,  3.6233e-01,  2.9303e+00],\n        [ 5.6404e-01,  1.3112e+00, -1.4610e+01, -5.5129e-01, -7.3192e-01,\n         -6.2388e-01, -1.3646e+00, -3.6969e-01,  4.5713e-01,  1.4946e+00],\n        [ 1.8349e+00,  3.8743e-01,  2.2851e+01,  9.5176e+00,  3.2526e+00,\n          3.9987e+00,  3.9907e-01,  6.9859e+00,  4.5959e-01,  2.5536e+00],\n        [ 1.4012e-01,  3.0445e-01,  3.3660e+00,  3.6189e+00,  5.6512e-01,\n          1.6817e+00,  1.6724e+01,  1.2033e+01,  3.1943e-01, -1.2344e+00],\n        [ 4.0966e-01,  5.9155e-01,  2.4535e+00,  3.2239e+00,  7.0914e-01,\n          1.3291e+00,  3.0239e+00,  5.2441e+00,  1.4251e+00,  1.1689e+00],\n        [ 2.6933e+00,  2.2949e+00,  3.6446e+00,  4.7861e+00,  1.8930e+00,\n          3.1729e+00, -3.7154e-01, -4.3632e-01, -4.0752e-01,  3.0464e+00],\n        [-4.0325e-01, -1.9121e+00, -1.3844e+00,  9.3613e+00,  3.9900e-01,\n         -1.3956e+00,  1.4606e+01,  2.3335e+01, -2.3192e+00, -1.3762e+00]],\n       device='mps:0', grad_fn=&lt;MmBackward0&gt;)\n\n\n\ndf.values.shape\n\n(83, 10)\n\n\n\nA = torch.tensor(df.values, dtype=torch.float)\nW, H, loss = factorize(A, 3, device=device)\nprint(loss)\n\ntensor(13.0500, device='mps:0', grad_fn=&lt;NormBackward1&gt;)\n\n\n\n# Dropdown menu for user and predict for all movies\nfrom ipywidgets import interact, widgets\n\n\n\ndef predict_movie_ratings(user, df, W, H):\n    idx = df.index.get_loc(user)\n    user_ratings = df.iloc[idx]\n    user_ratings = user_ratings.dropna()\n    user_ratings = user_ratings.to_frame().T\n    user_ratings = user_ratings.reindex(columns=df.columns, fill_value=float('nan'))\n    user_ratings = user_ratings.to_numpy()\n    \n    predicted_ratings = 5*torch.nn.Sigmoid()(torch.mm(W, H)).cpu().detach().numpy()\n    return pd.DataFrame({\"Observed\": user_ratings.flatten(), \"Predicted\": predicted_ratings[idx].flatten()}, index=df.columns)\n\n\npredict_movie_ratings('Ayush Shrivastava', df, W, H)\n\n\n\n\n\n\n\n\nObserved\nPredicted\n\n\n\n\nSholay\n4.0\n3.632530\n\n\nSwades (We The People)\n4.0\n4.347816\n\n\nThe Matrix (I)\n5.0\n4.850659\n\n\nInterstellar\n5.0\n4.998452\n\n\nDangal\n3.0\n3.240659\n\n\nTaare Zameen Par\n4.0\n3.987770\n\n\nShawshank Redemption\nNaN\n4.999985\n\n\nThe Dark Knight\nNaN\n5.000000\n\n\nNotting Hill\nNaN\n3.611275\n\n\nUri: The Surgical Strike\n4.0\n3.840538\n\n\n\n\n\n\n\n\ninteract(predict_movie_ratings, user=widgets.Dropdown(options=df.index, value=df.index[0], description='User'), df=widgets.fixed(df), W=widgets.fixed(W), H=widgets.fixed(H))\n\n\n\n\n&lt;function __main__.predict_movie_ratings(user, df, W, H)&gt;\n\n\n\n# Image completion\nimport os\nif os.path.exists('dog.jpg'):\n    print('dog.jpg exists')\nelse:\n    !wget https://segment-anything.com/assets/gallery/AdobeStock_94274587_welsh_corgi_pembroke_CD.jpg -O dog.jpg\n\ndog.jpg exists\n\n\n\n# Read in a image from torchvision\nimport torchvision\nimg = torchvision.io.read_image(\"dog.jpg\")\nprint(img.shape)\n\ntorch.Size([3, 1365, 2048])\n\n\n\n# Make grayscale\nimg = torch.tensor(img, dtype=torch.float)\nimg = img.mean(dim=0, keepdim=False)\nprint(img.shape)\n\ntorch.Size([1365, 2048])\n\n\n/var/folders/z8/gpvqr8mn3w9_f38byxhnsk780000gn/T/ipykernel_9900/232810751.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  img = torch.tensor(img, dtype=torch.float)\n\n\n\nimg.shape\n\ntorch.Size([1365, 2048])\n\n\n\nplt.imshow(img, cmap='gray')\n\n\n\n\n\n\n\n\n\ncrop = torchvision.transforms.functional.crop(img, 600, 800, 300, 300)\ncrop.shape\n\ntorch.Size([300, 300])\n\n\n\nplt.imshow(crop, cmap='gray')\n\n\n\n\n\n\n\n\n\n# Mask the image with NaN values \ndef mask_image(img, prop):\n    img_copy = img.clone()\n    mask = torch.rand(img.shape) &lt; prop\n    img_copy[mask] = float('nan')\n    return img_copy, mask\n\n\nmasked_img = mask_image(crop, 0.3)\n\n\nmasked_img[1].sum()\n\ntensor(26858)\n\n\n\nplt.imshow(masked_img[0], cmap='gray')\n\n\n\n\n\n\n\n\n\nW, H, loss = factorize(masked_img[0], 50, device=device)\n\n\nloss\n\ntensor(1303.8575, device='mps:0', grad_fn=&lt;NormBackward1&gt;)\n\n\n\nplt.imshow(torch.mm(W, H).cpu().detach().numpy(), cmap='gray')\n\n\n\n\n\n\n\n\n\ndef plot_image_completion(prop=0.1, factors=50):\n    masked_img, mask = mask_image(crop, prop)\n    W, H, loss = factorize(masked_img, factors, device=device)\n    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n    reconstructed_img = torch.mm(W, H).cpu().detach().numpy()\n    ax[0].imshow(masked_img, cmap='gray')\n    ax[0].set_title(\"Masked image\")\n    ax[1].imshow(reconstructed_img, cmap='gray')\n    ax[1].set_title(\"Reconstructed image\")\n\n\ninteract(plot_image_completion, prop=widgets.FloatSlider(min=0.01, max=0.9, step=0.01, value=0.3), factors=widgets.IntSlider(min=1, max=150, step=1, value=50))\n\n\n\n\n&lt;function __main__.plot_image_completion(prop=0.1, factors=50)&gt;\n\n\n\n# Now use matrix faactaorization to predict the ratings\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Create a class for the model\n\nclass MatrixFactorization(nn.Module):\n    def __init__(self, n_users, n_movies, n_factors=20):\n        super().__init__()\n        self.user_factors = nn.Embedding(n_users, n_factors)\n        self.movie_factors = nn.Embedding(n_movies, n_factors)\n\n    def forward(self, user, movie):\n        return (self.user_factors(user) * self.movie_factors(movie)).sum(1)"
  },
  {
    "objectID": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html",
    "href": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html",
    "title": "Adversarial Examples for ML",
    "section": "",
    "text": "The colab environment already has all the necessary Python packages installed. Specifically, we are using numpy, torch and torchvision.\n\nimport os\nimport time\n\nimport numpy as np\nimport torch\n\nimport argparse\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n\n# Choosing backend\nif torch.backends.mps.is_available():\n    device=torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device=torch.device(\"cuda\")\nelse:\n    device=torch.device(\"cpu\")\n\n\n\nWe load in the data using the in-built data loaders in PyTorch. It offers functionality for many commonly used computer vision datasets, but we will just use MNIST (a dataset of black and white handwritten digits) for now.\n\ndef load_dataset(dataset, data_dir, training_time):\n    if dataset == 'CIFAR-10':\n        loader_train, loader_test, data_details = load_cifar_dataset(data_dir, training_time)\n    elif 'MNIST' in dataset:\n        loader_train, loader_test, data_details = load_mnist_dataset(data_dir, training_time)\n    else:\n        raise ValueError('No support for dataset %s' % args.dataset)\n\n    return loader_train, loader_test, data_details\n\n\ndef load_mnist_dataset(data_dir, training_time):\n    # MNIST data loaders\n    trainset = datasets.MNIST(root=data_dir, train=True,\n                                download=True, transform=transforms.ToTensor())\n    testset = datasets.MNIST(root=data_dir, train=False,\n                                download=True, transform=transforms.ToTensor())\n\n    loader_train = torch.utils.data.DataLoader(trainset,\n                                batch_size=128,\n                                shuffle=True)\n\n    loader_test = torch.utils.data.DataLoader(testset,\n                                batch_size=128,\n                                shuffle=False)\n    data_details = {'n_channels':1, 'h_in':28, 'w_in':28, 'scale':255.0}\n    return loader_train, loader_test, data_details\n\nHaving defined the data loaders, we now create the data loaders to be used throughout, as well as a dictionary with the details of the dataset, in case we need it.\n\nloader_train, loader_test, data_details = load_dataset('MNIST','data',training_time=True)\n\n\n\n\nSince we need the path to the directory where we will storing our models (pre-trained or not), and we also need to instantiate a copy of the model we defined above, we will run the following commands to have everything setup for test/evaluation.\n\n\n\nWe use a 2-layer fully connected network for the experiments in this tutorial. The definition of a 3 layer convolutional neural network is also provided. The former is sufficient for MNIST, but may not be large enough for more complex tasks.\n\nmodel_name='fcn'\n\n\nclass cnn_3l_bn(nn.Module):\n    def __init__(self, n_classes=10):\n        super(cnn_3l_bn, self).__init__()\n        #in-channels, no. filters, filter size, stride\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.bn1 = nn.BatchNorm2d(20)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.bn2 = nn.BatchNorm2d(50)\n        # Number of neurons in preceding layer, Number in current layer\n        self.fc1 = nn.Linear(4*4*50, 500)\n        self.fc2 = nn.Linear(500, n_classes)\n\n    def forward(self, x):\n        # Rectified linear unit activation\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4*4*50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\nclass fcn(nn.Module):\n  def __init__(self, n_classes=10):\n    super(fcn, self).__init__()\n    self.fc1 = nn.Linear(784,200)\n    self.fc2 = nn.Linear(200,200)\n    self.fc3 = nn.Linear(200,n_classes)\n\n  def forward(self, x):\n    x = x.view(-1, 28*28)\n    x = F.relu(self.fc1(x))\n    x = F.relu(self.fc2(x))\n    x = self.fc3(x)\n    return F.log_softmax(x, dim=1)\n\n\nif 'fcn' in model_name:\n  model_dir_name='models'+'/'+'MNIST'+'/fcn/'\n  if not os.path.exists(model_dir_name):\n    os.makedirs(model_dir_name)\n\n  # Basic setup\n  net = fcn(10)\nelif 'cnn' in model_name:\n  model_dir_name='models'+'/'+'MNIST'+'/cnn_3l_bn/'\n  if not os.path.exists(model_dir_name):\n    os.makedirs(model_dir_name)\n\n  # Basic setup\n  net = cnn_3l_bn(10)\n\nnet.to(device)\n\ncriterion = nn.CrossEntropyLoss(reduction='none')"
  },
  {
    "objectID": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#data-model-and-attack-utilities",
    "href": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#data-model-and-attack-utilities",
    "title": "Adversarial Examples for ML",
    "section": "",
    "text": "The colab environment already has all the necessary Python packages installed. Specifically, we are using numpy, torch and torchvision.\n\nimport os\nimport time\n\nimport numpy as np\nimport torch\n\nimport argparse\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n\n# Choosing backend\nif torch.backends.mps.is_available():\n    device=torch.device(\"mps\")\nelif torch.cuda.is_available():\n    device=torch.device(\"cuda\")\nelse:\n    device=torch.device(\"cpu\")\n\n\n\nWe load in the data using the in-built data loaders in PyTorch. It offers functionality for many commonly used computer vision datasets, but we will just use MNIST (a dataset of black and white handwritten digits) for now.\n\ndef load_dataset(dataset, data_dir, training_time):\n    if dataset == 'CIFAR-10':\n        loader_train, loader_test, data_details = load_cifar_dataset(data_dir, training_time)\n    elif 'MNIST' in dataset:\n        loader_train, loader_test, data_details = load_mnist_dataset(data_dir, training_time)\n    else:\n        raise ValueError('No support for dataset %s' % args.dataset)\n\n    return loader_train, loader_test, data_details\n\n\ndef load_mnist_dataset(data_dir, training_time):\n    # MNIST data loaders\n    trainset = datasets.MNIST(root=data_dir, train=True,\n                                download=True, transform=transforms.ToTensor())\n    testset = datasets.MNIST(root=data_dir, train=False,\n                                download=True, transform=transforms.ToTensor())\n\n    loader_train = torch.utils.data.DataLoader(trainset,\n                                batch_size=128,\n                                shuffle=True)\n\n    loader_test = torch.utils.data.DataLoader(testset,\n                                batch_size=128,\n                                shuffle=False)\n    data_details = {'n_channels':1, 'h_in':28, 'w_in':28, 'scale':255.0}\n    return loader_train, loader_test, data_details\n\nHaving defined the data loaders, we now create the data loaders to be used throughout, as well as a dictionary with the details of the dataset, in case we need it.\n\nloader_train, loader_test, data_details = load_dataset('MNIST','data',training_time=True)\n\n\n\n\nSince we need the path to the directory where we will storing our models (pre-trained or not), and we also need to instantiate a copy of the model we defined above, we will run the following commands to have everything setup for test/evaluation.\n\n\n\nWe use a 2-layer fully connected network for the experiments in this tutorial. The definition of a 3 layer convolutional neural network is also provided. The former is sufficient for MNIST, but may not be large enough for more complex tasks.\n\nmodel_name='fcn'\n\n\nclass cnn_3l_bn(nn.Module):\n    def __init__(self, n_classes=10):\n        super(cnn_3l_bn, self).__init__()\n        #in-channels, no. filters, filter size, stride\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.bn1 = nn.BatchNorm2d(20)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.bn2 = nn.BatchNorm2d(50)\n        # Number of neurons in preceding layer, Number in current layer\n        self.fc1 = nn.Linear(4*4*50, 500)\n        self.fc2 = nn.Linear(500, n_classes)\n\n    def forward(self, x):\n        # Rectified linear unit activation\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4*4*50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\nclass fcn(nn.Module):\n  def __init__(self, n_classes=10):\n    super(fcn, self).__init__()\n    self.fc1 = nn.Linear(784,200)\n    self.fc2 = nn.Linear(200,200)\n    self.fc3 = nn.Linear(200,n_classes)\n\n  def forward(self, x):\n    x = x.view(-1, 28*28)\n    x = F.relu(self.fc1(x))\n    x = F.relu(self.fc2(x))\n    x = self.fc3(x)\n    return F.log_softmax(x, dim=1)\n\n\nif 'fcn' in model_name:\n  model_dir_name='models'+'/'+'MNIST'+'/fcn/'\n  if not os.path.exists(model_dir_name):\n    os.makedirs(model_dir_name)\n\n  # Basic setup\n  net = fcn(10)\nelif 'cnn' in model_name:\n  model_dir_name='models'+'/'+'MNIST'+'/cnn_3l_bn/'\n  if not os.path.exists(model_dir_name):\n    os.makedirs(model_dir_name)\n\n  # Basic setup\n  net = cnn_3l_bn(10)\n\nnet.to(device)\n\ncriterion = nn.CrossEntropyLoss(reduction='none')"
  },
  {
    "objectID": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#training-the-benignstandard-model",
    "href": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#training-the-benignstandard-model",
    "title": "Adversarial Examples for ML",
    "section": "Training the benign/standard model",
    "text": "Training the benign/standard model\nThis is sample code for training your own model. Since it takes time to run, for the purposes of the tutorial, we will assume we already have trained models.\n\n########################################  Benign/standard training ########################################\ndef train_one_epoch(model, optimizer, loader_train, verbose=True):\n    losses = []\n    model.train()\n    for t, (x, y) in enumerate(loader_train):\n        x.to(device)\n        y.to(device)\n        x_var = Variable(x, requires_grad= True).to(device)\n        y_var = Variable(y, requires_grad= False).to(device)\n        scores = model(x_var)\n        # loss = loss_fn(scores, y_var)\n        loss_function = nn.CrossEntropyLoss(reduction='none')\n        batch_loss = loss_function(scores, y_var)\n        loss = torch.mean(batch_loss)\n        losses.append(loss.data.cpu().numpy())\n        optimizer.zero_grad()\n        loss.backward()\n        # print(model.conv1.weight.grad)\n        optimizer.step()\n    if verbose:\n        print('loss = %.8f' % (loss.data))\n    return np.mean(losses)\n\n\nActual training loop\nWe define the necessary parameters for training (batch size, learning rate etc.), instantiate the optimizer and then train for 50 epochs.\nIn each epoch, the model is trained using all of the training data, which is split into batches of size 128. Thus, one step of the optimizer uses 128 samples, and there are a total of 50*(50,000/128) steps in the entire process.\n\n# Training parameters\nbatch_size=128\nlearning_rate=0.1 #\nweight_decay=2e-4\nsave_checkpoint=True\n\n# Torch optimizer\noptimizer = torch.optim.SGD(net.parameters(),\n                            lr=learning_rate,\n                            momentum=0.9,\n                            weight_decay=weight_decay)\n\n# if args.lr_schedule == 'cosine':\n#     scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n#             T_max=args.train_epochs, eta_min=0, last_epoch=-1)\n# elif args.lr_schedule == 'linear0':\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,150,200], gamma=0.1)\n\n\nfor epoch in range(0, 10):\n    start_time = time.time()\n    # lr = update_hyparam(epoch, args)\n    lr = optimizer.param_groups[0]['lr']\n    print('Current learning rate: {}'.format(lr))\n    # if not args.is_adv:\n    ben_loss = train_one_epoch(net, optimizer,\n                          loader_train, verbose=False)\n    print('time_taken for #{} epoch = {:.3f}'.format(epoch+1, time.time()-start_time))\n    if save_checkpoint:\n        ckpt_path = 'checkpoint_' + str(0)\n        torch.save(net.state_dict(), model_dir_name + ckpt_path)\n    print('Train loss - Ben: %s' %\n        (ben_loss))\n    scheduler.step()"
  },
  {
    "objectID": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#generating-adversarial-examples",
    "href": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#generating-adversarial-examples",
    "title": "Adversarial Examples for ML",
    "section": "Generating Adversarial Examples",
    "text": "Generating Adversarial Examples\nWe will look at how to generate adversarial examples using the Projected Gradient Descent (PGD) method for the model we have trained and visualize the adversarial examples thus generated.\n\n# Attack utils\n\n# Random initialization within the L2 ball\ndef rand_init_l2(img_variable, eps_max):\n    random_vec = torch.FloatTensor(*img_variable.shape).normal_(0, 1).to(device)\n    random_vec_norm = torch.max(\n               random_vec.view(random_vec.size(0), -1).norm(2, 1), torch.tensor(1e-9).to(device))\n    random_dir = random_vec/random_vec_norm.view(random_vec.size(0),1,1,1)\n    random_scale = torch.FloatTensor(img_variable.size(0)).uniform_(0, eps_max).to(device)\n    random_noise = random_scale.view(random_vec.size(0),1,1,1)*random_dir\n    img_variable = Variable(img_variable.data + random_noise, requires_grad=True).to(device)\n\n    return img_variable\n\n# Random initialization within the L_inf ball\ndef rand_init_linf(img_variable, eps_max):\n    random_noise = torch.FloatTensor(*img_variable.shape).uniform_(-eps_max, eps_max).to(device)\n    img_variable = Variable(img_variable.data + random_noise, requires_grad=True).to(device)\n\n    return img_variable\n\n# Tracking the best adversarial examples during the generation process\ndef track_best(blosses, b_adv_x, curr_losses, curr_adv_x):\n    if blosses is None:\n        b_adv_x = curr_adv_x.clone().detach()\n        blosses = curr_losses.clone().detach()\n    else:\n        replace = curr_losses &lt; blosses\n        b_adv_x[replace] = curr_adv_x[replace].clone().detach()\n        blosses[replace] = curr_losses[replace]\n\n    return blosses.to(device), b_adv_x.to(device)\n\n# Loss calculation\ndef cal_loss(y_out, y_true, targeted):\n    losses = torch.nn.CrossEntropyLoss(reduction='none')\n    losses_cal = losses(y_out, y_true).to(device)\n    loss_cal = torch.mean(losses_cal).to(device)\n    if targeted:\n        return loss_cal, losses_cal\n    else:\n        return -1*loss_cal, -1*losses_cal\n\n# Generating targets for each adversarial example\ndef generate_target_label_tensor(true_label, n_classes):\n    t = torch.floor(n_classes*torch.rand(true_label.shape)).type(torch.int64)\n    m = t == true_label\n    t[m] = (t[m]+ torch.ceil((n_classes-1)*torch.rand(t[m].shape)).type(torch.int64)) % n_classes\n    return t.to(device)\n\nThis provides the core loop of the attack algorithm which goes as follows: 1. The perturbation is initialized to 0. 2. The gradient of the model with respect to the current state of the adversarial example is found. 3. The gradient is appropriately normalized and added to the current state of the example 4. The complete adversarial example is clipped to lie within the input bounds 5. Steps 2,3 and 4 are repeated for a fixed number of steps or until some condition is met\n\n# Attack code\ndef pgd_attack(model, image_tensor, img_variable, tar_label_variable,\n               n_steps, eps_max, eps_step, clip_min, clip_max, targeted, rand_init):\n    \"\"\"\n    image_tensor: tensor which holds the clean images.\n    img_variable: Corresponding pytorch variable for image_tensor.\n    tar_label_variable: Assuming targeted attack, this variable holds the targeted labels.\n    n_steps: number of attack iterations.\n    eps_max: maximum l_inf attack perturbations.\n    eps_step: l_inf attack perturbation per step\n    \"\"\"\n\n    best_losses = None\n    best_adv_x = None\n    image_tensor = image_tensor.to(device)\n\n    if rand_init:\n        img_variable = rand_init_linf(img_variable, eps_max)\n\n    output = model.forward(img_variable)\n    for i in range(n_steps):\n        if img_variable.grad is not None:\n            img_variable.grad.zero_()\n        output = model.forward(img_variable)\n        loss_cal, losses_cal = cal_loss(output, tar_label_variable, targeted)\n        best_losses, best_adv_x = track_best(best_losses, best_adv_x, losses_cal, img_variable)\n\n        loss_cal, losses_cal = cal_loss(output, tar_label_variable, targeted)\n        loss_cal.backward()\n        # Finding the gradient of the loss\n        x_grad = -1 * eps_step * torch.sign(img_variable.grad.data)\n        # Adding gradient to current state of the example\n        adv_temp = img_variable.data + x_grad\n        total_grad = adv_temp - image_tensor\n        total_grad = torch.clamp(total_grad, -eps_max, eps_max)\n        x_adv = image_tensor + total_grad\n        # Projecting adversarial example back onto the constraint set\n        x_adv = torch.clamp(torch.clamp(\n            x_adv-image_tensor, -1*eps_max, eps_max)+image_tensor, clip_min, clip_max)\n        img_variable.data = x_adv\n\n    best_losses, best_adv_x = track_best(best_losses, best_adv_x, losses_cal, img_variable)\n\n    return best_adv_x\n\ndef pgd_l2_attack(model, image_tensor, img_variable, tar_label_variable,\n               n_steps, eps_max, eps_step, clip_min, clip_max, targeted,\n               rand_init, num_restarts):\n    \"\"\"\n    image_tensor: tensor which holds the clean images.\n    img_variable: Corresponding pytorch variable for image_tensor.\n    tar_label_variable: Assuming targeted attack, this variable holds the targeted labels.\n    n_steps: number of attack iterations.\n    eps_max: maximum l_inf attack perturbations.\n    eps_step: l_inf attack perturbation per step\n    \"\"\"\n\n    best_losses = None\n    best_adv_x = None\n    image_tensor_orig = image_tensor.clone().detach()\n    tar_label_orig = tar_label_variable.clone().detach()\n\n    for j in range(num_restarts):\n        if rand_init:\n            img_variable = rand_init_l2(img_variable, eps_max)\n\n        output = model.forward(img_variable)\n        for i in range(n_steps):\n            if img_variable.grad is not None:\n                img_variable.grad.zero_()\n            output = model.forward(img_variable)\n            loss_cal, losses_cal = cal_loss(output, tar_label_variable, targeted)\n            best_losses, best_adv_x = track_best(best_losses, best_adv_x, losses_cal, img_variable)\n            loss_cal.backward()\n            raw_grad = img_variable.grad.data\n            grad_norm = torch.max(\n                   raw_grad.view(raw_grad.size(0), -1).norm(2, 1), torch.tensor(1e-9))\n            grad_dir = raw_grad/grad_norm.view(raw_grad.size(0),1,1,1)\n            adv_temp = img_variable.data +  -1 * eps_step * grad_dir\n            # Clipping total perturbation\n            total_grad = adv_temp - image_tensor\n            total_grad_norm = torch.max(\n                   total_grad.view(total_grad.size(0), -1).norm(2, 1), torch.tensor(1e-9))\n            total_grad_dir = total_grad/total_grad_norm.view(total_grad.size(0),1,1,1)\n            total_grad_norm_rescale = torch.min(total_grad_norm, torch.tensor(eps_max))\n            clipped_grad = total_grad_norm_rescale.view(total_grad.size(0),1,1,1) * total_grad_dir\n            x_adv = image_tensor + clipped_grad\n            x_adv = torch.clamp(x_adv, clip_min, clip_max)\n            img_variable.data = x_adv\n\n        best_losses, best_adv_x = track_best(best_losses, best_adv_x, losses_cal, img_variable)\n\n        diff_array = np.array(x_adv.cpu())-np.array(image_tensor.data.cpu())\n        diff_array = diff_array.reshape(len(diff_array),-1)\n\n        img_variable.data = image_tensor_orig\n\n    return best_adv_x\n\nNow we can call the core adversarial example generation function over our data and model to determine how robust the model actually is!\n\ndef robust_test(model, loss_fn, loader, att_dir, n_batches=0, train_data=False,\n                training_time=False):\n    \"\"\"\n    n_batches (int): Number of batches for evaluation.\n    \"\"\"\n    model.eval()\n    num_correct, num_correct_adv, num_samples = 0, 0, 0\n    steps = 1\n    losses_adv = []\n    losses_ben = []\n    adv_images = []\n    adv_labels = []\n    clean_images = []\n    correct_labels = []\n\n    for t, (x, y) in enumerate(loader):\n        x=x.to(device)\n        y=y.to(device)\n        x_var = Variable(x, requires_grad= True).to(device)\n        y_var = Variable(y, requires_grad=False).to(device)\n        if att_dir['targeted']:\n            y_target = generate_target_label_tensor(\n                               y_var.cpu(), 10).to(device)\n        else:\n            y_target = y_var\n        if 'PGD_linf' in att_dir['attack']:\n            adv_x = pgd_attack(model, x, x_var, y_target, att_dir['attack_iter'],\n                           att_dir['epsilon'], att_dir['eps_step'], att_dir['clip_min'],\n                           att_dir['clip_max'], att_dir['targeted'], att_dir['rand_init'])\n        elif 'PGD_l2' in att_dir['attack']:\n            adv_x = pgd_l2_attack(model, x, x_var, y_target, att_dir['attack_iter'],\n                           att_dir['epsilon'], att_dir['eps_step'], att_dir['clip_min'],\n                           att_dir['clip_max'], att_dir['targeted'], att_dir['rand_init'],\n                           att_dir['num_restarts'])\n        # Predictions\n        # scores = model(x.cuda())\n        scores = model(x)\n        _, preds = scores.data.max(1)\n        scores_adv = model(adv_x)\n        _, preds_adv = scores_adv.data.max(1)\n        # Losses\n        batch_loss_adv = loss_fn(scores_adv, y)\n        loss_adv = torch.mean(batch_loss_adv)\n        losses_adv.append(loss_adv.data.cpu().numpy())\n        batch_loss_ben = loss_fn(scores, y)\n        loss_ben = torch.mean(batch_loss_ben)\n        losses_ben.append(loss_ben.data.cpu().numpy())\n        # Correct count\n        num_correct += (preds == y).sum()\n        num_correct_adv += (preds_adv == y).sum()\n        num_samples += len(preds)\n        # Adding images and labels to list\n        adv_images.extend(adv_x)\n        adv_labels.extend(preds_adv)\n        clean_images.extend(x)\n        correct_labels.extend(preds)\n\n        if n_batches &gt; 0 and steps==n_batches:\n            break\n        steps += 1\n\n    acc = float(num_correct) / num_samples\n    acc_adv = float(num_correct_adv) / num_samples\n    print('Clean accuracy: {:.2f}% ({}/{})'.format(\n        100.*acc,\n        num_correct,\n        num_samples,\n    ))\n    print('Adversarial accuracy: {:.2f}% ({}/{})'.format(\n        100.*acc_adv,\n        num_correct_adv,\n        num_samples,\n    ))\n\n    return 100.*acc, 100.*acc_adv, np.mean(losses_ben), np.mean(losses_adv), adv_images, adv_labels, clean_images, correct_labels\n\nThe most important parameters below are epsilon (which controls the magnitude of the perturbation), gamma (which determines how far outside the constraint set intial search is allowed) and attack_iter (which is just the number of attack iterations).\n\n# Attack setup\ngamma=2.5\nepsilon=0.2\nattack_iter=10\ndelta=epsilon*gamma/attack_iter\nattack_params = {'attack': 'PGD_linf', 'epsilon': epsilon,\n              'attack_iter': 10, 'eps_step': delta,\n              'targeted': True, 'clip_min': 0.0,\n              'clip_max': 1.0,'rand_init': True,\n              'num_restarts': 1}\n\nNow, we load the model (remember to first upload it into the models folder!) and then generate adversarial examples.\n\nckpt_path = 'checkpoint_' + str(0)\nnet.to(device)\nnet.eval()\nnet.load_state_dict(torch.load(model_dir_name + ckpt_path, map_location=device))\nn_batches_eval = 10\nprint('Test set validation')\n# Running validation\nacc_test, acc_adv_test, test_loss, test_loss_adv, adv_images, adv_labels, clean_images, correct_labels = robust_test(net,\n    criterion, loader_test, attack_params, n_batches=n_batches_eval,\n    train_data=False, training_time=True)\n# print('Training set validation')\n# acc_train, acc_adv_train, train_loss, train_loss_adv, _ = robust_test(net,\n#     criterion, loader_train_all, args, attack_params, n_batches=n_batches_eval,\n#     train_data=True, training_time=True)\n\n\nVisualizing the adversarial examples\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nfig = plt.figure(figsize=(9, 13))\ncolumns = 4\nrows = 5\n\n# ax enables access to manipulate each of subplots\nax = []\n\nfor i in range(columns*rows):\n    image_count=int(i/2)\n    if i%2==1:\n      img = adv_images[image_count].reshape(28,28).cpu()\n      # create subplot and append to ax\n      ax.append( fig.add_subplot(rows, columns, i+1) )\n      ax[-1].set_title(\"output:\"+str(adv_labels[image_count].cpu().numpy()))  # set title\n      ax[-1].set_xticks([])\n      ax[-1].set_yticks([])\n    else:\n      img = clean_images[image_count].reshape(28,28).cpu()\n      # create subplot and append to ax\n      ax.append( fig.add_subplot(rows, columns, i+1) )\n      ax[-1].set_title(\"output:\"+str(correct_labels[image_count].cpu().numpy()))  # set title\n      ax[-1].set_xticks([])\n      ax[-1].set_yticks([])\n    plt.imshow(img, interpolation='nearest',cmap='gray')\n\n\nplt.show()  # finally, render the plot"
  },
  {
    "objectID": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#training-robust-models",
    "href": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#training-robust-models",
    "title": "Adversarial Examples for ML",
    "section": "Training robust models",
    "text": "Training robust models\nThis training loop is very similar to the benign one, except that we now call the adversarial example generation function to generate adversarial examples during the training process.\n\n########################################  Adversarial training ########################################\ndef robust_train_one_epoch(model, optimizer, loader_train, att_dir,\n                           epoch):\n    # print('Current eps: {}, delta: {}'.format(eps, delta))\n    losses_adv = []\n    losses_ben = []\n    model.train()\n    for t, (x, y) in enumerate(loader_train):\n        x=x.to(device)\n        y=y.to(device)\n        x_var = Variable(x, requires_grad= True)\n        y_var = Variable(y, requires_grad= False)\n        if att_dir['targeted']:\n            y_target = generate_target_label_tensor(\n                               y_var.cpu(), 10).to(device)\n        else:\n            y_target = y_var\n        if 'PGD_linf' in att_dir['attack']:\n            adv_x = pgd_attack(model, x, x_var, y_target, att_dir['attack_iter'],\n                           att_dir['epsilon'], att_dir['eps_step'], att_dir['clip_min'],\n                           att_dir['clip_max'], att_dir['targeted'], att_dir['rand_init'])\n        elif 'PGD_l2' in att_dir['attack']:\n            adv_x = pgd_l2_attack(model, x, x_var, y_target, att_dir['attack_iter'],\n                           att_dir['epsilon'], att_dir['eps_step'], att_dir['clip_min'],\n                           att_dir['clip_max'], att_dir['targeted'], att_dir['rand_init'],\n                           att_dir['num_restarts'])\n        scores = model(adv_x)\n        loss_function = nn.CrossEntropyLoss(reduction='none')\n        batch_loss_adv = loss_function(scores, y_var)\n        batch_loss_ben = loss_function(model(x),y_var)\n        loss = torch.mean(batch_loss_adv)\n        loss_ben = torch.mean(batch_loss_ben)\n        losses_ben.append(loss_ben.data.cpu().numpy())\n        losses_adv.append(loss.data.cpu().numpy())\n        # GD step\n        optimizer.zero_grad()\n        loss.backward()\n        # print(model.conv1.weight.grad)\n        optimizer.step()\n    return np.mean(losses_adv), np.mean(losses_ben)\n\n\nfor epoch in range(0, 10):\n    start_time = time.time()\n    # lr = update_hyparam(epoch, args)\n    lr = optimizer.param_groups[0]['lr']\n    print('Current learning rate: {}'.format(lr))\n    curr_loss, ben_loss = robust_train_one_epoch(net,\n                            optimizer, loader_train, attack_params,\n                            epoch)\n    print('time_taken for #{} epoch = {:.3f}'.format(epoch+1, time.time()-start_time))\n    if save_checkpoint:\n        ckpt_path = 'checkpoint_adv' + str(0)\n        torch.save(net.state_dict(), model_dir_name + ckpt_path)\n    print('Train loss - Ben: %s, Adv: %s' %\n        (ben_loss, curr_loss))\n    scheduler.step()"
  },
  {
    "objectID": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#evaluating-the-robust-model",
    "href": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#evaluating-the-robust-model",
    "title": "Adversarial Examples for ML",
    "section": "Evaluating the robust model",
    "text": "Evaluating the robust model\nEvaluating the robust model, we find its accuracy on adversarial examples has increased significantly!\n\nckpt_path = 'checkpoint_adv' + str(0)\nnet.eval()\nnet.load_state_dict(torch.load(model_dir_name + ckpt_path, map_location=device))\nn_batches_eval = 10\nprint('Test set validation')\n# Running validation\nacc_test_r, acc_adv_test_r, test_loss_r, test_loss_adv_r, adv_images_r, adv_labels_r, clean_images_r, correct_labels_r = robust_test(net,\n    criterion, loader_test, attack_params, n_batches=n_batches_eval,\n    train_data=False, training_time=True)\n# print('Training set validation')\n# acc_train, acc_adv_train, train_loss, train_loss_adv, _ = robust_test(net,\n#     criterion, loader_train_all, args, attack_params, n_batches=n_batches_eval,\n#     train_data=True, training_time=True)\n\n\nfig = plt.figure(figsize=(9, 13))\ncolumns = 4\nrows = 5\n\n# ax enables access to manipulate each of subplots\nax = []\n\nfor i in range(columns*rows):\n    image_count=int(i/2)\n    if i%2==1:\n      img = adv_images_r[image_count].reshape(28,28).cpu()\n      # create subplot and append to ax\n      ax.append( fig.add_subplot(rows, columns, i+1) )\n      ax[-1].set_title(\"output:\"+str(adv_labels_r[image_count].cpu().numpy()))  # set title\n      ax[-1].set_xticks([])\n      ax[-1].set_yticks([])\n    else:\n      img = clean_images_r[image_count].reshape(28,28).cpu()\n      # create subplot and append to ax\n      ax.append( fig.add_subplot(rows, columns, i+1) )\n      ax[-1].set_title(\"output:\"+str(correct_labels_r[image_count].cpu().numpy()))  # set title\n      ax[-1].set_xticks([])\n      ax[-1].set_yticks([])\n    plt.imshow(img, interpolation='nearest',cmap='gray')\n\n\nplt.show()  # finally, render the plot"
  },
  {
    "objectID": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#discussion-questions",
    "href": "notebooks/guest-lecture/Student_copy_adv_examples_tutorial__Arjun_Bhagoji.html#discussion-questions",
    "title": "Adversarial Examples for ML",
    "section": "Discussion questions",
    "text": "Discussion questions\n\nDoesn’t robust training solve the problem of adversarial examples? Why is there still so much research on the topic?\nHow would a real-world attacker try to carry out this attack without access to the classifier being used?\nWhat does the existence of adversarial examples tell us about modern ML models?"
  },
  {
    "objectID": "notebooks/logistic-regression-cost.html",
    "href": "notebooks/logistic-regression-cost.html",
    "title": "Logistic Regression - Cost Function",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom latexify import *\n# Matplotlib retina\n%config InlineBackend.figure_format = 'retina'\n\n\nX = np.array([\n    [1],\n    [2],\n    [3],\n    [4],\n    [5],\n    [6]\n])\n\ny = np.array([1, 1, 1, 0, 0, 0])\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nlr = LogisticRegression(penalty='none',solver='newton-cg')\n\n\nlr.fit(X, y)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n\n\nLogisticRegression(penalty='none', solver='newton-cg')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='none', solver='newton-cg')\n\n\n\nlr.coef_\n\narray([[-18.33148189]])\n\n\n\nlr.intercept_\n\narray([64.11147504])\n\n\n\ndef sigmoid(z):\n    return 1/(1+np.exp(z))\n\n\ntheta_0_li, theta_1_li = np.meshgrid(np.linspace(-10, 10, 200), np.linspace(-10, 10, 200))\n\n\ndef cost_rmse(theta_0, theta_1):\n    y_hat = sigmoid(theta_0 + theta_1*X)\n    err = np.sum((y-y_hat)**2)\n    return err\n\n\nz = np.zeros((len(theta_0_li), len(theta_0_li)))\nfor i in range(len(theta_0_li)):\n    for j in range(len(theta_0_li)):\n        z[i, j] = cost_rmse(theta_0_li[i, j], theta_1_li[i, j])\n\n\nlatexify()\nplt.contourf(theta_0_li, theta_1_li, z)\nplt.xlabel(r'$\\theta_0$')\nplt.ylabel(r'$\\theta_1$')\nplt.colorbar()\nplt.title('RMSE contour plot')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/logistic-regression/logistic-sse-loss-contour.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nlatexify()\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(theta_0_li, theta_1_li, z)\nax.set_title('RMSE surface plot')\nax.set_xlabel(r'$\\theta_0$')\nax.set_ylabel(r'$\\theta_1$')\nplt.tight_layout()\nplt.savefig(\"../figures/logistic-regression/logistic-sse-loss-3d.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nimport pandas as pd\n\n\npd.DataFrame(z).min().min()\n\n9.01794626038055\n\n\n\ndef cost_2(theta_0, theta_1):\n    y_hat = sigmoid(theta_0 + theta_1*X)\n    \n    err = -np.sum((y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))\n    return err\n\n\nz2 = np.zeros((len(theta_0_li), len(theta_0_li)))\nfor i in range(len(theta_0_li)):\n    for j in range(len(theta_0_li)):\n        z2[i, j] = cost_2(theta_0_li[i, j], theta_1_li[i, j])\n\n/tmp/ipykernel_851067/1266618369.py:4: RuntimeWarning: divide by zero encountered in log\n  err = -np.sum((y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))\n/tmp/ipykernel_851067/1266618369.py:4: RuntimeWarning: invalid value encountered in multiply\n  err = -np.sum((y*np.log(y_hat) + (1-y)*np.log(1-y_hat)))\n\n\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nlatexify()\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(theta_0_li, theta_1_li, z2)\nax.set_title('Cross-entropy surface plot')\nax.set_xlabel(r'$\\theta_0$')\nax.set_ylabel(r'$\\theta_1$')\nplt.tight_layout()\nplt.savefig(\"../figures/logistic-regression/logistic-cross-loss-surface.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nlatexify()\nplt.contourf(theta_0_li, theta_1_li, z2)\nplt.title('Cross-entropy contour plot')\nplt.colorbar()\nplt.xlabel(r'$\\theta_0$')\nplt.ylabel(r'$\\theta_1$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/logistic-regression/logistic-cross-loss-contour.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\ny.shape, y_bar.shape\n\n((6,), (10000,))\n\n\n\ny = 0\ny_bar = np.linspace(0, 1.1, 10000)\nplt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\nformat_axes(plt.gca())\nplt.ylabel(\"Cost when y = 0\")\nplt.xlabel(r'$\\hat{y}$')\nplt.savefig(\"../figures/logistic-regression/logistic-cross-cost-0.pdf\", bbox_inches=\"tight\", transparent=True)\n\n/tmp/ipykernel_851067/3960806875.py:3: RuntimeWarning: divide by zero encountered in log\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n/tmp/ipykernel_851067/3960806875.py:3: RuntimeWarning: invalid value encountered in multiply\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n/tmp/ipykernel_851067/3960806875.py:3: RuntimeWarning: invalid value encountered in log\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n\n\n\n\n\n\n\n\n\n\ny = 1\ny_bar = np.linspace(0, 1.1, 10000)\nplt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\nformat_axes(plt.gca())\nplt.ylabel(\"Cost when y = 1\")\nplt.xlabel(r'$\\hat{y}$')\nplt.savefig(\"../figures/logistic-regression/logistic-cross-cost-1.pdf\", bbox_inches=\"tight\", transparent=True)\n\n/tmp/ipykernel_851067/210742206.py:3: RuntimeWarning: divide by zero encountered in log\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n/tmp/ipykernel_851067/210742206.py:3: RuntimeWarning: invalid value encountered in log\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n/tmp/ipykernel_851067/210742206.py:3: RuntimeWarning: invalid value encountered in multiply\n  plt.plot(y_bar, -y*np.log(y_bar) - (1-y)*np.log(1-y_bar))\n\n\n\n\n\n\n\n\n\n\nLikelihood\n\nX_with_one = np.hstack((np.ones_like(X), X))\n\n\n\\[\\begin{align*}\nP(y | X, \\theta) &= \\prod_{i=1}^{n} P(y_{i} | x_{i}, \\theta) \\\\ &= \\prod_{i=1}^{n} \\Big\\{\\frac{1}{1 + e^{-x_{i}^{T}\\theta}}\\Big\\}^{y_{i}}\\Big\\{1 - \\frac{1}{1 + e^{-x_{i}^{T}\\theta}}\\Big\\}^{1 - y_{i}} \\\\\n\\end{align*}\\]\n\nX_with_one[1]\n\narray([1, 2])\n\n\n\ndef likelihood(theta_0, theta_1):\n    s = 1\n\n    for i in range(len(X)):\n        y_i_hat = sigmoid(-X_with_one[i]@np.array([theta_0, theta_1]))\n        s = s* ((y_i_hat**y[i])*(1-y_i_hat)**(1-y[i]))\n    \n    \n    return s\n\nx_grid_2, y_grid_2 = np.mgrid[-5:100:0.5, -30:10:.1]\n\nli = np.zeros_like(x_grid_2)\nfor i in range(x_grid_2.shape[0]):\n    for j in range(x_grid_2.shape[1]):\n        li[i, j] = likelihood(x_grid_2[i, j], y_grid_2[i, j])\n        \n\n\nplt.contourf(x_grid_2, y_grid_2, li)\n#plt.gca().set_aspect('equal')\nplt.xlabel(r\"$\\theta_0$\")\nplt.ylabel(r\"$\\theta_1$\")\nplt.colorbar()\nplt.scatter(lr.intercept_[0], lr.coef_[0], s=200, marker='*', color='r', label='MLE')\nplt.title(r\"Likelihood as a function of ($\\theta_0, \\theta_1$)\")\n#plt.gca().set_aspect('equal')\nplt.legend()\nplt.savefig(\"../figures/logistic-regression/logistic-likelihood.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "notebooks/autodiff.html",
    "href": "notebooks/autodiff.html",
    "title": "Autodiff",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, TensorDataset\nimport seaborn as sns\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n# Set random seed for reproducibility\ntorch.manual_seed(0)\n\n# Torch version\ntorch.__version__\n\n'2.0.1'\n\n\n\n### Derviatives using numerical differentiation\n\ndef f(x):\n    return 3 * x ** 2 + 2 * x + 1\n\ndef numerical_derivative_single_side(f, x, h=0.001):\n    return (f(x + h) - f(x)) / h\n\ndef numerical_derivative_double_side(f, x, h=0.001):\n    return (f(x + h) - f(x - h)) / (2 * h)\n\nx = torch.tensor(2.0, requires_grad=False)\nprint(f'f\\'(2) = {numerical_derivative_single_side(f, x, 0.00001)}')\nprint(f'f\\'(2) = {numerical_derivative_double_side(f, x, 0.00001)}')\n\nf'(2) = 14.1143798828125\nf'(2) = 14.1143798828125\n\n\n\ndef f(theta_0, theta_1, theta_2, theta_3, theta_4):\n    return theta_0 + 2 * theta_1 + 3 * theta_2 + 4 * theta_3 + 5 * theta_4\n\n\n\ntheta_0 = torch.tensor(1.0, requires_grad=False)\ntheta_1 = torch.tensor(2.0, requires_grad=False)\ntheta_2 = torch.tensor(3.0, requires_grad=False)\ntheta_3 = torch.tensor(4.0, requires_grad=False)\ntheta_4 = torch.tensor(5.0, requires_grad=False)\n\n\ndf_dtheta_0 = numerical_derivative_single_side(lambda theta_0: f(theta_0, theta_1, theta_2, theta_3, theta_4), theta_0, 0.0001)\ndf_dtheta_0\n\ntensor(0.9918)\n\n\n\n## Above method is very expensive and not practical for large number of parameters\n\n\ntheta_0 = torch.tensor(1.0, requires_grad=True)\ntheta_1 = torch.tensor(1.0, requires_grad=True)\ntheta_2 = torch.tensor(2.0, requires_grad=True)\n\nx1 = torch.tensor(1.0)\nx2 = torch.tensor(2.0)\n\nf1 = theta_1*x1\nf2 = theta_2*x2\n\nf3 = f1 + f2\n\nf4 = f3 + theta_0\n\nf5 = f4*-1\n\nf6 = torch.exp(f5)\n\nf7 = 1 + f6\n\nf8 = 1/f7\n\nf9 = torch.log(f8)\n\nL = f9*-1\n\nall_nodes = {\"theta_0\": theta_0, \"theta_1\": theta_1, \"theta_2\": theta_2,  \n             \"f1\": f1, \"f2\": f2, \"f3\": f3, \"f4\": f4, \"f5\": f5, \"f6\": f6, \"f7\": f7, \"f8\": f8, \"f9\": f9, \"L\": L}\n\n# Retain grad for all nodes\nfor node in all_nodes.values():\n    node.retain_grad()\n\n\n# Print out the function evaluation for all nodes along with name of the node\nfor name, node in all_nodes.items():\n    print(f\"{name}: {node.item()}\")\n\ntheta_0: 1.0\ntheta_1: 1.0\ntheta_2: 2.0\nf1: 1.0\nf2: 4.0\nf3: 5.0\nf4: 6.0\nf5: -6.0\nf6: 0.0024787522852420807\nf7: 1.0024787187576294\nf8: 0.9975274205207825\nf9: -0.0024756414350122213\nL: 0.0024756414350122213\n\n\n\nL.backward()\n\n# Print out the gradient for all nodes along with name of the node\nfor name, node in all_nodes.items():\n    print(f\"{name}: {node.grad.item()}\")\n\ntheta_0: -0.00247262348420918\ntheta_1: -0.00247262348420918\ntheta_2: -0.00494524696841836\nf1: -0.00247262348420918\nf2: -0.00247262348420918\nf3: -0.00247262348420918\nf4: -0.00247262348420918\nf5: 0.00247262348420918\nf6: 0.9975274801254272\nf7: 0.9975274801254272\nf8: -1.0024787187576294\nf9: -1.0\nL: 1.0\n\n\n\n(-1/(f7**2))*-1.00247\n\ntensor(0.9975, grad_fn=&lt;MulBackward0&gt;)\n\n\n\ntorch.exp(f5)*0.9975\n\ntensor(0.0025, grad_fn=&lt;MulBackward0&gt;)\n\n\n\n### Micrograd demo: https://github.com/karpathy/micrograd/blob/master/micrograd/engine.py\n\n\n### Example to illustrate accumulation of gradients\n\ntheta = torch.tensor(1.0, requires_grad=True)\n\nx1 = torch.tensor(1.0)\nx2 = torch.tensor(2.0)\n\nL1 = theta*x1\nL2 = theta*x2\n\nL = L1 + L2\nL.backward()\n\n\ntheta.grad\n\ntensor(3.)\n\n\n\nWhy do we need to use torch.no_grad() in the test phase?\n\n\nWhy do we need to zero out the gradients in the training phase after each update?\nmodel = SimpleModel()\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Dummy data\ninputs = torch.randn((100, 10))\ntargets = torch.randn((100, 1))\n\n# Training loop\nfor epoch in range(100):\n    # Forward pass\n    outputs = model(inputs)\n    \n    # Compute the loss\n    loss = criterion(outputs, targets)\n    \n    # Zero the gradients\n    optimizer.zero_grad()\n    \n    # Backward pass\n    loss.backward()\n    \n    # Update the weights\n    optimizer.step()\n\n    # Print the loss every 10 epochs\n    if epoch % 10 == 0:\n        print(f'Epoch {epoch}, Loss: {loss.item()}')"
  },
  {
    "objectID": "notebooks/Gradient Descent-2d.html",
    "href": "notebooks/Gradient Descent-2d.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n\nsns.despine()\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\ninit_x = 2\ninit_y = 5\n\n\n%matplotlib inline\n\n\nimport numpy as np\n\n\n4.1*4.1\n\n16.81\n\n\n\n4.1-0.2*2*4.1\n\n2.46\n\n\n\nx = 4.1\nalpha = 0.2\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    x = x- (alpha*2*x)\n    print(\"(\",round(x, 2),\",\" ,round(x*x, 2),\")\")\n\n( 2.46 , 6.05 )\n( 1.48 , 2.18 )\n( 0.89 , 0.78 )\n( 0.53 , 0.28 )\n( 0.32 , 0.1 )\n( 0.19 , 0.04 )\n( 0.11 , 0.01 )\n( 0.07 , 0.0 )\n( 0.04 , 0.0 )\n( 0.02 , 0.0 )\n\n\n\nx = 4.1\nalpha = 0.05\niterations = 20\n\n# for i in range(10):\nfor i in range(iterations):\n    x = x- (alpha*2*x)\n    st = \"\"\"\n    \n    \\begin{frame}{Iteration %d}\n    \\begin{columns}\n\n\n        \\begin{column}{0.6\\textwidth}\n            \\begin{adjustbox}{max totalsize={\\textwidth},center}\n                \\begin{tikzpicture}\n\n                    \\begin{axis}[\n                        xlabel=$x$,\n                        ylabel=$y$,\n                        xmin=-4.2,\n                        xmax=4.2,\n                        axis x line*=bottom,\n                        axis y line*=left,\n                        xtick align=outside,\n                        ytick align=outside,\n                        legend pos=outer north east\n                        ]\n                        \\addplot[mark=none, gray] {x^2};\\addlegendentry{$y=x^2$}\n                        \\addplot[only marks, mark=*]\n                        coordinates{ % plot 1 data set\n                            (%s,%s)\n                            }; \n\n\n\n                        \\end{axis}\n\n                \\end{tikzpicture}\n            \\end{adjustbox}\n        \\end{column}\n    \\begin{column}{0.5\\textwidth}\n    \\begin{adjustbox}{max totalsize={\\textwidth},center}\n        \\begin{tikzpicture}\n        \\begin{axis}\n        [\n        title={Contour plot, view from top},\n        view={0}{90},\n        xlabel=$x$,\n        ylabel=$y$,\n        axis x line*=bottom,\n        axis y line*=left,\n        xtick align=outside,\n        ytick align=outside,\n        unit vector ratio*=1 1 1,\n        ]\n        \\addplot3[\n        contour gnuplot={number=14,}\n        ]\n        {x^2};\n        \\addplot[only marks, mark=*]\n        coordinates{ % plot 1 data set\n            (%f,%f)\n        }; \n        \\end{axis}\n        \\end{tikzpicture}\n        \\end{adjustbox}\n    \\end{column}\n    \\end{columns}\n\n\n    \\end{frame}\n    \"\"\" %(i, i, i, i, i)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-10-3f4f6db3ccf0&gt; in &lt;module&gt;\n     71 \n     72     \\end{frame}\n---&gt; 73     \"\"\" %(i, i, i, i, i)\n\nValueError: unsupported format character 'p' (0x70) at index 793\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Gradient Descent}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-10.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-11.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-12.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-13.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-14.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-15.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-16.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-17.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-18.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-19.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-20.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\n8.2*4.1\n\n33.62\n\n\n\ndef func(x):\n    return np.sin(x) + np.sin(x/2) + np.sin(x/3)\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\nlatexify()\nval = -7.2\n\nplt.scatter([val],func(np.array([val])), color='k')\nax.annotate('Local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='grey', shrink=0.0001))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y, color='grey')\nformat_axes(plt.gca())\nplt.xlabel(\"x\")\nplt.ylabel(\"y=f(x)\")\nplt.savefig(\"../gradient-descent/local-minima.eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\nimport sys\nsys.path.append(\"../\")\n\n\nfrom latexify import format_axes, latexify\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = 0.95\niterations = 10\nlatexify()\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y,'k')\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5, color='grey')\n    plt.scatter([prev],[prev**2],s=40, color='grey')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/overshooting-\"+str(i+1)+\".eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Overshooting}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\nlatexify()\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y,'k')\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=40, color='grey')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Slow Convergence}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\nlatexify()\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y,'k')\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=40, color='grey')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True, bbox_inches=\"tight\")\n\n\nx = np.linspace(1,10,100)\ny = 1/x\nplt.plot(y,label=\"GD\")\nnoise = np.random.random((len(x)))\nnoise[0] = 0\nnoise[1] = 0\nnoise[2] = 0\nplt.plot(y+0.2*(noise-0.5),label=\"SGD\")\nplt.legend()\nplt.title(\"Iterations vs Cost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\nplt.savefig(\"gd-sgd.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nval = 4.1\nalpha = 0.05\n\nfor i in range(10):\n    val = val - alpha * 2* val\n    print (val)\n\n3.6899999999999995\n3.3209999999999997\n2.9888999999999997\n2.6900099999999996\n2.4210089999999997\n2.1789080999999997\n1.9610172899999996\n1.7649155609999996\n1.5884240048999996\n1.4295816044099996"
  },
  {
    "objectID": "notebooks/decision-tree-discrete-input-discrete-output.html",
    "href": "notebooks/decision-tree-discrete-input-discrete-output.html",
    "title": "Decision Trees Discrete Input and Discrete Output",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom latexify import latexify, format_axes\n\n\ndf = pd.read_csv(\"../datasets/tennis-discrete-output.csv\", index_col=0)\n\n\ndf\n\n\n\n\n\n\n\n\nOutlook\nTemp\nHumidity\nWindy\nPlay\n\n\nDay\n\n\n\n\n\n\n\n\n\nD1\nSunny\nHot\nHigh\nWeak\nNo\n\n\nD2\nSunny\nHot\nHigh\nStrong\nNo\n\n\nD3\nOvercast\nHot\nHigh\nWeak\nYes\n\n\nD4\nRain\nMild\nHigh\nWeak\nYes\n\n\nD5\nRain\nCool\nNormal\nWeak\nYes\n\n\nD6\nRain\nCool\nNormal\nStrong\nNo\n\n\nD7\nOvercast\nCool\nNormal\nStrong\nYes\n\n\nD8\nSunny\nMild\nHigh\nWeak\nNo\n\n\nD9\nSunny\nCool\nNormal\nWeak\nYes\n\n\nD10\nRain\nMild\nNormal\nWeak\nYes\n\n\nD11\nSunny\nMild\nNormal\nStrong\nYes\n\n\nD12\nOvercast\nMild\nHigh\nStrong\nYes\n\n\nD13\nOvercast\nHot\nNormal\nWeak\nYes\n\n\nD14\nRain\nMild\nHigh\nStrong\nNo\n\n\n\n\n\n\n\n\ndef entropy(ser):\n    \"\"\"\n    Calculate entropy for a categorical variable.\n\n    Parameters:\n    - ser: pd.Series of categorical data\n\n    Returns:\n    - Entropy value\n    \"\"\"\n    # Count the occurrences of each unique value in the series\n    value_counts = ser.value_counts()\n\n    # Calculate the probabilities of each unique value\n    probabilities = value_counts / len(ser)\n\n    # Calculate entropy using the formula: H(S) = -p1*log2(p1) - p2*log2(p2) - ...\n    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n\n    return entropy_value\n    \n\n\nentropy(df[\"Play\"])\n\n0.9402859586706311"
  },
  {
    "objectID": "notebooks/perceptron-learning.html",
    "href": "notebooks/perceptron-learning.html",
    "title": "Perceptron learning algorithm",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny_and = np.array([0, 0, 0, 1])\ny_or = np.array([0, 1, 1, 1])\ny_xor = np.array([0, 1, 1, 0])\n\n\nclass Perceptron(object):\n    def __init__(self, lr=0.01, iterations=100):\n        self.lr = lr\n        self.iterations = iterations\n        \n    def activation(self, z):\n        ac = np.zeros_like(z)\n        ac[z&gt;0] = 1\n        return ac\n    \n    def fit(self, X, y):\n        X_with_one = np.append(np.ones((len(X), 1)), X, axis=1)\n        self.W = np.zeros((X_with_one.shape[1], 1))\n        for i in range(self.iterations):\n            for j in range(len(X)):\n                summation = (X_with_one@self.W).flatten()\n                y_hat = self.activation(summation)\n                err = y - y_hat.flatten() \n                self.W = self.W + (self.lr*err[j]*X_with_one[j]).reshape(*(self.W.shape))\n        \n    def predict(self, X):\n        X_with_one = np.append(np.ones((len(X), 1)), X, axis=1)\n        summation = (X_with_one@self.W).flatten()\n        y_hat = self.activation(summation)           \n        return y_hat\n\n    \n\n\nperceptron = Perceptron()\n\n\nperceptron.fit(X, y_or)\n\n\nperceptron.W\n\narray([[0.  ],\n       [0.01],\n       [0.01]])\n\n\n\nperceptron.predict(X)\n\narray([0., 1., 1., 1.])\n\n\n\nperceptron.fit(X, y_and)\n\n\nperceptron.W\n\narray([[-0.02],\n       [ 0.02],\n       [ 0.01]])\n\n\n\nperceptron.predict(X)\n\narray([0., 0., 0., 1.])\n\n\n\nperceptron.fit(X, y_xor)\n\n\nperceptron.W\n\narray([[ 0.01],\n       [-0.01],\n       [ 0.  ]])\n\n\n\nperceptron.predict(X)\n\narray([1., 1., 0., 0.])\n\n\n\nXOR using feature transformation\n\n# Transformation: 1 \n# x1, x2, x1x2\nX_xor_1 = np.append(X, (X[:, 0]*X[:, 1]).reshape(-1, 1), axis=1)\n\n\nperceptron = Perceptron()\n\n\nperceptron.fit(X_xor_1, y_xor)\n\n\nperceptron.W\n\narray([[ 0.  ],\n       [ 0.01],\n       [ 0.01],\n       [-0.04]])\n\n\n\nnp.allclose(perceptron.predict(X_xor_1), y_xor)\n\nTrue\n\n\n\n(X[:, 0]*X[:, 1]).reshape(-1, 1)\n\narray([[0],\n       [0],\n       [0],\n       [1]])\n\n\n\n# Transformation: 1 \n# x1, x2, x1x2\n\n\nX_xor_2 = np.array([(1-X[:, 0])*X[:,1], (1-X[:, 1])*X[:,0]]).T\n\n\nperceptron = Perceptron()\nperceptron.fit(X_xor_2, y_xor)\n\n\nperceptron.W\n\narray([[0.  ],\n       [0.01],\n       [0.01]])"
  },
  {
    "objectID": "notebooks/cross-validation-diagrams.html",
    "href": "notebooks/cross-validation-diagrams.html",
    "title": "Cross Validation Diagrams",
    "section": "",
    "text": "Adapted from https://matplotlib.org/stable/gallery/specialty_plots/anscombe.html\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify(columns=2)\n\n\nimport matplotlib.pyplot as plt\n\n# Define the train/test split percentages\ntrain_percentage = 0.7\ntest_percentage = 1 - train_percentage\n\n# Create a rectangular plot to represent the train/test split\ntrain_rectangle = plt.Rectangle((0, 0), train_percentage, 1, fill=True, color='lightgreen', label='Train Set')\ntest_rectangle = plt.Rectangle((train_percentage, 0), test_percentage, 1, fill=True, color='lightcoral', label='Test Set')\n\n# Add rectangles to the plot\nplt.gca().add_patch(train_rectangle)\nplt.gca().add_patch(test_rectangle)\n\n# Set labels and legend\nplt.xlabel('Data Split')\nplt.ylabel('Data Points')\nplt.title('Train/Test Split Illustration')\nplt.legend()\n\n# Remove x and y ticks\nplt.xticks([])\nplt.yticks([])\n\n\n\n\n\n\n\n\n\nx = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\ny1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\ny2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\ny3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\nx4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\ny4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n\ndatasets = {\n    'I': (x, y1),\n    'II': (x, y2),\n    'III': (x, y3),\n    'IV': (x4, y4)\n}\n\nfig, axs = plt.subplots(2, 2, sharex=True, sharey=True,\n                        gridspec_kw={'wspace': 0.08, 'hspace': 0.08})\naxs[0, 0].set(xlim=(0, 20), ylim=(2, 14))\naxs[0, 0].set(xticks=(0, 10, 20), yticks=(4, 8, 12))\n\nfor ax, (label, (x, y)) in zip(axs.flat, datasets.items()):\n    ax.text(0.1, 0.9, label, fontsize=20, transform=ax.transAxes, va='top')\n    ax.tick_params(direction='in', top=True, right=True)\n    ax.plot(x, y, 'o')\n\n    # linear regression\n    p1, p0 = np.polyfit(x, y, deg=1)  # slope, intercept\n    ax.axline(xy1=(0, p0), slope=p1, color='r', lw=2)\n\n    # add text box for the statistics\n    stats = (f'$\\\\mu$ = {np.mean(y):.2f}\\n'\n             f'$\\\\sigma$ = {np.std(y):.2f}\\n'\n             f'$r$ = {np.corrcoef(x, y)[0][1]:.2f}')\n    bbox = dict(boxstyle='round', fc='blanchedalmond', ec='orange', alpha=0.5)\n    ax.text(0.95, 0.07, stats, fontsize=9, bbox=bbox,\n            transform=ax.transAxes, horizontalalignment='right')\n    #format_axes(ax)\n\nplt.savefig(\"../figures/anscombe.pdf\")"
  },
  {
    "objectID": "notebooks/bias-variance.html",
    "href": "notebooks/bias-variance.html",
    "title": "Bias Variance Tradeoff",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\nfrom latexify import latexify, format_axes\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\nimport pandas as pd\nimport ipywidgets as widgets\n\n\nlatexify(columns=2)\n\n\nx_overall = np.linspace(0, 10, 50)\nf_x = 0.2*np.sin(x_overall) + 0.2*np.cos(2*x_overall)+ 0.6*x_overall - 0.05*x_overall**2 - 0.003*x_overall**3\n\neps = np.random.normal(0, 1, 50)\ny_overall = f_x + eps\nplt.plot(x_overall, f_x, label = 'True function')\nplt.scatter(x_overall, y_overall, s=10, c='r', label = 'Noisy data')\nformat_axes(plt.gca())\nplt.legend()\n\n\n\n\n\n\n\n\n\ndef fit_plot_tree(x, y, depth=1, extra=None):\n    dt = DecisionTreeRegressor(max_depth=depth)\n    dt.fit(x.reshape(-1, 1), y)\n    y_pred = dt.predict(x.reshape(-1, 1))\n    plt.figure()\n\n    plt.plot(x_overall, f_x, label = r'$f_{true}$', lw=2)\n    plt.scatter(x_overall, y_overall, s=10, c='r', label = 'Noisy data')\n    label = r\"$\\hat{f}$\" if not extra else fr\"$\\hat{{f}}_{{{extra}}}$\"\n\n    plt.plot(x, y_pred, label = label, lw=2)\n\n    format_axes(plt.gca())\n    plt.legend()\n    plt.title(f\"Depth = {depth}\")\n    return dt\n\n\nfor i in range(1, 10):\n    fit_plot_tree(x_overall, y_overall, i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef fit_plot_polynomial(x, y, degree=1, extra=None, ax=None):\n    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n    model.fit(x.reshape(-1, 1), y)\n    y_pred = model.predict(x.reshape(-1, 1))\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    ax.plot(x_overall, f_x, label = r'$f_{true}$', lw=2)\n    ax.scatter(x_overall, y_overall, s=10, c='r', label = 'Noisy data')\n    label = r\"$\\hat{f}$\" if not extra else fr\"$\\hat{{f}}_{{{extra}}}$\"\n\n    ax.plot(x, y_pred, label = label, lw=2)\n\n    format_axes(ax)\n    ax.legend()\n    ax.set_title(f\"Degree = {degree}\")\n    return model\n\n\nfit_plot_polynomial(x_overall, y_overall, 5)\n\nPipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=5)),\n                ('linearregression', LinearRegression())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=5)),\n                ('linearregression', LinearRegression())])PolynomialFeaturesPolynomialFeatures(degree=5)LinearRegressionLinearRegression()\n\n\n\n\n\n\n\n\n\n\ndef plot_degree(degree=1):\n    regs = []\n    fig, axes = plt.subplots(5, 2, figsize=(8, 12), sharex=True, sharey=True)\n\n    for i, ax in enumerate(axes.flatten()):\n        idx = np.random.choice(np.arange(1, 49), 15, replace=False)\n        idx = np.concatenate([[0], idx, [49]])\n        idx.sort()\n        x = x_overall[idx]\n        y = y_overall[idx]\n        regs.append(fit_plot_polynomial(x, y, degree=degree, extra=i, ax=ax))\n        # remove legend\n        #ax.legend().remove()\n        ax.scatter(x_overall[idx], y_overall[idx], s=50, c='b', label='Sample', alpha=0.1)\n        ax.legend()\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return regs\n\n\n_ = plot_degree(5)\n\n\n\n\n\n\n\n\n\nregs = {}\nfor i in range(0, 10):\n    regs[i] = plot_degree(i)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef plot_predictions(reg):\n    x_test = np.linspace(0, 10, 50)\n    y_pred = np.zeros((10, 50))\n    for i in range(10):\n        y_pred[i] = reg[i].predict(x_test.reshape(-1, 1))\n    plt.plot(x_test, y_pred.mean(axis=0), label = r'$\\hat{f}$', lw=2)\n    plt.plot(x_test, f_x, label = r'$f_{true}$', lw=2)\n    plt.plot(x_test, y_pred.T, lw=1, c='k', alpha=0.5)  \n    format_axes(plt.gca())\n    plt.legend()\n\nplot_predictions(regs[1])\n\n\n\n\n\n\n\n\n\ndef plot_bias(reg):\n    x_test = np.linspace(0, 10, 50)\n    y_pred = np.zeros((10, 50))\n    for i in range(10):\n        y_pred[i] = reg[i].predict(x_test.reshape(-1, 1))\n    y_pred_mean = np.mean(y_pred, axis=0)\n    y_pred_var = np.var(y_pred, axis=0)\n\n    plt.plot(x_overall, f_x, label = r'$f_{true}$', lw=2)\n    #plt.scatter(x_overall, y_overall, s=10, c='r', label = 'Noisy data')\n    plt.plot(x_test, y_pred_mean, label = r'$\\bar{f}$', lw=2)\n    plt.fill_between(x_test, y_pred_mean, f_x, alpha=0.2, color='green', label = 'Bias')\n    plt.legend()\n\nplot_bias(regs[7])\n\n\n\n\n\n\n\n\n\ndef plot_variance(reg):\n    x_test = np.linspace(0, 10, 50)\n    y_pred = np.zeros((10, 50))\n    for i in range(10):\n        y_pred[i] = reg[i].predict(x_test.reshape(-1, 1))\n    y_pred_mean = np.mean(y_pred, axis=0)\n    y_pred_var = np.var(y_pred, axis=0)\n\n    plt.plot(x_overall, f_x, label = r'$f_{true}$', lw=2)\n    #plt.scatter(x_overall, y_overall, s=10, c='r', label = 'Noisy data')\n    plt.plot(x_test, y_pred_mean, label = r'$\\bar{f}$', lw=2)\n    plt.fill_between(x_test, y_pred_mean - y_pred_var, y_pred_mean + y_pred_var, alpha=0.2, color='red', label = 'Variance')\n    plt.legend()\n\n\nplot_variance(regs[8])\n\n\n\n\n\n\n\n\n\n# Plot bias^2 and variance for different depths as bar plot\n\ndef plot_bias_variance(reg):\n    x_test = np.linspace(0, 10, 50)\n    y_pred = np.zeros((10, 50))\n    for i in range(10):\n        y_pred[i] = reg[i].predict(x_test.reshape(-1, 1))\n    y_pred_mean = np.mean(y_pred, axis=0)\n    y_pred_var = np.var(y_pred, axis=0)\n\n    bias = (y_pred_mean - f_x)**2\n    var = y_pred_var\n    return bias.sum(), var.sum()\n\n\nbs = {}\nvs = {}\nfor i in range(1, 8):\n    bs[i], vs[i] = plot_bias_variance(regs[i])\n\n\ndf = pd.DataFrame({'Bias': bs, 'Variance': vs})\n\n\ndf.plot.bar(rot=0)"
  },
  {
    "objectID": "notebooks/object-detection-segmentation.html",
    "href": "notebooks/object-detection-segmentation.html",
    "title": "Object detection using YOLO and segmentation using Segment Anything",
    "section": "",
    "text": "References\n\nhttps://blog.roboflow.com/how-to-use-segment-anything-model-sam/\n\n\nimport torch\nimport torchvision\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.transforms import functional as F\nfrom PIL import Image\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n#config retina\n%config InlineBackend.figure_format = 'retina'\n\n\n# Load pre-trained Faster R-CNN model\nmodel = fasterrcnn_resnet50_fpn(pretrained=True)\n_ = model.eval()\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\n\n\nimg = Image.open('../datasets/images/office.jpg')\n\n\n# Transform the image\nimg_tensor = F.to_tensor(img)\nimg_tensor = img_tensor.unsqueeze(0)  # Add batch dimension\n\n\nfig = plt.figure(figsize=(6, 6))\nplt.imshow(img)\n_ = plt.axis('off')\n\n\n\n\n\n\n\n\n\n# Forward pass through the model\nwith torch.no_grad():\n    prediction = model(img_tensor)\n\n\nclass_names = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A',\n    'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\n\nthreshold = 0.5\n\n# Filter boxes based on confidence scores\nfiltered_boxes = prediction[0]['boxes'][prediction[0]['scores'] &gt; threshold]\nfiltered_scores = prediction[0]['scores'][prediction[0]['scores'] &gt; threshold]\nfiltered_labels = prediction[0]['labels'][prediction[0]['scores'] &gt; threshold]\n\n# Plot the image with bounding boxes and class names\nfig, ax = plt.subplots(1, figsize=(10, 10))\nax.imshow(img)\n\n# Add bounding boxes and labels to the plot\nfor i in range(filtered_boxes.size(0)):\n    box = filtered_boxes[i].cpu().numpy()\n    score = filtered_scores[i].item()\n    label = filtered_labels[i].item()\n\n    # Create a Rectangle patch\n    rect = patches.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='r', facecolor='none')\n\n    # Add the patch to the Axes\n    ax.add_patch(rect)\n\n    # Add class name and score as text\n    class_name = class_names[label]\n    ax.text(box[0], box[1], f'{class_name} ({score:.2f})', color='r', verticalalignment='top')\n\nplt.axis('off')  # Turn off axis labels\nplt.show()\n\n\n\n\n\n\n\n\n\n%pip install -q roboflow supervision\n\nNote: you may need to restart the kernel to use updated packages.\n\n\n\ntry:\n    from segment_anything import SamPredictor, sam_model_registry, SamAutomaticMaskGenerator\nexcept ImportError:\n    %pip install git+https://github.com/facebookresearch/segment-anything.git\n    pip install -q roboflow supervision\n    from segment_anything import SamPredictor, sam_model_registry\n\n\n# Place the model weights\nimport os\npth_path = os.path.expanduser('~/.cache/torch/sam.pth')\npth_url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n\nif not os.path.exists(pth_path):\n    import urllib.request\n    print(f'Downloading SAM weights to {pth_path}')\n    urllib.request.urlretrieve(pth_url, pth_path)\n\n\nDEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nMODEL_TYPE = \"vit_h\"\n\nsam = sam_model_registry[MODEL_TYPE](checkpoint=pth_path)\n_ = sam.to(device=DEVICE)\n\n\nimport cv2\nfrom segment_anything import SamAutomaticMaskGenerator\n\nmask_generator = SamAutomaticMaskGenerator(sam)\n\nimage_bgr = cv2.imread('../datasets/images/office.jpg')\nimage_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\nresult = mask_generator.generate(image_rgb)\n\n\nimport supervision as sv\n\nmask_annotator = sv.MaskAnnotator(color_lookup = sv.ColorLookup.INDEX)\ndetections = sv.Detections.from_sam(result)\nannotated_image = mask_annotator.annotate(image_bgr, detections)\n\n\n# Blending the annotated image with the original image\nalpha = 0.5  # Adjust the alpha value as needed\nblended_image = cv2.addWeighted(image_bgr, 1 - alpha, annotated_image, alpha, 0)\n\n# Display the original image, annotated image, and the blended result\nfig, ax = plt.subplots(1, 3, figsize=(18, 6))\nax[0].imshow(img_tensor.squeeze(0).permute(1, 2, 0))\nax[0].set_title('Original Image')\nax[1].imshow(annotated_image)\nax[1].set_title('Annotated Image')\nax[2].imshow(blended_image)\nax[2].set_title('Blended Result')\n\nfor a in ax:\n    a.axis('off')"
  },
  {
    "objectID": "notebooks/cost-iteration-notebook.html",
    "href": "notebooks/cost-iteration-notebook.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n/home/nipunbatra-pc/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n  return f(*args, **kwds)\n\n\n\nX = np.array([1, 2, 3])\ny = np.array([1, 2, 3])\n\n\ndef y_hat(X, theta_0, theta_1):\n    return theta_0 + theta_1*X\n\n\ndef cost(X, y, theta_0, theta_1):\n    yh = y_hat(X, theta_0, theta_1)\n    return (y-yh).T@(y-yh)\n\n\ntheta_0 = 4\ntheta_1 = 0\nalpha = 0.1\ncosts = np.zeros(1000)\ntheta_0_list = np.zeros(1000)\ntheta_1_list = np.zeros(1000)\n\nfor i in range(1000):\n    costs[i] = cost(X, y, theta_0, theta_1)\n    theta_0 = theta_0 - 2*alpha*((y_hat(X, theta_0, theta_1)-y).mean())\n    theta_1 = theta_1 - 2*alpha*((y_hat(X, theta_0, theta_1)-y).T@X)/len(X)\n    theta_0_list[i] = theta_0\n    theta_1_list[i] = theta_1\n\n\nimport sys\nsys.path.append(\"../\")\nfrom latexify import *\n\n\nlatexify()\nplt.plot(costs[:200], 'k')\nformat_axes(plt.gca())\nplt.ylabel(\"Cost\")\nplt.xlabel(\"Iteration\")\nplt.savefig(\"../gradient-descent/gd-iter-cost.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\n\nfor i in range(0, 200, 20):\n    plt.title(label=\"Fit at iteration {}\".format(i))\n    plt.plot(X, theta_0_list[i]+theta_1_list[i]*X, color='k')\n    plt.scatter(X, y, color='k')\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    format_axes(plt.gca())\n    plt.savefig(\"../gradient-descent/fit-iteration-{}.pdf\".format(i), bbox_inches=\"tight\", transparent=True)\n    plt.cla()\n\n\n\n\n\n\n\n\n\ntheta_0 = 4\ntheta_1 = 0\n(y_hat(X, theta_0, theta_1)-y).mean()\n\n2.0\n\n\n\n(y-y_hat(X, theta_0, theta_1)).mean()\n\n-2.0\n\n\n\n(y-y_hat(X, theta_0, theta_1))@X\n\n-10"
  },
  {
    "objectID": "notebooks/curse-dimensionality.html",
    "href": "notebooks/curse-dimensionality.html",
    "title": "Curse of Dimensionality",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\n# \nn = 10\nnp.random.seed(42)\n# Get `n` points in 1d space uniformly distributed from 0 to 1\nx = np.random.uniform(0, 1, n)\nplt.scatter(x, np.zeros(n), c='r', s=100)\n\n\n\n\n\n\n\n\n\n# Pick a random test point\n\nx_test = np.random.uniform(0, 1, 1)\n\n\n# Mark the nearest point and farthest point\nx_nearest = x[np.argmin(np.abs(x - x_test))]\nx_farthest = x[np.argmax(np.abs(x - x_test))]\n\nplt.scatter(x, np.zeros(n), c='r', s=100)\nplt.scatter(x_test, 0, c='b', s=100, marker='*', label='test')\nplt.scatter(x_nearest, 0, c='g', s=100, label='nearest')\nplt.scatter(x_farthest, 0, c='y', s=100, label='farthest')\nplt.legend()\n\nratio = np.abs(x_test - x_nearest) / np.abs(x_test - x_farthest)\nprint('Ratio of distances: {}'.format(ratio))\n\nRatio of distances: [0.04356313]\n\n\n\n\n\n\n\n\n\n\n# Do the above experiment for 1000 times\n\nn = 10\nnp.random.seed(42)\n\nn_exp = 1000\nratios = np.zeros(n_exp)\nfor i in range(n_exp):\n    x = np.random.uniform(0, 1, n)\n    x_test = np.random.uniform(0, 1, 1)\n    x_nearest = x[np.argmin(np.abs(x - x_test))]\n    x_farthest = x[np.argmax(np.abs(x - x_test))]\n    ratios[i] = np.abs(x_test - x_nearest) / np.abs(x_test - x_farthest)\n\n\nimport seaborn as sns\nsns.displot(ratios, kde=False, bins=20)\n\n\n\n\n\n\n\n\n\n# Repeat the experiment in 2d\n\nn = 10\nnp.random.seed(42)\nx = np.random.uniform(0, 1, (n, 2))\nplt.scatter(x[:, 0], x[:, 1], c='r', s=100)\n\n\n\n\n\n\n\n\n\n# Pick a random test point\n\nx_test = np.random.uniform(0, 1, 2)\n\n# Mark the nearest point and farthest point\nx_nearest = x[np.argmin(np.linalg.norm(x - x_test, axis=1))]\nx_farthest = x[np.argmax(np.linalg.norm(x - x_test, axis=1))]\n\nplt.scatter(x[:, 0], x[:, 1], c='r', s=100)\nplt.scatter(x_test[0], x_test[1], c='b', s=100, marker='*', label='test')\nplt.scatter(x_nearest[0], x_nearest[1], c='g', s=100, label='nearest')\nplt.scatter(x_farthest[0], x_farthest[1], c='y', s=100, label='farthest')\nplt.legend()\n\n\n\n\n\n\n\n\n\n# Find the ratio of distances between the nearest and farthest points in 1000 experiments\n\nn = 10\nnp.random.seed(42)\n\nratios_2d = np.zeros(n_exp)\nfor i in range(n_exp):\n    x = np.random.uniform(0, 1, (n, 2))\n    x_test = np.random.uniform(0, 1, 2)\n    x_nearest = x[np.argmin(np.linalg.norm(x - x_test, axis=1))]\n    x_farthest = x[np.argmax(np.linalg.norm(x - x_test, axis=1))]\n    ratios_2d[i] = np.linalg.norm(x_test - x_nearest) / np.linalg.norm(x_test - x_farthest)\n\n\nsns.displot(ratios_2d, kde=False, bins=20)\n\n\n\n\n\n\n\n\n\n# Now, let's do the same experiment in dimensions varying from 1 to 20\n\nn = 10\nnp.random.seed(42)\nn_dim = 101\nratios_nd = np.zeros((n_exp, n_dim))\nfor i in range(n_exp):\n    for d in range(1, n_dim + 1):\n        x = np.random.uniform(0, 1, (n, d))\n        x_test = np.random.uniform(0, 1, d)\n        x_nearest = x[np.argmin(np.linalg.norm(x - x_test, axis=1))]\n        x_farthest = x[np.argmax(np.linalg.norm(x - x_test, axis=1))]\n        ratios_nd[i, d - 1] = np.linalg.norm(x_test - x_nearest) / np.linalg.norm(x_test - x_farthest)\n\n\n# Plot the ratio of distances between the nearest and farthest points in 1000 experiments for each dimension\n\nfrom matplotlib import markers\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(np.arange(1, n_dim + 1), np.mean(ratios_nd, axis=0), 'o-', markersize=3)\nplt.xlabel('Dimension')\nplt.ylabel('Ratio of distances')\nplt.title('Ratio of distances between the nearest and farthest points in 1000 experiments for each dimension')\nplt.ylim(0, 1.)\n\n\n\n\n\n\n\n\n\n# Let us now see what happens if we have more points in higher dimensions\n# 1d space: 10 points\n\nn = 10\nnp.random.seed(42)\nn_dim = 4\nratios_nd_more_points = np.zeros((n_exp, n_dim))\nnum_points = [10, 50, 200, 2000]\nfor i in range(n_exp):\n    for d in range(1, n_dim + 1):\n        x = np.random.uniform(0, 1, (num_points[d-1], d))\n        x_test = np.random.uniform(0, 1, d)\n        x_nearest = x[np.argmin(np.linalg.norm(x - x_test, axis=1))]\n        x_farthest = x[np.argmax(np.linalg.norm(x - x_test, axis=1))]\n        ratios_nd_more_points[i, d - 1] = np.linalg.norm(x_test - x_nearest) / np.linalg.norm(x_test - x_farthest)\n\n\nimport pandas as pd\ncols = [(1, 10), (2, 50), (3, 200), (4, 2000)]\ndf = pd.DataFrame(ratios_nd_more_points, columns=cols)\ndf\n\n\n\n\n\n\n\n\n(1, 10)\n(2, 50)\n(3, 200)\n(4, 2000)\n\n\n\n\n0\n0.040316\n0.040880\n0.081673\n0.055671\n\n\n1\n0.056051\n0.094828\n0.139413\n0.063989\n\n\n2\n0.098272\n0.076760\n0.163102\n0.078649\n\n\n3\n0.213181\n0.165776\n0.055523\n0.033316\n\n\n4\n0.011092\n0.105754\n0.070214\n0.081415\n\n\n...\n...\n...\n...\n...\n\n\n995\n0.005806\n0.108972\n0.075787\n0.104469\n\n\n996\n0.178349\n0.056938\n0.067411\n0.047147\n\n\n997\n0.006966\n0.070710\n0.061830\n0.095790\n\n\n998\n0.047800\n0.082439\n0.095661\n0.049558\n\n\n999\n0.080716\n0.042173\n0.033175\n0.082464\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\ndf.mean().plot(kind='bar', rot=0)\nplt.ylabel('Ratio of distances')\nplt.xlabel('Dimension, number of points')\n_= plt.title('Ratio of distances between the nearest and farthest points \\nin 1000 experiments for each dimension and number of points')\nplt.ylim(0, 1.)\n\n\n\n\n\n\n\n\n\n# Now showing how linear regression is affected by curse of dimensionality\n\nn_points = 20\n\nx = np.random.uniform(0, 1, (n_points, 1))\n# sort the x values\nx = np.sort(x, axis=0)\ny = 4 * x + 3 + np.random.normal(0, 0.5, (n_points, 1))\n\nplt.scatter(x, y)\n\n\n\n\n\n\n\n\n\n# Let us fit a linear regression model of degree d\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef fit_linear_regression(x, y, d):\n    poly = PolynomialFeatures(degree=d)\n    x = poly.fit_transform(x)\n    model = LinearRegression()\n    model.fit(x, y)\n    return model\n\ndef plot_linear_regression(x, y, d):\n    model = fit_linear_regression(x, y, d)\n    y_pred = model.predict(PolynomialFeatures(degree=d).fit_transform(x))\n    plt.scatter(x, y)\n    plt.plot(x, y_pred, c='r')\n    plt.show()\n\n\nplot_linear_regression(x, y, 1)\n\n\n\n\n\n\n\n\n\nplot_linear_regression(x, y, 5)\n\n\n\n\n\n\n\n\n\nplot_linear_regression(x, y, 10)\n\n\n\n\n\n\n\n\n\nplot_linear_regression(x, y, 15)\n\n\n\n\n\n\n\n\n\n# Now, we see that if we increase the number of points, the model will fit better\n\nn_points = 1000\n\nx = np.random.uniform(0, 1, (n_points, 1))\n# sort the x values\nx = np.sort(x, axis=0)\ny = 4 * x + 3 + np.random.normal(0, 0.5, (n_points, 1))\n\nplot_linear_regression(x, y, 1)\n\n\n\n\n\n\n\n\n\nplot_linear_regression(x, y, 5)\n\n\n\n\n\n\n\n\n\nplot_linear_regression(x, y, 25)"
  },
  {
    "objectID": "notebooks/entropy.html",
    "href": "notebooks/entropy.html",
    "title": "Decision Trees Entropy",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom latexify import latexify, format_axes\nfrom scipy.special import xlogy\n\n# Function to calculate entropy\ndef entropy(p):\n    return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n# Generate data\nx_values = np.linspace(0.000, 1.0, 100)  # Avoid log(0) in the calculation\ny_values = entropy(x_values)\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73626/845472961.py:6: RuntimeWarning: divide by zero encountered in log2\n  return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73626/845472961.py:6: RuntimeWarning: invalid value encountered in multiply\n  return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n\n\n\ny_values\n\narray([       nan, 0.08146203, 0.14257333, 0.19590927, 0.24414164,\n       0.28853851, 0.32984607, 0.36855678, 0.40502013, 0.43949699,\n       0.47218938, 0.50325833, 0.53283506, 0.56102849, 0.58793037,\n       0.61361902, 0.63816195, 0.66161791, 0.68403844, 0.70546904,\n       0.72595015, 0.74551784, 0.76420451, 0.78203929, 0.79904852,\n       0.81525608, 0.83068364, 0.84535094, 0.85927598, 0.87247521,\n       0.88496364, 0.89675502, 0.90786192, 0.91829583, 0.92806728,\n       0.93718586, 0.9456603 , 0.95349858, 0.9607079 , 0.96729478,\n       0.97326507, 0.97862399, 0.98337619, 0.98752571, 0.99107606,\n       0.99403021, 0.99639062, 0.99815923, 0.9993375 , 0.9999264 ,\n       0.9999264 , 0.9993375 , 0.99815923, 0.99639062, 0.99403021,\n       0.99107606, 0.98752571, 0.98337619, 0.97862399, 0.97326507,\n       0.96729478, 0.9607079 , 0.95349858, 0.9456603 , 0.93718586,\n       0.92806728, 0.91829583, 0.90786192, 0.89675502, 0.88496364,\n       0.87247521, 0.85927598, 0.84535094, 0.83068364, 0.81525608,\n       0.79904852, 0.78203929, 0.76420451, 0.74551784, 0.72595015,\n       0.70546904, 0.68403844, 0.66161791, 0.63816195, 0.61361902,\n       0.58793037, 0.56102849, 0.53283506, 0.50325833, 0.47218938,\n       0.43949699, 0.40502013, 0.36855678, 0.32984607, 0.28853851,\n       0.24414164, 0.19590927, 0.14257333, 0.08146203,        nan])\n\n\n\n# Replace NaN values with 0\ny_values = np.nan_to_num(y_values, nan=0.0)\n\n\nlatexify(columns=2)\n\n\nplt.plot(x_values, y_values, color='black')\n\n# Set labels and title\nplt.xlabel('$P(+)$')\nplt.ylabel('Entropy')\nplt.title('Entropy vs. $P(+)$')\nformat_axes(plt.gca())\nplt.savefig(\"../figures/decision-trees/entropy.pdf\", bbox_inches=\"tight\")\n\n\n\n\n\n\n\n\n\n# Function to calculate entropy with numerical stability\ndef entropy_numerically_stable(p):\n    return (-xlogy(p, p) - xlogy(1 - p, 1 - p))/np.log(2)\n\ny_values = entropy_numerically_stable(x_values)\n\n\nplt.plot(x_values, y_values)\n\n\n\n\n\n\n\n\nHow does xlogy handle the corner case?\n\nxlogy??\n\nCall signature:  xlogy(*args, **kwargs)\nType:            ufunc\nString form:     &lt;ufunc 'xlogy'&gt;\nFile:            ~/miniconda3/lib/python3.9/site-packages/numpy/__init__.py\nDocstring:      \nxlogy(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nxlogy(x, y, out=None)\n\nCompute ``x*log(y)`` so that the result is 0 if ``x = 0``.\n\nParameters\n----------\nx : array_like\n    Multiplier\ny : array_like\n    Argument\nout : ndarray, optional\n    Optional output array for the function results\n\nReturns\n-------\nz : scalar or ndarray\n    Computed x*log(y)\n\nNotes\n-----\nThe log function used in the computation is the natural log.\n\n.. versionadded:: 0.13.0\n\nExamples\n--------\nWe can use this function to calculate the binary logistic loss also\nknown as the binary cross entropy. This loss function is used for\nbinary classification problems and is defined as:\n\n.. math::\n    L = 1/n * \\sum_{i=0}^n -(y_i*log(y\\_pred_i) + (1-y_i)*log(1-y\\_pred_i))\n\nWe can define the parameters `x` and `y` as y and y_pred respectively.\ny is the array of the actual labels which over here can be either 0 or 1.\ny_pred is the array of the predicted probabilities with respect to\nthe positive class (1).\n\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.special import xlogy\n&gt;&gt;&gt; y = np.array([0, 1, 0, 1, 1, 0])\n&gt;&gt;&gt; y_pred = np.array([0.3, 0.8, 0.4, 0.7, 0.9, 0.2])\n&gt;&gt;&gt; n = len(y)\n&gt;&gt;&gt; loss = -(xlogy(y, y_pred) + xlogy(1 - y, 1 - y_pred)).sum()\n&gt;&gt;&gt; loss /= n\n&gt;&gt;&gt; loss\n0.29597052165495025\n\nA lower loss is usually better as it indicates that the predictions are\nsimilar to the actual labels. In this example since our predicted\nprobabilties are close to the actual labels, we get an overall loss\nthat is reasonably low and appropriate.\nClass docstring:\nFunctions that operate element by element on whole arrays.\n\nTo see the documentation for a specific ufunc, use `info`.  For\nexample, ``np.info(np.sin)``.  Because ufuncs are written in C\n(for speed) and linked into Python with NumPy's ufunc facility,\nPython's help() function finds this page whenever help() is called\non a ufunc.\n\nA detailed explanation of ufuncs can be found in the docs for :ref:`ufuncs`.\n\n**Calling ufuncs:** ``op(*x[, out], where=True, **kwargs)``\n\nApply `op` to the arguments `*x` elementwise, broadcasting the arguments.\n\nThe broadcasting rules are:\n\n* Dimensions of length 1 may be prepended to either array.\n* Arrays may be repeated along dimensions of length 1.\n\nParameters\n----------\n*x : array_like\n    Input arrays.\nout : ndarray, None, or tuple of ndarray and None, optional\n    Alternate array object(s) in which to put the result; if provided, it\n    must have a shape that the inputs broadcast to. A tuple of arrays\n    (possible only as a keyword argument) must have length equal to the\n    number of outputs; use None for uninitialized outputs to be\n    allocated by the ufunc.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the :ref:`ufunc docs &lt;ufuncs.kwargs&gt;`.\n\nReturns\n-------\nr : ndarray or tuple of ndarray\n    `r` will have the shape that the arrays in `x` broadcast to; if `out` is\n    provided, it will be returned. If not, `r` will be allocated and\n    may contain uninitialized values. If the function has more than one\n    output, then the result will be a tuple of arrays."
  },
  {
    "objectID": "notebooks/meshgrid.html",
    "href": "notebooks/meshgrid.html",
    "title": "Meshgrid",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n# from latexify import latexify\n# latexify(columns = 2)\n%matplotlib inline\n%config InlineBackend.figure_format = \"retina\"\n\n\nimport ipywidgets as widgets\nfrom ipywidgets import interactive\n\n\nfrom sklearn.datasets import make_moons\n\nX, y = make_moons(n_samples = 1000, noise = 0.1, random_state = 0)\n\nplt.figure(figsize = (8, 6))\nplt.scatter(X[:, 0], X[:, 1], c = y, s = 40, cmap = plt.cm.Spectral);\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100, random_state = 0)\nrf.fit(X, y)\n\nRandomForestClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(random_state=0) \n\n\n\n# Decision surface\nplt.figure(figsize = (8, 6))\nplt.scatter(X[:, 0], X[:, 1], c = y, s = 40, cmap = plt.cm.viridis)\nax = plt.gca()\nxlim = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\nylim = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n\n# Create grid to evaluate model\nx_lin = np.linspace(xlim[0], xlim[1], 30)\ny_lin = np.linspace(ylim[0], ylim[1], 30)\n\nXX, YY = np.meshgrid(x_lin, y_lin)\nxy = np.vstack([XX.ravel(), YY.ravel()]).T\nZ = rf.predict_proba(xy)[:, 1].reshape(XX.shape)\n\n# Plot decision boundary\nax.contourf(XX, YY, Z, cmap = plt.cm.viridis, alpha = 0.2)\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\nX_arr = np.array([1, 2, 3, 4])\nY_arr = np.array([5, 6, 7])\n\nXX, YY = np.meshgrid(X_arr, Y_arr)\n\n\nXX\n\narray([[1, 2, 3, 4],\n       [1, 2, 3, 4],\n       [1, 2, 3, 4]])\n\n\n\nYY\n\narray([[5, 5, 5, 5],\n       [6, 6, 6, 6],\n       [7, 7, 7, 7]])\n\n\n\nXX.shape, YY.shape\n\n((3, 4), (3, 4))\n\n\n\nout = {}\ncount = 0\nfor i in range(XX.shape[0]):\n    for j in range(XX.shape[1]):\n        count = count + 1\n        out[count] = {\"i\": i, \"j\": j, \"XX\": XX[i, j], \"YY\": YY[i, j]}\n\n\npd.DataFrame(out).T\n\n\n\n\n\n\n\n\ni\nj\nXX\nYY\n\n\n\n\n1\n0\n0\n1\n5\n\n\n2\n0\n1\n2\n5\n\n\n3\n0\n2\n3\n5\n\n\n4\n0\n3\n4\n5\n\n\n5\n1\n0\n1\n6\n\n\n6\n1\n1\n2\n6\n\n\n7\n1\n2\n3\n6\n\n\n8\n1\n3\n4\n6\n\n\n9\n2\n0\n1\n7\n\n\n10\n2\n1\n2\n7\n\n\n11\n2\n2\n3\n7\n\n\n12\n2\n3\n4\n7\n\n\n\n\n\n\n\n\nXX[0], YY[0]\n\n(array([1, 2, 3, 4]), array([5, 5, 5, 5]))\n\n\n\nplt.figure(figsize = (6, 4))\nfor i in range(XX.shape[0]):\n    plt.plot(XX[i], YY[i], 'o', label=i)\nplt.legend();\n\n\n\n\n\n\n\n\n\nplt.figure(figsize = (6, 4))\nplt.plot(XX, YY, 'o')\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s see some Plots and their Contours\n\n\\[ \\color{red}{F(x, y) = \\sin(x)\\cos(y)}, -\\frac{3\\pi}{2} \\le x \\le \\frac{3\\pi}{2}, -\\pi \\le y \\le \\pi \\]\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nX1 = np.linspace(-3/2 * np.pi, 3/2 * np.pi, 100)\nY1 = np.linspace(-np.pi, np.pi, 100)\nXX1, YY1 = np.meshgrid(X1, Y1)\nF = np.sin(XX1) * np.cos(YY1)\n\n\nplt.figure(figsize=(6, 3))\nplt.contourf(XX1, YY1, F, cmap = \"viridis\")\nplt.colorbar()\nplt.title(r\"Contour Plot of $F(x, y) = \\sin(x)\\cos(y)$\", fontsize = 14)\nplt.show()\n\n\nfig = plt.figure(figsize=(8, 4))\nax4 = fig.add_subplot(111, projection = \"3d\")\nax4.plot_surface(XX1, YY1, F, cmap = \"viridis\")\nax4.set_title(r\"Graph of $F(x, y) = \\sin(x)\\cos(y)$\", fontsize = 14)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxlim = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\nylim = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n\n# Create grid to evaluate model\nx_lin = np.linspace(xlim[0], xlim[1], 30)\ny_lin = np.linspace(ylim[0], ylim[1], 30)\n\n\nx_lin\n\narray([-1.67150293, -1.52070662, -1.36991031, -1.219114  , -1.06831769,\n       -0.91752137, -0.76672506, -0.61592875, -0.46513244, -0.31433613,\n       -0.16353982, -0.01274351,  0.1380528 ,  0.28884911,  0.43964542,\n        0.59044173,  0.74123804,  0.89203435,  1.04283066,  1.19362697,\n        1.34442328,  1.49521959,  1.6460159 ,  1.79681221,  1.94760852,\n        2.09840483,  2.24920114,  2.39999745,  2.55079376,  2.70159007])\n\n\n\nXX, YY = np.meshgrid(x_lin, y_lin)\n\n\ndef update_plot(i = 0, j = 2):\n    x_point = XX[i, j]\n    y_point = YY[i, j]\n\n    plt.figure(figsize = (8, 6))\n    plt.plot(XX, YY, 'o', alpha = 0.1, color = 'k')\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.viridis)\n\n    pred = rf.predict_proba([[x_point, y_point]])[:, 1]\n\n    plt.scatter(x_point, y_point, s = 100, c = \"r\")\n    plt.title(f\"Prediction P(Class 1): {pred[0]:.2f}\")\n    plt.show()\n\nupdate_plot(0, 0)\n\n\n\n\n\n\n\n\n\nwidget = interactive(update_plot, i = (0, XX.shape[0] - 1), j = (0, XX.shape[1] - 1))\ndisplay(widget)\n\n\n\n\n\nXX[0], YY[:, 0]\n\n(array([-1.67150293, -1.52070662, -1.36991031, -1.219114  , -1.06831769,\n        -0.91752137, -0.76672506, -0.61592875, -0.46513244, -0.31433613,\n        -0.16353982, -0.01274351,  0.1380528 ,  0.28884911,  0.43964542,\n         0.59044173,  0.74123804,  0.89203435,  1.04283066,  1.19362697,\n         1.34442328,  1.49521959,  1.6460159 ,  1.79681221,  1.94760852,\n         2.09840483,  2.24920114,  2.39999745,  2.55079376,  2.70159007]),\n array([-1.23673767, -1.13313159, -1.02952551, -0.92591943, -0.82231335,\n        -0.71870727, -0.61510119, -0.51149511, -0.40788903, -0.30428295,\n        -0.20067687, -0.09707079,  0.00653529,  0.11014137,  0.21374745,\n         0.31735353,  0.42095961,  0.52456569,  0.62817177,  0.73177785,\n         0.83538393,  0.93899001,  1.04259609,  1.14620217,  1.24980825,\n         1.35341433,  1.45702041,  1.56062649,  1.66423257,  1.76783865]))\n\n\n\nXX.shape\n\n(30, 30)\n\n\n\nfrom einops import rearrange, repeat, reduce\n\nUsually when dealing with images, the shapes are of the form \\((B \\times C \\times H \\times W)\\) where  \\(B\\): Batch Size  \\(C\\): Number of Image Channels  \\(H\\): Image Height  \\(W\\): Image Width  Inorder to flatten the image data into a linear array \\((B \\times C \\times H \\times W) \\to (B \\times C \\cdot H \\cdot W)\\), we may use  \\[ \\texttt{einops.rearrange(img, \"b c h w -&gt; b (c h w)\")} \\]\n\nXX.shape\n\n(30, 30)\n\n\n\nXX.ravel().shape\n\n(900,)\n\n\n\nrearrange(XX, 'i j -&gt; (i j) 1').shape, rearrange(XX, 'i j -&gt; (i j)').shape\n\n((900, 1), (900,))\n\n\n\nrearrange(YY, 'i j -&gt; (i j) 1').shape\n\n(900, 1)\n\n\n\nXX_flat = rearrange(XX, 'i j -&gt; (i j) 1')\nYY_flat = rearrange(YY, 'i j -&gt; (i j) 1')\n\n\nnp.array([XX_flat, YY_flat]).shape\n\n(2, 900, 1)\n\n\n\nrearrange([XX_flat, YY_flat], 'f n 1 -&gt; n f').shape\n\n(900, 2)\n\n\n\nX_feature = rearrange([XX_flat, YY_flat], 'f n 1 -&gt; n f')\n\n\nX_feature[:32]\n\narray([[-1.67150293, -1.23673767],\n       [-1.52070662, -1.23673767],\n       [-1.36991031, -1.23673767],\n       [-1.219114  , -1.23673767],\n       [-1.06831769, -1.23673767],\n       [-0.91752137, -1.23673767],\n       [-0.76672506, -1.23673767],\n       [-0.61592875, -1.23673767],\n       [-0.46513244, -1.23673767],\n       [-0.31433613, -1.23673767],\n       [-0.16353982, -1.23673767],\n       [-0.01274351, -1.23673767],\n       [ 0.1380528 , -1.23673767],\n       [ 0.28884911, -1.23673767],\n       [ 0.43964542, -1.23673767],\n       [ 0.59044173, -1.23673767],\n       [ 0.74123804, -1.23673767],\n       [ 0.89203435, -1.23673767],\n       [ 1.04283066, -1.23673767],\n       [ 1.19362697, -1.23673767],\n       [ 1.34442328, -1.23673767],\n       [ 1.49521959, -1.23673767],\n       [ 1.6460159 , -1.23673767],\n       [ 1.79681221, -1.23673767],\n       [ 1.94760852, -1.23673767],\n       [ 2.09840483, -1.23673767],\n       [ 2.24920114, -1.23673767],\n       [ 2.39999745, -1.23673767],\n       [ 2.55079376, -1.23673767],\n       [ 2.70159007, -1.23673767],\n       [-1.67150293, -1.13313159],\n       [-1.52070662, -1.13313159]])\n\n\n\nZ = rf.predict_proba(X_feature)[:, 1]\n\n\nZ.shape\n\n(900,)\n\n\n\nplt.figure(figsize = (8, 6))\nplt.scatter(XX_flat, YY_flat, c = Z, cmap = plt.cm.viridis)\nplt.show()\n\n\n\n\n\n\n\n\n\nZ[:10]\n\narray([0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.19, 0.36, 0.73])\n\n\n\n# Divide Z into k levels\nk = 10\nmin_Z = Z.min()\nmax_Z = Z.max()\n\nlevels = np.linspace(min_Z, max_Z, k)\n\nlevels\n\narray([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])\n\n\n\n# Create an image from Z \nimg = rearrange(Z, '(h w) -&gt; h w', h=XX.shape[0])\nplt.figure(figsize = (8, 6))\nplt.imshow(img, cmap=plt.cm.viridis, \n           extent=[XX.min(), XX.max(), YY.min(), YY.max()], \n           origin='lower',\n           interpolation='spline36')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize = (8, 6))\nplt.contourf(XX, YY, Z.reshape(XX.shape), cmap=plt.cm.viridis, levels=10);\nplt.colorbar()\nplt.show()"
  },
  {
    "objectID": "notebooks/siren.html",
    "href": "notebooks/siren.html",
    "title": "Sirens v/s Linear Regression",
    "section": "",
    "text": "import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Remove all the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set env CUDA_LAUNCH_BLOCKING=1\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Retina display\n%config InlineBackend.figure_format = 'retina'\n\ntry:\n    from einops import rearrange\nexcept ImportError:\n    %pip install einops\n    from einops import rearrange\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 1\n----&gt; 1 import torch\n      2 import torchvision\n      3 import torchvision.transforms as transforms\n\nModuleNotFoundError: No module named 'torch'\n\n\n\n\nif os.path.exists('dog.jpg'):\n    print('dog.jpg exists')\nelse:\n    !wget https://segment-anything.com/assets/gallery/AdobeStock_94274587_welsh_corgi_pembroke_CD.jpg -O dog.jpg\n\ndog.jpg exists\n\n\n\n# Read in a image from torchvision\nimg = torchvision.io.read_image(\"dog.jpg\")\nprint(img.shape)\n\ntorch.Size([3, 1365, 2048])\n\n\n\nplt.imshow(rearrange(img, 'c h w -&gt; h w c').numpy())\n\n\n\n\n\n\n\n\n\nfrom sklearn import preprocessing\n\nscaler_img = preprocessing.MinMaxScaler().fit(img.reshape(-1, 1))\nscaler_img\n\nMinMaxScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MinMaxScalerMinMaxScaler()\n\n\n\nimg_scaled = scaler_img.transform(img.reshape(-1, 1)).reshape(img.shape)\nimg_scaled.shape\n\nimg_scaled = torch.tensor(img_scaled)\n\n\nimg_scaled = img_scaled.to(device)\nimg_scaled\n\ntensor([[[0.3098, 0.3137, 0.3137,  ..., 0.2941, 0.2941, 0.2980],\n         [0.3098, 0.3137, 0.3137,  ..., 0.2941, 0.2941, 0.2980],\n         [0.3098, 0.3137, 0.3137,  ..., 0.2941, 0.2941, 0.2980],\n         ...,\n         [0.4745, 0.4745, 0.4784,  ..., 0.3804, 0.3765, 0.3765],\n         [0.4745, 0.4745, 0.4784,  ..., 0.3804, 0.3804, 0.3765],\n         [0.4745, 0.4745, 0.4784,  ..., 0.3843, 0.3804, 0.3804]],\n\n        [[0.2039, 0.2078, 0.2078,  ..., 0.2157, 0.2157, 0.2118],\n         [0.2039, 0.2078, 0.2078,  ..., 0.2157, 0.2157, 0.2118],\n         [0.2039, 0.2078, 0.2078,  ..., 0.2157, 0.2157, 0.2118],\n         ...,\n         [0.4039, 0.4039, 0.4078,  ..., 0.3216, 0.3176, 0.3176],\n         [0.4039, 0.4039, 0.4078,  ..., 0.3216, 0.3216, 0.3176],\n         [0.4039, 0.4039, 0.4078,  ..., 0.3255, 0.3216, 0.3216]],\n\n        [[0.1373, 0.1412, 0.1412,  ..., 0.1176, 0.1176, 0.1176],\n         [0.1373, 0.1412, 0.1412,  ..., 0.1176, 0.1176, 0.1176],\n         [0.1373, 0.1412, 0.1412,  ..., 0.1176, 0.1176, 0.1176],\n         ...,\n         [0.1451, 0.1451, 0.1490,  ..., 0.1686, 0.1647, 0.1647],\n         [0.1451, 0.1451, 0.1490,  ..., 0.1686, 0.1686, 0.1647],\n         [0.1451, 0.1451, 0.1490,  ..., 0.1725, 0.1686, 0.1686]]],\n       device='cuda:0', dtype=torch.float64)\n\n\n\ncrop = torchvision.transforms.functional.crop(img_scaled.cpu(), 600, 800, 300, 300)\ncrop.shape\n\ntorch.Size([3, 300, 300])\n\n\n\nplt.imshow(rearrange(crop, 'c h w -&gt; h w c').cpu().numpy())\n\n\n\n\n\n\n\n\n\ncrop = crop.to(device)\n\n\n# Get the dimensions of the image tensor\nnum_channels, height, width = crop.shape\nprint(num_channels, height, width)\n\n3 300 300\n\n\n\nnum_channels, height, width = 2, 3, 4\n\n    \n# Create a 2D grid of (x,y) coordinates\nw_coords = torch.arange(width).repeat(height, 1)\nh_coords = torch.arange(height).repeat(width, 1).t()\nw_coords = w_coords.reshape(-1)\nh_coords = h_coords.reshape(-1)\n\n# Combine the x and y coordinates into a single tensor\nX = torch.stack([h_coords, w_coords], dim=1).float()\n\n\nX.shape\n\ntorch.Size([12, 2])\n\n\n\ndef create_coordinate_map(img):\n    \"\"\"\n    img: torch.Tensor of shape (num_channels, height, width)\n    \n    return: tuple of torch.Tensor of shape (height * width, 2) and torch.Tensor of shape (height * width, num_channels)\n    \"\"\"\n    \n    num_channels, height, width = img.shape\n    \n    # Create a 2D grid of (x,y) coordinates (h, w)\n    # width values change faster than height values\n    w_coords = torch.arange(width).repeat(height, 1)\n    h_coords = torch.arange(height).repeat(width, 1).t()\n    w_coords = w_coords.reshape(-1)\n    h_coords = h_coords.reshape(-1)\n\n    # Combine the x and y coordinates into a single tensor\n    X = torch.stack([h_coords, w_coords], dim=1).float()\n\n    # Move X to GPU if available\n    X = X.to(device)\n\n    # Reshape the image to (h * w, num_channels)\n    Y = rearrange(img, 'c h w -&gt; (h w) c').float()\n    return X, Y\n\n\ndog_X, dog_Y = create_coordinate_map(crop)\n\ndog_X.shape, dog_Y.shape\n\n(torch.Size([90000, 2]), torch.Size([90000, 3]))\n\n\n\n# MinMaxScaler from -1 to 1\nscaler_X = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(dog_X.cpu())\n\n# Scale the X coordinates\ndog_X_scaled = scaler_X.transform(dog_X.cpu())\n\n# Move the scaled X coordinates to the GPU\ndog_X_scaled = torch.tensor(dog_X_scaled).to(device)\n\n# Set to dtype float32\ndog_X_scaled = dog_X_scaled.float()\n\n\nclass LinearModel(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(LinearModel, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        \n    def forward(self, x):\n        return self.linear(x)\n    \n\n\nnet = LinearModel(2, 3)\nnet.to(device)\n\nLinearModel(\n  (linear): Linear(in_features=2, out_features=3, bias=True)\n)\n\n\n\ndef train(net, lr, X, Y, epochs, verbose=True):\n    \"\"\"\n    net: torch.nn.Module\n    lr: float\n    X: torch.Tensor of shape (num_samples, 2)\n    Y: torch.Tensor of shape (num_samples, 3)\n    \"\"\"\n\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        outputs = net(X)\n        \n        \n        loss = criterion(outputs, Y)\n        loss.backward()\n        optimizer.step()\n        if verbose and epoch % 100 == 0:\n            print(f\"Epoch {epoch} loss: {loss.item():.6f}\")\n    return loss.item()\n\n\ntrain(net, 0.01, dog_X_scaled, dog_Y, 1000)\n\nEpoch 0 loss: 0.504670\nEpoch 100 loss: 0.046783\nEpoch 200 loss: 0.036839\nEpoch 300 loss: 0.036823\nEpoch 400 loss: 0.036823\nEpoch 500 loss: 0.036823\nEpoch 600 loss: 0.036823\nEpoch 700 loss: 0.036823\nEpoch 800 loss: 0.036823\nEpoch 900 loss: 0.036823\n\n\n0.03682254999876022\n\n\n\ndef plot_reconstructed_and_original_image(original_img, net, X, title=\"\"):\n    \"\"\"\n    net: torch.nn.Module\n    X: torch.Tensor of shape (num_samples, 2)\n    Y: torch.Tensor of shape (num_samples, 3)\n    \"\"\"\n    num_channels, height, width = original_img.shape\n    net.eval()\n    with torch.no_grad():\n        outputs = net(X)\n        outputs = outputs.reshape(height, width, num_channels)\n        #outputs = outputs.permute(1, 2, 0)\n    fig = plt.figure(figsize=(6, 4))\n    gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1])\n\n    ax0 = plt.subplot(gs[0])\n    ax1 = plt.subplot(gs[1])\n\n    ax0.imshow(outputs.cpu())\n    ax0.set_title(\"Reconstructed Image\")\n    \n\n    ax1.imshow(original_img.cpu().permute(1, 2, 0))\n    ax1.set_title(\"Original Image\")\n    \n    for a in [ax0, ax1]:\n        a.axis(\"off\")\n\n\n    fig.suptitle(title, y=0.9)\n    plt.tight_layout()\n\n\nplot_reconstructed_and_original_image(crop, net, dog_X_scaled, title=\"Reconstructed Image\")\n\n\n\n\n\n\n\n\n\n# Use polynomial features of degree \"d\"\n\ndef poly_features(X, degree):\n    \"\"\"\n    X: torch.Tensor of shape (num_samples, 2)\n    degree: int\n    \n    return: torch.Tensor of shape (num_samples, degree * (degree + 1) / 2)\n    \"\"\"\n    X1 = X[:, 0]\n    X2 = X[:, 1]\n    X1 = X1.unsqueeze(1)\n    X2 = X2.unsqueeze(1)\n    X = torch.cat([X1, X2], dim=1)\n    poly = preprocessing.PolynomialFeatures(degree=degree)\n    X = poly.fit_transform(X.cpu())\n    return torch.tensor(X, dtype=torch.float32).to(device)\n\n\ndog_X_scaled_poly = poly_features(dog_X_scaled, 50)\n\n\ndog_X_scaled_poly.dtype, dog_X_scaled_poly.shape, dog_Y.shape, dog_Y.dtype\n\n(torch.float32,\n torch.Size([90000, 1326]),\n torch.Size([90000, 3]),\n torch.float32)\n\n\n\nnet = LinearModel(dog_X_scaled_poly.shape[1], 3)\nnet.to(device)\n\ntrain(net, 0.005, dog_X_scaled_poly, dog_Y, 1500)\n\nEpoch 0 loss: 0.353235\nEpoch 100 loss: 0.028444\nEpoch 200 loss: 0.025136\nEpoch 300 loss: 0.024183\nEpoch 400 loss: 0.023526\nEpoch 500 loss: 0.023012\nEpoch 600 loss: 0.022591\nEpoch 700 loss: 0.022229\nEpoch 800 loss: 0.021912\nEpoch 900 loss: 0.021658\nEpoch 1000 loss: 0.021389\nEpoch 1100 loss: 0.021166\nEpoch 1200 loss: 0.020970\nEpoch 1300 loss: 0.020785\nEpoch 1400 loss: 0.020631\n\n\n0.020467281341552734\n\n\n\nplot_reconstructed_and_original_image(crop, net, dog_X_scaled_poly, title=\"Reconstructed Image with Polynomial Features\")\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\n\n# create RFF features\ndef create_rff_features(X, num_features, sigma):\n    from sklearn.kernel_approximation import RBFSampler\n    rff = RBFSampler(n_components=num_features, gamma=1/(2 * sigma**2))\n    X = X.cpu().numpy()\n    X = rff.fit_transform(X)\n    return torch.tensor(X, dtype=torch.float32).to(device)\n\n\nX_rff = create_rff_features(dog_X_scaled, 37500, 0.008)\n\n\nX_rff.shape\n\ntorch.Size([90000, 37500])\n\n\n\nnet = LinearModel(X_rff.shape[1], 3)\nnet.to(device)\n\ntrain(net, 0.005, X_rff, dog_Y, 2500)\n\nEpoch 0 loss: 0.375324\nEpoch 100 loss: 0.047630\nEpoch 200 loss: 0.009158\nEpoch 300 loss: 0.003941\nEpoch 400 loss: 0.002057\nEpoch 500 loss: 0.001118\nEpoch 600 loss: 0.000640\nEpoch 700 loss: 0.000404\nEpoch 800 loss: 0.000292\nEpoch 900 loss: 0.000241\nEpoch 1000 loss: 0.000218\nEpoch 1100 loss: 0.000208\nEpoch 1200 loss: 0.000204\nEpoch 1300 loss: 0.000201\nEpoch 1400 loss: 0.000200\nEpoch 1500 loss: 0.000199\nEpoch 1600 loss: 0.000198\nEpoch 1700 loss: 0.000197\nEpoch 1800 loss: 0.000196\nEpoch 1900 loss: 0.000195\nEpoch 2000 loss: 0.000195\nEpoch 2100 loss: 0.000194\nEpoch 2200 loss: 0.000194\nEpoch 2300 loss: 0.000193\nEpoch 2400 loss: 0.000193\n\n\n0.00019248582248110324\n\n\n\nplot_reconstructed_and_original_image(crop, net, X_rff, title=\"Reconstructed Image with RFF Features\")\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\n\nw = 100\nscale=2\ntorch.arange(0, w, 1/scale)\n\ntensor([ 0.0000,  0.5000,  1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,\n         4.0000,  4.5000,  5.0000,  5.5000,  6.0000,  6.5000,  7.0000,  7.5000,\n         8.0000,  8.5000,  9.0000,  9.5000, 10.0000, 10.5000, 11.0000, 11.5000,\n        12.0000, 12.5000, 13.0000, 13.5000, 14.0000, 14.5000, 15.0000, 15.5000,\n        16.0000, 16.5000, 17.0000, 17.5000, 18.0000, 18.5000, 19.0000, 19.5000,\n        20.0000, 20.5000, 21.0000, 21.5000, 22.0000, 22.5000, 23.0000, 23.5000,\n        24.0000, 24.5000, 25.0000, 25.5000, 26.0000, 26.5000, 27.0000, 27.5000,\n        28.0000, 28.5000, 29.0000, 29.5000, 30.0000, 30.5000, 31.0000, 31.5000,\n        32.0000, 32.5000, 33.0000, 33.5000, 34.0000, 34.5000, 35.0000, 35.5000,\n        36.0000, 36.5000, 37.0000, 37.5000, 38.0000, 38.5000, 39.0000, 39.5000,\n        40.0000, 40.5000, 41.0000, 41.5000, 42.0000, 42.5000, 43.0000, 43.5000,\n        44.0000, 44.5000, 45.0000, 45.5000, 46.0000, 46.5000, 47.0000, 47.5000,\n        48.0000, 48.5000, 49.0000, 49.5000, 50.0000, 50.5000, 51.0000, 51.5000,\n        52.0000, 52.5000, 53.0000, 53.5000, 54.0000, 54.5000, 55.0000, 55.5000,\n        56.0000, 56.5000, 57.0000, 57.5000, 58.0000, 58.5000, 59.0000, 59.5000,\n        60.0000, 60.5000, 61.0000, 61.5000, 62.0000, 62.5000, 63.0000, 63.5000,\n        64.0000, 64.5000, 65.0000, 65.5000, 66.0000, 66.5000, 67.0000, 67.5000,\n        68.0000, 68.5000, 69.0000, 69.5000, 70.0000, 70.5000, 71.0000, 71.5000,\n        72.0000, 72.5000, 73.0000, 73.5000, 74.0000, 74.5000, 75.0000, 75.5000,\n        76.0000, 76.5000, 77.0000, 77.5000, 78.0000, 78.5000, 79.0000, 79.5000,\n        80.0000, 80.5000, 81.0000, 81.5000, 82.0000, 82.5000, 83.0000, 83.5000,\n        84.0000, 84.5000, 85.0000, 85.5000, 86.0000, 86.5000, 87.0000, 87.5000,\n        88.0000, 88.5000, 89.0000, 89.5000, 90.0000, 90.5000, 91.0000, 91.5000,\n        92.0000, 92.5000, 93.0000, 93.5000, 94.0000, 94.5000, 95.0000, 95.5000,\n        96.0000, 96.5000, 97.0000, 97.5000, 98.0000, 98.5000, 99.0000, 99.5000])\n\n\n\ndef create_coordinate_map(img, scale=1):\n    \"\"\"\n    img: torch.Tensor of shape (num_channels, height, width)\n    \n    return: tuple of torch.Tensor of shape (height * width, 2) and torch.Tensor of shape (height * width, num_channels)\n    \"\"\"\n    \n    num_channels, height, width = img.shape\n    \n    # Create a 2D grid of (x,y) coordinates (h, w)\n    # width values change faster than height values\n    w_coords = torch.arange(0, width,  1/scale).repeat(int(height*scale), 1)\n    h_coords = torch.arange(0, height, 1/scale).repeat(int(width*scale), 1).t()\n    w_coords = w_coords.reshape(-1)\n    h_coords = h_coords.reshape(-1)\n\n    # Combine the x and y coordinates into a single tensor\n    X = torch.stack([h_coords, w_coords], dim=1).float()\n\n    # Move X to GPU if available\n    X = X.to(device)\n\n    # Reshape the image to (h * w, num_channels)\n    Y = rearrange(img, 'c h w -&gt; (h w) c').float()\n    return X, Y\n\n\ncreate_coordinate_map(crop, scale=2)[0].shape\n\ntorch.Size([360000, 2])\n\n\n\ncreate_coordinate_map(crop, scale=1)[0].shape\n\ntorch.Size([90000, 2])"
  },
  {
    "objectID": "notebooks/rl-3.html",
    "href": "notebooks/rl-3.html",
    "title": "Reinforcement Learning 3 Deep Q Learning",
    "section": "",
    "text": "Reference\n\nDetailed Explanation and Python Implementation of Q-Learning Algorithm in OpenAI Gym (Cart-Pole)\n\n\nBasic Imports\nhttps://www.gymlibrary.dev/environments/classic_control/mountain_car/\n\n%pip install stable-baselines\n\nCollecting stable-baselines\n  Downloading stable_baselines-2.10.2-py3-none-any.whl (240 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 240.9/240.9 kB 2.2 MB/s eta 0:00:00a 0:00:01\nCollecting gym[atari,classic_control]&gt;=0.11 (from stable-baselines)\n  Downloading gym-0.26.2.tar.gz (721 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 721.7/721.7 kB 1.5 MB/s eta 0:00:0000:0100:01\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: scipy in /Users/nipun/mambaforge/lib/python3.10/site-packages (from stable-baselines) (1.10.1)\nRequirement already satisfied: joblib in /Users/nipun/mambaforge/lib/python3.10/site-packages (from stable-baselines) (1.3.2)\nRequirement already satisfied: cloudpickle&gt;=0.5.5 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from stable-baselines) (3.0.0)\nCollecting opencv-python (from stable-baselines)\n  Downloading opencv_python-4.9.0.80-cp37-abi3-macosx_11_0_arm64.whl (35.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35.4/35.4 MB 5.7 MB/s eta 0:00:0000:0100:01\nRequirement already satisfied: numpy in /Users/nipun/mambaforge/lib/python3.10/site-packages (from stable-baselines) (1.24.3)\nRequirement already satisfied: pandas in /Users/nipun/mambaforge/lib/python3.10/site-packages (from stable-baselines) (2.0.1)\nRequirement already satisfied: matplotlib in /Users/nipun/mambaforge/lib/python3.10/site-packages (from stable-baselines) (3.7.1)\nCollecting gym-notices&gt;=0.0.4 (from gym[atari,classic_control]&gt;=0.11-&gt;stable-baselines)\n  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\nCollecting ale-py~=0.8.0 (from gym[atari,classic_control]&gt;=0.11-&gt;stable-baselines)\n  Downloading ale_py-0.8.1-cp310-cp310-macosx_11_0_arm64.whl (1.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 5.5 MB/s eta 0:00:0000:0100:01\nCollecting pygame==2.1.0 (from gym[atari,classic_control]&gt;=0.11-&gt;stable-baselines)\n  Downloading pygame-2.1.0-cp310-cp310-macosx_11_0_arm64.whl (4.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 4.4 MB/s eta 0:00:0000:0100:01\nRequirement already satisfied: contourpy&gt;=1.0.1 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from matplotlib-&gt;stable-baselines) (1.0.7)\nRequirement already satisfied: cycler&gt;=0.10 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from matplotlib-&gt;stable-baselines) (0.11.0)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from matplotlib-&gt;stable-baselines) (4.39.4)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from matplotlib-&gt;stable-baselines) (1.4.4)\nRequirement already satisfied: packaging&gt;=20.0 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from matplotlib-&gt;stable-baselines) (23.1)\nRequirement already satisfied: pillow&gt;=6.2.0 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from matplotlib-&gt;stable-baselines) (9.5.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from matplotlib-&gt;stable-baselines) (3.0.9)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from matplotlib-&gt;stable-baselines) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from pandas-&gt;stable-baselines) (2023.3)\nRequirement already satisfied: tzdata&gt;=2022.1 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from pandas-&gt;stable-baselines) (2023.3)\nRequirement already satisfied: importlib-resources in /Users/nipun/mambaforge/lib/python3.10/site-packages (from ale-py~=0.8.0-&gt;gym[atari,classic_control]&gt;=0.11-&gt;stable-baselines) (5.12.0)\nRequirement already satisfied: typing-extensions in /Users/nipun/mambaforge/lib/python3.10/site-packages (from ale-py~=0.8.0-&gt;gym[atari,classic_control]&gt;=0.11-&gt;stable-baselines) (4.5.0)\nRequirement already satisfied: six&gt;=1.5 in /Users/nipun/mambaforge/lib/python3.10/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;stable-baselines) (1.16.0)\nBuilding wheels for collected packages: gym\n  Building wheel for gym (pyproject.toml) ... done\n  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827621 sha256=42d82791c287db865f66ed914f24b5b3d62e1a142c3b83c9c21d877545b9385b\n  Stored in directory: /Users/nipun/Library/Caches/pip/wheels/b9/22/6d/3e7b32d98451b4cd9d12417052affbeeeea012955d437da1da\nSuccessfully built gym\nInstalling collected packages: gym-notices, pygame, opencv-python, gym, ale-py, stable-baselines\n  Attempting uninstall: pygame\n    Found existing installation: pygame 2.5.2\n    Uninstalling pygame-2.5.2:\n      Successfully uninstalled pygame-2.5.2\nSuccessfully installed ale-py-0.8.1 gym-0.26.2 gym-notices-0.0.8 opencv-python-4.9.0.80 pygame-2.1.0 stable-baselines-2.10.2\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport gymnasium as gym\nimport math\nimport random\n\n\nenv = gym.make(\"CartPole-v1\")\n\n# set up matplotlib\nis_ipython = 'inline' in matplotlib.get_backend()\nif is_ipython:\n    from IPython import display\n\nplt.ion()\n\n# if GPU is to be used\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nfrom stable_baselines import DQN\nfrom stable_baselines.common.evaluation import evaluate_policy\n\n\n# Create environment\nenv = gym.make('LunarLander-v2')\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 from stable_baselines import DQN\n      2 from stable_baselines.common.evaluation import evaluate_policy\n      5 # Create environment\n\nFile ~/mambaforge/lib/python3.10/site-packages/stable_baselines/__init__.py:4\n      1 import os\n      2 import warnings\n----&gt; 4 from stable_baselines.a2c import A2C\n      5 from stable_baselines.acer import ACER\n      6 from stable_baselines.acktr import ACKTR\n\nFile ~/mambaforge/lib/python3.10/site-packages/stable_baselines/a2c/__init__.py:1\n----&gt; 1 from stable_baselines.a2c.a2c import A2C\n\nFile ~/mambaforge/lib/python3.10/site-packages/stable_baselines/a2c/a2c.py:5\n      3 import gym\n      4 import numpy as np\n----&gt; 5 import tensorflow as tf\n      7 from stable_baselines import logger\n      8 from stable_baselines.common import explained_variance, tf_util, ActorCriticRLModel, SetVerbosity, TensorboardWriter\n\nModuleNotFoundError: No module named 'tensorflow'\n\n\n\n\nTransition = namedtuple('Transition',\n                        ('state', 'action', 'next_state', 'reward'))\n\n\nclass ReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.memory = deque([], maxlen=capacity)\n\n    def push(self, *args):\n        \"\"\"Save a transition\"\"\"\n        self.memory.append(Transition(*args))\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)\n\n\nclass DQN(nn.Module):\n\n    def __init__(self, n_observations, n_actions):\n        super(DQN, self).__init__()\n        self.layer1 = nn.Linear(n_observations, 128)\n        self.layer2 = nn.Linear(128, 128)\n        self.layer3 = nn.Linear(128, n_actions)\n\n    # Called with either one element to determine next action, or a batch\n    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        return self.layer3(x)\n\n\nenv = gym.make('MountainCar-v0')\n\n\n# BATCH_SIZE is the number of transitions sampled from the replay buffer\n# GAMMA is the discount factor as mentioned in the previous section\n# EPS_START is the starting value of epsilon\n# EPS_END is the final value of epsilon\n# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n# TAU is the update rate of the target network\n# LR is the learning rate of the ``AdamW`` optimizer\nBATCH_SIZE = 128\nGAMMA = 0.99\nEPS_START = 0.9\nEPS_END = 0.05\nEPS_DECAY = 1000\nTAU = 0.005\nLR = 1e-4\n\n# Get number of actions from gym action space\nn_actions = env.action_space.n\n# Get the number of state observations\nstate, info = env.reset()\nn_observations = len(state)\n\npolicy_net = DQN(n_observations, n_actions).to(device)\ntarget_net = DQN(n_observations, n_actions).to(device)\ntarget_net.load_state_dict(policy_net.state_dict())\n\noptimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\nmemory = ReplayMemory(20000)\n\n\nsteps_done = 0\n\n\ndef select_action(state):\n    global steps_done\n    sample = random.random()\n    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n        math.exp(-1. * steps_done / EPS_DECAY)\n    steps_done += 1\n    if sample &gt; eps_threshold:\n        with torch.no_grad():\n            # t.max(1) will return the largest column value of each row.\n            # second column on max result is index of where max element was\n            # found, so we pick action with the larger expected reward.\n            return policy_net(state).max(1).indices.view(1, 1)\n    else:\n        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n\n\nepisode_durations = []\n\n\ndef plot_durations(show_result=False):\n    plt.figure(1)\n    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n    if show_result:\n        plt.title('Result')\n    else:\n        plt.clf()\n        plt.title('Training...')\n    plt.xlabel('Episode')\n    plt.ylabel('Duration')\n    plt.plot(durations_t.numpy())\n    # Take 100 episode averages and plot them too\n    if len(durations_t) &gt;= 100:\n        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n        means = torch.cat((torch.zeros(99), means))\n        plt.plot(means.numpy())\n\n    plt.pause(0.001)  # pause a bit so that plots are updated\n    if is_ipython:\n        if not show_result:\n            display.display(plt.gcf())\n            display.clear_output(wait=True)\n        else:\n            display.display(plt.gcf())\n\n\ndef optimize_model():\n    if len(memory) &lt; BATCH_SIZE:\n        return\n    transitions = memory.sample(BATCH_SIZE)\n    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n    # detailed explanation). This converts batch-array of Transitions\n    # to Transition of batch-arrays.\n    batch = Transition(*zip(*transitions))\n\n    # Compute a mask of non-final states and concatenate the batch elements\n    # (a final state would've been the one after which simulation ended)\n    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n                                          batch.next_state)), device=device, dtype=torch.bool)\n    non_final_next_states = torch.cat([s for s in batch.next_state\n                                                if s is not None])\n    state_batch = torch.cat(batch.state)\n    action_batch = torch.cat(batch.action)\n    reward_batch = torch.cat(batch.reward)\n\n    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n    # columns of actions taken. These are the actions which would've been taken\n    # for each batch state according to policy_net\n    state_action_values = policy_net(state_batch).gather(1, action_batch)\n\n    # Compute V(s_{t+1}) for all next states.\n    # Expected values of actions for non_final_next_states are computed based\n    # on the \"older\" target_net; selecting their best reward with max(1).values\n    # This is merged based on the mask, such that we'll have either the expected\n    # state value or 0 in case the state was final.\n    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n    with torch.no_grad():\n        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n    # Compute the expected Q values\n    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n\n    # Compute Huber loss\n    criterion = nn.SmoothL1Loss()\n    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n\n    # Optimize the model\n    optimizer.zero_grad()\n    loss.backward()\n    # In-place gradient clipping\n    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n    optimizer.step()\n\n\nif torch.cuda.is_available():\n    num_episodes = 600\nelse:\n    num_episodes = 100\n\nfor i_episode in range(num_episodes):\n    # Initialize the environment and get its state\n    state, info = env.reset()\n    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n    for t in count():\n        action = select_action(state)\n        observation, reward, terminated, truncated, _ = env.step(action.item())\n        reward = torch.tensor([reward], device=device)\n        done = terminated or truncated\n\n        if terminated:\n            next_state = None\n        else:\n            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n\n        # Store the transition in memory\n        memory.push(state, action, next_state, reward)\n\n        # Move to the next state\n        state = next_state\n\n        # Perform one step of the optimization (on the policy network)\n        optimize_model()\n\n        # Soft update of the target network's weights\n        # θ′ ← τ θ + (1 −τ )θ′\n        target_net_state_dict = target_net.state_dict()\n        policy_net_state_dict = policy_net.state_dict()\n        for key in policy_net_state_dict:\n            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n        target_net.load_state_dict(target_net_state_dict)\n\n        if done:\n            episode_durations.append(t + 1)\n            plot_durations()\n            break\n\nprint('Complete')\nplot_durations(show_result=True)\nplt.ioff()\nplt.show()\n\nComplete\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;"
  },
  {
    "objectID": "notebooks/logistic-iris.html",
    "href": "notebooks/logistic-iris.html",
    "title": "Logistic Regression - Iris dataset",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom latexify import *\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.patches as mpatches\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom sklearn.datasets import load_iris\n\n\nd = load_iris()\nX = d['data'][:, :2]\ny = d['target']\n\n\nd['feature_names']\n\n['sepal length (cm)',\n 'sepal width (cm)',\n 'petal length (cm)',\n 'petal width (cm)']\n\n\n\nlatexify()\ncolours = ['blue', 'red', 'green']\nspecies = ['I. setosa', 'I. versicolor', 'I. virginica']\nfor i in range(0, 3):    \n    df_ = X[y == i]\n    plt.scatter(        \n        df_[:, 0],        \n        df_[:, 1],\n        color=colours[i],        \n        alpha=0.5,        \n        label=species[i] ,\n        s=10\n    )\nformat_axes(plt.gca())\nplt.legend()\nplt.xlabel(d['feature_names'][0])\nplt.ylabel(d['feature_names'][1])\n\n\nplt.savefig(\"../figures/logistic-regression/logisitic-iris.pdf\", bbox_inches=\"tight\", transparent=True)\n\n\n\n\n\n\n\n\n\nclf = LogisticRegression(penalty='none',solver='newton-cg')\n\n\nclf.fit(X, y)\n\n/home/nipun.batra/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n\n\nLogisticRegression(penalty='none', solver='newton-cg')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='none', solver='newton-cg')\n\n\n\nclf.coef_\n\narray([[-55.82562338,  47.29592374],\n       [ 26.96162409, -23.85029157],\n       [ 28.86399931, -23.44563218]])\n\n\n\nX.shape\n\n(150, 2)\n\n\n\ny.shape\n\n(150,)\n\n\n\n# create a mesh to plot in\nx_min, x_max = X[:, 0].min() - 0.3, X[:, 0].max() + 0.3\ny_min, y_max = X[:, 1].min() - 0.3, X[:, 1].max() + 0.3\nh = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\n\n\n\n\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.4)\nplt.gca().set_aspect('equal')\n#plt.scatter(X[:, 0], X[:, 1], c=y)\nlatexify()\nfor i in range(0, 3):    \n    df_ = X[y == i]\n    plt.scatter(        \n        df_[:, 0],        \n        df_[:, 1],\n        color=colours[i],        \n        alpha=0.5,        \n        label=species[i],\n        s=10\n    )\nformat_axes(plt.gca())\nplt.legend()\nplt.xlabel(d['feature_names'][0])\nplt.ylabel(d['feature_names'][1])\nplt.savefig(\"../figures/logistic-regression/logisitic-iris-prediction.pdf\", bbox_inches=\"tight\", transparent=True)"
  },
  {
    "objectID": "notebooks/svm-cvxopt.html",
    "href": "notebooks/svm-cvxopt.html",
    "title": "SVM implementation using CVXOpt",
    "section": "",
    "text": "This post is pretty much borrowed with slight modifications from this excellent blog post from Matthieu Blondel\n\nGeneral QP problem\nA general QP problem can be formulated as follows:\n\\[\n\\begin{align*}\n\\text{minimize} \\quad & \\frac{1}{2} x^T P x + q^T x \\\\\n\\text{subject to} \\quad & G x \\leq h \\\\\n& A x = b\n\\end{align*}\n\\]\n\n\nSolving in Dual\nIn dual in hard margin SVM, the dual problem is formulated as follows:\n\\[\n\\begin{align*}\n\\text{maximize} \\quad & \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_i \\alpha_j y^{(i)} y^{(j)} (x^{(i)})^T x^{(j)} \\\\\n\\text{subject to} \\quad & \\alpha_i \\geq 0 \\quad \\text{for} \\quad i = 1, 2, \\ldots, m \\\\\n& \\sum_{i=1}^{m} \\alpha_i y^{(i)} = 0\n\\end{align*}\n\\]\nOr, we can write it in the following form (minimization problem): \\[\n\\begin{align*}\n\\text{minimize} \\quad & \\frac{1}{2} \\alpha^T Q \\alpha + e^T \\alpha \\\\\n\\text{subject to} \\quad & y^T \\alpha = 0 \\\\\n& \\alpha \\geq 0\n\\end{align*}\n\\]\nwhere \\[\n\\begin{align*}\nQ_{ij} & = y^{(i)} y^{(j)} (x^{(i)})^T x^{(j)} \\\\\ne_i & = -1\n\\end{align*}\n\\]\nwe can again write it in the following form using kernel trick:\n\\[\nQ_{ij} = y^{(i)} y^{(j)} K(x^{(i)}, x^{(j)})\n\\]\nwhere \\(K(x^{(i)}, x^{(j)}) = (x^{(i)})^T x^{(j)}\\) is the kernel function.\nThus, we can now set \\[\n\\begin{align*}\nP & = Q \\\\\nq & = e \\\\\nG & = -I \\\\\nh & = 0 \\\\\nA & = y^T \\\\\nb & = 0\n\\end{align*}\n\\]\nwhere \\(I\\) is the identity matrix.\n\ntry: \n    import cvxopt\n    import cvxopt.solvers\n\nexcept:\n    %pip install cvxopt\n    import cvxopt\n    import cvxopt.solvers\n\nimport numpy as np\nfrom sklearn.datasets import make_blobs\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\ndef linear_kernel(x1, x2):\n    return np.dot(x1, x2)\n\ndef polynomial_kernel(x, y, p=3):\n    return (1 + np.dot(x, y)) ** p\n\ndef gaussian_kernel(x, y, sigma=5.0):\n    return np.exp(-np.linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n\n\nX, y = make_blobs(n_samples=40, centers=2, n_features=2, random_state=42)\n\ny[y == 0] = -1.0\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n\n\n\n\n\n\n\n\n\nn_samples, n_features = X.shape\nkernel = linear_kernel\n\n# Gram matrix\nK = np.zeros((n_samples, n_samples))\nfor i in range(n_samples):\n    for j in range(n_samples):\n        K[i,j] = kernel(X[i], X[j])\n\n\n# QP problem formulation\nP = cvxopt.matrix(np.outer(y, y) * K)\nq = cvxopt.matrix(-np.ones(n_samples))\nA = cvxopt.matrix(y, (1, n_samples), 'd')\nb = cvxopt.matrix(0.0)\nG = cvxopt.matrix(-np.eye(n_samples))\nh = cvxopt.matrix(np.zeros(n_samples))\n\n\ndictionary = {\"P\": P, \"q\": q, \"A\": A, \"b\": b, \"G\": G, \"h\": h}\nfor key, val in dictionary.items():\n    print(key, val.size)\n\nP (40, 40)\nq (40, 1)\nA (1, 40)\nb (1, 1)\nG (40, 40)\nh (40, 1)\n\n\n\nsolution = cvxopt.solvers.qp(P, q, G, h, A, b)\n\n     pcost       dcost       gap    pres   dres\n 0: -1.4386e+00 -2.2358e+00  8e+01  8e+00  2e+00\n 1: -5.3423e-01 -2.0710e-01  6e+00  7e-01  1e-01\n 2:  3.5560e-03 -1.3362e-01  1e-01  4e-16  6e-15\n 3: -3.7579e-02 -5.5490e-02  2e-02  4e-17  7e-16\n 4: -5.0978e-02 -5.6104e-02  5e-03  9e-19  6e-16\n 5: -5.4114e-02 -5.4359e-02  2e-04  1e-17  6e-16\n 6: -5.4321e-02 -5.4336e-02  1e-05  7e-18  8e-16\n 7: -5.4333e-02 -5.4333e-02  1e-07  1e-17  7e-16\n 8: -5.4333e-02 -5.4333e-02  1e-09  2e-17  6e-16\nOptimal solution found.\n\n\n\nalphas_sol = np.array(solution['x']).flatten()\nalphas_sol\n\narray([4.81116795e-11, 5.57561687e-11, 4.14022254e-11, 5.64570278e-11,\n       3.19971594e-11, 1.13766765e-09, 1.10946709e-10, 5.43328120e-02,\n       9.43563733e-11, 3.59644883e-11, 2.72365634e-11, 1.78702226e-08,\n       7.91651647e-11, 4.45973348e-11, 6.48044988e-11, 3.09926966e-11,\n       3.26557350e-11, 4.12118866e-11, 2.53367783e-10, 4.55829611e-11,\n       4.43685787e-11, 6.12190291e-11, 5.72210101e-11, 3.48077854e-11,\n       2.70414582e-11, 3.72073366e-11, 4.52326326e-11, 6.21623048e-11,\n       3.61239404e-11, 3.61946755e-11, 2.70499880e-11, 6.76419169e-11,\n       3.77922165e-11, 9.70279801e-11, 4.63391531e-11, 6.74255180e-11,\n       3.77950852e-11, 5.47202347e-11, 5.02000180e-11, 5.43328313e-02])\n\n\n\nalphas_sol[alphas_sol &gt; 1e-5]\n\narray([0.05433281, 0.05433283])\n\n\n\n# Support vectors have non zero lagrange multipliers\nsv = alphas_sol &gt; 1e-5\nind = np.arange(len(alphas_sol))[sv]\na = alphas_sol[sv]\nsv_x = X[sv]\nsv_y = y[sv]\n\n# Plot the support vectors\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n\n# Mark the support vectors with thick black circles\nplt.scatter(sv_x[:, 0], sv_x[:, 1], c=sv_y, s=100, cmap=plt.cm.Paired, marker='o', edgecolors='k')\n\n\n\n\n\n\n\n\n\nw = np.sum(a * sv_y[:, None] * sv_x, axis=0)\nw\n\narray([ 0.23474316, -0.23143308])\n\n\n\nb = np.sum(sv_y - np.dot(sv_x, w)) / len(sv_y)\nb\n\n1.1726593683601716\n\n\n\n# Plot the decision boundary and margins\nplt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\nplt.scatter(sv_x[:, 0], sv_x[:, 1], c=sv_y, s=100, cmap=plt.cm.Paired, marker='o', edgecolors='k')\n\nx_min = np.min(X[:, 0])\nx_max = np.max(X[:, 0])\n\ny_min = (-w[0] * x_min - b) / w[1]\ny_max = (-w[0] * x_max - b) / w[1]\n\nplt.plot([x_min, x_max], [y_min, y_max], 'k-')\n\n# Margin 1\ny_min = (-w[0] * x_min - b + 1) / w[1]\ny_max = (-w[0] * x_max - b + 1) / w[1]\n\nplt.plot([x_min, x_max], [y_min, y_max], 'k--')\n\n# Margin 2\ny_min = (-w[0] * x_min - b - 1) / w[1]\ny_max = (-w[0] * x_max - b - 1) / w[1]\n\nplt.plot([x_min, x_max], [y_min, y_max], 'k--')\n\n\n\n\n\n\n\n\n\n\n\nUsing cvxpy\n\nimport cvxpy as cp\n\nalpha = cp.Variable(n_samples)\n\ninequality_constraints = [alpha &gt;= 0]\nequality_constraints = [cp.multiply(alpha, y) == 0]\n\noverall_constraints = inequality_constraints + equality_constraints\nP_numpy = np.outer(y, y)* K   + 3e-8 * np.eye(n_samples)\nobjective = cp.Maximize(cp.sum(alpha) - 0.5 * cp.quad_form(alpha, P_numpy))\n\nprob = cp.Problem(objective, overall_constraints)\nresult = prob.solve()\n\n\nprob.status\nalpha.value.sort()\nalpha.value\n\narray([-1.49664286e-08, -7.86036073e-09, -7.48959975e-09, -4.95631234e-09,\n       -4.80337014e-09, -4.21382858e-09, -4.17627547e-09, -3.60319297e-09,\n       -2.54786233e-09, -1.86476801e-09, -1.28264318e-09, -6.50572915e-10,\n       -3.44845584e-10, -4.45332158e-11,  2.50213529e-10,  2.67913419e-10,\n        8.79522453e-10,  8.82710270e-10,  9.50098303e-10,  1.82268844e-09,\n        2.74355414e-09,  3.45032195e-09,  4.95527787e-09,  5.02308218e-09,\n        5.34165002e-09,  6.71939414e-09,  6.94177696e-09,  7.06890144e-09,\n        9.38395895e-09,  9.49925189e-09,  9.72666606e-09,  1.15208492e-08,\n        1.18351650e-08,  1.20920336e-08,  1.22215620e-08,  1.69516039e-08,\n        2.28584041e-08,  3.45155480e-08,  5.49049991e-08,  1.01339280e-07])\n\n\n\n\nPrimal using cvxopt\n\n# Number of features\nn_features = X.shape[1]\n\n# Identity matrix\nP = cvxopt.matrix(np.block([[np.eye(n_features), np.zeros((n_features, 1))], [np.zeros((1, n_features + 1))]]))\n\n# Inequality constraints matrix\nG = cvxopt.matrix(-np.block([np.diag(y) @ X, y.reshape(-1, 1)]))\n\n# Inequality constraints bounds\nh = cvxopt.matrix(-np.ones(X.shape[0]))\n\n# Solve the QP problem\nsolution = cvxopt.solvers.qp(P, cvxopt.matrix(np.zeros(n_features + 1)), G, h)\n\n# Extract optimized w and b values\nw_opt = np.array(solution['x'][:-1])\nb_opt = np.array(solution['x'][-1])\n\n     pcost       dcost       gap    pres   dres\n 0:  1.9726e-02  3.3304e+00  8e+01  2e+00  3e+02\n 1:  1.2482e-01 -5.6113e+00  6e+00  1e-01  2e+01\n 2:  1.3362e-01 -3.5560e-03  1e-01  4e-16  5e-15\n 3:  5.5490e-02  3.7579e-02  2e-02  3e-16  3e-16\n 4:  5.6104e-02  5.0978e-02  5e-03  2e-16  1e-16\n 5:  5.4359e-02  5.4114e-02  2e-04  2e-16  1e-16\n 6:  5.4336e-02  5.4321e-02  1e-05  2e-16  2e-15\n 7:  5.4333e-02  5.4333e-02  1e-07  3e-16  2e-16\n 8:  5.4333e-02  5.4333e-02  1e-09  2e-16  3e-16\nOptimal solution found.\n\n\n\nw_opt, w\n\n(array([[ 0.23474325],\n        [-0.23143308]]),\n array([ 0.23474316, -0.23143308]))\n\n\n\nb_opt, b\n\n(array(1.17265942), 1.1726593683601716)"
  },
  {
    "objectID": "notebooks/pr-curve.html",
    "href": "notebooks/pr-curve.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nX, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=42, cluster_std=8.0)\n\ntrain_samples = 40\nX_train = X[:train_samples]\ny_train = y[:train_samples]\n\nX_test = X[train_samples:]\ny_test = y[train_samples:]\n\n# Plot training fata with small markers\nplt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], marker='o', label='class 0 Train', color='b')\nplt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], marker='s', label='class 1 Train', color='r')\n\n# Plot test data with larger markers\nplt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1], marker='o', s=100, label='class 0 Test', color='b', alpha=0.3)\nplt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1], marker='s', s=100, label='class 1 Test', color='r', alpha=0.3)\n\nplt.legend()\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(penalty='none', max_iter=1000)\n\nlr.fit(X_train, y_train)\n\n/Users/nipun/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1183: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n  warnings.warn(\n\n\nLogisticRegression(max_iter=1000, penalty='none')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(max_iter=1000, penalty='none')\n\n\n\npred_test = lr.predict_proba(X_test)\n\n\npred_test[:5]\n\narray([[0.37896698, 0.62103302],\n       [0.0502929 , 0.9497071 ],\n       [0.69240206, 0.30759794],\n       [0.1239149 , 0.8760851 ],\n       [0.27543045, 0.72456955]])\n\n\n\ndf = pd.DataFrame(pred_test, columns=['class 0', 'class 1'])\ndf['gt'] = y_test\n\n\ndf.head()\n\n\n\n\n\n\n\n\nclass 0\nclass 1\ngt\n\n\n\n\n0\n0.378967\n0.621033\n0\n\n\n1\n0.050293\n0.949707\n1\n\n\n2\n0.692402\n0.307598\n1\n\n\n3\n0.123915\n0.876085\n1\n\n\n4\n0.275430\n0.724570\n1\n\n\n\n\n\n\n\n\ndf['pred'] = (df['class 1'] &gt; 0.5).astype(int)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nclass 0\nclass 1\ngt\npred\n\n\n\n\n0\n0.378967\n0.621033\n0\n1\n\n\n1\n0.050293\n0.949707\n1\n1\n\n\n2\n0.692402\n0.307598\n1\n0\n\n\n3\n0.123915\n0.876085\n1\n1\n\n\n4\n0.275430\n0.724570\n1\n1\n\n\n\n\n\n\n\n\ndef confusion_p_r(model, X, y, threshold=0.5, eps=1e-8, img=False):\n    pred_prob = model.predict_proba(X)\n    pred = (pred_prob[:, 1] &gt;= threshold).astype(int)\n    df = pd.DataFrame(pred_prob, columns=['class 0', 'class1'])\n    df['gt'] = y\n    df['pred'] = pred\n    TP = ((df['gt'] == 1) & (df['pred'] == 1)).sum()\n    TN = ((df['gt'] == 0) & (df['pred'] == 0)).sum()\n    FP = ((df['gt'] == 0) & (df['pred'] == 1)).sum()\n    FN = ((df['gt'] == 1) & (df['pred'] == 0)).sum()\n    precision = TP / (TP + FP + eps)\n    recall = TP / (TP + FN + eps)\n    # plot confusion matrix\n    if img:\n        # Compute confusion matrix\n        cm = confusion_matrix(y, pred)\n        print(pd.DataFrame(cm, index=['True 0', 'True 1'], columns=['Pred 0', 'Pred 1']))\n    return precision, recall\n\n\nconfusion_p_r(lr, X_test, y_test, img=True)\n\n        Pred 0  Pred 1\nTrue 0      24       7\nTrue 1       9      20\n\n\n(0.7407407404663923, 0.6896551721759809)\n\n\n\nfor threshold in np.linspace(0, 1, 21):\n    precision, recall = confusion_p_r(lr, X_test, y_test, threshold)\n    print(f'Threshold: {threshold:.2f} Precision: {precision:.2f} Recall: {recall:.2f}')\n\nThreshold: 0.00 Precision: 0.48 Recall: 1.00\nThreshold: 0.05 Precision: 0.51 Recall: 1.00\nThreshold: 0.10 Precision: 0.53 Recall: 1.00\nThreshold: 0.15 Precision: 0.58 Recall: 0.97\nThreshold: 0.20 Precision: 0.59 Recall: 0.93\nThreshold: 0.25 Precision: 0.63 Recall: 0.90\nThreshold: 0.30 Precision: 0.65 Recall: 0.83\nThreshold: 0.35 Precision: 0.66 Recall: 0.79\nThreshold: 0.40 Precision: 0.69 Recall: 0.76\nThreshold: 0.45 Precision: 0.72 Recall: 0.72\nThreshold: 0.50 Precision: 0.74 Recall: 0.69\nThreshold: 0.55 Precision: 0.76 Recall: 0.66\nThreshold: 0.60 Precision: 0.78 Recall: 0.62\nThreshold: 0.65 Precision: 0.81 Recall: 0.59\nThreshold: 0.70 Precision: 0.88 Recall: 0.52\nThreshold: 0.75 Precision: 0.87 Recall: 0.48\nThreshold: 0.80 Precision: 0.93 Recall: 0.48\nThreshold: 0.85 Precision: 0.90 Recall: 0.31\nThreshold: 0.90 Precision: 1.00 Recall: 0.24\nThreshold: 0.95 Precision: 1.00 Recall: 0.14\nThreshold: 1.00 Precision: 0.00 Recall: 0.00\n\n\n\nconfusion_p_r(lr, X_test, y_test, threshold=0.99, img=True)\n\n        Pred 0  Pred 1\nTrue 0      31       0\nTrue 1      29       0\n\n\n(0.0, 0.0)\n\n\n\n# Plo9t Precicion curve\nmin_prob = lr.predict_proba(X_test)[:, 1].min()\nmax_prob = lr.predict_proba(X_test)[:, 1].max()\nthresholds = np.linspace(min_prob, max_prob, 100)\nprecisions = []\nrecalls = []\n\nfor threshold in thresholds:\n    precision, recall = confusion_p_r(lr, X_test, y_test, threshold)\n    precisions.append(precision)\n    recalls.append(recall)\n\nplt.plot(thresholds, precisions, label='Precision')\nplt.plot(thresholds, recalls, label='Recall')\n\nplt.xlabel('Threshold')\nplt.legend()\n\n\n\n\n\n\n\n\n\n# Plot Precision-Recall curve\nplt.plot(recalls, precisions, marker='o')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n# make plot square\nplt.gca().set_aspect('equal', adjustable='box')\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import precision_recall_curve\n\nprecision, recall, thresholds = precision_recall_curve(y_test, lr.predict_proba(X_test)[:, 1])\n\nplt.plot(recall, precision, marker='o')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n\nText(0, 0.5, 'Precision')\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import PrecisionRecallDisplay\ndisplay = PrecisionRecallDisplay.from_estimator(\n    lr, X_test, y_test, name=\"Logistic\", plot_chance_level=True\n)\n_ = display.ax_.set_title(\"2-class Precision-Recall curve\")\n\n\n\n\n\n\n\n\n\n(y_train==0).sum(), (y_train==1).sum(), (y_test==0).sum(), (y_test==1).sum()\n\n(19, 21, 31, 29)\n\n\n\n21/(19+21), 29/(29+31)\n\n(0.525, 0.48333333333333334)\n\n\n\n# Plot the decision boundary\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\nZ = lr.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, alpha=0.2, cmap='coolwarm', levels=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n\n# Colorbar\nplt.colorbar()\n\n# Plot test data with larger markers\nplt.scatter(X_test[y_test == 0, 0], X_test[y_test == 0, 1], marker='o', s=100, label='class 0 Test', color='b', alpha=0.3)\nplt.scatter(X_test[y_test == 1, 0], X_test[y_test == 1, 1], marker='s', s=100, label='class 1 Test', color='r', alpha=0.3)\n\nplt.legend()"
  },
  {
    "objectID": "notebooks/decision-tree-real-output.html",
    "href": "notebooks/decision-tree-real-output.html",
    "title": "Decision Trees Real Output",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom latexify import latexify, format_axes\n\n\ndf = pd.read_csv(\"../datasets/tennis-real-output.csv\", index_col=[0])\n\n\ndf\n\n\n\n\n\n\n\n\nOutlook\nTemp\nHumidity\nWind\nMinutes Played\n\n\nDay\n\n\n\n\n\n\n\n\n\nD1\nSunny\nHot\nHigh\nWeak\n20\n\n\nD2\nSunny\nHot\nHigh\nStrong\n24\n\n\nD3\nOvercast\nHot\nHigh\nWeak\n40\n\n\nD4\nRain\nMild\nHigh\nWeak\n50\n\n\nD5\nRain\nCool\nNormal\nWeak\n60\n\n\nD6\nRain\nCool\nNormal\nStrong\n10\n\n\nD7\nOvercast\nCool\nNormal\nStrong\n4\n\n\nD8\nSunny\nMild\nHigh\nWeak\n10\n\n\nD9\nSunny\nCool\nNormal\nWeak\n60\n\n\nD10\nRain\nMild\nNormal\nWeak\n40\n\n\nD11\nSunny\nMild\nHigh\nStrong\n45\n\n\nD12\nOvercast\nMild\nHigh\nStrong\n40\n\n\nD13\nOvercast\nHot\nNormal\nWeak\n35\n\n\nD14\nRain\nMild\nHigh\nStrong\n20\n\n\n\n\n\n\n\n\nmean_mins = df[\"Minutes Played\"].mean()\nprint(mean_mins)\n\n32.714285714285715\n\n\n\ninitial_mse = ((df[\"Minutes Played\"] - mean_mins) ** 2).mean()\nprint(initial_mse)\n\n311.3469387755102\n\n\n\n# Explore MSE for different splits based on the \"Outlook\" attribute\nweighted_total_mse = 0.0\nfor category in df[\"Wind\"].unique():\n    subset = df[df[\"Wind\"] == category]\n    \n    # Calculate MSE for the subset\n    mse_subset = ((subset[\"Minutes Played\"] - subset[\"Minutes Played\"].mean()) ** 2).mean()\n    \n    # Calculate the weighted MSE\n    weighted_mse = (len(subset) / len(df)) * mse_subset\n    weighted_total_mse = weighted_total_mse + weighted_mse\n    \n    print(subset[\"Minutes Played\"].values)\n    print(f\"Wind: {category}\")\n    print(\"Subset MSE:\", mse_subset)\n    print(f\"Weighted MSE = {len(subset)}/{len(df)} * {mse_subset:0.4} = {weighted_mse:0.4}\")\n    print(\"\\n\")\n\nprint(\"Weighted total MSE:\", weighted_total_mse)\n\n[20 40 50 60 10 60 40 35]\nWind: Weak\nSubset MSE: 277.734375\nWeighted MSE = 8/14 * 277.7 = 158.7\n\n\n[24 10  4 45 40 20]\nWind: Strong\nSubset MSE: 218.13888888888889\nWeighted MSE = 6/14 * 218.1 = 93.49\n\n\nWeighted total MSE: 252.19345238095235\n\n\n\nreduction_mse_wind = initial_mse - weighted_total_mse\nprint(reduction_mse_wind)\n\n59.15348639455783\n\n\n\ndef reduction_mse(df_dataset, input_attribute, target_attribute):\n    # Calculate the initial MSE\n    mean_target = df_dataset[target_attribute].mean()\n    initial_mse = ((df_dataset[target_attribute] - mean_target) ** 2).mean()\n    weighted_total_mse = 0.0\n\n    for category in df_dataset[input_attribute].unique():\n        subset = df_dataset[df_dataset[input_attribute] == category]\n        mse_subset = ((subset[target_attribute] - subset[target_attribute].mean()) ** 2).mean()\n        \n        weighted_mse = (len(subset) / len(df_dataset)) * mse_subset\n        weighted_total_mse = weighted_total_mse + weighted_mse\n    \n    return initial_mse - weighted_total_mse\n\n    \n\n\nreduction = {}\nfor attribute in [\"Outlook\", \"Temp\", \"Humidity\", \"Wind\"]:\n    reduction[attribute] = reduction_mse(df, attribute, \"Minutes Played\")\n    \nreduction_ser = pd.Series(reduction)\n\n\nlatexify()\n\n\nbars = reduction_ser.plot(kind='bar', rot=0, color='k')\nformat_axes(plt.gca())\n\n# Add values on top of the bars\nfor bar in bars.patches:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n\nplt.xlabel(\"Attribute\")\nplt.ylabel(\"Reduction in MSE\")\nplt.savefig(\"../figures/decision-trees/discrete-input-real-output-level-1.pdf\")"
  },
  {
    "objectID": "notebooks/svm-kernel.html",
    "href": "notebooks/svm-kernel.html",
    "title": "SVM Kernel",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nX = np.array([-2, -1, 0, 1, 2, 3, 4]).reshape(-1, 1)\ny = np.array([-1, -1, 1, 1, 1, -1, -1])\n\n\nplt.scatter(X.flatten(), np.zeros_like(X), c=y, cmap=cm.Paired)\n\n\n\n\n\n\n\n\n\ndef phi(x):\n    return np.hstack([x, x**2])\n\n\nphi(X)\n\narray([[-2,  4],\n       [-1,  1],\n       [ 0,  0],\n       [ 1,  1],\n       [ 2,  4],\n       [ 3,  9],\n       [ 4, 16]])\n\n\n\nplt.scatter(phi(X)[:, 0], phi(X)[:, 1], c = y,cmap = cm.Paired)\n\n\n\n\n\n\n\n\n\nfrom sklearn.svm import SVC\n\n\nc = SVC(kernel='linear', C = 1e6)\n\n\nc.fit(phi(X), y)\n\nSVC(C=1000000.0, kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(C=1000000.0, kernel='linear')\n\n\n\nplt.scatter(phi(X)[:, 0], phi(X)[:, 1], c = y, zorder=10, cmap =cm.Paired, edgecolors='k', alpha=1, s=200)\nx_min = phi(X)[:, 0].min()-1\nx_max = phi(X)[:, 0].max()+1\ny_min = phi(X)[:, 1].min()-1\ny_max = phi(X)[:, 1].max()+1\n\nXX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\nZ = c.decision_function(np.c_[XX.ravel(), YY.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(XX.shape)\nplt.pcolormesh(XX, YY, Z &gt; 0, cmap=plt.cm.Paired, alpha=0.6)\nplt.contour(XX, YY, Z, colors=['k', 'k', 'k'],\n            linestyles=['--', '-', '--'], levels=[-1, 0, 1])\nplt.xlabel(r\"$x_1 = X$\")\nplt.ylabel(r\"$x_2 = X^2$\")\nplt.title(r\"Decision surface: {:0.1f}*x_1 + {:0.1f}*x_2 + {:0.1f} = 0\".format(c.coef_[0, 0], c.coef_[0, 1], c.intercept_[0]))\n\nText(0.5, 1.0, 'Decision surface: 1.3*x_1 + -0.7*x_2 + 1.0 = 0')\n\n\n\n\n\n\n\n\n\n\n# Now using non-linearly separable data in 2D\n\nfrom sklearn.datasets import make_blobs, make_circles\n\nX, y = make_circles(n_samples=100, factor=0.5, noise=0.4)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm.Paired)\n\n\n\n\n\n\n\n\n\ndef plot_contour(X, y, clf):\n    xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 40), np.linspace(X[:, 1].min(), X[:, 1].max(), 40))\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.9,\n            linestyles=['--', '-', '--'])\n\n    plt.scatter(c.support_vectors_[:, 0], c.support_vectors_[:, 1], s=100,\n            linewidth=1, facecolors='none', edgecolors='k')\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm.Paired)\n    \n    plt.pcolormesh(xx, yy, Z &gt; 0, cmap=plt.cm.Paired, alpha=0.2)\n\n    \n    \n    # Train accuracy\n    y_pred = clf.predict(X)\n    plt.title(\"Train accuracy: {:0.2f}\\n Number of support vectors: {}\".format((y_pred == y).mean(), len(clf.support_vectors_)))\n\n    \n    \n\n\n# First linear SVM\n\nc = SVC(kernel='linear', C=10)\nc.fit(X, y)\n\nplot_contour(X, y, c)\n\n\n\n\n\n\n\n\n\nc_poly2 = SVC(kernel='poly', degree=2, C=10)\n\nc_poly2.fit(X, y)\n\nplot_contour(X, y, c_poly2)\n\n\n\n\n\n\n\n\n\n# RBF kernel\n\nc_rbf = SVC(kernel='rbf', C=10, gamma=1e-1)\nc_rbf.fit(X, y)\n\nplot_contour(X, y, c_rbf)\n\n\n\n\n\n\n\n\n\n# interactive widget to show the effect of gamma\n\nfrom ipywidgets import interact\n\ndef plot_svm(gamma=0.05):\n    c = SVC(kernel='rbf', C=10, gamma=gamma)\n    c.fit(X, y)\n    plot_contour(X, y, c)\n\ninteract(plot_svm, gamma=(0.01, 60, 0.05))\n\n\n\n\n&lt;function __main__.plot_svm(gamma=0.05)&gt;"
  },
  {
    "objectID": "notebooks/transcript.html",
    "href": "notebooks/transcript.html",
    "title": "YouTube video to transcript using openAI whisper and summary using OLLama",
    "section": "",
    "text": "try:\n    from pydub import AudioSegment\nexcept ImportError:\n    %pip install pydub\n    %pip install pydub[extras]\n    from pydub import AudioSegment\n    from pydub.playback import play\n\n\nfrom IPython.display import Audio\naudio_path = '../datasets/audio/Prime-minister.m4a'\naudio = AudioSegment.from_file(audio_path, format=\"m4a\")\naudio\n\n\n                    \n                        \n                        Your browser does not support the audio element.\n                    \n                  \n\n\n\ntry:    \n    import whisper\nexcept ImportError:\n    %pip install openai-whisper\n    import whisper\n\n\nwhisper_model = whisper.load_model(\"base.en\")\n\n\ntranscription = whisper_model.transcribe(audio_path, fp16=True, verbose=False)\n\n100%|██████████| 347/347 [00:00&lt;00:00, 367.83frames/s]\n\n\n\ntranscription\n\n{'text': ' Who is the Prime Minister of India?',\n 'segments': [{'id': 0,\n   'seek': 0,\n   'start': 0.0,\n   'end': 3.0,\n   'text': ' Who is the Prime Minister of India?',\n   'tokens': [50363, 5338, 318, 262, 5537, 4139, 286, 3794, 30, 50513],\n   'temperature': 0.0,\n   'avg_logprob': -0.34697675704956055,\n   'compression_ratio': 0.813953488372093,\n   'no_speech_prob': 0.005249415524303913}],\n 'language': 'en'}\n\n\n\nfrom IPython.display import Audio\n\n\ntry:\n    from gtts import gTTS\nexcept ImportError:\n    %pip install gtts\n    from gtts import gTTS\n\n\ndef speak(text, file):\n    tts = gTTS(text, lang='en')\n    with open(file, 'wb') as f:\n        tts.write_to_fp(f)\n    return Audio(file)\n\n\nspeak(transcription['text'], '../datasets/audio/pm-2.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nfrom langchain.llms import Ollama\nfrom langchain.callbacks.manager import CallbackManager\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler \n\n\nllm = Ollama(model=\"llama2\", \n             callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))\n\n\ndef answers(llm, prompt_qs, prompts, text):\n    outputs = []\n    for prompt, prompt_qs in zip(prompts, prompt_qs):\n        print(prompt_qs, end=\"\\n\")\n        output = llm(prompt, temperature=0.5)\n        #print(output, end=\"\\n\\n\")\n        print(\"\\n\" + \"==\"*50, end=\"\\n\\n\")\n    outputs.append(output) \n    return outputs\n\n\nprompt_qs = [\"Please be concise.\"] \nprompts = [q + \":\"+ transcription[\"text\"] for q in prompt_qs]\n\noutputs = answers(llm, prompt_qs, prompts, transcription[\"text\"])\n\nPlease be concise.\n\nThe Prime Minister of India is Narendra Modi.\n====================================================================================================\n\n\n\n\nspeak(outputs[0].replace(\"\\n\", \"\"), '../datasets/audio/pm-answer.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nReferences\n\nWhisper\nLangchain and LLama\nEnglish to Hindi using Transformers\n\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('https://www.youtube.com/watch?v=CuBzyh4Xmvk', width=500, height=300)\n\n\n        \n        \n\n\n\ntry:\n    import yt_dlp\nexcept ImportError:\n    %pip install yt_dlp\n    import yt_dlp\n\n\ndef download(video_id: str, save_path: str) -&gt; str:\n    video_url = f'https://www.youtube.com/watch?v={video_id}'\n    ydl_opts = {\n        'format': 'm4a/bestaudio/best',\n        'paths': {'home': save_path},\n        'outtmpl': {'default': \"lecture.m4a\"},\n        'postprocessors': [{\n            'key': 'FFmpegExtractAudio',\n            'preferredcodec': 'm4a',\n        }]\n    }\n    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n        error_code = ydl.download([video_url])\n        if error_code != 0:\n            raise Exception('Failed to download video')\n\n    return save_path\n\n\ndownload('CuBzyh4Xmvk', '../datasets/audio/')\n\n[youtube] Extracting URL: https://www.youtube.com/watch?v=CuBzyh4Xmvk\n[youtube] CuBzyh4Xmvk: Downloading webpage\n[youtube] CuBzyh4Xmvk: Downloading ios player API JSON\n[youtube] CuBzyh4Xmvk: Downloading android player API JSON\n[youtube] CuBzyh4Xmvk: Downloading m3u8 information\n[info] CuBzyh4Xmvk: Downloading 1 format(s): 140\n[download] ../datasets/audio/lecture.m4a has already been downloaded\n[download] 100% of   72.26MiB\n[ExtractAudio] Not converting audio ../datasets/audio/lecture.m4a; file is already in target format m4a\n\n\n'../datasets/audio/'\n\n\n\naudio_path = '../datasets/audio/lecture.m4a'\naudio = AudioSegment.from_file(audio_path, format=\"m4a\")\n\n\naudio[:13000]\n\n\n                    \n                        \n                        Your browser does not support the audio element.\n                    \n                  \n\n\n\ntranscription = whisper_model.transcribe(\"../datasets/audio/lecture.m4a\", fp16=True, verbose=False)\n\n 99%|█████████▉| 465481/468481 [02:07&lt;00:00, 3643.86frames/s]\n\n\n\nprint(transcription[\"text\"][:500].replace(\". \", \"\\n\"))\n\n Please look at the code mentioned above and please sign up on the Google Cloud\nWe've already started making some announcements\nYou will likely end up missing the announcements and you'll have no one else to play with\nThe second quick logistical announcement is that we'll have an extra lecture on Saturday, 11th Jan at 11am in 1.101\nSo a lot of ones over there\nAnd I think one or two people still have conflict, but in the larger, in the larger phone we'll have almost everyone available, so we\n\n\n\ntranscription.keys()\n\ndict_keys(['text', 'segments', 'language'])\n\n\n\ndef create_srt_from_transcription(transcription_objects, srt_file_path):\n    with open(srt_file_path, 'w') as srt_file:\n        index = 1  # SRT format starts with index 1\n\n        for entry in transcription_objects['segments']:\n            start_time = entry['start']\n            end_time = entry['end']\n            text = entry['text']\n\n            # Convert time to SRT format\n            start_time_str = format_time(start_time)\n            end_time_str = format_time(end_time)\n\n            # Write entry to SRT file\n            srt_file.write(f\"{index}\\n\")\n            srt_file.write(f\"{start_time_str} --&gt; {end_time_str}\\n\")\n            srt_file.write(f\"{text}\\n\\n\")\n\n            index += 1\n\ndef format_time(time_seconds):\n    minutes, seconds = divmod(time_seconds, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f\"{int(hours):02d}:{int(minutes):02d}:{int(seconds):02d},000\"\n\n\ncreate_srt_from_transcription(transcription, \"../datasets/audio/lecture.srt\")\n\n\n!head ../datasets/audio/lecture.srt\n\n1\n00:00:00,000 --&gt; 00:00:05,000\n Please look at the code mentioned above and please sign up on the Google Cloud.\n\n2\n00:00:05,000 --&gt; 00:00:08,000\n We've already started making some announcements.\n\n3\n00:00:08,000 --&gt; 00:00:14,000\n\n\n\nspeak(transcription['text'][:1300], '../datasets/audio/hello.mp3')\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ntry:\n    from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\nexcept:\n    %pip install transformers -U -q\n    %pip install sentencepiece\n    from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n    \n\n\n\n# download and save model\nmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\")\n\n# import tokenizer\ntokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-one-to-many-mmt\", src_lang=\"en_XX\")\n\n\ntext_to_translate = transcription[\"text\"][:500].split(\". \")\ntext_to_translate\n\n[' Please look at the code mentioned above and please sign up on the Google Cloud',\n \"We've already started making some announcements\",\n \"You will likely end up missing the announcements and you'll have no one else to play with\",\n \"The second quick logistical announcement is that we'll have an extra lecture on Saturday, 11th Jan at 11am in 1.101\",\n 'So a lot of ones over there',\n \"And I think one or two people still have conflict, but in the larger, in the larger phone we'll have almost everyone available, so we\"]\n\n\n\nmodel_inputs = tokenizer(text_to_translate, return_tensors=\"pt\", padding=True, truncation=True)\n\n\ngenerated_tokens = model.generate(\n    **model_inputs,\n    forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"]\n)\n\ntranslation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n\n\n\ntranslation\n\n['कृपया उपर्युक्त कोड को देखें और कृपया Google क्लाउड पर साइन अप करें',\n 'हम पहले से ही कुछ घोषणाएं करने शुरू कर दी हैं',\n 'आप शायद अंत में घोषणाओं को खो देंगे और आप के साथ खेलने के लिए कोई अन्य नहीं होगा',\n 'दूसरा त्वरित लॉजिस्टिक घोषणा यह है कि हम एक अतिरिक्त व्याख्यान Saturday, 11th Jan 11am में 1.101 में होगा',\n 'तो वहाँ के बहुत से',\n 'और मुझे लगता है कि एक या दो लोग अभी भी संघर्ष है, लेकिन बड़ी, बड़ी फोन में हम लगभग सभी उपलब्ध हो जाएगा, तो हम']\n\n\n\nllm = Ollama(model=\"mistral\", \n             callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]))\nprompt_qs = [\"Please provide a bullet-point summary for the given text:\",\n             \"Highlight the important topics and subtopics in the given lecture:\",\n             \"Give us some question for a quiz based on the following text:\",\n             \"Summarize the following text in Hindi in 10 lines or less:\",\n            ]\n\nprompts = [q + \"\\n\\n\" + transcription[\"text\"] for q in prompt_qs]\n\noutputs = answers(llm, prompt_qs, prompts, transcription[\"text\"])\n\nPlease provide a bullet-point summary for the given text:\n * The text discusses a machine learning course and announces several logistical matters, including signing up for Google Cloud, an extra lecture on Saturday, and providing access to Google Docs for FAQ and project questions.\n* The definition of machine learning is discussed, with the ability to learn without explicit programming being highlighted.\n* A task to recognize digits from a dataset is introduced as an example, and rules are suggested for recognizing the digit \"4\".\n* It is explained that traditional programming involves explicitly programming rules, while machine learning involves using data and experience to learn patterns and make predictions.\n* An example of predicting tomato quality based on visual features is given, with the goal being to scale up this process in a business setting.\n* The concept of precision and recall in machine learning evaluation metrics is touched upon, as well as the idea of a decision tree algorithm for classification tasks.\n* The text encourages students to come up with simple rules for recognizing patterns and using decision trees to make predictions based on those rules.\n* The greedy algorithm for finding the best attribute for splitting data in a decision tree is mentioned, along with the concept of entropy as a measure of disorder or uncertainty in a dataset.\n====================================================================================================\n\nHighlight the important topics and subtopics in the given lecture:\n The given lecture covers several important topics related to machine learning, including:\n\n1. Machine Learning Definition and Concepts\n* Explicit programming vs. machine learning\n* Linear programming vs. machine learning\n* Learning into a computer program\n2. Recognizing Digits using Machine Learning\n* Writing rules to recognize digits from dataset\n3. Machine Learning Algorithms and Techniques\n* Decision Trees for Classification Problems\n4. Performance Measures in Machine Learning\n* Accuracy, Precision, Recall, F-score, and Matthew's Correlation Coefficient\n5. Optimal Decision Tree and Greedy Algorithm\n6. Data Preprocessing and Feature Selection\n7. Entropy, Information Gain, and Attribute Selection\n8. Decision Tree Implementation and Details\n9. Limitations and Future Work in Machine Learning\n\nThe lecture also includes discussions on the importance of data preprocessing, feature selection, and understanding performance measures for evaluating machine learning models effectively. It is important to note that this list might not be exhaustive, but it covers the main topics mentioned in the given lecture.\n====================================================================================================\n\nGive us some question for a quiz based on the following text:\n 1. What is machine learning and when was it first introduced?\n2. What is the difference between explicit programming and machine learning?\n3. In the context of machine learning, what is a training set and a test set?\n4. What are some rules for recognizing the digit \"4\" in an image dataset?\n5. What is precision and recall in machine learning?\n6. What is the difference between precision and Matthew's correlation coefficient?\n7. In the given example, what is the precision, recall, F score, and Matthew's correlation coefficient for predicting cancerous or not based on a dataset with 91 entries, of which 90 are not cancerous and 1 is cancerous?\n8. What is the main difference between decision trees and other machine learning algorithms?\n9. How does a decision tree algorithm work to classify data based on attributes?\n10. What is entropy in information theory and how is it related to decision trees?\n====================================================================================================\n\nSummarize the following text in Hindi in 10 lines or less:\n हेज़रूदीन भाषा में 10 शोधनावलिका:\n\n1. यहाँ देखें लोगों को Google Cloud पर स्IGN UP करें। अगर आप मिटावे पहुँचती हैं, तो बेहतरीन शायद अपने साथ लगे जाएँगे। पहले तक हमें कुछ अख्बरें दिये गये हैं, वहाँ आप नहीं पहूँचेंगे और कोई भी साथी नहीं होगा।\n2. सब्बत, 11-01-2023 रात्रिकाल 11:00 वज़न में एक और पदार्थ होगा। यहाँ कुछ लोगों की संख्या काफी बढ़ जाती है, और अधिक लोगों में सभी पहुँचेंगे।\n3. FAQ और प्रोजेक्ट्स जिसे Google Docs में पहले शार्तीय थे, आप सभी टिप्साहित रखा जाएंगे। अगर आपको कुछ सवाल है, तो इसमें टिप्साहित शून्य भागान बनाएं और हमें दिजिएं।\n4. अगर आपको कुछ प्रश्न है, तो Google Docs पर प्रोजेक्ट्स के लिए टिप्साहित दूं।\n5. पहले ध्यान रखें कि वही चीज़ पर और जो वही समझाई गयी थी, वही समझाएं।\n6. Arthur Sandler द्वारा 1959 से पहले के साथ उन्होंने \"मशीन लर्निंग\" (Machine Learning) का शब्द पहली बातचीत की।\n7. एक सामग्री के लिए वेज़न से अपनी शिकाई करने के लिए इंजामत है।\n8. प्रोग्राम स्वयं प्रोग्राम है, नहीं दोनों तक प्रोग्राम होते हैं।\n9. आज के लिए काम करने वाले प्रश्रेण की बताव क्या है? यह सामग्री के वेज़न से अपनी शिकाई करने के लिए इंजामत है।\n10. दिए गये डिगिट्स (0-9) के लिए, पहली अध्ययन के लिए एक प्रोग्राम बनाएं जिससे वे दिगिट्स recognize करें।\n===================================================================================================="
  },
  {
    "objectID": "notebooks/lin-reg-tutorial.html",
    "href": "notebooks/lin-reg-tutorial.html",
    "title": "Some practice problems",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\n\n# Create some data\nx = np.linspace(0, 1, 100).reshape(-1, 1)\nf_x = 2 * x + 1\neps = np.random.randn(100, 1)*0.5\ny = f_x + eps\n\nplt.scatter(x, y, label='Data')\nplt.plot(x, f_x, 'r', label='True function')\n\n\n\n\n\n\n\n\n\nlr = LinearRegression()\npenalty_matrix = np.diag(np.eye(x.shape[0]))\npenalty_matrix\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\nlr.fit(x, y, sample_weight=penalty_matrix)\nlr.coef_, lr.intercept_\nplt.plot(x, lr.predict(x), 'g', label='Fitted model')\nplt.plot(x, f_x, 'r', label='True function')\nplt.scatter(x, y, label='Data')\nplt.legend()\n\n\n\n\n\n\n\n\n\nlr = LinearRegression()\nlr.fit(x, y)\nprint(lr.coef_, lr.intercept_)\nplt.plot(x, lr.predict(x), 'g', label='Fitted model')\nplt.plot(x, f_x, 'r', label='True function')\nplt.scatter(x, y, label='Data')\nplt.legend()\n\n[[1.9804156]] [1.05585787]\n\n\n\n\n\n\n\n\n\n\npenalty_matrix = np.linspace(0, 1, 100)\n\nlr = LinearRegression()\nlr.fit(x, y, sample_weight=penalty_matrix)\nprint(lr.coef_, lr.intercept_)\n\nplt.plot(x, lr.predict(x), 'g', label='Fitted model')\nplt.plot(x, f_x, 'r', label='True function')\nplt.scatter(x, y, label='Data')\nplt.legend()\n\n[[1.94435821]] [1.08001753]\n\n\n\n\n\n\n\n\n\n\nR = np.diag(penalty_matrix)\nprint(R)\n\n[[0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.01010101 0.         ... 0.         0.         0.        ]\n [0.         0.         0.02020202 ... 0.         0.         0.        ]\n ...\n [0.         0.         0.         ... 0.97979798 0.         0.        ]\n [0.         0.         0.         ... 0.         0.98989899 0.        ]\n [0.         0.         0.         ... 0.         0.         1.        ]]\n\n\n\nx_aug = np.hstack([np.ones_like(x), x])\ntheta_hat= np.linalg.inv(x_aug.T @ R @ x_aug) @ x_aug.T @ R @ y\nprint(theta_hat)\n\n[[1.08001753]\n [1.94435821]]\n\n\n\\(\\hat{y} = x^T\\theta\\)\n\nlr = LinearRegression(fit_intercept=False)\nlr.fit(x, y)\nprint(lr.coef_, lr.intercept_)\n\n[[3.55624367]] 0.0\n\n\n\nsum_x_y = np.sum(x * y)\nsum_x_x = np.sum(x * x)\n\nsum_x_y/sum_x_x\n\n3.5562436711266945\n\n\n\\(\\hat{y} = \\theta\\)\n\nlr = LinearRegression(fit_intercept=False)\nlr.fit(np.ones_like(y), y)\nprint(lr.coef_, lr.intercept_)\n\n[[2.04606567]] 0.0\n\n\n\nnp.mean(y)\n\n2.0460656658200334"
  },
  {
    "objectID": "notebooks/basis.html",
    "href": "notebooks/basis.html",
    "title": "Basis Expansion in Linear Regression",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.linear_model import Ridge\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\n# Interactive widget\nfrom ipywidgets import interact\n\n\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\n# Download CO2 data from NOAA\nurl = 'https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv'\n\nnames = 'year,month,decimal date,average,deseasonalized,ndays,sdev,unc'.split(',')\n\n# no index\ndf = pd.read_csv(url, skiprows=72, names=names, index_col=False)\n\n\ndf\n\n\n\n\n\n\n\n\nyear\nmonth\ndecimal date\naverage\ndeseasonalized\nndays\nsdev\nunc\n\n\n\n\n0\n1960\n10\n1960.7896\n313.83\n316.85\n-1\n-9.99\n-0.99\n\n\n1\n1960\n11\n1960.8743\n315.00\n316.89\n-1\n-9.99\n-0.99\n\n\n2\n1960\n12\n1960.9563\n316.19\n316.96\n-1\n-9.99\n-0.99\n\n\n3\n1961\n1\n1961.0411\n316.89\n316.84\n-1\n-9.99\n-0.99\n\n\n4\n1961\n2\n1961.1260\n317.70\n317.05\n-1\n-9.99\n-0.99\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n761\n2024\n3\n2024.2083\n425.38\n423.92\n22\n0.99\n0.40\n\n\n762\n2024\n4\n2024.2917\n426.57\n424.03\n24\n0.98\n0.38\n\n\n763\n2024\n5\n2024.3750\n426.90\n423.61\n29\n0.76\n0.27\n\n\n764\n2024\n6\n2024.4583\n426.91\n424.44\n20\n0.65\n0.28\n\n\n765\n2024\n7\n2024.5417\n425.55\n425.10\n24\n0.69\n0.27\n\n\n\n\n766 rows × 8 columns\n\n\n\n\ndf.index = pd.to_datetime(df[['year', 'month']].apply(lambda x: '{}-{}'.format(x[0], x[1]), axis=1))\n\n\ndf.average.plot(figsize=(6, 4), title='CO2 Levels at Mauna Loa Observatory')\nplt.xlabel('Year')\nplt.ylabel('CO2 Level')\n\nText(0, 0.5, 'CO2 Level')\n\n\n\n\n\n\n\n\n\n\n# Create X and y\n\n# X = months since first measurement\nX = np.array(range(len(df)))\ny = df.average.values\n\n\nplt.plot(X, y)\nplt.xlabel('Months since first measurement')\nplt.ylabel('CO2 Level')\n\nText(0, 0.5, 'CO2 Level')\n\n\n\n\n\n\n\n\n\n\n# Normalize X and y\n\ns1 = StandardScaler()\ns2 = StandardScaler()\n\nX_norm = s1.fit_transform(X.reshape(-1, 1))\ny_norm = s2.fit_transform(y.reshape(-1, 1))\n\n\nX_norm.mean(), X_norm.std()\n\n(0.0, 0.9999999999999999)\n\n\n\ndf = pd.DataFrame({\"x\":X.flatten(), \"transformed\":X_norm.flatten()})\ndf\n\n\n\n\n\n\n\n\nx\ntransformed\n\n\n\n\n0\n0\n-1.729791\n\n\n1\n1\n-1.725269\n\n\n2\n2\n-1.720746\n\n\n3\n3\n-1.716224\n\n\n4\n4\n-1.711702\n\n\n...\n...\n...\n\n\n761\n761\n1.711702\n\n\n762\n762\n1.716224\n\n\n763\n763\n1.720746\n\n\n764\n764\n1.725269\n\n\n765\n765\n1.729791\n\n\n\n\n766 rows × 2 columns\n\n\n\n\ndf[\"re-transformed\"] = s1.inverse_transform(df[\"transformed\"].values.reshape(-1, 1))\ndf\n\n\n\n\n\n\n\n\nx\ntransformed\nre-transformed\n\n\n\n\n0\n0\n-1.729791\n0.0\n\n\n1\n1\n-1.725269\n1.0\n\n\n2\n2\n-1.720746\n2.0\n\n\n3\n3\n-1.716224\n3.0\n\n\n4\n4\n-1.711702\n4.0\n\n\n...\n...\n...\n...\n\n\n761\n761\n1.711702\n761.0\n\n\n762\n762\n1.716224\n762.0\n\n\n763\n763\n1.720746\n763.0\n\n\n764\n764\n1.725269\n764.0\n\n\n765\n765\n1.729791\n765.0\n\n\n\n\n766 rows × 3 columns\n\n\n\n\ndf.mean()\n\nx                 382.5\ntransformed         0.0\nre-transformed    382.5\ndtype: float64\n\n\n\ndf.std()\n\nx                 221.269444\ntransformed         1.000653\nre-transformed    221.269444\ndtype: float64\n\n\n\nx_test = np.array([800])\ns1.transform(x_test.reshape(-1, 1))\n\narray([[1.88807266]])\n\n\n\ny_norm.mean(), y_norm.std()\n\n(-2.003619202665557e-15, 0.9999999999999998)\n\n\n\nplt.plot(X_norm, y_norm)\nplt.xlabel('(Normalized) Months since first measurement')\nplt.ylabel('(Normalized) CO2 Level')\n\nText(0, 0.5, '(Normalized) CO2 Level')\n\n\n\n\n\n\n\n\n\n\nTask 1: Interpolation\n\nnp.random.seed(42)\ntrain_idx = np.random.choice(range(len(X_norm)), size=int(len(X_norm) * 0.7), replace=False)\ntest_idx = list(set(range(len(X_norm))) - set(train_idx))\n\nX_train = X[train_idx]\ny_train = y[train_idx]\n\nX_test = X[test_idx]\ny_test = y[test_idx]\n\nX_norm_train = X_norm[train_idx]\ny_norm_train = y_norm[train_idx]\n\nX_norm_test = X_norm[test_idx]\ny_norm_test = y_norm[test_idx]\n\n\nplt.plot(X_norm_train, y_norm_train, 'o', label='train',markersize=1)\nplt.plot(X_norm_test, y_norm_test, 'o', label='test', ms=3)\nplt.xlabel('(Normalized) Months since first measurement')\nplt.ylabel('(Normalized) CO2 Level')\nplt.legend()\n\n\n\n\n\n\n\n\n\nerrors= {}\nX_lin_1d = np.linspace(X_norm.min(), X_norm.max(), 100).reshape(-1, 1)\n\n\n\nModel 1: Vanilla Linear Regression\n\ndef plot_fit_predict(model, X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin, title, plot=True):\n    model.fit(X_norm_train, y_norm_train)\n\n    y_hat_train = model.predict(X_norm_train).reshape(-1, 1)\n    y_hat_test = model.predict(X_norm_test).reshape(-1, 1)\n\n    # Transform back to original scale\n    y_hat_train = s2.inverse_transform(y_hat_train)\n    y_hat_test = s2.inverse_transform(y_hat_test)\n\n    y_hat_lin = s2.inverse_transform(model.predict(X_lin).reshape(-1, 1))\n\n    errors[title] = {\"train\": mean_squared_error(y_train, y_hat_train),\n                     \"test\": mean_squared_error(y_test, y_hat_test)}\n\n    if plot:\n        plt.plot(X_train, y_train, 'o', label='train', markersize=1)\n        plt.plot(X_test, y_test, 'o', label='test', ms=3)\n        plt.plot(s1.inverse_transform(X_lin_1d), y_hat_lin, label='model')\n        plt.xlabel('Months since first measurement')\n        plt.ylabel('CO2 Levels')\n        plt.legend()\n        plt.title('{}\\n Train MSE: {:.2f} | Test MSE: {:.2f}'.format(title, errors[title][\"train\"], errors[title][\"test\"]))\n\n    return errors[title]\n\n\nmodel = LinearRegression()\nplot_fit_predict(model, X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin_1d, \"Linear Regression\")\n\n{'train': 22.02534268790436, 'test': 18.72254862852729}\n\n\n\n\n\n\n\n\n\n\n\nMLP\n\n# use sk-learn for MLP\nmlp_model = MLPRegressor(hidden_layer_sizes=[128, 256, 512, 256, 128], max_iter = 10000)\n\nplot_fit_predict(mlp_model, X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin_1d, \"MLP Regression\")\n\n/Users/nipun/mambaforge/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:1624: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n\n\n{'train': 5.935971669202183, 'test': 6.511128786459153}\n\n\n\n\n\n\n\n\n\n\n\nPolynomial Regression of degree “d”\n\ndef create_poly_features(X, d):\n    \"\"\"\n    X is (N, 1) array\n    d is degree of polynomial\n    returns normalized polynomial features of X\n    \"\"\"\n    \n    X_poly = np.zeros((len(X), d))\n    X_poly[:, 0] = X[:, 0]\n    for i in range(1, d):\n        X_poly[:, i] = X[:, 0] ** (i + 1)\n    \n    # Normalize each column\n    X_poly = StandardScaler().fit_transform(X_poly)\n    return X_poly\n\n\nxs = np.linspace(-5, 5, 50).reshape(-1, 1)\npoly_f = create_poly_features(xs, 4)\nfor i in range(4):\n    plt.plot(xs, poly_f[:, i], label='x^{}'.format(i+1))\nplt.legend()\n\n\n\n\n\n\n\n\n\ndef show_poly_features(degree):\n    X_poly = create_poly_features(X_norm, degree)\n    plt.plot(X_norm, X_poly)\n    plt.xlabel('X')\n    plt.ylabel('Polynomial Features')\n    plt.title('Degree {}'.format(degree))\n\nshow_poly_features(2)\n\n\n\n\n\n\n\n\n\ninteract(show_poly_features, degree=(1, 10, 1))\n\n\n\n\n&lt;function __main__.show_poly_features(degree)&gt;\n\n\n\nmodel2 = LinearRegression()\ndegree = 4\nXf_norm_train = create_poly_features(X_norm_train.reshape(-1, 1), degree)\nXf_norm_test = create_poly_features(X_norm_test.reshape(-1, 1), degree)\nX_lin_poly = create_poly_features(X_lin_1d, degree)\n\nplot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Polynomial Regression (d={degree})\")\n\n{'train': 4.911310087864347, 'test': 6.6713097074692875}\n\n\n\n\n\n\n\n\n\n\nmodel2.coef_, model2.intercept_\n\n(array([[0.97361847, 0.09894695, 0.02772375, 0.03416679]]),\n array([-0.00901074]))\n\n\n\nX_lin_poly.shape\n\n(100, 4)\n\n\n\nX_lin_poly.shape\n\n(100, 4)\n\n\n\nmodel2.coef_.shape\n\n(1, 4)\n\n\n\nX_lin_1d.shape\n\n(100, 1)\n\n\n\nplt.plot(X_lin_1d, model2.intercept_.repeat(len(X_lin_1d)), label='Degree 0')\nplt.plot(X_lin_1d, X_lin_poly[:, 0:1]@model2.coef_[:, 0], label='Degree 1')\nplt.plot(X_lin_1d, X_lin_poly[:, 1:2]@model2.coef_[:, 1], label='Degree 2')\nplt.plot(X_lin_1d, X_lin_poly[:, 2:3]@model2.coef_[:, 2], label='Degree 3')\nplt.plot(X_lin_1d, X_lin_poly[:, 3:4]@model2.coef_[:, 3], label='Degree 4')\nplt.legend()\n\n\n\n\n\n\n\n\n\ndef show_additive(model, X_lin_1d, max_degree):\n    ys = model.intercept_.repeat(len(X_lin_1d))\n    #plt.fill_between(X_lin_1d.squeeze(), s2.inverse_transform(ys.reshape(-1, 1)).squeeze(), alpha=0.1)\n    print(ys.shape, X_lin_1d.shape)\n    label = '{:0.2f}x'.format(model.intercept_[0])\n    \n    for i in range(1, max_degree + 1):\n        yd = X_lin_poly[:, i-1:i]@model.coef_[:, i-1]\n        ys = ys + yd\n        label += ' + {:0.2f} x^{}'.format(model.coef_[:, i-1][0], i)\n    ys = s2.inverse_transform(ys.reshape(-1, 1))\n    plt.plot(X_lin_1d, ys, label = label)\n    plt.plot(X_norm_train, y_train, 'o', label='train', markersize=1)\n    plt.legend()\n\nshow_additive(model2, X_lin_1d, 3)\n\n(100,) (100, 1)\n\n\n\n\n\n\n\n\n\n\nfrom ipywidgets import interact, fixed\nm = model2\ninteract(show_additive, model=fixed(m), X_lin_1d=fixed(X_lin_1d), max_degree=(1, len(m.coef_[0]), 1))\n\n\n\n\n&lt;function __main__.show_additive(model, X_lin_1d, max_degree)&gt;\n\n\n\nfor degree in range(1, 10):\n    Xf_norm_train = create_poly_features(X_norm_train.reshape(-1, 1), degree)\n    Xf_norm_test = create_poly_features(X_norm_test.reshape(-1, 1), degree)\n\n    X_lin_poly = create_poly_features(X_lin_1d, degree)\n\n    plot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Polynomial Regression (d={degree})\", plot=False)\n\n\nerrors_df = pd.DataFrame(errors).T\nerrors_df\n\n\n\n\n\n\n\n\ntrain\ntest\n\n\n\n\nLinear Regression\n22.025343\n18.722549\n\n\nMLP Regression\n5.935972\n6.511129\n\n\nPolynomial Regression (d=4)\n4.911310\n6.671310\n\n\nPolynomial Regression (d=1)\n22.025343\n20.401004\n\n\nPolynomial Regression (d=2)\n5.120285\n6.983209\n\n\nPolynomial Regression (d=3)\n5.001988\n6.904987\n\n\nPolynomial Regression (d=5)\n4.802187\n6.752464\n\n\nPolynomial Regression (d=6)\n4.800177\n6.755818\n\n\nPolynomial Regression (d=7)\n4.741101\n6.819747\n\n\nPolynomial Regression (d=8)\n4.735053\n6.846223\n\n\nPolynomial Regression (d=9)\n4.710354\n6.859936\n\n\n1\n22.025343\n20.401004\n\n\n2\n5.120285\n6.983209\n\n\n3\n5.001988\n6.904987\n\n\n4\n4.911310\n6.671310\n\n\n5\n4.802187\n6.752464\n\n\n6\n4.800177\n6.755818\n\n\n7\n4.741101\n6.819747\n\n\n8\n4.735053\n6.846223\n\n\n9\n4.710354\n6.859936\n\n\n10\n4.704567\n6.857088\n\n\n11\n4.592348\n6.814545\n\n\n12\n4.586188\n6.859695\n\n\n13\n4.586165\n6.858467\n\n\n14\n4.584720\n6.872560\n\n\n15\n4.579522\n6.854161\n\n\n16\n4.577088\n6.856695\n\n\n17\n4.577062\n6.857773\n\n\n18\n4.576811\n6.862393\n\n\n19\n4.536762\n6.881145\n\n\n\n\n\n\n\n\nerrors_df.plot(kind='bar', figsize=(12, 6), title='MSE for Train and Test Sets')\n\n\n\n\n\n\n\n\n\n# bias variance tradeoff\n\nerrors_poly = {}\n\nfor degree in range(1, 20):\n    Xf_norm_train = create_poly_features(X_norm_train.reshape(-1, 1), degree)\n    Xf_norm_test = create_poly_features(X_norm_test.reshape(-1, 1), degree)\n\n    X_lin_poly = create_poly_features(X_lin_1d, degree)\n\n    plot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, degree, plot=False)\n\n    # geting errors for polynomial regression only for plotting\n    errors_poly[degree] = errors[degree]\n\n\nerrors_poly_df = pd.DataFrame(errors_poly).T\nbest_degree = np.argmin(errors_poly_df.test) + 1\nmin_error = errors_poly_df.test[best_degree - 1]    # index of df = degree - 1\nprint(f\"Best degree: {best_degree}, Min error: {min_error}\")\n\n# set figure size\nplt.figure(figsize=(12, 6))\nplt.plot(errors_poly_df.index.values, errors_poly_df.train.values, label='train')\nplt.plot(errors_poly_df.index.values, errors_poly_df.test.values, label='test')\nplt.axvline(best_degree, color='black', linestyle='--', label='best degree')\nplt.xticks(np.arange(min(errors_poly_df.index), max(errors_poly_df.index)+1, 1.0))\nplt.ylim(4.5, 7.5)      # set y limit - to show the difference between train and test clearly\nplt.xlabel('Degree')\nplt.ylabel('MSE')\nplt.title('Bias-Variance Tradeoff')\nplt.legend()\nplt.show()\n\nBest degree: 4, Min error: 6.904987394528579\n\n\n\n\n\n\n\n\n\n\n\nRidge Regression with polynomial basis\n\n# initiate ridge regression model\nmodel_ridge = Ridge(alpha=0.3)\n\nerrors_ridge = {}\n\nfor degree in range(1, 20):\n    Xf_norm_train = create_poly_features(X_norm_train.reshape(-1, 1), degree)\n    Xf_norm_test = create_poly_features(X_norm_test.reshape(-1, 1), degree)\n\n    X_lin_poly = create_poly_features(X_lin_1d, degree)\n\n    plot_fit_predict(model_ridge, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f'ridge_{degree}', plot=False)\n\n    # geting errors for polynomial regression only for plotting\n    errors_ridge[degree] = errors[f'ridge_{degree}']\n\n\nerrors_ridge_df = pd.DataFrame(errors_ridge).T\nbest_degree_ridge = np.argmin(errors_ridge_df.test) + 1\nmin_error = errors_ridge_df.test[best_degree_ridge - 1]    # index of df = degree - 1\nprint(f\"Best degree: {best_degree_ridge}, Min error: {min_error}\")\n\n# set figure size\nplt.figure(figsize=(12, 6))\nplt.plot(errors_ridge_df.index.values, errors_ridge_df.train.values, label='train')\nplt.plot(errors_ridge_df.index.values, errors_ridge_df.test.values, label='test')\nplt.axvline(best_degree_ridge, color='black', linestyle='--', label='best degree')\nplt.xticks(np.arange(min(errors_ridge_df.index), max(errors_ridge_df.index)+1, 1.0))\nplt.ylim(4.5, 7.5)      # set y limit - to show the difference between train and test clearly\nplt.xlabel('Degree')\nplt.ylabel('MSE')\nplt.title('Bias-Variance Tradeoff')\nplt.legend()\nplt.show()\n\nBest degree: 4, Min error: 6.535907094411071\n\n\n\n\n\n\n\n\n\n\nplot_fit_predict(model_ridge, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Ridge Regression with Polynomial Features (d={best_degree_ridge})\", plot=True)\n\n{'train': 4.767435271070607, 'test': 6.39886853568042}\n\n\n\n\n\n\n\n\n\n\n\nGausian Basis Function\n\ndef create_guassian_basis(X , d , std = 1):\n    \"\"\"\n    X is (N, 1) array\n    d is number of basis functions\n    \"\"\"\n    means = np.linspace(X.min(), X.max(), d)\n    X = np.repeat(X, d, axis=1)\n    means = np.repeat(means.reshape(-1, 1), len(X), axis=1).T\n\n    return np.exp(-(X - means) ** 2 / (2 * std ** 2))\n\n\ndef show_gaussian_basis(d, stdev):\n    xs = np.linspace(-5, 5, 100).reshape(-1, 1)\n    X_gauss = create_guassian_basis(xs, d, std=stdev)\n    plt.plot(xs, X_gauss)\n    plt.xlabel('X')\n    plt.ylabel('Gaussian Basis')\n    plt.title('Degree {} \\nStddev {}'.format(d, stdev))\n    plt.ylim(0, 1)\n\nshow_gaussian_basis(4, 1)\n\n\n\n\n\n\n\n\n\ninteract(show_gaussian_basis, d=(1, 10, 1), stdev=(0.1, 20, 0.1))\n\n\n\n\n&lt;function __main__.show_gaussian_basis(d, stdev)&gt;\n\n\n\nmodel_gauss = LinearRegression()\ndegree = 4\nXf_norm_train = create_guassian_basis(X_norm_train.reshape(-1, 1), degree)\nXf_norm_test = create_guassian_basis(X_norm_test.reshape(-1, 1), degree)\n\nX_lin_poly = create_guassian_basis(X_lin_1d, degree)\n\n\nplot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Gaussian Basis Linear Regression (d={degree})\")\n\n{'train': 5.390651367676581, 'test': 5.618947635873222}\n\n\n\n\n\n\n\n\n\n\nerrors.clear()\n\n\nfor degree in range(3, 7):\n    Xf_norm_train = create_guassian_basis(X_norm_train.reshape(-1, 1), degree)\n    Xf_norm_test = create_guassian_basis(X_norm_test.reshape(-1, 1), degree)\n\n    X_lin_poly = create_guassian_basis(X_lin_1d, degree)\n\n\n    plot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Gaussian Basis Linear Regression (d={degree})\", plot=False)\n\n\nerrors_df = pd.DataFrame(errors).T\n\n\nerrors_df.plot(kind='bar', figsize=(12, 6), title='MSE for Train and Test Sets')\n\n\n\n\n\n\n\n\n\nerrors.clear()\n\n\n# Bias Variance Tradeoff wrt Sigma\n\nnum_gaussians = 3\nfor std in [0.1, 0.5, 1, 2, 5, 10]:\n    Xf_norm_train = create_guassian_basis(X_norm_train.reshape(-1, 1), num_gaussians, std)\n    Xf_norm_test = create_guassian_basis(X_norm_test.reshape(-1, 1), num_gaussians, std)\n\n    X_lin_poly = create_guassian_basis(X_lin_1d, num_gaussians, std)\n\n    plot_fit_predict(model2, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_poly, f\"Gaussian Basis Linear Regression (d=3, std={std})\", plot=False)\n\n\n# Plot the train and test errors for different values of sigma\n\nerrors_df = pd.DataFrame(errors).T\n\ntest_errors = errors_df.test.values\ntrain_errors = errors_df.train.values\n\nlog_test_errors = np.log(test_errors)\nlog_train_errors = np.log(train_errors)\n\nstds = [0.1, 0.5, 1, 2, 5, 10]\n\nplt.plot(stds , log_test_errors, label='Log Test Loss')\nplt.plot(stds , log_train_errors, label='Log Train Loss')\nplt.scatter(stds, log_test_errors)\nplt.scatter(stds, log_train_errors)\nplt.legend()\nplt.xlabel('Sigma')\nplt.ylabel('Log(MSE)')\nplt.title('Bias Variance Tradeoff wrt Sigma')\n\nText(0.5, 1.0, 'Bias Variance Tradeoff wrt Sigma')\n\n\n\n\n\n\n\n\n\n\n\nGaussian Process\n\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.gaussian_process.kernels import ExpSineSquared\nfrom sklearn.gaussian_process.kernels import RationalQuadratic\nfrom sklearn.gaussian_process.kernels import WhiteKernel\n\nlong_term_trend_kernel = 50.0**2 * RBF(length_scale=50.0)\n\nseasonal_kernel = (\n    2.0**2\n    * RBF(length_scale=100.0)\n    * ExpSineSquared(length_scale=1.0, periodicity=1.0, periodicity_bounds=\"fixed\")\n)\n\nirregularities_kernel = 0.5**2 * RationalQuadratic(length_scale=1.0, alpha=1.0)\n\nnoise_kernel = 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(\n    noise_level=0.1**2, noise_level_bounds=(1e-5, 1e5)\n)\n\nco2_kernel = (\n    long_term_trend_kernel + seasonal_kernel + irregularities_kernel + noise_kernel\n)\nco2_kernel\n\n50**2 * RBF(length_scale=50) + 2**2 * RBF(length_scale=100) * ExpSineSquared(length_scale=1, periodicity=1) + 0.5**2 * RationalQuadratic(alpha=1, length_scale=1) + 0.1**2 * RBF(length_scale=0.1) + WhiteKernel(noise_level=0.01)\n\n\n\n# Using GP for the interpolation problem\n\nfrom sklearn.gaussian_process import GaussianProcessRegressor\n\ndef plot_fit_gp(X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin, title, plot=True):\n\n    gaussian_process = GaussianProcessRegressor(\n        kernel=co2_kernel,n_restarts_optimizer=9\n    )\n\n    gaussian_process.fit(X_norm_train, y_norm_train)\n\n    y_hat_train, std_prediction_train = gaussian_process.predict(X_norm_train, return_std=True)\n    y_hat_test , std_prediction_test = gaussian_process.predict(X_norm_test, return_std=True)\n\n    y_hat_train = y_hat_train.reshape(-1, 1)\n    y_hat_test = y_hat_test.reshape(-1, 1)\n\n    # Transform back to original scale\n    y_hat_train = s2.inverse_transform(y_hat_train)\n    y_hat_test = s2.inverse_transform(y_hat_test)\n\n    y_hat_lin , std_prediction_lin = gaussian_process.predict(X_lin , return_std=True)\n    y_hat_lin = y_hat_lin.reshape(-1, 1)\n    y_hat_lin = s2.inverse_transform(y_hat_lin)\n\n    errors[title] = {\"train\": mean_squared_error(y_train, y_hat_train),\n                     \"test\": mean_squared_error(y_test, y_hat_test)}\n\n    if plot:\n        plt.plot(X_train, y_train, 'o', label='train',markersize=1)\n        plt.plot(X_test, y_test, 'o', label='test', ms=3)\n        plt.plot(s1.inverse_transform(X_lin_1d), y_hat_lin, label='model')\n        plt.fill_between(s1.inverse_transform(X_lin_1d).reshape(-1), \n                         (y_hat_lin - 1.96*std_prediction_lin.reshape(-1,1)).reshape(-1), \n                         (y_hat_lin + 1.96*std_prediction_lin.reshape(-1,1)).reshape(-1), alpha=0.5 , label='95% Confidence interval')\n        plt.xlabel('Months since first measurement')\n        plt.ylabel('CO2 Levels')\n        plt.legend()\n\n        plt.title('{}\\n Train MSE: {:.2f} | Test MSE: {:.2f}'.format(title, errors[title][\"train\"], errors[title][\"test\"]))\n\n    return errors[title]\n\n\nplot_fit_gp(X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin_1d, \"Gaussian Process Regression\")\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nInput In [58], in &lt;module&gt;\n----&gt; 1 plot_fit_gp(X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin_1d, \"Gaussian Process Regression\")\n\nInput In [57], in plot_fit_gp(X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin, title, plot)\n      5 def plot_fit_gp(X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin, title, plot=True):\n      7     gaussian_process = GaussianProcessRegressor(\n      8         kernel=co2_kernel,n_restarts_optimizer=9\n      9     )\n---&gt; 11     gaussian_process.fit(X_norm_train, y_norm_train)\n     13     y_hat_train, std_prediction_train = gaussian_process.predict(X_norm_train, return_std=True)\n     14     y_hat_test , std_prediction_test = gaussian_process.predict(X_norm_test, return_std=True)\n\nFile ~/miniforge3/lib/python3.9/site-packages/sklearn/base.py:1152, in _fit_context.&lt;locals&gt;.decorator.&lt;locals&gt;.wrapper(estimator, *args, **kwargs)\n   1145     estimator._validate_params()\n   1147 with config_context(\n   1148     skip_parameter_validation=(\n   1149         prefer_skip_nested_validation or global_skip_validation\n   1150     )\n   1151 ):\n-&gt; 1152     return fit_method(estimator, *args, **kwargs)\n\nFile ~/miniforge3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpr.py:325, in GaussianProcessRegressor.fit(self, X, y)\n    322     for iteration in range(self.n_restarts_optimizer):\n    323         theta_initial = self._rng.uniform(bounds[:, 0], bounds[:, 1])\n    324         optima.append(\n--&gt; 325             self._constrained_optimization(obj_func, theta_initial, bounds)\n    326         )\n    327 # Select result from run with minimal (negative) log-marginal\n    328 # likelihood\n    329 lml_values = list(map(itemgetter(1), optima))\n\nFile ~/miniforge3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpr.py:656, in GaussianProcessRegressor._constrained_optimization(self, obj_func, initial_theta, bounds)\n    654 def _constrained_optimization(self, obj_func, initial_theta, bounds):\n    655     if self.optimizer == \"fmin_l_bfgs_b\":\n--&gt; 656         opt_res = scipy.optimize.minimize(\n    657             obj_func,\n    658             initial_theta,\n    659             method=\"L-BFGS-B\",\n    660             jac=True,\n    661             bounds=bounds,\n    662         )\n    663         _check_optimize_result(\"lbfgs\", opt_res)\n    664         theta_opt, func_min = opt_res.x, opt_res.fun\n\nFile ~/miniforge3/lib/python3.9/site-packages/scipy/optimize/_minimize.py:710, in minimize(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\n    707     res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n    708                              **options)\n    709 elif meth == 'l-bfgs-b':\n--&gt; 710     res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n    711                            callback=callback, **options)\n    712 elif meth == 'tnc':\n    713     res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n    714                         **options)\n\nFile ~/miniforge3/lib/python3.9/site-packages/scipy/optimize/_lbfgsb_py.py:365, in _minimize_lbfgsb(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\n    359 task_str = task.tobytes()\n    360 if task_str.startswith(b'FG'):\n    361     # The minimization routine wants f and g at the current x.\n    362     # Note that interruptions due to maxfun are postponed\n    363     # until the completion of the current minimization iteration.\n    364     # Overwrite f and g:\n--&gt; 365     f, g = func_and_grad(x)\n    366 elif task_str.startswith(b'NEW_X'):\n    367     # new iteration\n    368     n_iterations += 1\n\nFile ~/miniforge3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:285, in ScalarFunction.fun_and_grad(self, x)\n    283 if not np.array_equal(x, self.x):\n    284     self._update_x_impl(x)\n--&gt; 285 self._update_fun()\n    286 self._update_grad()\n    287 return self.f, self.g\n\nFile ~/miniforge3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:251, in ScalarFunction._update_fun(self)\n    249 def _update_fun(self):\n    250     if not self.f_updated:\n--&gt; 251         self._update_fun_impl()\n    252         self.f_updated = True\n\nFile ~/miniforge3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:155, in ScalarFunction.__init__.&lt;locals&gt;.update_fun()\n    154 def update_fun():\n--&gt; 155     self.f = fun_wrapped(self.x)\n\nFile ~/miniforge3/lib/python3.9/site-packages/scipy/optimize/_differentiable_functions.py:137, in ScalarFunction.__init__.&lt;locals&gt;.fun_wrapped(x)\n    133 self.nfev += 1\n    134 # Send a copy because the user may overwrite it.\n    135 # Overwriting results in undefined behaviour because\n    136 # fun(self.x) will change self.x, with the two no longer linked.\n--&gt; 137 fx = fun(np.copy(x), *args)\n    138 # Make sure the function returns a true scalar\n    139 if not np.isscalar(fx):\n\nFile ~/miniforge3/lib/python3.9/site-packages/scipy/optimize/_optimize.py:77, in MemoizeJac.__call__(self, x, *args)\n     75 def __call__(self, x, *args):\n     76     \"\"\" returns the function value \"\"\"\n---&gt; 77     self._compute_if_needed(x, *args)\n     78     return self._value\n\nFile ~/miniforge3/lib/python3.9/site-packages/scipy/optimize/_optimize.py:71, in MemoizeJac._compute_if_needed(self, x, *args)\n     69 if not np.all(x == self.x) or self._value is None or self.jac is None:\n     70     self.x = np.asarray(x).copy()\n---&gt; 71     fg = self.fun(x, *args)\n     72     self.jac = fg[1]\n     73     self._value = fg[0]\n\nFile ~/miniforge3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpr.py:297, in GaussianProcessRegressor.fit.&lt;locals&gt;.obj_func(theta, eval_gradient)\n    295 def obj_func(theta, eval_gradient=True):\n    296     if eval_gradient:\n--&gt; 297         lml, grad = self.log_marginal_likelihood(\n    298             theta, eval_gradient=True, clone_kernel=False\n    299         )\n    300         return -lml, -grad\n    301     else:\n\nFile ~/miniforge3/lib/python3.9/site-packages/sklearn/gaussian_process/_gpr.py:580, in GaussianProcessRegressor.log_marginal_likelihood(self, theta, eval_gradient, clone_kernel)\n    577     kernel.theta = theta\n    579 if eval_gradient:\n--&gt; 580     K, K_gradient = kernel(self.X_train_, eval_gradient=True)\n    581 else:\n    582     K = kernel(self.X_train_)\n\nFile ~/miniforge3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:841, in Sum.__call__(self, X, Y, eval_gradient)\n    813 \"\"\"Return the kernel k(X, Y) and optionally its gradient.\n    814 \n    815 Parameters\n   (...)\n    838     is True.\n    839 \"\"\"\n    840 if eval_gradient:\n--&gt; 841     K1, K1_gradient = self.k1(X, Y, eval_gradient=True)\n    842     K2, K2_gradient = self.k2(X, Y, eval_gradient=True)\n    843     return K1 + K2, np.dstack((K1_gradient, K2_gradient))\n\nFile ~/miniforge3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:842, in Sum.__call__(self, X, Y, eval_gradient)\n    840 if eval_gradient:\n    841     K1, K1_gradient = self.k1(X, Y, eval_gradient=True)\n--&gt; 842     K2, K2_gradient = self.k2(X, Y, eval_gradient=True)\n    843     return K1 + K2, np.dstack((K1_gradient, K2_gradient))\n    844 else:\n\nFile ~/miniforge3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:940, in Product.__call__(self, X, Y, eval_gradient)\n    938 if eval_gradient:\n    939     K1, K1_gradient = self.k1(X, Y, eval_gradient=True)\n--&gt; 940     K2, K2_gradient = self.k2(X, Y, eval_gradient=True)\n    941     return K1 * K2, np.dstack(\n    942         (K1_gradient * K2[:, :, np.newaxis], K2_gradient * K1[:, :, np.newaxis])\n    943     )\n    944 else:\n\nFile ~/miniforge3/lib/python3.9/site-packages/sklearn/gaussian_process/kernels.py:1916, in RationalQuadratic.__call__(self, X, Y, eval_gradient)\n   1913 # gradient with respect to alpha\n   1914 if not self.hyperparameter_alpha.fixed:\n   1915     alpha_gradient = K * (\n-&gt; 1916         -self.alpha * np.log(base)\n   1917         + dists / (2 * self.length_scale**2 * base)\n   1918     )\n   1919     alpha_gradient = alpha_gradient[:, :, np.newaxis]\n   1920 else:  # alpha is kept fixed\n\nKeyboardInterrupt: \n\n\n\n\n\nFourier Features\nReference: https://bmild.github.io/fourfeat/\n\nFourier Feature Mapping\nFor every input x, we map it to a higher dimensional space using the following function:\n\\[\\gamma(x) = [\\cos(2\\pi Bx), \\sin(2\\pi Bx)]^{T} \\]\nwhere \\(B\\) is a random Gaussian matrix, where each entry is drawn independently from a normal distribution N(0, \\(σ^{2}\\))\n\nnp.random.seed(42)\nsigma = 5\nNUM_features = 5\nfs = sigma*np.random.randn(NUM_features)\nprint(fs)\n\nfor i in range(NUM_features):\n    plt.plot(X_norm, np.sin(fs[i]*X_norm), label=f'feature {i}-sin')\n    plt.plot(X_norm, np.cos(fs[i]*X_norm), label=f'feature {i}-cos')\nplt.legend()\nplt.title('Fourier Featurization of X manually')\n\n[ 2.48357077 -0.69132151  3.23844269  7.61514928 -1.17076687]\n\n\nText(0.5, 1.0, 'Fourier Featurization of X manually')\n\n\n\n\n\n\n\n\n\n\n# Explicit implementation of RFF\n\ndef create_random_features(X, gamma, NUM_features):\n    \"\"\"\n    X is (N, 1) array\n    gamma is a scalar\n    NUM_features is a scalar\n    \"\"\"\n    \n    X_rff = np.zeros((len(X), 2*NUM_features + 1))\n    X_rff[:, 0] = X[:, 0]\n    for i in range(NUM_features):\n        b = np.random.randn()\n        X_rff[:, i+1] = np.sin(2*np.pi*gamma*b*X[:, 0])\n        X_rff[:, i + NUM_features+1] = np.cos(2*np.pi*gamma*b*X[:, 0])\n    \n    # Normalize each column\n    X_rff = StandardScaler().fit_transform(X_rff)\n    return X_rff\n\n\n\nSklearn Implementation\n\n# Sklearn's implementation of RFF\n\nfrom sklearn.kernel_approximation import RBFSampler\nRBFSampler?\n\nInit signature: RBFSampler(*, gamma=1.0, n_components=100, random_state=None)\nDocstring:     \nApproximate a RBF kernel feature map using random Fourier features.\n\nIt implements a variant of Random Kitchen Sinks.[1]\n\nRead more in the :ref:`User Guide &lt;rbf_kernel_approx&gt;`.\n\nParameters\n----------\ngamma : 'scale' or float, default=1.0\n    Parameter of RBF kernel: exp(-gamma * x^2).\n    If ``gamma='scale'`` is passed then it uses\n    1 / (n_features * X.var()) as value of gamma.\n\n    .. versionadded:: 1.2\n       The option `\"scale\"` was added in 1.2.\n\nn_components : int, default=100\n    Number of Monte Carlo samples per original feature.\n    Equals the dimensionality of the computed feature space.\n\nrandom_state : int, RandomState instance or None, default=None\n    Pseudo-random number generator to control the generation of the random\n    weights and random offset when fitting the training data.\n    Pass an int for reproducible output across multiple function calls.\n    See :term:`Glossary &lt;random_state&gt;`.\n\nAttributes\n----------\nrandom_offset_ : ndarray of shape (n_components,), dtype={np.float64, np.float32}\n    Random offset used to compute the projection in the `n_components`\n    dimensions of the feature space.\n\nrandom_weights_ : ndarray of shape (n_features, n_components),        dtype={np.float64, np.float32}\n    Random projection directions drawn from the Fourier transform\n    of the RBF kernel.\n\nn_features_in_ : int\n    Number of features seen during :term:`fit`.\n\n    .. versionadded:: 0.24\n\nfeature_names_in_ : ndarray of shape (`n_features_in_`,)\n    Names of features seen during :term:`fit`. Defined only when `X`\n    has feature names that are all strings.\n\n    .. versionadded:: 1.0\n\nSee Also\n--------\nAdditiveChi2Sampler : Approximate feature map for additive chi2 kernel.\nNystroem : Approximate a kernel map using a subset of the training data.\nPolynomialCountSketch : Polynomial kernel approximation via Tensor Sketch.\nSkewedChi2Sampler : Approximate feature map for\n    \"skewed chi-squared\" kernel.\nsklearn.metrics.pairwise.kernel_metrics : List of built-in kernels.\n\nNotes\n-----\nSee \"Random Features for Large-Scale Kernel Machines\" by A. Rahimi and\nBenjamin Recht.\n\n[1] \"Weighted Sums of Random Kitchen Sinks: Replacing\nminimization with randomization in learning\" by A. Rahimi and\nBenjamin Recht.\n(https://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf)\n\nExamples\n--------\n&gt;&gt;&gt; from sklearn.kernel_approximation import RBFSampler\n&gt;&gt;&gt; from sklearn.linear_model import SGDClassifier\n&gt;&gt;&gt; X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n&gt;&gt;&gt; y = [0, 0, 1, 1]\n&gt;&gt;&gt; rbf_feature = RBFSampler(gamma=1, random_state=1)\n&gt;&gt;&gt; X_features = rbf_feature.fit_transform(X)\n&gt;&gt;&gt; clf = SGDClassifier(max_iter=5, tol=1e-3)\n&gt;&gt;&gt; clf.fit(X_features, y)\nSGDClassifier(max_iter=5)\n&gt;&gt;&gt; clf.score(X_features, y)\n1.0\nFile:           ~/mambaforge/lib/python3.10/site-packages/sklearn/kernel_approximation.py\nType:           type\nSubclasses:     \n\n\n\nr= RBFSampler(n_components=5)\nplt.plot(X_norm, r.fit_transform(X_norm.reshape(-1,1)))\nplt.title('Fourier Featurization using RBFSampler (sklearn)')\n\nText(0.5, 1.0, 'Fourier Featurization using RBFSampler (sklearn)')\n\n\n\n\n\n\n\n\n\n\nr = RBFSampler(n_components=4, gamma=0.1)\nplt.plot(X_norm, r.fit_transform(X_norm.reshape(-1,1)))\nplt.title('Fourier Featurization using RBFSampler (sklearn)')\n\nText(0.5, 1.0, 'Fourier Featurization using RBFSampler (sklearn)')\n\n\n\n\n\n\n\n\n\n\nr = RBFSampler(n_components=4, gamma=20)\nplt.plot(X_norm, r.fit_transform(X_norm.reshape(-1,1)))\nplt.title('Fourier Featurization using RBFSampler (sklearn)')\n\nText(0.5, 1.0, 'Fourier Featurization using RBFSampler (sklearn)')\n\n\n\n\n\n\n\n\n\n\n# Implicit implementation of RFF using sklearn\n\ndef create_rff(X, gamma, NUM_features):\n    # Random Fourier Features\n    # https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html\n    rbf_feature = RBFSampler(gamma=gamma, n_components=NUM_features, random_state=1)\n    X_features = rbf_feature.fit_transform(X)\n    return X_features\n\n\nmodel3 = LinearRegression()\ngamma = 2.0\nNUM_features = 10\n\nXf_norm_train = create_rff(X_norm_train.reshape(-1, 1), gamma, NUM_features)\nXf_norm_test = create_rff(X_norm_test.reshape(-1, 1), gamma, NUM_features)\n\nX_lin_rff = create_rff(X_lin_1d, gamma, NUM_features)\n\nplot_fit_predict(model3, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_rff, f\"Random Fourier Features (gamma={gamma}, NUM_features={NUM_features})\")\n\n{'train': 4.630893826442012, 'test': 4.626609156411284}\n\n\n\n\n\n\n\n\n\n\n\nBias-Variance Tradeoff for Fourier Features\nw.r.t gamma\n\nmodel4 = LinearRegression()\n\n# NUM_features_values = [1, 2, 3, 4, 5, 10, 20, 50, 100]\ngamma_values = [0.01, 0.1, 1, 2, 5, 10]\n\nerrors_rff = {}\n\nfor gamma in gamma_values:\n    # gamma = 2.0\n    NUM_features_ = 100\n    Xf_norm_train = create_rff(X_norm_train.reshape(-1, 1), gamma, NUM_features_)\n    Xf_norm_test = create_rff(X_norm_test.reshape(-1, 1), gamma, NUM_features_)\n\n    X_lin_rff = create_rff(X_lin_1d, gamma, NUM_features_)\n\n    plot_fit_predict(model4, Xf_norm_train, y_norm_train, Xf_norm_test, y_norm_test, X_lin_rff, gamma, plot=False)\n\n    errors_rff[gamma] = errors[gamma]\n\n\npd.DataFrame(errors_rff).T.plot(logy=True, logx=True)\n\n\n\n\n\n\n\n\n\n\n\nExtrapolation using Gaussian Process\n\nX_norm_train = X_norm[:int(len(X_norm)*0.7)]\nX_norm_test = X_norm[int(len(X_norm)*0.7):]\n\ny_norm_train = y_norm[:int(len(y_norm)*0.7)]\ny_norm_test = y_norm[int(len(y_norm)*0.7):]\n\nX_train = X[:int(len(X)*0.7)]\nX_test = X[int(len(X)*0.7):]\n\ny_train = y[:int(len(y)*0.7)]\ny_test = y[int(len(y)*0.7):]\n\n\nplt.plot(X_norm_train, y_norm_train, 'o', label='train',markersize=1)\nplt.plot(X_norm_test, y_norm_test, 'o', label='test', ms=3)\n\n\n\n\n\n\n\n\n\nplot_fit_gp(X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin_1d, \"Gaussian Process Extrapolation Regression\")\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nInput In [235], in &lt;module&gt;\n----&gt; 1 plot_fit_gp(X_norm_train, y_norm_train, X_norm_test, y_norm_test, X_lin_1d, \"Gaussian Process Extrapolation Regression\")\n\nNameError: name 'plot_fit_gp' is not defined\n\n\n\n\n\nBefore this…\n\n\nDataset 1: Sine wave with noise\n\n# Generate some data\nrng = np.random.RandomState(1)\nx = 15 * rng.rand(200)\ny = np.sin(x) + 0.1 * rng.randn(200)\ndf = pd.DataFrame({'x': x, 'y': y})\n\n# plot the data\nplt.scatter(df.x, df.y)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Train test split\ntrain = df.sample(frac=0.8, random_state=1)\ntest = df.drop(train.index)\n\nplt.scatter(train.x, train.y, color='blue', label='train')\nplt.scatter(test.x, test.y, color='orange', label='test')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\ndef plot_predictions(train, test, yhat_train, yhat_test):\n        \n    # add yhat_train to train and yhat_test to test\n    train['yhat'] = yhat_train\n    test['yhat'] = yhat_test\n\n    # sort train and test by x\n    train = train.sort_values(by='x')\n    test = test.sort_values(by='x')\n\n    # Train and test error\n    train_rmse = np.sqrt(np.mean((train.yhat - train.y)**2))\n    test_rmse = np.sqrt(np.mean((test.yhat - test.y)**2))\n\n    plt.scatter(train.x, train.y, color='blue', label='train')\n    plt.scatter(test.x, test.y, color='orange', label='test')\n    plt.plot(train.x, train.yhat, color='red', label='train prediction')\n    plt.plot(test.x, test.yhat, color='green', label='test prediction')\n    plt.title('Train RMSE: {:.3f}, Test RMSE: {:.3f}'.format(train_rmse, test_rmse))\n    plt.legend()\n    plt.show()\n\n    return train_rmse, test_rmse\n\n\n# Hyperparameter tuning using grid search and showing bias variance tradeoff\ndef hyperparameter_tuning(params, train, test, model):\n    train_rmse = []\n    test_rmse = []\n\n    for d in params:\n        yhat_train, yhat_test = model(d, train, test)\n        train_rmse.append(np.sqrt(np.mean((yhat_train - train.y)**2)))\n        test_rmse.append(np.sqrt(np.mean((yhat_test - test.y)**2)))\n\n    plt.plot(params, train_rmse, label='train')\n    plt.plot(params, test_rmse, label='test')\n    plt.xlabel('params')\n    plt.ylabel('RMSE')\n    plt.legend()\n    plt.show()\n\n    optimal_param = params[np.argmin(test_rmse)]\n\n    return optimal_param\n\n\nrmse_dict = {}\n\n\nModel 1: MLP\n\n# use sk-learn for MLP\nmlp_model = MLPRegressor(hidden_layer_sizes=[128, 256, 512, 256, 128], max_iter = 10000)\nmlp_model.fit(np.array(train.x).reshape(-1, 1), train.y)\n\nMLPRegressor(hidden_layer_sizes=[128, 256, 512, 256, 128], max_iter=10000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPRegressorMLPRegressor(hidden_layer_sizes=[128, 256, 512, 256, 128], max_iter=10000)\n\n\n\nyhat_train = mlp_model.predict(np.array(train.x).reshape(-1, 1))\nyhat_test = mlp_model.predict(np.array(test.x).reshape(-1, 1))\ntrain_rmse, test_rmse = plot_predictions(train, test, yhat_train, yhat_test)\nrmse_dict['MLP'] = (train_rmse, test_rmse)\n\n\n\n\n\n\n\n\n\n\nModel 2: Vanilla Linear Regression\n\nlr1 = LinearRegression()\nlr1.fit(np.array(train.x).reshape(-1, 1), train.y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nyhat_train = lr1.predict(np.array(train.x).reshape(-1, 1))\nyhat_test = lr1.predict(np.array(test.x).reshape(-1, 1))\ntrain_rmse, test_rmse = plot_predictions(train, test, yhat_train, yhat_test)\nrmse_dict['Vanilla LR'] = (train_rmse, test_rmse)\n\n\n\n\n\n\n\n\n\n\nModel 3: Polynomial regression with degree d\n\ndef poly_regression(d, train, test):\n    lr = LinearRegression()\n    pf = PolynomialFeatures(degree=d)\n    \n    X_train = pf.fit_transform(train.x.values.reshape(-1, 1))\n    X_test = pf.fit_transform(test.x.values.reshape(-1, 1))\n    \n    lr.fit(X_train, train.y)\n    \n    yhat_train = lr.predict(X_train)\n    yhat_test = lr.predict(X_test)\n\n    return yhat_train, yhat_test\n\n\n# Hyperparameter tuning using grid search and showing bias variance tradeoff\ndegrees = range(1, 20)\nbest_degree = hyperparameter_tuning(degrees, train, test, poly_regression)\nyhat_train, yhat_test = poly_regression(best_degree, train, test)\ntrain_rmse, test_rmse = plot_predictions(train, test, yhat_train, yhat_test)\nrmse_dict['Polynomial Regression'] = (train_rmse, test_rmse)\nprint(\"Best degree: \", best_degree)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBest degree:  10\n\n\n\n\nModel 4: Linear regression with sine and cosine basis functions\n\ndef sine_basis_regression(num_basis, train, test):\n    lr = LinearRegression()\n    for i in range(1, num_basis+1):\n        train[f\"sine_{i}\"] = np.sin(i*train.x)\n        train[f\"cosine_{i}\"] = np.cos(i*train.x)\n        test[f\"sine_{i}\"] = np.sin(i*test.x)\n        test[f\"cosine_{i}\"] = np.cos(i*test.x)\n    \n    X_train = train.drop(['y'], axis=1)\n    X_test = test.drop(['y'], axis=1)\n    \n    lr.fit(X_train, train.y)\n\n    yhat_train = lr.predict(X_train)\n    yhat_test = lr.predict(X_test)\n\n    return yhat_train, yhat_test\n\n\nbasis = range(1, 20)\nbest_num_basis = hyperparameter_tuning(basis, train, test, sine_basis_regression)\nyhat_train, yhat_test = sine_basis_regression(best_num_basis, train, test)\ntrain_rmse, test_rmse = plot_predictions(train, test, yhat_train, yhat_test)\nrmse_dict['Sine Basis Regression'] = (train_rmse, test_rmse)\nprint(\"Best number of basis: \", best_num_basis)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBest number of basis:  14\n\n\n\n\nModel 5: Linear regression with Gaussian basis functions\n\n# Source: https://jakevdp.github.io/PythonDataScienceHandbook/05.06-linear-regression.html#Gaussian-basis-functions\n\nclass GaussianFeatures(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Uniformly spaced Gaussian features for one-dimensional input\n    \n    Constructor with N centers and width_factor as hyperparameters\n    N comes from the number of basis functions\n    width_factor is the width of each basis function\n    \"\"\"\n    \n    def __init__(self, N, width_factor=2.0):\n        self.N = N\n        self.width_factor = width_factor\n    \n    @staticmethod\n    def _gauss_basis(x, y, width, axis=None):\n        arg = (x - y) / width\n        return np.exp(-0.5 * np.sum(arg ** 2, axis))\n        \n    def fit(self, X, y=None):\n        # create N centers spread along the data range\n        self.centers_ = np.linspace(X.min(), X.max(), self.N)\n        self.width_ = self.width_factor * (self.centers_[1] - self.centers_[0])\n        return self\n        \n    def transform(self, X):\n        return self._gauss_basis(X[:, :, np.newaxis], self.centers_, self.width_, axis=1)\n\n\n# Hyperparameter tuning\nbasis = range(2, 20)\n\ntrain_rmse = []\ntest_rmse = []\nfor d in basis:\n    model = make_pipeline(GaussianFeatures(d), LinearRegression())\n    model.fit(np.array(train.x).reshape(-1, 1), train.y)\n    yhat_train = model.predict(np.array(train.x).reshape(-1, 1))\n    yhat_test = model.predict(np.array(test.x).reshape(-1, 1))\n    train_rmse.append(np.sqrt(np.mean((yhat_train - train.y)**2)))\n    test_rmse.append(np.sqrt(np.mean((yhat_test - test.y)**2)))\n\nbest_num_basis = basis[np.argmin(test_rmse)]\nprint(\"Best number of basis: \", best_num_basis)\nplt.plot(basis, train_rmse, label='train')\nplt.plot(basis, test_rmse, label='test')\nplt.xlabel('degree')\nplt.ylabel('RMSE')\nplt.legend()\nplt.show()\n\nBest number of basis:  13\n\n\n\n\n\n\n\n\n\n\ngauss_model = make_pipeline(GaussianFeatures(best_num_basis), LinearRegression())\ngauss_model.fit(np.array(train.x).reshape(-1, 1), train.y)\nyhat_train = gauss_model.predict(train.x.values.reshape(-1, 1))\nyhat_test = gauss_model.predict(test.x.values.reshape(-1, 1))\ntrain_rmse, test_rmse = plot_predictions(train, test, yhat_train, yhat_test)\nrmse_dict['Gaussian Basis Regression'] = (train_rmse, test_rmse)\n\n\n\n\n\n\n\n\n\nPlotting rmse using different variants of linear regression\n\n# create a bar plot of train and test RMSE\n\ntrain_rmse = [rmse_dict[key][0] for key in rmse_dict.keys()]\ntest_rmse = [rmse_dict[key][1] for key in rmse_dict.keys()]\nlabels = [key for key in rmse_dict.keys()]\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(10, 5))\nrects1 = ax.bar(x - width/2, train_rmse, width, label='Train RMSE')\nrects2 = ax.bar(x + width/2, test_rmse, width, label='Test RMSE')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('RMSE')\nax.set_title('RMSE by model')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\nfig.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\nDataset 2: CO2 Dataset\n\ndf\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n6.255330\n-0.020070\n\n\n1\n10.804867\n-0.920032\n\n\n2\n0.001716\n0.024965\n\n\n3\n4.534989\n-0.916051\n\n\n4\n2.201338\n0.776696\n\n\n...\n...\n...\n\n\n195\n13.979581\n1.115462\n\n\n196\n0.209274\n0.163526\n\n\n197\n3.515431\n-0.332839\n\n\n198\n9.251675\n0.161240\n\n\n199\n14.235245\n0.996049\n\n\n\n\n200 rows × 2 columns\n\n\n\n\ndf.index = pd.to_datetime(df[['year', 'month']].apply(lambda x: '{}-{}'.format(x[0], x[1]), axis=1))\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n/Users/kalash/Documents/Work/Semester_8/ML_TA/ml-teaching/notebooks/basis.ipynb Cell 108 line 1\n----&gt; &lt;a href='vscode-notebook-cell:/Users/kalash/Documents/Work/Semester_8/ML_TA/ml-teaching/notebooks/basis.ipynb#Y425sZmlsZQ%3D%3D?line=0'&gt;1&lt;/a&gt; df.index = pd.to_datetime(df[['year', 'month']].apply(lambda x: '{}-{}'.format(x[0], x[1]), axis=1))\n\nFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:3767, in DataFrame.__getitem__(self, key)\n   3765     if is_iterator(key):\n   3766         key = list(key)\n-&gt; 3767     indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n   3769 # take() does not accept boolean indexers\n   3770 if getattr(indexer, \"dtype\", None) == bool:\n\nFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:5876, in Index._get_indexer_strict(self, key, axis_name)\n   5873 else:\n   5874     keyarr, indexer, new_indexer = self._reindex_non_unique(keyarr)\n-&gt; 5876 self._raise_if_missing(keyarr, indexer, axis_name)\n   5878 keyarr = self.take(indexer)\n   5879 if isinstance(key, Index):\n   5880     # GH 42790 - Preserve name from an Index\n\nFile /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:5935, in Index._raise_if_missing(self, key, indexer, axis_name)\n   5933     if use_interval_msg:\n   5934         key = list(key)\n-&gt; 5935     raise KeyError(f\"None of [{key}] are in the [{axis_name}]\")\n   5937 not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())\n   5938 raise KeyError(f\"{not_found} not in index\")\n\nKeyError: \"None of [Index(['year', 'month'], dtype='object')] are in the [columns]\"\n\n\n\n\ndf\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n6.255330\n-0.020070\n\n\n1\n10.804867\n-0.920032\n\n\n2\n0.001716\n0.024965\n\n\n3\n4.534989\n-0.916051\n\n\n4\n2.201338\n0.776696\n\n\n...\n...\n...\n\n\n195\n13.979581\n1.115462\n\n\n196\n0.209274\n0.163526\n\n\n197\n3.515431\n-0.332839\n\n\n198\n9.251675\n0.161240\n\n\n199\n14.235245\n0.996049\n\n\n\n\n200 rows × 2 columns\n\n\n\n\ndf.average.plot()\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n/var/folders/nf/tcn1v_gx4pvc23db1_jd0_zr0000gq/T/ipykernel_44290/1265214205.py in ?()\n----&gt; 1 df.average.plot()\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/generic.py in ?(self, name)\n   5985             and name not in self._accessors\n   5986             and self._info_axis._can_hold_identifiers_and_holds_name(name)\n   5987         ):\n   5988             return self[name]\n-&gt; 5989         return object.__getattribute__(self, name)\n\nAttributeError: 'DataFrame' object has no attribute 'average'\n\n\n\n\ntrain_cutoff = 2000\ntrain = df[df.year &lt; train_cutoff]\ntest = df[df.year &gt;= train_cutoff]\ndf.average.plot()\n\ntrain.average.plot(color='blue')\ntest.average.plot(color='orange')\n\nlen(train), len(test)\n\n\nmonths_from_start = range(len(df))\nmonths_from_start = np.array(months_from_start).reshape(-1, 1)\n\n\n# use sk-learn for MLP\n\nmlp_model = MLPRegressor(hidden_layer_sizes=[512, 512, 512, 512, 512], max_iter = 5000)\nmlp_model.fit(months_from_start[:len(train)], train.average.values)\n\n\nyhat_train = mlp_model.predict(months_from_start[:len(train)])\nyhat_test = mlp_model.predict(months_from_start[len(train):])\n\nyhat_train = pd.Series(yhat_train, index=train.index)\nyhat_test = pd.Series(yhat_test, index=test.index)\n\ndf.average.plot()\nyhat_train.plot()\nyhat_test.plot()\n\n# Train error\ntrain_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\ntest_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\nplt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\n# normalize data\n\ntrain_scaled = (train - train.mean()) / train.std()\ntest_scaled = (test - test.mean()) / test.std()\nmonths_from_start_scaled = (months_from_start - months_from_start.mean()) / months_from_start.std()\n\n# train_scaled = (train - train.mean()) / train.std()\n# test_scaled = (test - test.mean()) / test.std()\n# months_from_start_scaled = (months_from_start - months_from_start.mean()) / months_from_start.std()\n\nmlp_model = MLPRegressor(hidden_layer_sizes=512, max_iter = 1000)\nmlp_model.fit(months_from_start_scaled[:len(train)], train_scaled.average.values)\n\nyhat_train = mlp_model.predict(months_from_start_scaled[:len(train)])\nyhat_test = mlp_model.predict(months_from_start_scaled[len(train):])\n\nyhat_train_scaled = pd.Series(yhat_train, index=train.index)\nyhat_test_scaled = pd.Series(yhat_test, index=test.index)\n\nyhat_train = yhat_train_scaled * train.std() + train.mean()\n# yhat_test = yhat_test_scaled * test.std() + test.mean()\n\n\ndf.average.plot()\nyhat_train.plot()\nyhat_test.plot()\n\n# Train error\ntrain_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\ntest_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\nplt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\nModel 2: Vanilla Linear Regression\n\nlr1 = LinearRegression()\nlr1.fit(months_from_start[:len(train)], train.average.values)\nyhat1_test = lr1.predict(months_from_start[len(train):])\nyhat1_train = lr1.predict(months_from_start[:len(train)])\n\nyhat_train = pd.Series(yhat1_train, index=train.index)\nyhat_test = pd.Series(yhat1_test, index=test.index)\n\ndf.average.plot()\nyhat_train.plot()\nyhat_test.plot()\n\n# Train error\ntrain_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\ntest_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\nplt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\n\nModel 3: Polynomial regression with degree d\n\ndef poly_regression(d, train, test):\n    months_from_start = range(len(df))\n    months_from_start = np.array(months_from_start).reshape(-1, 1)\n\n    lr = LinearRegression()\n    pf = PolynomialFeatures(degree=d)\n    X_train = pf.fit_transform(months_from_start[:len(train)])\n    X_test = pf.fit_transform(months_from_start[len(train):])\n    \n    lr.fit(X_train, train.average.values)\n    \n    yhat_test = lr.predict(X_test)\n    yhat_train = lr.predict(X_train)\n    \n\n    yhat_train = pd.Series(yhat_train, index=train.index)\n    yhat_test = pd.Series(yhat_test, index=test.index)\n\n    df.average.plot()\n    yhat_train.plot()\n    yhat_test.plot()\n    \n    # Train error\n    train_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\n    test_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\n    plt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\npoly_regression(2, train, test)\n\n\n\nModel 4: Linear Regression with sine and cosine basis functions\n\n### Adding sine and cosine terms\ndef sine_cosine_features(X, n):\n    \"\"\"\n    X: array of shape (n_samples, 1)\n    n: number of sine and cosine features to add\n    \"\"\"\n    for i in range(1, n+1):\n        X = np.hstack([X, np.sin(i*X), np.cos(i*X)])\n    return X\n\n\nX = np.linspace(-1, 1, 100).reshape(-1, 1)\n\n\n_ = plt.plot(X, sine_cosine_features(X, 0))\n\n\n\nModel 5: Gaussian basis functions\n\n\nModel 6: Linear Regression with polynomial and sine/cosine basis functions\n\ndef poly_sine_cosine_regression(n, train, test):\n    months_from_start = range(len(df))\n    months_from_start = np.array(months_from_start).reshape(-1, 1)\n\n    lr = LinearRegression()\n    \n    X_train = sine_cosine_features(months_from_start[:len(train)], n)\n    \n    X_test = sine_cosine_features(months_from_start[len(train):], n)\n    print(X_train.shape, X_test.shape)\n    \n    lr.fit(X_train, train.average.values)\n    \n    yhat_test = lr.predict(X_test)\n    yhat_train = lr.predict(X_train)\n    \n\n    yhat_train = pd.Series(yhat_train, index=train.index)\n    yhat_test = pd.Series(yhat_test, index=test.index)\n\n    \n    yhat_train.plot(alpha=0.2, lw=4)\n    yhat_test.plot(alpha=0.2, lw=4)\n    df.average.plot(color='k', lw=1)\n    \n    # Train error\n    train_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\n    test_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\n    plt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\npoly_sine_cosine_regression(6, train, test)\n\n\n\nModel 7: Random Fourier Features\n\ndef rff_featurise(X, n_components=100):\n    # Random Fourier Features\n    # https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html\n    from sklearn.kernel_approximation import RBFSampler\n    rbf_feature = RBFSampler(gamma=1, n_components=n_components, random_state=1)\n    X_features = rbf_feature.fit_transform(X)\n    return X_features\n\n\ndef poly_rff_regression(n, train, test):\n    months_from_start = range(len(df))\n    months_from_start = np.array(months_from_start).reshape(-1, 1)\n\n    lr = LinearRegression()\n    \n    X_train = rff_featurise(months_from_start[:len(train)], n)\n    \n    X_test = rff_featurise(months_from_start[len(train):], n)\n    print(X_train.shape, X_test.shape)\n    \n    lr.fit(X_train, train.average.values)\n    \n    yhat_test = lr.predict(X_test)\n    yhat_train = lr.predict(X_train)\n    \n\n    yhat_train = pd.Series(yhat_train, index=train.index)\n    yhat_test = pd.Series(yhat_test, index=test.index)\n\n    \n    yhat_train.plot(alpha=0.2, lw=4)\n    yhat_test.plot(alpha=0.2, lw=4)\n    df.average.plot(color='k', lw=1)\n    \n    # Train error\n    train_rmse = np.sqrt(np.mean((yhat_train - train.average)**2))\n    test_rmse = np.sqrt(np.mean((yhat_test - test.average)**2))\n    plt.title('Train RMSE: {:.2f}, Test RMSE: {:.2f}'.format(train_rmse, test_rmse))\n\n\npoly_rff_regression(440, train, test)"
  },
  {
    "objectID": "notebooks/anscombe.html",
    "href": "notebooks/anscombe.html",
    "title": "Anscombe’s Quartet",
    "section": "",
    "text": "Adapted from https://matplotlib.org/stable/gallery/specialty_plots/anscombe.html\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom latexify import latexify, format_axes\n\n\nlatexify(columns=2)\n\n\nx = [10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5]\ny1 = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\ny2 = [9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74]\ny3 = [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73]\nx4 = [8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8]\ny4 = [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89]\n\ndatasets = {\n    'I': (x, y1),\n    'II': (x, y2),\n    'III': (x, y3),\n    'IV': (x4, y4)\n}\n\nfig, axs = plt.subplots(2, 2, sharex=True, sharey=True,\n                        gridspec_kw={'wspace': 0.08, 'hspace': 0.08})\naxs[0, 0].set(xlim=(0, 20), ylim=(2, 14))\naxs[0, 0].set(xticks=(0, 10, 20), yticks=(4, 8, 12))\n\nfor ax, (label, (x, y)) in zip(axs.flat, datasets.items()):\n    ax.text(0.1, 0.9, label, fontsize=20, transform=ax.transAxes, va='top')\n    ax.tick_params(direction='in', top=True, right=True)\n    ax.plot(x, y, 'o')\n\n    # linear regression\n    p1, p0 = np.polyfit(x, y, deg=1)  # slope, intercept\n    ax.axline(xy1=(0, p0), slope=p1, color='r', lw=2)\n\n    # add text box for the statistics\n    stats = (f'$\\\\mu$ = {np.mean(y):.2f}\\n'\n             f'$\\\\sigma$ = {np.std(y):.2f}\\n'\n             f'$r$ = {np.corrcoef(x, y)[0][1]:.2f}')\n    bbox = dict(boxstyle='round', fc='blanchedalmond', ec='orange', alpha=0.5)\n    ax.text(0.95, 0.07, stats, fontsize=9, bbox=bbox,\n            transform=ax.transAxes, horizontalalignment='right')\n    #format_axes(ax)\n\nplt.savefig(\"../figures/anscombe.pdf\")"
  },
  {
    "objectID": "notebooks/logistic-regression-torch.html",
    "href": "notebooks/logistic-regression-torch.html",
    "title": "Logistic Regression Torch",
    "section": "",
    "text": "import numpy as np\nimport sklearn \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom latexify import *\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nclass LogisticRegression(nn.Module):\n    def __init__(self, input_dim):\n        super(LogisticRegression, self).__init__()\n        self.linear = nn.Linear(input_dim, 1)\n    \n    def forward(self, x):\n        logits = self.linear(x)\n        return logits\n\n\nfrom sklearn.datasets import make_moons\n\n\nX, y = make_moons(n_samples=100, noise=0.1)\n\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n\n\n\n\n\n\n\n\n\nlog_reg = LogisticRegression(2)\n\n\nlog_reg.linear.weight, log_reg.linear.bias\n\n(Parameter containing:\n tensor([[ 4.4356e-01, -2.6494e-04]], requires_grad=True),\n Parameter containing:\n tensor([0.3300], requires_grad=True))\n\n\n\nlog_reg(torch.tensor([1, 0.0]))\n\ntensor([0.7736], grad_fn=&lt;AddBackward0&gt;)\n\n\n\n# Predict with the model\n\ndef predict_plot_grid(model):\n    XX, YY = torch.meshgrid(torch.linspace(-2, 3, 100), torch.linspace(-2, 2, 100))\n    X_grid = torch.cat([XX.unsqueeze(-1), YY.unsqueeze(-1)], dim=-1)\n    logits = model(X_grid)\n    probs = torch.sigmoid(logits).reshape(100, 100)\n    plt.contourf(XX, YY, probs.detach().numpy(), levels=[0.0, 0.1, 0.2,0.3, 0.4,0.5, 0.6,0.7, 0.8,0.9, 1.0], \n                 cmap=plt.cm.Spectral, alpha=0.5)\n    plt.colorbar()\n    \n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n    \npredict_plot_grid(log_reg)\n    \n    \n\n/Users/nipun/mambaforge/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3484.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n\n\n\n\n\n\n\n\n\n\n\n\nopt = torch.optim.Adam(log_reg.parameters(), lr=0.01)\n\nconverged = False\nprev_loss = 1e8 \ni = 0\nwhile not converged:\n    opt.zero_grad()\n    logits = log_reg(torch.tensor(X, dtype=torch.float32))\n    loss = nn.BCEWithLogitsLoss()(logits, torch.tensor(y, dtype=torch.float32).view(-1, 1))\n    loss.backward()\n    opt.step()\n    if i%10==0:\n        print(i, loss.item())\n    if np.abs(prev_loss - loss.item()) &lt; 1e-5:\n        converged = True\n    prev_loss = loss.item() \n    i = i + 1\n\n0 0.6356857419013977\n10 0.594046413898468\n20 0.5578780174255371\n30 0.5273130536079407\n40 0.5019742846488953\n50 0.4811658561229706\n60 0.46408790349960327\n70 0.4499843716621399\n80 0.43821147084236145\n90 0.4282519817352295\n100 0.41970330476760864\n110 0.41225600242614746\n120 0.4056738317012787\n130 0.3997761607170105\n140 0.39442503452301025\n150 0.38951483368873596\n160 0.38496479392051697\n170 0.3807127773761749\n180 0.3767113983631134\n190 0.37292394042015076\n200 0.3693225383758545\n210 0.36588576436042786\n220 0.36259689927101135\n230 0.35944312810897827\n240 0.3564144968986511\n250 0.35350313782691956\n260 0.3507026731967926\n270 0.3480079174041748\n280 0.34541457891464233\n290 0.3429188132286072\n300 0.3405173122882843\n310 0.33820685744285583\n320 0.3359847366809845\n330 0.3338480293750763\n340 0.33179405331611633\n350 0.3298201858997345\n360 0.3279236853122711\n370 0.326102077960968\n380 0.3243526518344879\n390 0.3226730227470398\n400 0.3210606873035431\n410 0.3195130228996277\n420 0.3180277347564697\n430 0.3166024684906006\n440 0.3152349293231964\n450 0.3139227330684662\n460 0.3126639127731323\n470 0.31145623326301575\n480 0.3102976083755493\n490 0.3091861307621002\n500 0.30811992287635803\n510 0.30709704756736755\n520 0.30611586570739746\n530 0.3051745593547821\n540 0.30427151918411255\n550 0.30340519547462463\n560 0.3025740385055542\n570 0.30177661776542664\n580 0.3010115325450897\n590 0.3002774715423584\n600 0.2995731830596924\n610 0.298897385597229\n620 0.29824888706207275\n630 0.2976265847682953\n640 0.29702943563461304\n650 0.2964564263820648\n660 0.2959064245223999\n670 0.29537859559059143\n680 0.29487207531929016\n690 0.2943858802318573\n700 0.29391926527023315\n710 0.29347139596939087\n720 0.29304152727127075\n730 0.2926288843154907\n740 0.29223284125328064\n750 0.2918526232242584\n760 0.2914877235889435\n770 0.2911374270915985\n780 0.29080113768577576\n790 0.29047834873199463\n800 0.29016852378845215\n810 0.28987109661102295\n820 0.28958556056022644\n830 0.2893114984035492\n840 0.289048433303833\n850 0.28879591822624207\n860 0.28855353593826294\n870 0.2883208990097046\n880 0.2880975604057312\n890 0.2878831923007965\n900 0.28767746686935425\n910 0.28748005628585815\n920 0.2872906029224396\n930 0.28710871934890747\n940 0.2869342267513275\n950 0.28676676750183105\n960 0.286606103181839\n970 0.28645193576812744\n980 0.28630396723747253\n990 0.2861621081829071\n1000 0.2860259413719177\n1010 0.28589537739753723\n1020 0.285770058631897\n1030 0.28564995527267456\n1040 0.28553470969200134\n1050 0.28542420268058777\n1060 0.28531819581985474\n1070 0.2852165997028351\n\n\n\npredict_plot_grid(log_reg)\n\n\n\n\n\n\n\n\n\n# Iris dataset\nfrom sklearn.datasets import load_iris\niris = load_iris()\n\n\nX = iris.data\ny = iris.target\n\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nText(0, 0.5, 'Sepal width')\n\n\n\n\n\n\n\n\n\n\nclass ThreeClassLogisticRegression(nn.Module):\n    def __init__(self, input_dim):\n        super(ThreeClassLogisticRegression, self).__init__()\n        self.linear1 = nn.Linear(input_dim, 1)\n        self.linear2 = nn.Linear(input_dim, 1)\n        self.linear3 = nn.Linear(input_dim, 1)\n    \n    def forward(self, x):\n        logits1 = self.linear1(x) #x^T theta_1\n        logits2 = self.linear2(x) # x^T theta_2\n        logits3 = self.linear3(x) # x^T theta_3\n        return torch.cat([logits1, logits2, logits3], dim=-1)\n    \n\nclass MultiClassLogisticRegression(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(MultiClassLogisticRegression, self).__init__()\n        self.linear = nn.Linear(input_dim, num_classes)\n    \n    def forward(self, x):\n        logits = self.linear(x)\n        return logits\n\n\nmlr = ThreeClassLogisticRegression(2)\n\n\nX_tensor = torch.tensor(X, dtype=torch.float32)[:,:2]\ny_tensor = torch.tensor(y, dtype=torch.long)\n\n\nmlr(X_tensor).shape\n\ntorch.Size([150, 3])\n\n\n\nmlr_efficient = MultiClassLogisticRegression(2, 3)\nmlr_efficient(X_tensor).shape\n\ntorch.Size([150, 3])\n\n\n\nmlr_efficient(X_tensor[:5])\n\ntensor([[-1.0483, -4.8033,  1.9511],\n        [-1.0942, -4.4552,  1.9622],\n        [-0.9808, -4.4023,  1.8450],\n        [-0.9696, -4.2915,  1.8230],\n        [-0.9916, -4.7768,  1.8925]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\nnn.Softmax(dim=-1)(mlr_efficient(X_tensor[:5]))\n\ntensor([[0.0474, 0.0011, 0.9515],\n        [0.0449, 0.0016, 0.9536],\n        [0.0558, 0.0018, 0.9423],\n        [0.0576, 0.0021, 0.9403],\n        [0.0529, 0.0012, 0.9459]], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\n\nnn.Softmax(dim=-1)(mlr_efficient(X_tensor[:5])).sum(dim=-1)\n\ntensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], grad_fn=&lt;SumBackward1&gt;)\n\n\n\ndef plot_most_probable_class(model):\n    XX, YY = torch.meshgrid(torch.linspace(4, 8, 100), torch.linspace(1.5, 4.5, 100))\n    X_grid = torch.cat([XX.unsqueeze(-1), YY.unsqueeze(-1)], dim=-1)\n    logits = model(X_grid)\n    predicted_class = torch.argmax(logits, dim=-1)\n    plt.contourf(XX, YY, predicted_class.reshape(100, 100).detach().numpy(), levels=[-0.5, 0.5, 1.5, 2.5], \n                 cmap=plt.cm.Spectral, alpha=0.5)\n    plt.colorbar()\n\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n    \nplot_most_probable_class(mlr_efficient)\n\n\n\n\n\n\n\n\n\nlogits = mlr_efficient(X_tensor)\nnn.Softmax(dim=-1)(logits[:5])\n\ntensor([[0.0746, 0.8907, 0.0347],\n        [0.0938, 0.8669, 0.0393],\n        [0.0919, 0.8643, 0.0438],\n        [0.0976, 0.8560, 0.0464],\n        [0.0739, 0.8895, 0.0366]], grad_fn=&lt;SoftmaxBackward0&gt;)\n\n\n\ny_tensor\n\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2])\n\n\n\nopt = torch.optim.Adam(mlr_efficient.parameters(), lr=0.01)\n\nconverged = False\nprev_loss = 1e8\n\ni = 0\nwhile not converged:\n    opt.zero_grad()\n    logits = mlr_efficient(X_tensor)\n    loss = F.cross_entropy(logits, y_tensor)\n    loss.backward()\n    opt.step()\n    if i%10==0:\n        print(i, loss.item())\n    if np.abs(prev_loss - loss.item()) &lt; 1e-5:\n        converged = True\n    prev_loss = loss.item() \n    i = i + 1\n\n0 3.512922763824463\n10 2.396028518676758\n20 1.7443147897720337\n30 1.3343197107315063\n40 1.0142916440963745\n50 0.9684472680091858\n60 0.9662931561470032\n70 0.9421321749687195\n80 0.932521641254425\n90 0.9219393134117126\n100 0.9116792678833008\n110 0.9015797972679138\n120 0.8915135860443115\n130 0.8814629912376404\n140 0.8714943528175354\n150 0.8616196513175964\n160 0.8518762588500977\n170 0.84228515625\n180 0.8328675031661987\n190 0.8236393928527832\n200 0.8146136999130249\n210 0.8057998418807983\n220 0.7972047328948975\n230 0.7888327836990356\n240 0.7806861400604248\n250 0.7727659344673157\n260 0.7650708556175232\n270 0.7575994729995728\n280 0.7503483891487122\n290 0.7433139681816101\n300 0.7364917397499084\n310 0.7298766374588013\n320 0.7234635949134827\n330 0.7172466516494751\n340 0.7112202048301697\n350 0.7053780555725098\n360 0.6997144222259521\n370 0.6942230463027954\n380 0.6888980865478516\n390 0.6837335228919983\n400 0.6787236928939819\n410 0.673862636089325\n420 0.6691449284553528\n430 0.6645650863647461\n440 0.6601179838180542\n450 0.6557983160018921\n460 0.6516013145446777\n470 0.6475222110748291\n480 0.6435563564300537\n490 0.6396994590759277\n500 0.6359471082687378\n510 0.6322956085205078\n520 0.628740668296814\n530 0.6252787709236145\n540 0.6219063997268677\n550 0.6186199188232422\n560 0.6154161095619202\n570 0.6122919917106628\n580 0.6092444062232971\n590 0.6062706112861633\n600 0.6033677458763123\n610 0.6005332469940186\n620 0.5977646708488464\n630 0.5950595140457153\n640 0.592415452003479\n650 0.5898305177688599\n660 0.5873023271560669\n670 0.5848291516304016\n680 0.5824088454246521\n690 0.5800397992134094\n700 0.5777201056480408\n710 0.5754480957984924\n720 0.5732222199440002\n730 0.5710409283638\n740 0.5689027905464172\n750 0.5668062567710876\n760 0.5647502541542053\n770 0.5627332329750061\n780 0.560754120349884\n790 0.5588116645812988\n800 0.5569047331809998\n810 0.5550323128700256\n820 0.553193211555481\n830 0.5513865947723389\n840 0.5496113300323486\n850 0.547866702079773\n860 0.5461516976356506\n870 0.5444655418395996\n880 0.5428073406219482\n890 0.5411763787269592\n900 0.5395719408988953\n910 0.5379931926727295\n920 0.5364394187927246\n930 0.5349101424217224\n940 0.5334045886993408\n950 0.5319221615791321\n960 0.5304622650146484\n970 0.5290243625640869\n980 0.5276078581809998\n990 0.526212215423584\n1000 0.5248369574546814\n1010 0.5234816074371338\n1020 0.5221456289291382\n1030 0.5208286643028259\n1040 0.5195301175117493\n1050 0.5182497501373291\n1060 0.5169870257377625\n1070 0.5157415270805359\n1080 0.5145129561424255\n1090 0.5133009552955627\n1100 0.5121050477027893\n1110 0.5109249949455261\n1120 0.5097604990005493\n1130 0.5086110830307007\n1140 0.5074764490127563\n1150 0.5063564777374268\n1160 0.5052507519721985\n1170 0.5041589140892029\n1180 0.5030808448791504\n1190 0.5020161867141724\n1200 0.5009647011756897\n1210 0.4999260902404785\n1220 0.49890023469924927\n1230 0.4978867471218109\n1240 0.4968855381011963\n1250 0.49589625000953674\n1260 0.4949187636375427\n1270 0.49395284056663513\n1280 0.4929983615875244\n1290 0.49205493927001953\n1300 0.49112263321876526\n1310 0.4902009665966034\n1320 0.4892899990081787\n1330 0.48838943243026733\n1340 0.4874991476535797\n1350 0.48661890625953674\n1360 0.4857485890388489\n1370 0.48488810658454895\n1380 0.48403722047805786\n1390 0.4831957519054413\n1400 0.4823637306690216\n1410 0.4815407693386078\n1420 0.48072683811187744\n1430 0.47992178797721863\n1440 0.47912561893463135\n1450 0.47833800315856934\n1460 0.4775589108467102\n1470 0.4767882227897644\n1480 0.47602584958076477\n1490 0.4752714931964874\n1500 0.4745253026485443\n1510 0.47378697991371155\n1520 0.473056435585022\n1530 0.4723336398601532\n1540 0.4716183543205261\n1550 0.4709106385707855\n1560 0.4702102541923523\n1570 0.4695172607898712\n1580 0.4688314199447632\n1590 0.46815255284309387\n1600 0.46748092770576477\n1610 0.4668160378932953\n1620 0.46615809202194214\n1630 0.46550679206848145\n1640 0.46486222743988037\n1650 0.4642241895198822\n1660 0.46359261870384216\n1670 0.46296748518943787\n1680 0.4623486399650574\n1690 0.4617359936237335\n1700 0.4611296057701111\n1710 0.46052926778793335\n1720 0.4599350094795227\n1730 0.45934662222862244\n1740 0.45876413583755493\n1750 0.458187460899353\n1760 0.4576164782047272\n1770 0.45705124735832214\n1780 0.4564915597438812\n1790 0.45593738555908203\n1800 0.45538878440856934\n1810 0.45484548807144165\n1820 0.454307496547699\n1830 0.4537748694419861\n1840 0.45324742794036865\n1850 0.4527251124382019\n1860 0.45220789313316345\n1870 0.4516957998275757\n1880 0.4511885941028595\n1890 0.45068639516830444\n1900 0.4501889944076538\n1910 0.44969651103019714\n1920 0.44920867681503296\n1930 0.4487256407737732\n1940 0.4482472240924835\n1950 0.44777342677116394\n1960 0.4473041296005249\n1970 0.4468393921852112\n1980 0.4463789761066437\n1990 0.44592320919036865\n2000 0.4454716444015503\n2010 0.44502437114715576\n2020 0.44458135962486267\n2030 0.44414255023002625\n2040 0.4437079429626465\n2050 0.4432775378227234\n2060 0.44285109639167786\n2070 0.44242867827415466\n2080 0.4420103430747986\n2090 0.44159582257270813\n2100 0.4411852955818176\n2110 0.4407786428928375\n2120 0.44037577509880066\n2130 0.43997669219970703\n2140 0.43958136439323425\n2150 0.439189612865448\n2160 0.43880173563957214\n2170 0.43841731548309326\n2180 0.4380365014076233\n2190 0.43765926361083984\n2200 0.43728557229042053\n2210 0.43691518902778625\n2220 0.4365483224391937\n2230 0.4361847937107086\n2240 0.43582475185394287\n2250 0.435467928647995\n2260 0.43511438369750977\n2270 0.43476414680480957\n2280 0.43441712856292725\n2290 0.43407317996025085\n2300 0.4337325096130371\n2310 0.4333948791027069\n2320 0.43306025862693787\n2330 0.43272876739501953\n2340 0.43240025639533997\n2350 0.43207478523254395\n2360 0.4317522644996643\n2370 0.4314326047897339\n2380 0.4311158359050751\n2390 0.43080195784568787\n2400 0.4304908812046051\n2410 0.43018263578414917\n2420 0.4298771023750305\n2430 0.4295743405818939\n2440 0.4292742908000946\n2450 0.428976833820343\n2460 0.4286821484565735\n2470 0.4283899962902069\n2480 0.42810049653053284\n2490 0.4278135299682617\n2500 0.4275290369987488\n2510 0.42724716663360596\n2520 0.42696771025657654\n2530 0.4266907274723053\n2540 0.42641615867614746\n2550 0.426144003868103\n2560 0.4258742332458496\n2570 0.4256068468093872\n2580 0.42534172534942627\n2590 0.42507895827293396\n2600 0.4248184561729431\n2610 0.4245601296424866\n2620 0.4243040978908539\n2630 0.42405030131340027\n2640 0.42379865050315857\n2650 0.4235491156578064\n2660 0.4233017563819885\n2670 0.4230565130710602\n2680 0.4228133261203766\n2690 0.42257219552993774\n2700 0.42233312129974365\n2710 0.42209604382514954\n2720 0.42186105251312256\n2730 0.421627938747406\n2740 0.42139682173728943\n2750 0.42116764187812805\n2760 0.4209402799606323\n2770 0.4207148849964142\n2780 0.42049136757850647\n2790 0.420269638299942\n2800 0.4200498163700104\n2810 0.4198317527770996\n2820 0.4196154773235321\n2830 0.4194009304046631\n2840 0.4191882312297821\n2850 0.41897717118263245\n2860 0.4187678396701813\n2870 0.4185602068901062\n2880 0.41835424304008484\n2890 0.4181499481201172\n2900 0.4179472029209137\n2910 0.4177462160587311\n2920 0.41754665970802307\n2930 0.41734886169433594\n2940 0.4171524941921234\n2950 0.41695767641067505\n2960 0.41676443815231323\n2970 0.4165726900100708\n2980 0.41638243198394775\n2990 0.4161936044692993\n3000 0.4160062074661255\n3010 0.41582033038139343\n3020 0.415635883808136\n3030 0.41545283794403076\n3040 0.415271133184433\n3050 0.4150908291339874\n3060 0.4149118959903717\n3070 0.41473427414894104\n3080 0.4145580530166626\n3090 0.4143831133842468\n3100 0.4142094552516937\n3110 0.4140370488166809\n3120 0.41386595368385315\n3130 0.4136960804462433\n3140 0.4135274887084961\n3150 0.4133600890636444\n3160 0.413193941116333\n3170 0.4130289852619171\n3180 0.4128651022911072\n3190 0.4127025008201599\n3200 0.4125410318374634\n3210 0.4123806655406952\n3220 0.4122214615345001\n3230 0.4120633602142334\n3240 0.411906361579895\n3250 0.4117504358291626\n3260 0.4115956723690033\n3270 0.4114418923854828\n3280 0.41128915548324585\n3290 0.4111374616622925\n3300 0.4109868109226227\n3310 0.41083717346191406\n3320 0.41068848967552185\n3330 0.4105408489704132\n3340 0.41039419174194336\n3350 0.41024842858314514\n3360 0.4101036787033081\n3370 0.4099598526954651\n3380 0.4098169803619385\n3390 0.4096750020980835\n3400 0.40953394770622253\n3410 0.4093937873840332\n3420 0.4092545211315155\n3430 0.40911605954170227\n3440 0.4089786112308502\n3450 0.4088418781757355\n3460 0.40870606899261475\n3470 0.4085709750652313\n3480 0.4084368348121643\n3490 0.40830346941947937\n3500 0.4081708788871765\n3510 0.4080391824245453\n3520 0.4079081416130066\n3530 0.40777796506881714\n3540 0.4076485335826874\n3550 0.4075198173522949\n3560 0.40739184617996216\n3570 0.4072646200656891\n3580 0.40713822841644287\n3590 0.4070124626159668\n3600 0.40688738226890564\n3610 0.4067629873752594\n3620 0.4066393971443176\n3630 0.406516432762146\n3640 0.4063941240310669\n3650 0.4062725305557251\n3660 0.40615156292915344\n3670 0.4060312509536743\n3680 0.40591153502464294\n3690 0.4057925343513489\n3700 0.4056740999221802\n3710 0.405556321144104\n3720 0.4054391384124756\n3730 0.4053225815296173\n3740 0.4052066206932068\n3750 0.405091255903244\n3760 0.4049764573574066\n3770 0.4048621952533722\n3780 0.40474849939346313\n3790 0.4046354293823242\n3800 0.4045228958129883\n3810 0.40441086888313293\n3820 0.40429946780204773\n3830 0.40418851375579834\n3840 0.4040781259536743\n3850 0.4039682447910309\n3860 0.40385884046554565\n3870 0.4037500321865082\n3880 0.4036416709423065\n3890 0.40353381633758545\n3900 0.40342646837234497\n3910 0.4033195674419403\n3920 0.40321317315101624\n3930 0.40310725569725037\n3940 0.40300169587135315\n3950 0.4028966724872589\n3960 0.40279218554496765\n3970 0.40268802642822266\n3980 0.40258437395095825\n3990 0.40248122811317444\n4000 0.4023783504962921\n4010 0.40227600932121277\n4020 0.40217405557632446\n4030 0.4020724892616272\n4040 0.4019714295864105\n4050 0.4018706679344177\n\n\n\nmlr_efficient(X_tensor[:5])\n\ntensor([[ 3.5738, -1.9346, -3.4869],\n        [ 1.4138, -0.7382, -2.8186],\n        [ 3.6912, -1.3615, -3.6789],\n        [ 3.5621, -1.1532, -3.6723],\n        [ 4.7125, -2.2463, -3.9170]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\n\npred = F.softmax(mlr_efficient(X_tensor), dim=-1).detach().numpy()\npd.DataFrame(pred)\n\n\n\n\n\n\n\n\n0\n1\n2\n\n\n\n\n0\n0.995113\n0.004033\n0.000854\n\n\n1\n0.884354\n0.102808\n0.012838\n\n\n2\n0.993027\n0.006347\n0.000625\n\n\n3\n0.990414\n0.008871\n0.000714\n\n\n4\n0.998872\n0.000949\n0.000179\n\n\n...\n...\n...\n...\n\n\n145\n0.000119\n0.309663\n0.690218\n\n\n146\n0.000021\n0.511706\n0.488273\n\n\n147\n0.000446\n0.381773\n0.617781\n\n\n148\n0.092207\n0.416003\n0.491790\n\n\n149\n0.019944\n0.605274\n0.374782\n\n\n\n\n150 rows × 3 columns\n\n\n\n\nimport pandas as pd\n\n\ndf = pd.DataFrame(pred, columns=iris.target_names)\ndf[\"GT\"] = iris.target\ndf\n\n\n\n\n\n\n\n\nsetosa\nversicolor\nvirginica\nGT\n\n\n\n\n0\n0.995113\n0.004033\n0.000854\n0\n\n\n1\n0.884354\n0.102808\n0.012838\n0\n\n\n2\n0.993027\n0.006347\n0.000625\n0\n\n\n3\n0.990414\n0.008871\n0.000714\n0\n\n\n4\n0.998872\n0.000949\n0.000179\n0\n\n\n...\n...\n...\n...\n...\n\n\n145\n0.000119\n0.309663\n0.690218\n2\n\n\n146\n0.000021\n0.511706\n0.488273\n2\n\n\n147\n0.000446\n0.381773\n0.617781\n2\n\n\n148\n0.092207\n0.416003\n0.491790\n2\n\n\n149\n0.019944\n0.605274\n0.374782\n2\n\n\n\n\n150 rows × 4 columns\n\n\n\n\n# Find prediction\ny_pred = pred.argmax(axis=-1)\n\ndf[\"Predicted Class\"] = y_pred\n\n\ndf\n\n\n\n\n\n\n\n\nsetosa\nversicolor\nvirginica\nGT\nPredicted Class\n\n\n\n\n0\n0.995113\n0.004033\n0.000854\n0\n0\n\n\n1\n0.884354\n0.102808\n0.012838\n0\n0\n\n\n2\n0.993027\n0.006347\n0.000625\n0\n0\n\n\n3\n0.990414\n0.008871\n0.000714\n0\n0\n\n\n4\n0.998872\n0.000949\n0.000179\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n0.000119\n0.309663\n0.690218\n2\n2\n\n\n146\n0.000021\n0.511706\n0.488273\n2\n1\n\n\n147\n0.000446\n0.381773\n0.617781\n2\n2\n\n\n148\n0.092207\n0.416003\n0.491790\n2\n2\n\n\n149\n0.019944\n0.605274\n0.374782\n2\n1\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n# Accuracy\n(y_pred == iris.target).mean()\n\n0.8266666666666667\n\n\n\nplot_most_probable_class(mlr_efficient)\n\n\n\n\n\n\n\n\n\n# Add more features like x^2, xy, y^2\n\nX = iris.data\ny = iris.target\n\nX = np.concatenate([X, X**2, X[:, [0]]*X[:, [1]], X[:, [1]]**2], axis=-1)\n\nX_tensor = torch.tensor(X, dtype=torch.float32)\n\nmlr_efficient = MultiClassLogisticRegression(10, 3)\n\n\nopt = torch.optim.Adam(mlr_efficient.parameters(), lr=0.01)\n\nconverged = False\n\nprev_loss = 1e8\n\ni = 0\nwhile not converged:\n    opt.zero_grad()\n    logits = mlr_efficient(X_tensor)\n    loss = F.cross_entropy(logits, y_tensor)\n    loss.backward()\n    opt.step()\n    if i%10==0:\n        print(i, loss.item())\n    if np.abs(prev_loss - loss.item()) &lt; 1e-5:\n        converged = True\n    prev_loss = loss.item() \n    i = i + 1\n    \n\n0 14.176704406738281\n10 3.6011011600494385\n20 2.2202305793762207\n30 1.1102030277252197\n40 0.7572755217552185\n50 0.5920207500457764\n60 0.4846627414226532\n70 0.42518150806427\n80 0.3840698003768921\n90 0.3463784456253052\n100 0.3159296214580536\n110 0.29051730036735535\n120 0.26877450942993164\n130 0.25018322467803955\n140 0.23417198657989502\n150 0.22026310861110687\n160 0.208110511302948\n170 0.19741976261138916\n180 0.1879548579454422\n190 0.17952509224414825\n200 0.1719748079776764\n210 0.16517652571201324\n220 0.1590253710746765\n230 0.15343450009822845\n240 0.1483314335346222\n250 0.14365535974502563\n260 0.1393551379442215\n270 0.13538718223571777\n280 0.1317143738269806\n290 0.1283048838376999\n300 0.12513123452663422\n310 0.12216956913471222\n320 0.11939921230077744\n330 0.11680207401514053\n340 0.11436217278242111\n350 0.11206554621458054\n360 0.10989975929260254\n370 0.10785383731126785\n380 0.10591793060302734\n390 0.10408329963684082\n400 0.10234205424785614\n410 0.100687175989151\n420 0.09911229461431503\n430 0.09761164337396622\n440 0.0961800143122673\n450 0.09481271356344223\n460 0.0935053601861\n470 0.09225409477949142\n480 0.09105528891086578\n490 0.08990570902824402\n500 0.08880224823951721\n510 0.08774220943450928\n520 0.08672299981117249\n530 0.0857422798871994\n540 0.08479787409305573\n550 0.08388775587081909\n560 0.08301006257534027\n570 0.08216307312250137\n580 0.08134514838457108\n590 0.08055480569601059\n600 0.07979065924882889\n610 0.07905137538909912\n620 0.07833576202392578\n630 0.07764269411563873\n640 0.07697104662656784\n650 0.0763198509812355\n660 0.0756881982088089\n670 0.0750751718878746\n680 0.07447994500398636\n690 0.07390176504850388\n700 0.07333985716104507\n710 0.07279356569051743\n720 0.07226220518350601\n730 0.07174517214298248\n740 0.07124187797307968\n750 0.07075177878141403\n760 0.07027437537908554\n770 0.06980910897254944\n780 0.0693555623292923\n790 0.06891326606273651\n800 0.06848182529211044\n810 0.06806080043315887\n820 0.06764982640743256\n830 0.06724855303764343\n840 0.06685658544301987\n850 0.06647367775440216\n860 0.06609942764043808\n870 0.06573358923196793\n880 0.06537587195634842\n890 0.06502598524093628\n900 0.06468366086483002\n910 0.06434867531061172\n920 0.0640207827091217\n930 0.06369974464178085\n940 0.06338536739349365\n950 0.0630773976445198\n960 0.06277567893266678\n970 0.06247999891638756\n980 0.062190182507038116\n990 0.061906032264232635\n1000 0.06162742152810097\n1010 0.0613541342318058\n1020 0.06108605116605759\n1030 0.060823000967502594\n1040 0.06056485325098038\n1050 0.060311462730169296\n1060 0.060062702745199203\n1070 0.059818439185619354\n1080 0.0595785453915596\n1090 0.05934292450547218\n1100 0.059111423790454865\n1110 0.05888395383954048\n1120 0.05866041034460068\n1130 0.058440666645765305\n1140 0.05822468549013138\n1150 0.05801228806376457\n1160 0.0578034408390522\n1170 0.05759802460670471\n1180 0.057395968586206436\n1190 0.057197194546461105\n1200 0.05700158327817917\n1210 0.05680910125374794\n1220 0.056619662791490555\n1230 0.056433189660310745\n1240 0.05624958500266075\n1250 0.05606883391737938\n1260 0.055890828371047974\n1270 0.05571554973721504\n1280 0.05554285645484924\n1290 0.05537276715040207\n1300 0.05520516261458397\n1310 0.05504004284739494\n1320 0.05487731471657753\n1330 0.05471692979335785\n1340 0.054558854550123215\n1350 0.05440303310751915\n1360 0.054249413311481476\n1370 0.05409794673323631\n1380 0.05394855514168739\n1390 0.05380124971270561\n1400 0.053655948489904404\n1410 0.05351264774799347\n1420 0.05337127670645714\n1430 0.05323179438710213\n1440 0.05309417098760605\n1450 0.052958354353904724\n1460 0.05282432958483696\n1470 0.052692074328660965\n1480 0.052561502903699875\n1490 0.05243263393640518\n1500 0.05230540409684181\n1510 0.05217977985739708\n1520 0.052055761218070984\n1530 0.05193326622247696\n1540 0.051812317222356796\n1550 0.051692862063646317\n1560 0.05157487466931343\n1570 0.051458317786455154\n1580 0.0513431578874588\n1590 0.05122942104935646\n1600 0.05111704766750336\n1610 0.0510060079395771\n1620 0.05089626833796501\n1630 0.05078782141208649\n1640 0.05068064481019974\n1650 0.050574686378240585\n1660 0.05047000199556351\n1670 0.05036647617816925\n1680 0.050264161080121994\n1690 0.05016299709677696\n\n\n\n# Accuracy\npred = F.softmax(mlr_efficient(X_tensor), dim=-1).detach().numpy()\ny_pred = pred.argmax(axis=-1)\n(y_pred == iris.target).mean()\n\n0.9866666666666667"
  },
  {
    "objectID": "notebooks/classes-trees.html",
    "href": "notebooks/classes-trees.html",
    "title": "Basics of Classes and Plotting Trees",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport graphviz\n# Retina mode\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\nclass TreeNode:\n    def __init__(self, name, value=None, shape='rectangle'):\n        self.name = name\n        self.value = value\n        self.children = []\n        self.shape = shape\n\n    def add_child(self, child_node):\n        self.children.append(child_node)\n\n    def display_tree_text(self, level=0):\n        indent = \"  \" * level\n        print(f\"{indent}|- {self.name}: {self.value}\")\n        for child in self.children:\n            child.display_tree_text(level + 1)\n\n    def display_tree_graphviz(self, dot=None, parent_name=None, graph=None):\n        if graph is None:\n            graph = graphviz.Digraph(format='png')\n        graph.node(str(id(self)), str(self.name), shape=self.shape)\n\n        if parent_name is not None:\n            graph.edge(str(id(parent_name)), str(id(self)))\n\n        for child in self.children:\n            child.display_tree_graphviz(dot, self, graph)\n\n        return graph\n\n    def display_tree_directly(self):\n        graph = self.display_tree_graphviz()\n        src = graph.source\n        display(graphviz.Source(src, format='png'))\n\n\n# Creating nodes\nroot = TreeNode(\"Root\")\nchild1 = TreeNode(\"Child 1\")\nchild2 = TreeNode(\"Child 2\")\nchild3 = TreeNode(\"Child 3\")\n\n# Building the tree structure\nroot.add_child(child1)\nroot.add_child(child2)\nchild2.add_child(child3)\n\n\n# Displaying the tree in text format\nroot.display_tree_text()\n\n|- Root: None\n  |- Child 1: None\n  |- Child 2: None\n    |- Child 3: None\n\n\n\ngraph = root.display_tree_graphviz()\ngraph\n\n\n\n\n\n\n\n\n\nclass DecisionTreeNode:\n    def __init__(self, feature, threshold, decision=None, left=None, right=None, shape='box'):\n        self.feature = feature\n        self.threshold = threshold\n        self.decision = decision\n        self.left = left\n        self.right = right\n        self.shape = shape\n\n    def display_tree_graphviz(self, dot=None, parent_name=None, graph=None, edge_label=None):\n        if graph is None:\n            graph = graphviz.Digraph(format='png')\n\n        node_label = self.feature\n        \n        if self.threshold is not None:\n            node_label += f\" &lt;= {self.threshold}\"\n        \n        if self.decision is not None:\n            node_label += f\"\\nDecision: {self.decision}\"\n        \n        graph.node(str(id(self)), node_label, shape=self.shape)\n\n        if parent_name is not None:\n            if edge_label is not None:\n                graph.edge(str(id(parent_name)), str(id(self)), label=edge_label)\n            else:\n                graph.edge(str(id(parent_name)), str(id(self)))\n\n        if self.left is not None:\n            self.left.display_tree_graphviz(dot, self, graph, edge_label=\"True\")\n        if self.right is not None:\n            self.right.display_tree_graphviz(dot, self, graph, edge_label=\"False\")\n\n        return graph\n\n\nroot = DecisionTreeNode(\"Feature A\", 5.0, decision=None)\nleft_child = DecisionTreeNode(\"Feature B\", 3.0, decision=None)\nright_child = DecisionTreeNode(\"Feature C\", 8.0, decision=None)\nroot.left = left_child\nroot.right = right_child\n\nleft_left = DecisionTreeNode(\"\", None, decision = 20.0)\nleft_right = DecisionTreeNode(\"\", None, decision = 10.0)\n\nleft_child.left = left_left\nleft_child.right = left_right\n\nright_left = DecisionTreeNode(\"\", None, decision = 30.0)\nright_right = DecisionTreeNode(\"\", None, decision = 40.0)\n\nright_child.left = right_left\nright_child.right = right_right\n\n\nroot.display_tree_graphviz()"
  },
  {
    "objectID": "notebooks/cnn-edge.html",
    "href": "notebooks/cnn-edge.html",
    "title": "CNN Edge 2d",
    "section": "",
    "text": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\nimport pandas as pd\nimport matplotlib.pyplot as plt # for making figures\nimport seaborn as sns\n%matplotlib inline\n\n\n# Create a tensor of size 6x6 with first three columns as 1 and rest as 0\nx = torch.zeros(6, 6)\nx[:, :3] = 1\nprint(x)\n\ntensor([[1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.],\n        [1., 1., 1., 0., 0., 0.]])\n\n\n\nx.shape\n\ntorch.Size([6, 6])\n\n\n\n# Plot the tensor with equal aspect ratio\nplt.figure(figsize=(6, 6))\nsns.heatmap(x, cbar=False, xticklabels=False, yticklabels=False, cmap='gray', annot=True)\n\n\n\n\n\n\n\n\n\n# Create a 3x3 kernel with first column as 1, second as 0 and third as -1\n\nk = torch.tensor([[1, 0, -1], [1, 0, -1], [1, 0, -1]]).float()\nprint(k)\n\ntensor([[ 1.,  0., -1.],\n        [ 1.,  0., -1.],\n        [ 1.,  0., -1.]])\n\n\n\n# Apply the kernel to the image\n\n\ny = F.conv2d(x.view(1, 1, 6, 6), k.view(1, 1, 3, 3))\nprint(y)\n\n# Create figure of size of y\nplt.figure(figsize=(y.shape[2], y.shape[3]))\nsns.heatmap(y[0, 0], cbar=False, xticklabels=False, yticklabels=False, cmap='gray', annot=True)\n\ntensor([[[[0., 3., 3., 0.],\n          [0., 3., 3., 0.],\n          [0., 3., 3., 0.],\n          [0., 3., 3., 0.]]]])\n\n\n\n\n\n\n\n\n\n\nim = plt.imread('lm.jpeg')\nplt.imshow(im)\n\n\n\n\n\n\n\n\n\n# Crop to left 180 X 180 pixels\n\nim = im[:180, :180]\nplt.imshow(im, cmap='gray')\n\n\n\n\n\n\n\n\n\n# Convert to grayscale\nim = im.mean(axis=2)\nplt.imshow(im, cmap='gray')\n\n\n\n\n\n\n\n\n\nim.shape\n\n(180, 180)\n\n\n\n# Detect edges using our filter\n\nk = torch.tensor([[1, 0, -1], [1, 0, -1], [1, 0, -1]]).float()\n\n# Apply the kernel to the image\ny = F.conv2d(torch.tensor(im).float().view(1, 1, 180, 180), k.view(1, 1, 3, 3))\n\n\n\n# plot the result\n#plt.figure(figsize=(y.shape[2], y.shape[3]))\nplt.imshow(y[0, 0], cmap='gray')\n\n\n\n\n\n\n\n\n\n# Detect horizontal edges using our filter\n\nk = torch.tensor([[1, 0, -1], [1, 0, -1], [1, 0, -1]]).float().T\n\n# Apply the kernel to the image\ny = F.conv2d(torch.tensor(im).float().view(1, 1, 180, 180), k.view(1, 1, 3, 3))\nplt.imshow(y[0, 0], cmap='gray')"
  },
  {
    "objectID": "notebooks/logits-usage.html",
    "href": "notebooks/logits-usage.html",
    "title": "Why use logits",
    "section": "",
    "text": "import numpy as np\nimport sklearn \nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom latexify import *\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nimport torch\nimport torch.nn.functional as F\nimport ipywidgets as widgets\nfrom ipywidgets import interactive\nfrom IPython.display import display\n\n# Example ground truth probabilities\nground_truth_probs = torch.tensor([0.3, 0.7])\n\n# Example model predictions (logits)\nmodel_logits = torch.tensor([-2.0, 2.0])\n\n# Applying softmax to logits to get probabilities\nmodel_probs = F.sigmoid(model_logits)\n\n# Cross-entropy loss using probabilities\nloss_probs = F.binary_cross_entropy(model_probs, ground_truth_probs)\n\n# Cross-entropy loss using logits\nloss_logits = F.binary_cross_entropy_with_logits(model_logits, ground_truth_probs)\n\nprint(\"Loss using probabilities:\", loss_probs.item())\nprint(\"Loss using logits:\", loss_logits.item())\n\nLoss using probabilities: 0.7269279956817627\nLoss using logits: 0.7269280552864075\n\n\n\nl1 = widgets.FloatSlider(value=0.3, min=-300, max=300, step=0.01, description='Logits ex 1')\nl2 = widgets.FloatSlider(value=0.7, min=-300, max=300, step=0.01, description='Logits ex 2')\n\nbox = widgets.VBox([l1, l2])\n\n\ndef print_loss_using_both_methods(l1, l2):\n    logits = torch.tensor([l1, l2])\n    probs = F.sigmoid(logits)\n    loss_probs = F.binary_cross_entropy(probs, ground_truth_probs)\n    loss_logits = F.binary_cross_entropy_with_logits(logits, ground_truth_probs)\n    print(\"Loss using probabilities:\", loss_probs.item())\n    print(\"Loss using logits:\", loss_logits.item())\n\n\n# add interactivity\ninteractive(print_loss_using_both_methods, l1=l1, l2=l2)\n\n\n\n\n\nprint_loss_using_both_methods(l1.value, l2.value)\n\nLoss using probabilities: 8.003093719482422\nLoss using logits: 8.003093719482422\n\n\n\ndef our_sigmoid(z):\n    return 1/(1+torch.exp(-z))\n\n\nour_sigmoid(torch.tensor(-90.0))\n\ntensor(0.)\n\n\n\nF.binary_cross_entropy_with_logits?\n\nSignature:\nF.binary_cross_entropy_with_logits(\n    input: torch.Tensor,\n    target: torch.Tensor,\n    weight: Optional[torch.Tensor] = None,\n    size_average: Optional[bool] = None,\n    reduce: Optional[bool] = None,\n    reduction: str = 'mean',\n    pos_weight: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor\nDocstring:\nCalculate Binary Cross Entropy between target and input logits.\n\nSee :class:`~torch.nn.BCEWithLogitsLoss` for details.\n\nArgs:\n    input: Tensor of arbitrary shape as unnormalized scores (often referred to as logits).\n    target: Tensor of the same shape as input with values between 0 and 1\n    weight (Tensor, optional): a manual rescaling weight\n        if provided it's repeated to match input tensor shape\n    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n        the losses are averaged over each loss element in the batch. Note that for\n        some losses, there multiple elements per sample. If the field :attr:`size_average`\n        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n        when reduce is ``False``. Default: ``True``\n    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n        losses are averaged or summed over observations for each minibatch depending\n        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n        batch element instead and ignores :attr:`size_average`. Default: ``True``\n    reduction (str, optional): Specifies the reduction to apply to the output:\n        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n        ``'mean'``: the sum of the output will be divided by the number of\n        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n    pos_weight (Tensor, optional): a weight of positive examples to be broadcasted with target.\n        Must be a tensor with equal size along the class dimension to the number of classes.\n        Pay close attention to PyTorch's broadcasting semantics in order to achieve the desired\n        operations. For a target of size [B, C, H, W] (where B is batch size) pos_weight of\n        size [B, C, H, W] will apply different pos_weights to each element of the batch or\n        [C, H, W] the same pos_weights across the batch. To apply the same positive weight\n        along all spacial dimensions for a 2D multi-class target [C, H, W] use: [C, 1, 1].\n        Default: ``None``\n\nExamples::\n\n     &gt;&gt;&gt; input = torch.randn(3, requires_grad=True)\n     &gt;&gt;&gt; target = torch.empty(3).random_(2)\n     &gt;&gt;&gt; loss = F.binary_cross_entropy_with_logits(input, target)\n     &gt;&gt;&gt; loss.backward()\nFile:      ~/miniconda3/envs/ml/lib/python3.11/site-packages/torch/nn/functional.py\nType:      function"
  },
  {
    "objectID": "notebooks/decision-tree-real-input-discrete-output.html",
    "href": "notebooks/decision-tree-real-input-discrete-output.html",
    "title": "Decision Trees Real Input and Discrete Output",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom io import StringIO\n# Retina mode\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom latexify import latexify, format_axes\n\n\ndataset = \"\"\"Day,Temperature,PlayTennis\nD1,40,No\nD2,48,No\nD3,60,Yes\nD4,72,Yes\nD5,80,Yes\nD6,90,No\"\"\"\n\nf = StringIO(dataset)\ndf = pd.read_csv(f, sep=\",\")\n\n\ndf\n\n\n\n\n\n\n\n\nDay\nTemperature\nPlayTennis\n\n\n\n\n0\nD1\n40\nNo\n\n\n1\nD2\n48\nNo\n\n\n2\nD3\n60\nYes\n\n\n3\nD4\n72\nYes\n\n\n4\nD5\n80\nYes\n\n\n5\nD6\n90\nNo\n\n\n\n\n\n\n\n\nlatexify(columns=2)\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\ndt = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n\nX = df[['Temperature']]\ny = df['PlayTennis']\n\ndt.fit(X, y)\n\nDecisionTreeClassifier(criterion='entropy', max_depth=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(criterion='entropy', max_depth=1)\n\n\n\n# Function to strore the decision tree's graphviz representation as pdf\ndef save_decision_tree_as_pdf(dt, feature_names, class_names, filename):\n    dot_data = export_graphviz(dt, out_file=None, feature_names=feature_names, class_names=class_names, filled=True, rounded=True, special_characters=True)\n    graph = graphviz.Source(dot_data)\n    graph.render(filename, format='pdf')\n    \nsave_decision_tree_as_pdf(dt, ['Temperature'], ['No', 'Yes'], '../figures/decision-trees/real-ip-1')\n\n\ndt2 = DecisionTreeClassifier(criterion='entropy', max_depth=2)\n\ndt2.fit(X, y)\nsave_decision_tree_as_pdf(dt2, ['Temperature'], ['No', 'Yes'], '../figures/decision-trees/real-ip-2')\n\nHeavily borrowed and inspired from https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html\n\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(n_samples=300, centers=4,\n                  random_state=0, cluster_std=1.2)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap='rainbow')\nformat_axes(plt.gca())\nplt.xlabel(r\"$x_1$\")\nplt.ylabel(r\"$x_2$\")\n\nText(0, 0.5, '$x_2$')\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ndef visualize_tree(depth, X, y, ax=None, cmap='rainbow'):\n    model = DecisionTreeClassifier(max_depth=depth)\n    ax = ax or plt.gca()\n    print(model, depth)\n    \n    # Plot the training points\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap=cmap,\n               clim=(y.min(), y.max()), zorder=3)\n    ax.axis('tight')\n    format_axes(plt.gca())\n    plt.xlabel(r\"$x_1$\")\n    plt.ylabel(r\"$x_2$\")\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n    \n    # fit the estimator\n    model.fit(X, y)\n    xx, yy = np.meshgrid(np.linspace(*xlim, num=200),\n                         np.linspace(*ylim, num=200))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n    # Create a color plot with the results\n    n_classes = len(np.unique(y))\n    contours = ax.contourf(xx, yy, Z, alpha=0.2,\n                           levels=np.arange(n_classes + 1) - 0.5,\n                           cmap=cmap, clim=(y.min(), y.max()),\n                           zorder=1)\n\n    ax.set(xlim=xlim, ylim=ylim)\n    plt.tight_layout()\n    plt.savefig(f\"../figures/decision-trees/dt-{depth}.pdf\", bbox_inches=\"tight\")\n    plt.clf()\n\n\nfor depth in range(1, 11):\n    visualize_tree(depth, X, y)\n\nDecisionTreeClassifier(max_depth=1) 1\n\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n\n\nDecisionTreeClassifier(max_depth=2) 2\n\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n\n\nDecisionTreeClassifier(max_depth=3) 3\n\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n\n\nDecisionTreeClassifier(max_depth=4) 4\n\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n\n\nDecisionTreeClassifier(max_depth=5) 5\n\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n\n\nDecisionTreeClassifier(max_depth=6) 6\n\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n\n\nDecisionTreeClassifier(max_depth=7) 7\n\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n\n\nDecisionTreeClassifier(max_depth=8) 8\n\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n\n\nDecisionTreeClassifier(max_depth=9) 9\n\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n\n\nDecisionTreeClassifier(max_depth=10) 10\n\n\n/var/folders/1x/wmgn24mn1bbd2vgbqlk98tbc0000gn/T/ipykernel_73517/232881262.py:24: UserWarning: The following kwargs were not used by contour: 'clim'\n  contours = ax.contourf(xx, yy, Z, alpha=0.2,\n\n\n&lt;Figure size 400x246.914 with 0 Axes&gt;"
  },
  {
    "objectID": "notebooks/tips.html",
    "href": "notebooks/tips.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport time\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format='retina'\n\n\nM, N, K = 40, 30, 20\n\nA_np = np.random.randn(M, K)\nB_np = np.random.randn(K, N)\n\n\nA_np.shape\n\n(40, 20)\n\n\n\ntic = time.time()\nC_np = A_np@B_np\ntoc = time.time()\nprint(toc-tic)\n\n0.001062154769897461\n\n\n\ndef time_function(func, args=(), n_iter=10):\n    \"\"\"\n    Measures the execution time of a function over multiple iterations.\n\n    Parameters:\n    func (callable): The function to be timed.\n    args (tuple): The arguments to pass to the function.\n    n_iter (int): The number of iterations to run the function.\n\n    Returns:\n    np.ndarray: An array of execution times per iteration in seconds.\n    \"\"\"\n    timings = np.zeros(n_iter)\n\n    for i in range(n_iter):\n        start_time = time.time()\n        func(*args)\n        end_time = time.time()\n\n        timings[i] = end_time - start_time\n\n    return timings\n\n\ndef mm(A, B):\n  return A@B\n\n\nM, N, K = 1000, 1000, 1000\nA_np = np.random.randn(M, K)\nB_np = np.random.randn(K, N)\ntimings_numpy = time_function(mm, args=(A_np, B_np), n_iter=100)\n\n\ntimings_numpy\n\narray([0.016289  , 0.01717305, 0.0177319 , 0.01777315, 0.01613069,\n       0.01745892, 0.01651978, 0.01939702, 0.01579428, 0.01587319,\n       0.01608706, 0.01417089, 0.01438093, 0.01422   , 0.01515913,\n       0.01523018, 0.01510096, 0.01514196, 0.01523805, 0.02044392,\n       0.01564121, 0.01395583, 0.01647115, 0.01535106, 0.01428986,\n       0.01410389, 0.01599479, 0.01514792, 0.01513696, 0.01560783,\n       0.01368022, 0.01542401, 0.01463914, 0.01387072, 0.01423097,\n       0.0146029 , 0.0171392 , 0.01553369, 0.01486421, 0.01637197,\n       0.01645088, 0.01402497, 0.01469588, 0.01534796, 0.01405406,\n       0.01513219, 0.01508904, 0.01599312, 0.01427507, 0.01430011,\n       0.01630902, 0.01597834, 0.01514888, 0.01527619, 0.0153892 ,\n       0.01557493, 0.01491117, 0.01644635, 0.01448703, 0.01438189,\n       0.01430511, 0.01405096, 0.0156219 , 0.01632905, 0.01638031,\n       0.01555705, 0.01478577, 0.01565099, 0.0145731 , 0.0160861 ,\n       0.01578903, 0.01573801, 0.01575208, 0.01500297, 0.01475286,\n       0.01654625, 0.01642537, 0.01590681, 0.01389384, 0.01425624,\n       0.01581192, 0.01650596, 0.01550102, 0.01535511, 0.01536989,\n       0.01460409, 0.01639605, 0.01432776, 0.01465487, 0.01496196,\n       0.01525187, 0.01388502, 0.01564717, 0.01541686, 0.01616931,\n       0.01644707, 0.01413584, 0.01528502, 0.0147419 , 0.01503801])\n\n\n\nplt.plot(timings_numpy)\n\n\n\n\n\n\n\n\n\ndef mm_list(A, B):\n    C = [[0 for _ in range(len(B[0]))] for _ in range(len(A))]\n    for i in range(len(A)):\n        for j in range(len(B[0])):\n            for k in range(len(B)):\n                C[i][j] += A[i][k] * B[k][j]\n    return C\n\n\nA_list = A_np.tolist()\nB_list = B_np.tolist()\n\ntimings_list = time_function(mm_list, args=(A_list, B_list), n_iter=2)\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[15], line 4\n      1 A_list = A_np.tolist()\n      2 B_list = B_np.tolist()\n----&gt; 4 timings_list = time_function(mm_list, args=(A_list, B_list), n_iter=2)\n\nCell In[5], line 17, in time_function(func, args, n_iter)\n     15 for i in range(n_iter):\n     16     start_time = time.time()\n---&gt; 17     func(*args)\n     18     end_time = time.time()\n     20     timings[i] = end_time - start_time\n\nCell In[10], line 6, in mm_list(A, B)\n      4     for j in range(len(B[0])):\n      5         for k in range(len(B)):\n----&gt; 6             C[i][j] += A[i][k] * B[k][j]\n      7 return C\n\nKeyboardInterrupt: \n\n\n\n\n# use timeit to get a more accurate estimate along with the standard deviation\nimport timeit\n\ntime_numpy = timeit.timeit('mm(A_np, B_np)', globals=globals(), number=100)\n\n\n%timeit mm(A_np, B_np)\n\n15.5 ms ± 285 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
  },
  {
    "objectID": "notebooks/Linear Regression Notebook.html",
    "href": "notebooks/Linear Regression Notebook.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(5,5))\nx = [3,4,5,2,6]\ny = [25,35,39,20,41]\nplt.scatter(x,y)\nplt.xlabel(\"Height in feet\")\nplt.ylabel(\"Weight in KG\")\nplt.savefig(\"height-weight-scatterplot.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.savefig(\"scatterplot-2.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\n# plt.figure(fig)\nx = [0,1,2,3]\ny = [0,1,2,3]\nplt.scatter(x,y,label=\"Ordinary data\")\nplt.scatter([4],[0],label=\"Outlier\")\nplt.xlabel('x')\nplt.ylabel('y')\n# plt.legend(loc=(1.04,0)\nplt.legend()\nplt.savefig(\"scatterplot-3.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.linear_model import LinearRegression\n\n\nx = np.array(x).reshape((-1,1))\ny = np.array(y).reshape((-1,1))\nmodel = LinearRegression()\nmodel.fit(x,y)\nprediction = model.predict(x)\n\n\nplt.scatter(x,y)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.plot(x,prediction,label=\"Learnt Model\")\nfor i in range(len(x)):\n  plt.plot([x[i],x[i]],[prediction[i],y[i]],'r')\nplt.legend()\nplt.savefig(\"linear-fit.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n\nx\n\narray([-10.        ,  -9.7979798 ,  -9.5959596 ,  -9.39393939,\n        -9.19191919,  -8.98989899,  -8.78787879,  -8.58585859,\n        -8.38383838,  -8.18181818,  -7.97979798,  -7.77777778,\n        -7.57575758,  -7.37373737,  -7.17171717,  -6.96969697,\n        -6.76767677,  -6.56565657,  -6.36363636,  -6.16161616,\n        -5.95959596,  -5.75757576,  -5.55555556,  -5.35353535,\n        -5.15151515,  -4.94949495,  -4.74747475,  -4.54545455,\n        -4.34343434,  -4.14141414,  -3.93939394,  -3.73737374,\n        -3.53535354,  -3.33333333,  -3.13131313,  -2.92929293,\n        -2.72727273,  -2.52525253,  -2.32323232,  -2.12121212,\n        -1.91919192,  -1.71717172,  -1.51515152,  -1.31313131,\n        -1.11111111,  -0.90909091,  -0.70707071,  -0.50505051,\n        -0.3030303 ,  -0.1010101 ,   0.1010101 ,   0.3030303 ,\n         0.50505051,   0.70707071,   0.90909091,   1.11111111,\n         1.31313131,   1.51515152,   1.71717172,   1.91919192,\n         2.12121212,   2.32323232,   2.52525253,   2.72727273,\n         2.92929293,   3.13131313,   3.33333333,   3.53535354,\n         3.73737374,   3.93939394,   4.14141414,   4.34343434,\n         4.54545455,   4.74747475,   4.94949495,   5.15151515,\n         5.35353535,   5.55555556,   5.75757576,   5.95959596,\n         6.16161616,   6.36363636,   6.56565657,   6.76767677,\n         6.96969697,   7.17171717,   7.37373737,   7.57575758,\n         7.77777778,   7.97979798,   8.18181818,   8.38383838,\n         8.58585859,   8.78787879,   8.98989899,   9.19191919,\n         9.39393939,   9.5959596 ,   9.7979798 ,  10.        ])\n\n\n\nx[y==val]\n\narray([-2.12121212])\n\n\n\nval\n\n-2.3745682396702437\n\n\n\ny[x&lt;25]\n\narray([ 1.69351335,  1.47131924,  1.22371576,  0.9587681 ,  0.684928  ,\n        0.41069988,  0.14430802, -0.10662173, -0.33535303, -0.53629097,\n       -0.70518496, -0.83927443, -0.93737161, -0.99987837, -1.02873658,\n       -1.02731441, -1.00023337, -0.9531436 , -0.89245674, -0.82504765,\n       -0.7579375 , -0.69797133, -0.65150368, -0.62410537, -0.62030365,\n       -0.64336659, -0.69514097, -0.77595027, -0.88455735, -1.01819364,\n       -1.17265367, -1.34245142, -1.5210323 , -1.7010322 , -1.8745732 ,\n       -2.033584  , -2.17013196, -2.2767534 , -2.34676854, -2.37456824,\n       -2.35586079, -2.28786853, -2.16946611, -2.00125462, -1.7855683 ,\n       -1.52641324, -1.22934038, -0.90125763, -0.55018832, -0.18498567,\n        0.18498567,  0.55018832,  0.90125763,  1.22934038,  1.52641324,\n        1.7855683 ,  2.00125462,  2.16946611,  2.28786853,  2.35586079,\n        2.37456824,  2.34676854,  2.2767534 ,  2.17013196,  2.033584  ,\n        1.8745732 ,  1.7010322 ,  1.5210323 ,  1.34245142,  1.17265367,\n        1.01819364,  0.88455735,  0.77595027,  0.69514097,  0.64336659,\n        0.62030365,  0.62410537,  0.65150368,  0.69797133,  0.7579375 ,\n        0.82504765,  0.89245674,  0.9531436 ,  1.00023337,  1.02731441,\n        1.02873658,  0.99987837,  0.93737161,  0.83927443,  0.70518496,\n        0.53629097,  0.33535303,  0.10662173, -0.14430802, -0.41069988,\n       -0.684928  , -0.9587681 , -1.22371576, -1.47131924, -1.69351335])\n\n\n\nfunc([1.4])\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-69-a7647ebf0a8e&gt; in &lt;module&gt;\n----&gt; 1 func([1.4])\n\n&lt;ipython-input-66-27f8a49456fd&gt; in func(x)\n      1 def func(x):\n----&gt; 2     return np.sin(x) + np.sin(x/2) + np.sin(x/3)\n\nTypeError: unsupported operand type(s) for /: 'list' and 'int'"
  },
  {
    "objectID": "notebooks/ensemble-representation.html",
    "href": "notebooks/ensemble-representation.html",
    "title": "Representation of Ensemble Models",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nh = .02  # step size in the mesh\n\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2,\n                           random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [make_moons(noise=0.3, random_state=0),\n            make_circles(noise=0.2, factor=0.5, random_state=1),\n            ]\n\n\n\nfrom latexify import latexify\ntry:\n    latexify()\nexcept:\n    pass\n\n\ndef plot_comparison(maximum_depth):\n    names = [\"Decision Tree (Depth %d)\" %maximum_depth, \"Random Forest\"]\n    classifiers = [\n    DecisionTreeClassifier(max_depth=maximum_depth),\n    RandomForestClassifier(max_depth=maximum_depth, n_estimators=200, max_features=1),\n   ]\n    figure = plt.figure(figsize=(8, 4))\n    i = 1\n    # iterate over datasets\n    for ds_cnt, ds in enumerate(datasets):\n        # preprocess dataset, split into training and test part\n        X, y = ds\n        X = StandardScaler().fit_transform(X)\n        X_train, X_test, y_train, y_test = \\\n            train_test_split(X, y, test_size=.4, random_state=42)\n\n        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                            np.arange(y_min, y_max, h))\n\n        # just plot the dataset first\n        cm = plt.cm.RdBu\n        cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        if ds_cnt == 0:\n            ax.set_title(\"Input data\")\n        # Plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                edgecolors='k')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n                edgecolors='k')\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        i += 1\n\n        # iterate over classifiers\n        for name, clf in zip(names, classifiers):\n            ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n            clf.fit(X_train, y_train)\n            score = clf.score(X_test, y_test)\n\n            # Plot the decision boundary. For that, we will assign a color to each\n            # point in the mesh [x_min, x_max]x[y_min, y_max].\n            if hasattr(clf, \"decision_function\"):\n                Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n            else:\n                Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n            # Put the result into a color plot\n            Z = Z.reshape(xx.shape)\n            ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n            # Plot the training points\n            ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                    edgecolors='k')\n            # Plot the testing points\n            ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                    edgecolors='k', alpha=0.6)\n\n            ax.set_xlim(xx.min(), xx.max())\n            ax.set_ylim(yy.min(), yy.max())\n            ax.set_xticks(())\n            ax.set_yticks(())\n            if ds_cnt == 0:\n                ax.set_title(name)\n            ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                    size=15, horizontalalignment='right')\n            i += 1\n\n\n    plt.savefig(f\"../figures/ensemble/{str(maximum_depth)}-representation.pdf\" , transparent=True, bbox_inches=\"tight\")\n\n\nplot_comparison(2)\n\n\n\n\n\n\n\n\n\nplot_comparison(1)"
  },
  {
    "objectID": "gradient-descent/Gradient Descent.html",
    "href": "gradient-descent/Gradient Descent.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "#import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nsns.despine()\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-4-8cd18f7ee4aa&gt; in &lt;module&gt;\n----&gt; 1 sns.despine()\n\nNameError: name 'sns' is not defined\n\n\n\n\ninit_x = 2\ninit_y = 5\n\n\n%matplotlib notebook\n\n\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D  \n# Axes3D import has side effects, it enables using projection='3d' in add_subplot\nimport matplotlib.pyplot as plt\nimport random\n\ndef fun(x, y):\n    x = np.array(x)\n    y = np.array(y)\n    return 14+3*(x**2) + 14*(y**2) - 12*x- 28*y + 12*x*y\n\n\nlst_x = []\nlst_y = []\nx_ = init_x\ny_ = init_y\nalpha = 0.005\n\nlst_x.append(x_)\nlst_y.append(y_)\n\nfor i in range(10):\n\n    fig = plt.figure(figsize=(10,10))\n    ax = fig.add_subplot(111, projection='3d')\n    x = y = np.arange(-4.0, 4.0, 0.05)\n    X, Y = np.meshgrid(x, y)\n    zs = np.array(fun(np.ravel(X), np.ravel(Y)))\n    Z = zs.reshape(X.shape)\n    x_ = lst_x[-1]\n    y_ = lst_y[-1]\n#     ax.scatter3D(xdata, ydata, zdata, c=zdata, cmap='Greens')\n#     print (lst_x,lst_y,fun(lst_x,lst_y))\n    ax.scatter3D(lst_x,lst_y,fun(lst_x,lst_y),lw=10,alpha=1,cmap='hsv')\n    ax.plot_surface(X, Y, Z,color='orange',cmap='hsv')\n\n    ax.set_xlabel('X Label')\n    ax.set_ylabel('Y Label')\n    ax.set_zlabel('Z Label')\n    plt.title(\"Iteration \"+str(i+1))\n    lst_x.append(x_ - alpha * (3*x_ - 12 + 12*y_))\n    lst_y.append(y_  - alpha *(14*y_ -28 + 12*x_))\n    \n\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\nplt.plot(x,y)\nplt.title(\"Cost Function\")\n\nText(0.5, 1.0, 'Cost Function')\n\n\n\n\n\n\n\n\n\n\nplt.rcParams['axes.facecolor'] = '#fafafa'\n\n\n\np = 4.1\nalpha = 0.05\niterations = 20\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"iteration-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    \n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Gradient Descent}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-10.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-11.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-12.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-13.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-14.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-15.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-16.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-17.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-18.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-19.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Gradient Descent}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/iteration-20.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\ndef func(x):\n    return np.sin(x) + np.sin(x/2) + np.sin(x/3)\n\n\nfig, ax = plt.subplots()\nx = np.linspace(-10,10,100)\nx = x[x&lt;=0]\ny = func(x)\n\n\nval = -7.2\n\nplt.scatter([val],func(np.array([val])))\nax.annotate('local minima', xy=(val, func(val)), xytext=(val, 1),\n            arrowprops=dict(facecolor='black', shrink=0.05))\nplt.xticks([])\nplt.yticks([])\nplt.plot(x,y)\nplt.savefig(\"local-minima.eps\", format='eps',transparent=True)\n\n\n\n\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .95\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"overshooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Overshooting}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Overshooting}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/overshooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(-5,5,1000)\ny = x**2\n\np = 4.1\nalpha = .01\niterations = 10\n\n# for i in range(10):\nfor i in range(iterations):\n    plt.figure()\n    plt.plot(x,y)\n    prev = p\n    p = p - (alpha*2*p)\n    plt.arrow(prev,prev**2,p-prev,p**2-prev**2,head_width=0.5)\n    plt.scatter([prev],[prev**2],s=100)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cost\")\n    plt.title(\"Iteration \"+str(i+1)+\" (lr: \"+str(alpha)+\")\")\n    plt.savefig(\"undershooting-\"+str(i+1)+\".eps\", format='eps',transparent=True)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ns = \"\"\nfor i in range(iterations):\n    s+=\"\\\\begin{frame}{Slow Convergence}\\n\"\n    s+=\"  \\\\begin{center}\\n\"\n    s+=\"       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-\"+str(i+1)+\".eps}\\n\"\n    s+=\"   \\end{center}\\n\"\n    s+=\"\\end{frame}\\n\\n\"\n\n\nprint (s)\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-1.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-2.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-3.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-4.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-5.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-6.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-7.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-8.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-9.eps}\n   \\end{center}\n\\end{frame}\n\n\\begin{frame}{Slow Convergence}\n  \\begin{center}\n       \\includegraphics[totalheight=6cm]{gradient-descent/undershooting-10.eps}\n   \\end{center}\n\\end{frame}\n\n\n\n\n\nx = np.linspace(1,10,100)\ny = 1/x\nplt.plot(y,label=\"GD\")\nnoise = np.random.random((len(x)))\nnoise[0] = 0\nnoise[1] = 0\nnoise[2] = 0\nplt.plot(y+0.2*(noise-0.5),label=\"SGD\")\nplt.legend()\nplt.title(\"Iterations vs Cost\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Cost\")\nplt.savefig(\"gd-sgd.eps\", format='eps',transparent=True)\n\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\nThe PostScript backend does not support transparency; partially transparent artists will be rendered opaque.\n\n\n\n\n\n\n\n\n\n\nval = 4.1\nalpha = 0.05\n\nfor i in range(10):\n    val = val - alpha * 2* val\n    print (val)\n\n3.6899999999999995\n3.3209999999999997\n2.9888999999999997\n2.6900099999999996\n2.4210089999999997\n2.1789080999999997\n1.9610172899999996\n1.7649155609999996\n1.5884240048999996\n1.4295816044099996"
  },
  {
    "objectID": "cnn/visualisation.html",
    "href": "cnn/visualisation.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n%matplotlib inline\n\n\nsomeX, someY = 0.5, 0.5\nfig,ax = plt.subplots()\nax.set_aspect(\"equal\")\nax.add_patch(patches.Rectangle((0.5, 0.5), 0.1, 0.1,\n                      alpha=1, facecolor='gray'))\n\n\n\n\n\n\n\n\n\nimport sys\nsys.path.append(\"convnet-drawer/\")\n\n\nfrom convnet_drawer import Model, Conv2D, MaxPooling2D, Flatten, Dense\nfrom matplotlib_util import save_model_to_file\nchannel_scale = 1/5\n\nmodel = Model(input_shape=(32, 32, 1))\nmodel.add(Conv2D(6, (3, 3), (1, 1)))\nmodel.add(MaxPooling2D((3, 3), strides=(2, 2)))\nmodel.add(Conv2D(256, (5, 5), padding=\"same\"))\nmodel.add(MaxPooling2D((3, 3), strides=(2, 2)))\nmodel.add(Conv2D(384, (3, 3), padding=\"same\"))\nmodel.add(Conv2D(384, (3, 3), padding=\"same\"))\nmodel.add(Conv2D(256, (3, 3), padding=\"same\"))\nmodel.add(MaxPooling2D((3, 3), strides=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(4096))\nmodel.add(Dense(4096))\nmodel.add(Dense(1000))\n\n# save as svg file\nmodel.save_fig(\"example.svg\")\n\n\n\n# save via matplotlib\nsave_model_to_file(model, \"example.pdf\")\n\n\n\n\n\n\n\n\nmodel\n\n##### from mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef cuboid_data(o, size=(1,1,1)):\n    # code taken from\n    # https://stackoverflow.com/a/35978146/4124317\n    # suppose axis direction: x: to left; y: to inside; z: to upper\n    # get the length, width, and height\n    l, w, h = size\n    x = [[o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n         [o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n         [o[0], o[0] + l, o[0] + l, o[0], o[0]],  \n         [o[0], o[0] + l, o[0] + l, o[0], o[0]]]  \n    y = [[o[1], o[1], o[1] + w, o[1] + w, o[1]],  \n         [o[1], o[1], o[1] + w, o[1] + w, o[1]],  \n         [o[1], o[1], o[1], o[1], o[1]],          \n         [o[1] + w, o[1] + w, o[1] + w, o[1] + w, o[1] + w]]   \n    z = [[o[2], o[2], o[2], o[2], o[2]],                       \n         [o[2] + h, o[2] + h, o[2] + h, o[2] + h, o[2] + h],   \n         [o[2], o[2], o[2] + h, o[2] + h, o[2]],               \n         [o[2], o[2], o[2] + h, o[2] + h, o[2]]]               \n    return np.array(x), np.array(y), np.array(z)\n\ndef plotCubeAt(pos=(0,0,0), size=(1,1,1), ax=None,**kwargs):\n    # Plotting a cube element at position pos\n    if ax !=None:\n        X, Y, Z = cuboid_data( pos, size )\n        ax.plot_surface(X, Y, Z, rstride=1, cstride=1, **kwargs)\n\nsizes = [(32,32,1), (28, 28, 6), (14, 14, 6), (10, 10, 16), (5, 5, 16), (1, 120, 1)]\npositions = [(0, 0, 0)]*len(sizes)\nfor i in range(1, len(sizes)):\n    positions[i] = (positions[i-1][0] + sizes[i-1][0]+10, 0, 0)\ncolors = [\"grey\"]*len(sizes)\n\n\nfig = plt.figure()\nax = fig.gca(projection='3d')\nax.view_init(84, -90)\nax.set_aspect('equal')\nax.set_axis_off()\nax.set_xlabel('X')\nax.set_xlim(-5, positions[-1][0]+10)\nax.set_ylabel('Y')\nax.set_ylim(-1, 130)\nax.set_zlabel('Z')\nax.set_zlim(-1, 5)\n#ax.set_visible(False)\nfor p,s,c in zip(positions,sizes,colors):\n    plotCubeAt(pos=p, size=s, ax=ax, color=c)\nax.w_zaxis.line.set_lw(0.)\nax.set_zticks([])\n\nfor i in range(len(positions)):\n    ax.text(positions[i][0], -5, 0, \"X\".join(str(x) for x in sizes[i]), color='black', fontsize=4)\nfig.subplots_adjust(left=0, right=1, bottom=0, top=1)\nfig.tight_layout()\nplt.tight_layout()\nplt.savefig(\"lenet.pdf\", bbox_inches=\"tight\", transparent=True, dpi=600)"
  },
  {
    "objectID": "cnn/lenet.html",
    "href": "cnn/lenet.html",
    "title": "Figure out which ones we are getting wrong",
    "section": "",
    "text": "import tensorflow.keras as keras\nimport tensorflow.keras.layers as layers\n\nmodel = keras.Sequential()\nmodel.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(layers.AveragePooling2D())\nmodel.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\nmodel.add(layers.AveragePooling2D())\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(units=120, activation='relu'))\nmodel.add(layers.Dense(units=84, activation='relu'))\nmodel.add(layers.Dense(units=10, activation = 'softmax'))\n\n\nmodel\n\n&lt;tensorflow.python.keras.engine.sequential.Sequential at 0x7f96fa3cd4e0&gt;\n\n\n\nmodel.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfrom keras.datasets import mnist\nfrom keras import backend as K\n\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\n\nx_train.shape\n\n(60000, 28, 28, 1)\n\n\n\ny_train.shape\n\n(60000,)\n\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n%matplotlib inline\nplt.imshow(x_train[0][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\nplt.title(y_train[0])\n\nText(0.5, 1.0, '5')\n\n\n\n\n\n\n\n\n\n\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n\n\nEPOCHS = 10\nBATCH_SIZE = 128\n\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\n\n\nnum_classes = 10\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n\ny_train[0]\n\narray([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)\n\n\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/12\n60000/60000 [==============================] - 8s 138us/sample - loss: 0.3959 - accuracy: 0.8896 - val_loss: 0.1322 - val_accuracy: 0.9593\nEpoch 2/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.1192 - accuracy: 0.9637 - val_loss: 0.0776 - val_accuracy: 0.9744\nEpoch 3/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0831 - accuracy: 0.9751 - val_loss: 0.0646 - val_accuracy: 0.9789\nEpoch 4/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0668 - accuracy: 0.9804 - val_loss: 0.0557 - val_accuracy: 0.9822\nEpoch 5/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0575 - accuracy: 0.9826 - val_loss: 0.0455 - val_accuracy: 0.9853\nEpoch 6/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0501 - accuracy: 0.9849 - val_loss: 0.0484 - val_accuracy: 0.9846\nEpoch 7/12\n60000/60000 [==============================] - 6s 93us/sample - loss: 0.0423 - accuracy: 0.9870 - val_loss: 0.0431 - val_accuracy: 0.9854\nEpoch 8/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0371 - accuracy: 0.9886 - val_loss: 0.0365 - val_accuracy: 0.9876\nEpoch 9/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0339 - accuracy: 0.9894 - val_loss: 0.0430 - val_accuracy: 0.9868\nEpoch 10/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0320 - accuracy: 0.9902 - val_loss: 0.0406 - val_accuracy: 0.9870\nEpoch 11/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0271 - accuracy: 0.9915 - val_loss: 0.0360 - val_accuracy: 0.9891\nEpoch 12/12\n60000/60000 [==============================] - 6s 94us/sample - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.0367 - val_accuracy: 0.9881\nTest loss: 0.03666715431667981\nTest accuracy: 0.9881\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 0])\n\n\n\n\n\n\n\n\n\nplt.imshow(model.get_weights()[0][:, :, 0, 1])\n\n\n\n\n\n\n\n\n\nlen(model.get_weights())\n\n10\n\n\n\ne = model.layers[0]\n\n\ne.get_weights()[0]\n\n(3, 3, 1, 6)\n\n\n\ne.name\n\n'conv2d_3'\n\n\n\nw, b = model.get_layer(\"conv2d_3\").get_weights()\n\n\nimport pandas as pd\n\n\npd.Series(b).plot(kind='bar')\n\n\n\n\n\n\n\n\n\nimport seaborn as sns\nfig, ax = plt.subplots(ncols=6)\nfor i in range(6):\n    sns.heatmap(w[:, :, 0, i], ax=ax[i], annot=True)\n\n\n\n\n\n\n\n\n\nimport numpy as np\nnp.argmax(model.predict(x_test[0:1]))\n\n7\n\n\n\ntest_sample = 5\nplt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\npred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\nplt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\nText(0.5, 1.0, 'Predicted = 1, GT = 1')\n\n\n\n\n\n\n\n\n\n\nnp.argmax(model.predict(x_test[0:1])[0])\n\n7\n\n\n\npred_overall = np.argmax(model.predict(x_test), axis=1)\n\n\ngt_overall = np.argmax(y_test, axis=1)\n\n\nnp.where(np.not_equal(pred_overall, gt_overall))[0]\n\narray([ 247,  259,  321,  359,  445,  448,  449,  495,  582,  583,  625,\n        659,  684,  924,  947,  965, 1014, 1039, 1045, 1062, 1181, 1182,\n       1226, 1232, 1247, 1260, 1299, 1319, 1393, 1414, 1530, 1549, 1554,\n       1621, 1681, 1901, 1955, 1987, 2035, 2044, 2070, 2098, 2109, 2130,\n       2135, 2189, 2293, 2369, 2387, 2406, 2414, 2488, 2597, 2654, 2720,\n       2760, 2863, 2896, 2939, 2953, 2995, 3073, 3225, 3422, 3503, 3520,\n       3534, 3558, 3559, 3597, 3762, 3767, 3808, 3869, 3985, 4007, 4065,\n       4075, 4193, 4207, 4248, 4306, 4405, 4500, 4571, 4639, 4699, 4723,\n       4740, 4761, 4807, 4823, 5228, 5265, 5937, 5955, 5973, 6555, 6560,\n       6597, 6614, 6625, 6651, 6755, 6847, 7259, 7851, 7921, 8059, 8069,\n       8311, 8325, 8408, 9009, 9587, 9629, 9634, 9679, 9729])\n\n\n\npred_overall\n\narray([7, 2, 1, ..., 4, 5, 6])\n\n\n\ndef plot_prediction(test_sample):\n    plt.imshow(x_test[test_sample][:, :, 0], cmap=cm.get_cmap(\"Greys\"))\n    pred = np.argmax(model.predict(x_test[test_sample:test_sample+1])[0])\n    plt.title(\"Predicted = {}, GT = {}\".format(pred, np.argmax(y_test[test_sample])))\n\n\nplot_prediction(359)\n\n\n\n\n\n\n\n\n\nplot_prediction(9729)\n\n\n\n\n\n\n\n\n\nplot_prediction(9634)\n\n\n\n\n\n\n\n\n\n### Feature map\n\n\nfm_model = keras.Model(inputs=model.inputs, outputs=model.layers[2].output)\n\n\nmodel.summary()\n\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n_________________________________________________________________\naverage_pooling2d_3 (Average (None, 5, 5, 16)          0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 400)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 120)               48120     \n_________________________________________________________________\ndense_4 (Dense)              (None, 84)                10164     \n_________________________________________________________________\ndense_5 (Dense)              (None, 10)                850       \n=================================================================\nTotal params: 60,074\nTrainable params: 60,074\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.summary()\n\nModel: \"model_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_3_input (InputLayer)  [(None, 28, 28, 1)]       0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 26, 26, 6)         60        \n_________________________________________________________________\naverage_pooling2d_2 (Average (None, 13, 13, 6)         0         \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 11, 11, 16)        880       \n=================================================================\nTotal params: 940\nTrainable params: 940\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\nfm_model.predict(x_test[test_sample:test_sample+1]).shape\n\n(1, 11, 11, 16)\n\n\n\ntest_sample = 88\nfm_1 = fm_model.predict(x_test[test_sample:test_sample+1])[0, :, :, :]\n\n\nfig, ax = plt.subplots(ncols=16, figsize=(20, 4))\nfor i in range(16):\n    ax[i].imshow(fm_1[:, :, i], cmap=\"Greys\")"
  },
  {
    "objectID": "cnn/convolution-operation.html",
    "href": "cnn/convolution-operation.html",
    "title": "Machine Learning Resources",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom matplotlib import patches\n\n\ninp = np.random.choice(range(10), (5, 5))\nfilter_conv = np.array([\n    [1, 0, -1],\n    [1, 0, -1],\n    [1, 0, -1]\n])\n\n\nplt.imshow(inp, cmap='Greys')\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(ncols=3, figsize=(12, 4))\n\nsns.heatmap(inp, annot=True, cbar=None, ax=ax[0], cmap='Purples')\nsns.heatmap(filter_conv, annot=True, cbar=None, ax=ax[1], cmap='Purples')\ng = ax[0]\nrect = patches.Rectangle((0,0),3,3,linewidth=5,edgecolor='grey',facecolor='black', alpha=0.5)\n\n# Add the patch to the Axes\ng.add_patch(rect)\n\nax[0].set_title(\"Input\")\nax[1].set_title(\"Filter\")\n\nText(0.5, 1.0, 'Filter')\n\n\n\n\n\n\n\n\n\n\nfrom scipy.signal import convolve2d\n\n\nconvolve2d(inp, filter_conv, mode='valid')\n\narray([[ 2, -3, -4],\n       [ 4,  8, -9],\n       [ 0, 14, -1]])\n\n\n\n&gt;&gt;&gt; from scipy import signal\n&gt;&gt;&gt; from scipy.misc import lena as lena\n\n&gt;&gt;&gt; scharr = np.array([[ -3-3j, 0-10j,  +3 -3j],\n...                    [-10+0j, 0+ 0j, +10 +0j],\n...                    [ -3+3j, 0+10j,  +3 +3j]]) # Gx + j*Gy\n&gt;&gt;&gt; grad = signal.convolve2d(lena, scharr, boundary='symm', mode='same')\n\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n&lt;ipython-input-37-70a7c69e1898&gt; in &lt;module&gt;\n      1 from scipy import signal\n----&gt; 2 from scipy.misc import lena as lena\n      3 \n      4 scharr = np.array([[ -3-3j, 0-10j,  +3 -3j],\n      5                    [-10+0j, 0+ 0j, +10 +0j],\n\nImportError: cannot import name 'lena' from 'scipy.misc' (/home/nipunbatra-pc/anaconda3/lib/python3.7/site-packages/scipy/misc/__init__.py)\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mticker\nimport matplotlib.animation\n\n#####################\n# Array preparation\n#####################\n\n#input array\nn = 6\np = 1\na = np.random.randint(0, 5, size=(n, n))\n# kernel\nkernel = np.array([[ 1,0, -1], [1, 0,-1], [ 1,0, -1]])\nf = kernel.shape[0]\n\npadding = True\n\nif padding:\n    # visualization array (2 bigger in each direction)\n    va = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p), dtype=int)\n    va[p:-p,p:-p] = a\n    va_color = np.zeros((a.shape[0]+2*p, a.shape[1]+2*p)) \n    va_color[p:-p,p:-p] = 0.5\nelse:\n    va = a\n    va_color = np.zeros_like(a)\n\n#output array\nres = np.zeros((n-f+1+2*p, n-f+1+2*p))\n\n\n\n#####################\n# Create inital plot\n#####################\nfig = plt.figure(figsize=(8,4))\n\ndef add_axes_inches(fig, rect):\n    w,h = fig.get_size_inches()\n    return fig.add_axes([rect[0]/w, rect[1]/h, rect[2]/w, rect[3]/h])\n\naxwidth = 3.\ncellsize = axwidth/va.shape[1]\naxheight = cellsize*va.shape[0]\n\nax_va  = add_axes_inches(fig, [cellsize, cellsize, axwidth, axheight])\nax_kernel  = add_axes_inches(fig, [cellsize*2+axwidth,\n                                   (2+res.shape[0])*cellsize-kernel.shape[0]*cellsize,\n                                   kernel.shape[1]*cellsize,  \n                                   kernel.shape[0]*cellsize])\nax_res = add_axes_inches(fig, [cellsize*3+axwidth+kernel.shape[1]*cellsize,\n                               2*cellsize, \n                               res.shape[1]*cellsize,  \n                               res.shape[0]*cellsize])\nax_kernel.set_title(\"Kernel\", size=12)\n\nim_va = ax_va.imshow(va_color, vmin=0., vmax=1.3, cmap=\"Blues\")\nax_va.set_title(\"Image size: {}X{}\\n Padding: {}\".format(n, n, p))\nfor i in range(va.shape[0]):\n    for j in range(va.shape[1]):\n        ax_va.text(j,i, va[i,j], va=\"center\", ha=\"center\")\n\nax_kernel.imshow(np.zeros_like(kernel), vmin=-1, vmax=1, cmap=\"Pastel1\")\nfor i in range(kernel.shape[0]):\n    for j in range(kernel.shape[1]):\n        ax_kernel.text(j,i, kernel[i,j], va=\"center\", ha=\"center\")\n\n\nim_res = ax_res.imshow(res, vmin=0, vmax=1.3, cmap=\"Greens\")\nres_texts = []\nfor i in range(res.shape[0]):\n    row = []\n    for j in range(res.shape[1]):\n        row.append(ax_res.text(j,i, \"\", va=\"center\", ha=\"center\"))\n    res_texts.append(row)    \n\nax_res.set_title(\"Output size: {}X{}\".format(n+2*p-f+1, n+2*p-f+1))\n\nfor ax  in [ax_va, ax_kernel, ax_res]:\n    ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n    ax.yaxis.set_major_locator(mticker.IndexLocator(1,0))\n    ax.xaxis.set_major_locator(mticker.IndexLocator(1,0))\n    ax.grid(color=\"k\")\n\n###############\n# Animation\n###############\ndef init():\n    for row in res_texts:\n        for text in row:\n            text.set_text(\"\")\n\ndef animate(ij):\n    i,j=ij\n    o = kernel.shape[1]//2\n    # calculate result\n    \n   \n    res_ij = (kernel*va[1+i-o:1+i+o+1, 1+j-o:1+j+o+1]).sum()\n    \n    res_texts[i][j].set_text(res_ij)\n    # make colors\n    c = va_color.copy()\n    c[1+i-o:1+i+o+1, 1+j-o:1+j+o+1] = 1.\n    im_va.set_array(c)\n\n    r = res.copy()\n    r[i,j] = 1\n    im_res.set_array(r)\n    \n\n\ni,j = np.indices(res.shape)\nani = matplotlib.animation.FuncAnimation(fig, animate, init_func=init, \n                                         frames=zip(i.flat, j.flat), interval=5)\nani.save(\"algo.gif\", writer=\"imagemagick\")\n\n\n\n\n\n\n\n\n\nva\n\narray([[0, 2, 2, 2, 0],\n       [4, 3, 0, 2, 2],\n       [1, 3, 3, 4, 2],\n       [3, 0, 0, 0, 2],\n       [0, 3, 4, 2, 3]])\n\n\n\ni\n\narray([[0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1],\n       [2, 2, 2, 2, 2],\n       [3, 3, 3, 3, 3],\n       [4, 4, 4, 4, 4]])\n\n\n\ni = 0\nj = 3\no =kernel.shape[1]//2\n(kernel*va[1+i-o:1+i+o+1, 1+j-o:1+j+o+1])\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-127-a06f9a5b2501&gt; in &lt;module&gt;\n      2 j = 3\n      3 o =kernel.shape[1]//2\n----&gt; 4 (kernel*va[1+i-o:1+i+o+1, 1+j-o:1+j+o+1])\n\nValueError: operands could not be broadcast together with shapes (3,3) (3,2) \n\n\n\n\n(kernel)\n\narray([[ 1,  0, -1],\n       [ 1,  0, -1],\n       [ 1,  0, -1]])\n\n\n\nva[1+i-o:1+i+o+1, 1+j-o:1+j+o+1]\n\narray([[3, 2],\n       [2, 1],\n       [3, 0]])"
  }
]
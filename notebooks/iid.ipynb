{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": "---\nauthor: Nipun Batra\ntitle: Independent and Identically Distributed (i.i.d) Random Variables\ndescription: Understanding the concept of independent and identically distributed random variables, their properties, and applications in probability theory and statistics\ncategories:\n  - Probability\n  - Statistics\n  - Random Variables\n  - Mathematics\nkeywords: [independent, identically distributed, iid, random variables, joint probability, normal distribution]\ndate: '2025-03-17'\nbadges: true\ntoc: true\n---"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "# Retina mode\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Independent and Identically Distributed (i.i.d) Random Variables\n\n## Introduction\n\nIndependent and Identically Distributed (i.i.d) random variables are fundamental building blocks in probability theory and statistics. This concept forms the theoretical foundation for many statistical methods, from simple sampling to complex machine learning algorithms. When we say random variables are i.i.d, we mean two crucial things: they are **independent** (the outcome of one doesn't affect another) and **identically distributed** (they all follow the same probability distribution).\n\nUnderstanding i.i.d random variables is essential for:\n- Statistical inference and hypothesis testing\n- The Law of Large Numbers and Central Limit Theorem\n- Monte Carlo simulations\n- Machine learning model assumptions\n- Data sampling and experimental design\n\n## Learning Objectives\n\nBy the end of this notebook, you will be able to:\n\n1. **Define** independence and identical distribution for random variables\n2. **Compute** joint probability density functions for i.i.d random variables\n3. **Apply** the multiplication rule for independent random variables\n4. **Recognize** when the i.i.d assumption is appropriate in real-world scenarios\n5. **Implement** simulations involving i.i.d random variables using Python\n6. **Analyze** the properties and implications of i.i.d assumptions\n\n## Theoretical Background\n\n### Independence of Random Variables\n\nTwo random variables $X_1$ and $X_2$ are **independent** if:\n\n$$P(X_1 = x_1, X_2 = x_2) = P(X_1 = x_1) \\cdot P(X_2 = x_2)$$\n\nFor continuous random variables, this becomes:\n\n$$f_{X_1,X_2}(x_1, x_2) = f_{X_1}(x_1) \\cdot f_{X_2}(x_2)$$\n\nwhere $f_{X_1,X_2}(x_1, x_2)$ is the joint probability density function.\n\n### Identical Distribution\n\nRandom variables are **identically distributed** if they have the same probability distribution. This means:\n- Same probability density function (PDF) or probability mass function (PMF)\n- Same parameters (mean, variance, etc.)\n- Same support (the set of possible values)\n\n### The i.i.d Property\n\nWhen random variables $X_1, X_2, \\ldots, X_n$ are i.i.d:\n\n1. **Independence**: $f_{X_1,\\ldots,X_n}(x_1,\\ldots,x_n) = \\prod_{i=1}^n f_{X_i}(x_i)$\n2. **Identical Distribution**: $f_{X_1}(x) = f_{X_2}(x) = \\cdots = f_{X_n}(x) = f(x)$\n\nCombined: $f_{X_1,\\ldots,X_n}(x_1,\\ldots,x_n) = \\prod_{i=1}^n f(x_i)$\n\n---\n\n## Practical Implementation\n\nLet's explore these concepts through computational examples.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Example 1: Two Independent Normal Random Variables\n\nWe'll create two independent normal random variables, both following $N(0,1)$ (standard normal distribution). Since they have the same distribution parameters and are independent, they are i.i.d.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Computing Individual Probabilities\n\nFor independent random variables, we can compute their individual probability densities separately:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = torch.distributions.Normal(0, 1)\n",
    "X2 = torch.distributions.Normal(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Computing Joint Probability for i.i.d Variables\n\nFor i.i.d random variables, the joint probability density is the product of individual densities:\n\n$$f_{X_1,X_2}(x_1, x_2) = f_{X_1}(x_1) \\cdot f_{X_2}(x_2)$$\n\nLet's verify this with our example:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Summary and Key Takeaways\n\n### What We've Learned:\n\n1. **Definition**: i.i.d random variables are both **independent** (outcomes don't affect each other) and **identically distributed** (same probability distribution)\n\n2. **Mathematical Property**: For i.i.d variables $X_1, \\ldots, X_n$:\n   $$f_{X_1,\\ldots,X_n}(x_1,\\ldots,x_n) = \\prod_{i=1}^n f(x_i)$$\n\n3. **Visual Indicators**:\n   - Zero correlation between variables (independence)\n   - Identical marginal distributions (identical distribution)\n   - Circular scatter plots for bivariate normal i.i.d variables\n\n4. **Practical Importance**:\n   - Foundation for statistical inference\n   - Enables the Law of Large Numbers\n   - Assumption in many machine learning algorithms\n   - Critical for sampling theory\n\n### Key Connections to Broader Concepts:\n\n- **Law of Large Numbers**: Sample means of i.i.d variables converge to population mean\n- **Central Limit Theorem**: Sums of i.i.d variables approach normal distribution\n- **Statistical Inference**: Many hypothesis tests assume i.i.d observations\n- **Machine Learning**: Training examples are often assumed to be i.i.d\n- **Monte Carlo Methods**: Rely on i.i.d random sampling\n\n### When to Question i.i.d Assumptions:\n\n- Time series data (autocorrelation)\n- Spatial data (spatial correlation)  \n- Clustered data (within-cluster correlation)\n- Sequential learning (changing distributions)\n- Measurement instruments (systematic errors)\n\nUnderstanding i.i.d random variables provides the foundation for advanced topics in probability, statistics, and machine learning. This concept bridges theoretical probability with practical data analysis applications.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example 4: Simulating Coin Flips (Classic i.i.d Example)\ntorch.manual_seed(42)  # For reproducibility\n\n# Simulate 1000 coin flips (Bernoulli random variables)\nn_flips = 1000\np_heads = 0.5  # Fair coin\n\n# Each flip is an i.i.d Bernoulli(0.5) random variable\nflips = torch.distributions.Bernoulli(p_heads).sample((n_flips,))\n\n# Plot results\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# 1. Sequence of flips (first 100)\naxes[0].plot(range(100), flips[:100].numpy(), 'o-', markersize=3, alpha=0.7)\naxes[0].set_title('First 100 Coin Flips\\n(0=Tails, 1=Heads)')\naxes[0].set_xlabel('Flip Number')\naxes[0].set_ylabel('Outcome')\naxes[0].set_ylim(-0.1, 1.1)\naxes[0].grid(True, alpha=0.3)\n\n# 2. Running proportion of heads\ncumulative_heads = torch.cumsum(flips, dim=0)\nproportion_heads = cumulative_heads / torch.arange(1, n_flips + 1)\n\naxes[1].plot(range(1, n_flips + 1), proportion_heads.numpy(), 'b-', alpha=0.7)\naxes[1].axhline(y=0.5, color='red', linestyle='--', label='True probability (0.5)')\naxes[1].set_title('Running Proportion of Heads\\n(Converges to true probability)')\naxes[1].set_xlabel('Number of Flips')\naxes[1].set_ylabel('Proportion of Heads')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\n# 3. Histogram of outcomes\naxes[2].hist(flips.numpy(), bins=[-0.25, 0.25, 0.75, 1.25], alpha=0.7, \n             density=True, rwidth=0.8)\naxes[2].set_title(f'Distribution of Outcomes\\n({int(flips.sum())} heads, {n_flips - int(flips.sum())} tails)')\naxes[2].set_xlabel('Outcome')\naxes[2].set_ylabel('Probability')\naxes[2].set_xticks([0, 1])\naxes[2].set_xticklabels(['Tails', 'Heads'])\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Final proportion of heads: {proportion_heads[-1]:.4f}\")\nprint(f\"Expected proportion: {p_heads}\")\nprint(f\"Difference from expected: {abs(proportion_heads[-1] - p_heads):.4f}\")\nprint(\"\\nThis demonstrates the Law of Large Numbers:\")\nprint(\"As n increases, the sample proportion converges to the true probability.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Real-World Applications and When i.i.d Assumptions Hold\n\n### Common Examples of i.i.d Random Variables:\n\n1. **Coin Flips**: Each flip is independent of previous flips and has the same probability distribution\n2. **Measurement Errors**: In well-controlled experiments, measurement errors are often i.i.d\n3. **Random Sampling**: Drawing samples with replacement from a population\n4. **Manufacturing Quality**: Products from a stable manufacturing process\n5. **Network Packet Arrivals**: In some network models\n\n### When i.i.d Assumptions Break Down:\n\n1. **Time Series Data**: Today's stock price depends on yesterday's price (not independent)\n2. **Spatial Data**: Nearby locations are often similar (not independent)\n3. **Learning Systems**: Performance improves over time (not identically distributed)\n4. **Batch Effects**: Different experimental batches may have different distributions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Case 1: i.i.d variables (both N(0,1))\nX1_iid = torch.distributions.Normal(0, 1).sample((1000,))\nX2_iid = torch.distributions.Normal(0, 1).sample((1000,))\n\n# Case 2: Independent but NOT identically distributed\nX1_ind = torch.distributions.Normal(0, 1).sample((1000,))    # N(0,1)\nX2_ind = torch.distributions.Normal(2, 0.5).sample((1000,))  # N(2,0.5)\n\n# Case 3: Identically distributed but NOT independent (correlated)\n# Using multivariate normal with correlation\nmean = torch.tensor([0.0, 0.0])\ncov = torch.tensor([[1.0, 0.7], [0.7, 1.0]])  # correlation = 0.7\ncorrelated_samples = torch.distributions.MultivariateNormal(mean, cov).sample((1000,))\nX1_cor = correlated_samples[:, 0]\nX2_cor = correlated_samples[:, 1]\n\n# Create comparison plot\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Row 1: Scatter plots\ntitles = ['i.i.d Variables', 'Independent, Not Identical', 'Identical, Not Independent']\nX_pairs = [(X1_iid, X2_iid), (X1_ind, X2_ind), (X1_cor, X2_cor)]\n\nfor i, (X1, X2) in enumerate(X_pairs):\n    axes[0, i].scatter(X1.numpy(), X2.numpy(), alpha=0.5, s=10)\n    axes[0, i].set_title(titles[i])\n    axes[0, i].set_xlabel('X1')\n    axes[0, i].set_ylabel('X2')\n    axes[0, i].grid(True, alpha=0.3)\n    \n    # Add correlation info\n    corr = torch.corrcoef(torch.stack([X1, X2]))[0, 1]\n    axes[0, i].text(0.05, 0.95, f'Corr: {corr:.3f}', transform=axes[0, i].transAxes,\n                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\"))\n\n# Row 2: Histograms\nfor i, (X1, X2) in enumerate(X_pairs):\n    axes[1, i].hist(X1.numpy(), bins=30, alpha=0.6, label='X1', density=True)\n    axes[1, i].hist(X2.numpy(), bins=30, alpha=0.6, label='X2', density=True)\n    axes[1, i].set_title(f'Marginal Distributions')\n    axes[1, i].set_xlabel('Value')\n    axes[1, i].set_ylabel('Density')\n    axes[1, i].legend()\n    axes[1, i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nprint(\"COMPARISON SUMMARY:\")\nprint(\"=\"*50)\nfor i, (name, (X1, X2)) in enumerate(zip(titles, X_pairs)):\n    corr = torch.corrcoef(torch.stack([X1, X2]))[0, 1]\n    print(f\"\\n{name}:\")\n    print(f\"  X1: mean={X1.mean():.3f}, std={X1.std():.3f}\")\n    print(f\"  X2: mean={X2.mean():.3f}, std={X2.std():.3f}\")\n    print(f\"  Correlation: {corr:.3f}\")\n    \n    # Check properties\n    same_mean = abs(X1.mean() - X2.mean()) < 0.2\n    same_std = abs(X1.std() - X2.std()) < 0.2\n    independent = abs(corr) < 0.1\n    \n    print(f\"  ✓ Identically distributed: {same_mean and same_std}\")\n    print(f\"  ✓ Independent: {independent}\")\n    print(f\"  ✓ i.i.d: {same_mean and same_std and independent}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Example 3: Comparing i.i.d vs Non-i.i.d Variables\n\nLet's contrast i.i.d variables with non-i.i.d ones to understand the difference.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Generate samples from i.i.d normal random variables\nn_samples = 1000\n\n# Two i.i.d normal random variables\nX1_samples = torch.distributions.Normal(0, 1).sample((n_samples,))\nX2_samples = torch.distributions.Normal(0, 1).sample((n_samples,))\n\n# Plot the samples\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Individual distributions\naxes[0].hist(X1_samples.numpy(), bins=30, alpha=0.7, label='X1', color='blue', density=True)\naxes[0].hist(X2_samples.numpy(), bins=30, alpha=0.7, label='X2', color='red', density=True)\naxes[0].set_title('Individual Distributions\\n(Should be identical)')\naxes[0].set_xlabel('Value')\naxes[0].set_ylabel('Density')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\n# Joint distribution (scatter plot)\naxes[1].scatter(X1_samples.numpy(), X2_samples.numpy(), alpha=0.5, s=10)\naxes[1].set_title('Joint Distribution\\n(Should show no correlation)')\naxes[1].set_xlabel('X1')\naxes[1].set_ylabel('X2')\naxes[1].grid(True, alpha=0.3)\n\n# Correlation check\ncorrelation = torch.corrcoef(torch.stack([X1_samples, X2_samples]))[0, 1]\naxes[2].text(0.1, 0.7, f'Sample Correlation: {correlation:.4f}', fontsize=12, \n             transform=axes[2].transAxes, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\naxes[2].text(0.1, 0.5, f'Expected (theory): 0.0000', fontsize=12, \n             transform=axes[2].transAxes, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\naxes[2].text(0.1, 0.3, 'Independence verified if\\ncorrelation ≈ 0', fontsize=11, \n             transform=axes[2].transAxes)\naxes[2].set_title('Independence Check')\naxes[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Sample means: X1 = {X1_samples.mean():.4f}, X2 = {X2_samples.mean():.4f}\")\nprint(f\"Sample stds:  X1 = {X1_samples.std():.4f}, X2 = {X2_samples.std():.4f}\")\nprint(f\"Sample correlation: {correlation:.4f}\")\nprint(\"\\nFor i.i.d N(0,1) variables, we expect:\")\nprint(\"- Means ≈ 0, Standard deviations ≈ 1, Correlation ≈ 0\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Example 2: Visualizing i.i.d Random Variables\n\nLet's generate samples from i.i.d random variables and visualize their properties.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "**Result Interpretation**: \n- $P(X_1 = 0.2) \\approx 0.391$ - Individual probability density at $x_1 = 0.2$\n- $P(X_2 = 0.4) \\approx 0.368$ - Individual probability density at $x_2 = 0.4$  \n- $P(X_1 = 0.2, X_2 = 0.4) \\approx 0.144$ - Joint probability density\n\nNotice that the joint probability equals the product of individual probabilities, confirming independence: $0.391 \\times 0.368 \\approx 0.144$.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# say sample is\n",
    "sample = torch.tensor([0.2, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3910) tensor(0.3683)\n"
     ]
    }
   ],
   "source": [
    "P_X_x1_ = X1.log_prob(sample[0]).exp()\n",
    "P_X_x2_ = X2.log_prob(sample[1]).exp()\n",
    "\n",
    "print(P_X_x1_, P_X_x2_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1440)\n"
     ]
    }
   ],
   "source": [
    "joint_pdf = P_X_x1_ * P_X_x2_\n",
    "print(joint_pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
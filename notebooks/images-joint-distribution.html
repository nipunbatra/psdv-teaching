<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Nipun Batra">
<meta name="dcterms.date" content="2025-03-18">
<meta name="keywords" content="joint distributions, PCA, dimensionality reduction, image analysis, scikit-learn, digits dataset">
<meta name="description" content="Exploring joint distributions through image data analysis using PCA dimensionality reduction and visualization of multi-dimensional probability distributions">

<title>Images and Joint Distributions â€“ Probability Statistics Data Visualization Resources</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-c3c2c9e745155556954bc4da23476b10.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-c3c2c9e745155556954bc4da23476b10.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-c3c2c9e745155556954bc4da23476b10.css" rel="stylesheet" class="quarto-color-scheme-extra" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-1b24b88b73f8b56d80a2d5a00e70f885.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark-f1f85bb7dbd4314d5ad09add27c1e8f0.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<link href="../site_libs/bootstrap/bootstrap-1b24b88b73f8b56d80a2d5a00e70f885.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme-extra" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed quarto-light"><script id="quarto-html-before-body" type="application/javascript">
    const toggleBodyColorMode = (bsSheetEl) => {
      const mode = bsSheetEl.getAttribute("data-mode");
      const bodyEl = window.document.querySelector("body");
      if (mode === "dark") {
        bodyEl.classList.add("quarto-dark");
        bodyEl.classList.remove("quarto-light");
      } else {
        bodyEl.classList.add("quarto-light");
        bodyEl.classList.remove("quarto-dark");
      }
    }
    const toggleBodyColorPrimary = () => {
      const bsSheetEl = window.document.querySelector("link#quarto-bootstrap:not([rel=disabled-stylesheet])");
      if (bsSheetEl) {
        toggleBodyColorMode(bsSheetEl);
      }
    }
    const setColorSchemeToggle = (alternate) => {
      const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
      for (let i=0; i < toggles.length; i++) {
        const toggle = toggles[i];
        if (toggle) {
          if (alternate) {
            toggle.classList.add("alternate");
          } else {
            toggle.classList.remove("alternate");
          }
        }
      }
    };
    const toggleColorMode = (alternate) => {
      // Switch the stylesheets
      const primaryStylesheets = window.document.querySelectorAll('link.quarto-color-scheme:not(.quarto-color-alternate)');
      const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
      manageTransitions('#quarto-margin-sidebar .nav-link', false);
      if (alternate) {
        // note: dark is layered on light, we don't disable primary!
        enableStylesheet(alternateStylesheets);
        for (const sheetNode of alternateStylesheets) {
          if (sheetNode.id === "quarto-bootstrap") {
            toggleBodyColorMode(sheetNode);
          }
        }
      } else {
        disableStylesheet(alternateStylesheets);
        enableStylesheet(primaryStylesheets)
        toggleBodyColorPrimary();
      }
      manageTransitions('#quarto-margin-sidebar .nav-link', true);
      // Switch the toggles
      setColorSchemeToggle(alternate)
      // Hack to workaround the fact that safari doesn't
      // properly recolor the scrollbar when toggling (#1455)
      if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
        manageTransitions("body", false);
        window.scrollTo(0, 1);
        setTimeout(() => {
          window.scrollTo(0, 0);
          manageTransitions("body", true);
        }, 40);
      }
    }
    const disableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        stylesheet.rel = 'disabled-stylesheet';
      }
    }
    const enableStylesheet = (stylesheets) => {
      for (let i=0; i < stylesheets.length; i++) {
        const stylesheet = stylesheets[i];
        if(stylesheet.rel !== 'stylesheet') { // for Chrome, which will still FOUC without this check
          stylesheet.rel = 'stylesheet';
        }
      }
    }
    const manageTransitions = (selector, allowTransitions) => {
      const els = window.document.querySelectorAll(selector);
      for (let i=0; i < els.length; i++) {
        const el = els[i];
        if (allowTransitions) {
          el.classList.remove('notransition');
        } else {
          el.classList.add('notransition');
        }
      }
    }
    const isFileUrl = () => {
      return window.location.protocol === 'file:';
    }
    const hasAlternateSentinel = () => {
      let styleSentinel = getColorSchemeSentinel();
      if (styleSentinel !== null) {
        return styleSentinel === "alternate";
      } else {
        return false;
      }
    }
    const setStyleSentinel = (alternate) => {
      const value = alternate ? "alternate" : "default";
      if (!isFileUrl()) {
        window.localStorage.setItem("quarto-color-scheme", value);
      } else {
        localAlternateSentinel = value;
      }
    }
    const getColorSchemeSentinel = () => {
      if (!isFileUrl()) {
        const storageValue = window.localStorage.getItem("quarto-color-scheme");
        return storageValue != null ? storageValue : localAlternateSentinel;
      } else {
        return localAlternateSentinel;
      }
    }
    const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
      const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
      const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
      let newTheme = '';
      if(authorPrefersDark) {
        newTheme = isAlternate ? baseTheme : alternateTheme;
      } else {
        newTheme = isAlternate ? alternateTheme : baseTheme;
      }
      const changeGiscusTheme = () => {
        // From: https://github.com/giscus/giscus/issues/336
        const sendMessage = (message) => {
          const iframe = document.querySelector('iframe.giscus-frame');
          if (!iframe) return;
          iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
        }
        sendMessage({
          setConfig: {
            theme: newTheme
          }
        });
      }
      const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
      if (isGiscussLoaded) {
        changeGiscusTheme();
      }
    };
    const authorPrefersDark = false;
    const darkModeDefault = authorPrefersDark;
      document.querySelector('link#quarto-text-highlighting-styles.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
      document.querySelector('link#quarto-bootstrap.quarto-color-scheme-extra').rel = 'disabled-stylesheet';
    let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
    // Dark / light mode switch
    window.quartoToggleColorScheme = () => {
      // Read the current dark / light value
      let toAlternate = !hasAlternateSentinel();
      toggleColorMode(toAlternate);
      setStyleSentinel(toAlternate);
      toggleGiscusIfUsed(toAlternate, darkModeDefault);
      window.dispatchEvent(new Event('resize'));
    };
    // Switch to dark mode if need be
    if (hasAlternateSentinel()) {
      toggleColorMode(true);
    } else {
      toggleColorMode(false);
    }
  </script>

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Probability Statistics Data Visualization Resources</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://nipunbatra.github.io"> 
<span class="menu-text">Instructor Homepage</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../notebooks.html"> 
<span class="menu-text">Notebooks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../slides.html"> 
<span class="menu-text">Slides</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#joint-distributions-and-high-dimensional-data-analysis-introductionjoint-distributions-describe-the-probability-behavior-of-multiple-random-variables-simultaneously.-in-this-notebook-we-explore-joint-distributions-through-the-lens-of-high-dimensional-image-data-demonstrating-how-techniques-like-principal-component-analysis-pca-help-us-understand-and-visualize-joint-probability-structures-in-complex-datasets.when-dealing-with-image-data-each-pixel-can-be-thought-of-as-a-random-variable-and-the-entire-image-represents-a-realization-from-a-high-dimensional-joint-distribution.-understanding-these-joint-structures-is-crucial-for--image-compression-and-denoising--pattern-recognition-and-classification--dimensionality-reduction-for-visualization--feature-extraction-for-machine-learning--understanding-data-dependencies-and-correlations-learning-objectivesby-the-end-of-this-notebook-you-will-be-able-to1.-understand-joint-distributions-in-high-dimensional-spaces2.-apply-pca-to-discover-the-structure-of-joint-distributions3.-interpret-principal-components-as-directions-of-maximum-variance4.-visualize-high-dimensional-joint-distributions-in-lower-dimensions5.-analyze-the-relationship-between-data-correlation-and-joint-distribution-structure6.-connect-dimensionality-reduction-to-probabilistic-concepts-theoretical-background-joint-distributionsfor-random-variables-x_1-x_2-ldots-x_p-the-joint-distribution-describes-their-collective-probabilistic-behavior.-the-joint-probability-density-function-pdf-isf_x_1ldotsx_px_1-ldots-x_pthis-function-gives-the-probability-density-at-any-point-x_1-ldots-x_p-in-the-p-dimensional-space.-key-properties1.-marginal-distributions-f_x_ix_i-f_x_1x_px_1-x_p-dx_1-dx_i-1-dx_i1-dx_p2.-independence-variables-are-independent-if-f_x_1x_px_1-x_p-i1p-fx_ix_i3.-covariance-structure-textcovx_i-x_j-ex_i---mu_ix_j---mu_j-principal-component-analysis-pca-and-joint-distributionspca-finds-the-directions-of-maximum-variance-in-a-joint-distribution.-for-a-multivariate-dataset-with-covariance-matrix-sigma1.-eigenvalue-decomposition-vvt2.-principal-components-columns-of-v-eigenvectors3.-explained-variance-diagonal-elements-of-lambda-eigenvaluespca-essentially-rotates-the-coordinate-system-to-align-with-the-natural-axes-of-the-joint-distribution.-practical-implementation-analyzing-handwritten-digits" id="toc-joint-distributions-and-high-dimensional-data-analysis-introductionjoint-distributions-describe-the-probability-behavior-of-multiple-random-variables-simultaneously.-in-this-notebook-we-explore-joint-distributions-through-the-lens-of-high-dimensional-image-data-demonstrating-how-techniques-like-principal-component-analysis-pca-help-us-understand-and-visualize-joint-probability-structures-in-complex-datasets.when-dealing-with-image-data-each-pixel-can-be-thought-of-as-a-random-variable-and-the-entire-image-represents-a-realization-from-a-high-dimensional-joint-distribution.-understanding-these-joint-structures-is-crucial-for--image-compression-and-denoising--pattern-recognition-and-classification--dimensionality-reduction-for-visualization--feature-extraction-for-machine-learning--understanding-data-dependencies-and-correlations-learning-objectivesby-the-end-of-this-notebook-you-will-be-able-to1.-understand-joint-distributions-in-high-dimensional-spaces2.-apply-pca-to-discover-the-structure-of-joint-distributions3.-interpret-principal-components-as-directions-of-maximum-variance4.-visualize-high-dimensional-joint-distributions-in-lower-dimensions5.-analyze-the-relationship-between-data-correlation-and-joint-distribution-structure6.-connect-dimensionality-reduction-to-probabilistic-concepts-theoretical-background-joint-distributionsfor-random-variables-x_1-x_2-ldots-x_p-the-joint-distribution-describes-their-collective-probabilistic-behavior.-the-joint-probability-density-function-pdf-isf_x_1ldotsx_px_1-ldots-x_pthis-function-gives-the-probability-density-at-any-point-x_1-ldots-x_p-in-the-p-dimensional-space.-key-properties1.-marginal-distributions-f_x_ix_i-f_x_1x_px_1-x_p-dx_1-dx_i-1-dx_i1-dx_p2.-independence-variables-are-independent-if-f_x_1x_px_1-x_p-i1p-fx_ix_i3.-covariance-structure-textcovx_i-x_j-ex_i---mu_ix_j---mu_j-principal-component-analysis-pca-and-joint-distributionspca-finds-the-directions-of-maximum-variance-in-a-joint-distribution.-for-a-multivariate-dataset-with-covariance-matrix-sigma1.-eigenvalue-decomposition-vvt2.-principal-components-columns-of-v-eigenvectors3.-explained-variance-diagonal-elements-of-lambda-eigenvaluespca-essentially-rotates-the-coordinate-system-to-align-with-the-natural-axes-of-the-joint-distribution.-practical-implementation-analyzing-handwritten-digits" class="nav-link active" data-scroll-target="#joint-distributions-and-high-dimensional-data-analysis-introductionjoint-distributions-describe-the-probability-behavior-of-multiple-random-variables-simultaneously.-in-this-notebook-we-explore-joint-distributions-through-the-lens-of-high-dimensional-image-data-demonstrating-how-techniques-like-principal-component-analysis-pca-help-us-understand-and-visualize-joint-probability-structures-in-complex-datasets.when-dealing-with-image-data-each-pixel-can-be-thought-of-as-a-random-variable-and-the-entire-image-represents-a-realization-from-a-high-dimensional-joint-distribution.-understanding-these-joint-structures-is-crucial-for--image-compression-and-denoising--pattern-recognition-and-classification--dimensionality-reduction-for-visualization--feature-extraction-for-machine-learning--understanding-data-dependencies-and-correlations-learning-objectivesby-the-end-of-this-notebook-you-will-be-able-to1.-understand-joint-distributions-in-high-dimensional-spaces2.-apply-pca-to-discover-the-structure-of-joint-distributions3.-interpret-principal-components-as-directions-of-maximum-variance4.-visualize-high-dimensional-joint-distributions-in-lower-dimensions5.-analyze-the-relationship-between-data-correlation-and-joint-distribution-structure6.-connect-dimensionality-reduction-to-probabilistic-concepts-theoretical-background-joint-distributionsfor-random-variables-x_1-x_2-ldots-x_p-the-joint-distribution-describes-their-collective-probabilistic-behavior.-the-joint-probability-density-function-pdf-isf_x_1ldotsx_px_1-ldots-x_pthis-function-gives-the-probability-density-at-any-point-x_1-ldots-x_p-in-the-p-dimensional-space.-key-properties1.-marginal-distributions-f_x_ix_i-f_x_1x_px_1-x_p-dx_1-dx_i-1-dx_i1-dx_p2.-independence-variables-are-independent-if-f_x_1x_px_1-x_p-i1p-fx_ix_i3.-covariance-structure-textcovx_i-x_j-ex_i---mu_ix_j---mu_j-principal-component-analysis-pca-and-joint-distributionspca-finds-the-directions-of-maximum-variance-in-a-joint-distribution.-for-a-multivariate-dataset-with-covariance-matrix-sigma1.-eigenvalue-decomposition-vvt2.-principal-components-columns-of-v-eigenvectors3.-explained-variance-diagonal-elements-of-lambda-eigenvaluespca-essentially-rotates-the-coordinate-system-to-align-with-the-natural-axes-of-the-joint-distribution.-practical-implementation-analyzing-handwritten-digits">Joint Distributions and High-Dimensional Data Analysis## IntroductionJoint distributions describe the probability behavior of multiple random variables simultaneously. In this notebook, we explore joint distributions through the lens of high-dimensional image data, demonstrating how techniques like Principal Component Analysis (PCA) help us understand and visualize joint probability structures in complex datasets.When dealing with image data, each pixel can be thought of as a random variable, and the entire image represents a realization from a high-dimensional joint distribution. Understanding these joint structures is crucial for:- <strong>Image compression and denoising</strong>- <strong>Pattern recognition and classification</strong>- <strong>Dimensionality reduction for visualization</strong>- <strong>Feature extraction for machine learning</strong>- <strong>Understanding data dependencies and correlations</strong>## Learning ObjectivesBy the end of this notebook, you will be able to:1. <strong>Understand</strong> joint distributions in high-dimensional spaces2. <strong>Apply</strong> PCA to discover the structure of joint distributions3. <strong>Interpret</strong> principal components as directions of maximum variance4. <strong>Visualize</strong> high-dimensional joint distributions in lower dimensions5. <strong>Analyze</strong> the relationship between data correlation and joint distribution structure6. <strong>Connect</strong> dimensionality reduction to probabilistic concepts## Theoretical Background### Joint DistributionsFor random variables <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span>, the <strong>joint distribution</strong> describes their collective probabilistic behavior. The joint probability density function (PDF) is:<span class="math display">\[f_{X_1,\ldots,X_p}(x_1, \ldots, x_p)\]</span>This function gives the probability density at any point <span class="math inline">\((x_1, \ldots, x_p)\)</span> in the <span class="math inline">\(p\)</span>-dimensional space.### Key Properties:1. <strong>Marginal Distributions</strong>: $f_{X_i}(x_i) = f_{X_1,,X_p}(x_1, , x_p) dx_1 dx_{i-1} dx_{i+1} dx_p$2. <strong>Independence</strong>: Variables are independent if $f_{X_1,,X_p}(x_1, , x_p) = <em>{i=1}^p f</em>{X_i}(x_i)$3. <strong>Covariance Structure</strong>: <span class="math inline">\(\text{Cov}(X_i, X_j) = E[(X_i - \mu_i)(X_j - \mu_j)]\)</span>### Principal Component Analysis (PCA) and Joint DistributionsPCA finds the directions of maximum variance in a joint distribution. For a multivariate dataset with covariance matrix <span class="math inline">\(\Sigma\)</span>:1. <strong>Eigenvalue Decomposition</strong>: $= VV^T$2. <strong>Principal Components</strong>: Columns of <span class="math inline">\(V\)</span> (eigenvectors)3. <strong>Explained Variance</strong>: Diagonal elements of <span class="math inline">\(\Lambda\)</span> (eigenvalues)PCA essentially rotates the coordinate system to align with the natural axes of the joint distribution.â€”## Practical Implementation: Analyzing Handwritten Digits</a>
  <ul class="collapse">
  <li><a href="#loading-and-understanding-the-datasetwell-use-the-digits-dataset-where-each-image-is-an-88-pixel-grid.-this-gives-us-64-dimensional-data-points-representing-samples-from-a-64-dimensional-joint-distribution." id="toc-loading-and-understanding-the-datasetwell-use-the-digits-dataset-where-each-image-is-an-88-pixel-grid.-this-gives-us-64-dimensional-data-points-representing-samples-from-a-64-dimensional-joint-distribution." class="nav-link" data-scroll-target="#loading-and-understanding-the-datasetwell-use-the-digits-dataset-where-each-image-is-an-88-pixel-grid.-this-gives-us-64-dimensional-data-points-representing-samples-from-a-64-dimensional-joint-distribution.">Loading and Understanding the DatasetWeâ€™ll use the digits dataset, where each image is an 8Ã—8 pixel grid. This gives us 64-dimensional data points, representing samples from a 64-dimensional joint distribution.</a>
  <ul class="collapse">
  <li><a href="#step-2-principal-component-analysis---finding-joint-distribution-structurepca-helps-us-understand-the-joint-distribution-by-finding-the-directions-of-maximum-variance.-these-directions-reveal-the-underlying-structure-of-how-the-64-pixel-intensities-co-vary." id="toc-step-2-principal-component-analysis---finding-joint-distribution-structurepca-helps-us-understand-the-joint-distribution-by-finding-the-directions-of-maximum-variance.-these-directions-reveal-the-underlying-structure-of-how-the-64-pixel-intensities-co-vary." class="nav-link" data-scroll-target="#step-2-principal-component-analysis---finding-joint-distribution-structurepca-helps-us-understand-the-joint-distribution-by-finding-the-directions-of-maximum-variance.-these-directions-reveal-the-underlying-structure-of-how-the-64-pixel-intensities-co-vary.">Step 2: Principal Component Analysis - Finding Joint Distribution StructurePCA helps us understand the joint distribution by finding the directions of maximum variance. These directions reveal the underlying structure of how the 64 pixel intensities co-vary.</a></li>
  </ul></li>
  <li><a href="#connecting-pca-to-probability-theory-covariance-matrix-and-eigenstructurethe-principal-components-are-directly-related-to-the-covariance-structure-of-our-joint-distribution" id="toc-connecting-pca-to-probability-theory-covariance-matrix-and-eigenstructurethe-principal-components-are-directly-related-to-the-covariance-structure-of-our-joint-distribution" class="nav-link" data-scroll-target="#connecting-pca-to-probability-theory-covariance-matrix-and-eigenstructurethe-principal-components-are-directly-related-to-the-covariance-structure-of-our-joint-distribution">Connecting PCA to Probability Theory### Covariance Matrix and EigenstructureThe principal components are directly related to the covariance structure of our joint distribution:</a></li>
  <li><a href="#analyzing-the-joint-distribution-structure-step-1-understanding-marginal-distributionsbefore-applying-pca-lets-examine-the-marginal-distributions---how-individual-pixels-random-variables-behave-across-all-samples." id="toc-analyzing-the-joint-distribution-structure-step-1-understanding-marginal-distributionsbefore-applying-pca-lets-examine-the-marginal-distributions---how-individual-pixels-random-variables-behave-across-all-samples." class="nav-link" data-scroll-target="#analyzing-the-joint-distribution-structure-step-1-understanding-marginal-distributionsbefore-applying-pca-lets-examine-the-marginal-distributions---how-individual-pixels-random-variables-behave-across-all-samples.">Analyzing the Joint Distribution Structure### Step 1: Understanding Marginal DistributionsBefore applying PCA, letâ€™s examine the marginal distributions - how individual pixels (random variables) behave across all samples.</a>
  <ul class="collapse">
  <li><a href="#visualizing-individual-sampleslets-examine-individual-realizations-from-our-64-dimensional-joint-distribution" id="toc-visualizing-individual-sampleslets-examine-individual-realizations-from-our-64-dimensional-joint-distribution" class="nav-link" data-scroll-target="#visualizing-individual-sampleslets-examine-individual-realizations-from-our-64-dimensional-joint-distribution">Visualizing Individual SamplesLetâ€™s examine individual realizations from our 64-dimensional joint distribution:</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">


<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Images and Joint Distributions</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Machine Learning</div>
    <div class="quarto-category">Data Science</div>
    <div class="quarto-category">Probability</div>
    <div class="quarto-category">Distributions</div>
  </div>
  </div>

<div>
  <div class="description">
    Exploring joint distributions through image data analysis using PCA dimensionality reduction and visualization of multi-dimensional probability distributions
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Nipun Batra </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">March 18, 2025</p>
    </div>
  </div>
  
    
  </div>
  

<div>
  <div class="keywords">
    <div class="block-title">Keywords</div>
    <p>joint distributions, PCA, dimensionality reduction, image analysis, scikit-learn, digits dataset</p>
  </div>
</div>

</header>


<p>â€”author: Nipun Batratitle: Images and Joint Distributionsdescription: Exploring joint distributions through image data analysis using PCA dimensionality reduction and visualization of multi-dimensional probability distributionscategories: - Machine Learning - Data Science - Probability - Distributionskeywords: [joint distributions, PCA, dimensionality reduction, image analysis, scikit-learn, digits dataset]date: â€™2025-03-18â€™badges: truetoc: trueâ€”</p>
<div id="cell-1" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(np.__version__)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Retina mode</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>config InlineBackend.figure_format <span class="op">=</span> <span class="st">'retina'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>2.2.4</code></pre>
</div>
</div>
<section id="joint-distributions-and-high-dimensional-data-analysis-introductionjoint-distributions-describe-the-probability-behavior-of-multiple-random-variables-simultaneously.-in-this-notebook-we-explore-joint-distributions-through-the-lens-of-high-dimensional-image-data-demonstrating-how-techniques-like-principal-component-analysis-pca-help-us-understand-and-visualize-joint-probability-structures-in-complex-datasets.when-dealing-with-image-data-each-pixel-can-be-thought-of-as-a-random-variable-and-the-entire-image-represents-a-realization-from-a-high-dimensional-joint-distribution.-understanding-these-joint-structures-is-crucial-for--image-compression-and-denoising--pattern-recognition-and-classification--dimensionality-reduction-for-visualization--feature-extraction-for-machine-learning--understanding-data-dependencies-and-correlations-learning-objectivesby-the-end-of-this-notebook-you-will-be-able-to1.-understand-joint-distributions-in-high-dimensional-spaces2.-apply-pca-to-discover-the-structure-of-joint-distributions3.-interpret-principal-components-as-directions-of-maximum-variance4.-visualize-high-dimensional-joint-distributions-in-lower-dimensions5.-analyze-the-relationship-between-data-correlation-and-joint-distribution-structure6.-connect-dimensionality-reduction-to-probabilistic-concepts-theoretical-background-joint-distributionsfor-random-variables-x_1-x_2-ldots-x_p-the-joint-distribution-describes-their-collective-probabilistic-behavior.-the-joint-probability-density-function-pdf-isf_x_1ldotsx_px_1-ldots-x_pthis-function-gives-the-probability-density-at-any-point-x_1-ldots-x_p-in-the-p-dimensional-space.-key-properties1.-marginal-distributions-f_x_ix_i-f_x_1x_px_1-x_p-dx_1-dx_i-1-dx_i1-dx_p2.-independence-variables-are-independent-if-f_x_1x_px_1-x_p-i1p-fx_ix_i3.-covariance-structure-textcovx_i-x_j-ex_i---mu_ix_j---mu_j-principal-component-analysis-pca-and-joint-distributionspca-finds-the-directions-of-maximum-variance-in-a-joint-distribution.-for-a-multivariate-dataset-with-covariance-matrix-sigma1.-eigenvalue-decomposition-vvt2.-principal-components-columns-of-v-eigenvectors3.-explained-variance-diagonal-elements-of-lambda-eigenvaluespca-essentially-rotates-the-coordinate-system-to-align-with-the-natural-axes-of-the-joint-distribution.-practical-implementation-analyzing-handwritten-digits" class="level1">
<h1>Joint Distributions and High-Dimensional Data Analysis## IntroductionJoint distributions describe the probability behavior of multiple random variables simultaneously. In this notebook, we explore joint distributions through the lens of high-dimensional image data, demonstrating how techniques like Principal Component Analysis (PCA) help us understand and visualize joint probability structures in complex datasets.When dealing with image data, each pixel can be thought of as a random variable, and the entire image represents a realization from a high-dimensional joint distribution. Understanding these joint structures is crucial for:- <strong>Image compression and denoising</strong>- <strong>Pattern recognition and classification</strong>- <strong>Dimensionality reduction for visualization</strong>- <strong>Feature extraction for machine learning</strong>- <strong>Understanding data dependencies and correlations</strong>## Learning ObjectivesBy the end of this notebook, you will be able to:1. <strong>Understand</strong> joint distributions in high-dimensional spaces2. <strong>Apply</strong> PCA to discover the structure of joint distributions3. <strong>Interpret</strong> principal components as directions of maximum variance4. <strong>Visualize</strong> high-dimensional joint distributions in lower dimensions5. <strong>Analyze</strong> the relationship between data correlation and joint distribution structure6. <strong>Connect</strong> dimensionality reduction to probabilistic concepts## Theoretical Background### Joint DistributionsFor random variables <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span>, the <strong>joint distribution</strong> describes their collective probabilistic behavior. The joint probability density function (PDF) is:<span class="math display">\[f_{X_1,\ldots,X_p}(x_1, \ldots, x_p)\]</span>This function gives the probability density at any point <span class="math inline">\((x_1, \ldots, x_p)\)</span> in the <span class="math inline">\(p\)</span>-dimensional space.### Key Properties:1. <strong>Marginal Distributions</strong>: $f_{X_i}(x_i) = f_{X_1,,X_p}(x_1, , x_p) dx_1 dx_{i-1} dx_{i+1} dx_p$2. <strong>Independence</strong>: Variables are independent if $f_{X_1,,X_p}(x_1, , x_p) = <em>{i=1}^p f</em>{X_i}(x_i)$3. <strong>Covariance Structure</strong>: <span class="math inline">\(\text{Cov}(X_i, X_j) = E[(X_i - \mu_i)(X_j - \mu_j)]\)</span>### Principal Component Analysis (PCA) and Joint DistributionsPCA finds the directions of maximum variance in a joint distribution. For a multivariate dataset with covariance matrix <span class="math inline">\(\Sigma\)</span>:1. <strong>Eigenvalue Decomposition</strong>: $= VV^T$2. <strong>Principal Components</strong>: Columns of <span class="math inline">\(V\)</span> (eigenvectors)3. <strong>Explained Variance</strong>: Diagonal elements of <span class="math inline">\(\Lambda\)</span> (eigenvalues)PCA essentially rotates the coordinate system to align with the natural axes of the joint distribution.â€”## Practical Implementation: Analyzing Handwritten Digits</h1>
<section id="loading-and-understanding-the-datasetwell-use-the-digits-dataset-where-each-image-is-an-88-pixel-grid.-this-gives-us-64-dimensional-data-points-representing-samples-from-a-64-dimensional-joint-distribution." class="level2">
<h2 class="anchored" data-anchor-id="loading-and-understanding-the-datasetwell-use-the-digits-dataset-where-each-image-is-an-88-pixel-grid.-this-gives-us-64-dimensional-data-points-representing-samples-from-a-64-dimensional-joint-distribution.">Loading and Understanding the DatasetWeâ€™ll use the digits dataset, where each image is an 8Ã—8 pixel grid. This gives us 64-dimensional data points, representing samples from a 64-dimensional joint distribution.</h2>
<div id="cell-4" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_digits</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Data Interpretation:</strong>- <code>X.shape = (1797, 64)</code>: We have 1,797 samples from a 64-dimensional joint distribution- Each row represents one realization from this joint distribution - Each column represents one random variable (pixel intensity)- The joint distribution captures how all 64 pixels co-vary across different digit images</p>
<div id="cell-6" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize several samples from our joint distributionfig, axes = plt.subplots(2, 5, figsize=(12, 6))axes = axes.ravel()for i in range(10):    # Each image is a sample from the 64-dimensional joint distribution    sample_idx = i * 180  # Spread out the examples    image = X[sample_idx].reshape(8, 8)    axes[i].imshow(image, cmap='gray')    axes[i].set_title(f'Digit: {y[sample_idx]}\nSample #{sample_idx}')    axes[i].axis('off')plt.suptitle('Individual Samples from the 64-Dimensional Joint Distribution', fontsize=14)plt.tight_layout()plt.show()# Show the actual 64-dimensional vector for one sampleprint(f"Sample {sample_idx} as 64-dimensional vector:")print(f"First 10 values: {X[sample_idx][:10]}")print(f"Range: [{X[sample_idx].min():.1f}, {X[sample_idx].max():.1f}]")print(f"Mean: {X[sample_idx].mean():.2f}, Std: {X[sample_idx].std():.2f}")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-7" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Analyze marginal distributions of pixelsfig, axes = plt.subplots(2, 3, figsize=(15, 8))# 1. Mean image (expected value of the joint distribution)mean_image = X.mean(axis=0).reshape(8, 8)axes[0, 0].imshow(mean_image, cmap='gray')axes[0, 0].set_title('Mean Image\n(Expected Value)')axes[0, 0].axis('off')# 2. Standard deviation image (marginal variances)std_image = X.std(axis=0).reshape(8, 8)axes[0, 1].imshow(std_image, cmap='hot')axes[0, 1].set_title('Standard Deviation per Pixel\n(Marginal Variances)')axes[0, 1].axis('off')# 3. Sample of marginal distributionspixel_indices = [10, 28, 35, 50]  # Different pixel positionsfor i, px_idx in enumerate(pixel_indices):    if i &lt; 2:        ax = axes[0, 2]    else:        ax = axes[1, 2]        ax.hist(X[:, px_idx], bins=30, alpha=0.7, density=True,             label=f'Pixel {px_idx}')axes[0, 2].set_title('Marginal Distributions\nof Selected Pixels')axes[0, 2].set_xlabel('Pixel Intensity')axes[0, 2].set_ylabel('Density')axes[0, 2].legend()axes[1, 2].set_xlabel('Pixel Intensity')axes[1, 2].set_ylabel('Density')axes[1, 2].legend()# 4. Pixel correlation analysis# Sample a few pixels to show correlationsample_pixels = [20, 21, 28, 29]  # Adjacent pixelspixel_data = X[:, sample_pixels]correlation_matrix = np.corrcoef(pixel_data.T)im = axes[1, 0].imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)axes[1, 0].set_title('Correlation Matrix\n(Sample Adjacent Pixels)')axes[1, 0].set_xticks(range(len(sample_pixels)))axes[1, 0].set_yticks(range(len(sample_pixels)))axes[1, 0].set_xticklabels([f'Px{i}' for i in sample_pixels])axes[1, 0].set_yticklabels([f'Px{i}' for i in sample_pixels])plt.colorbar(im, ax=axes[1, 0])# 5. Scatter plot showing dependenceaxes[1, 1].scatter(X[:, 20], X[:, 21], alpha=0.3, s=10)axes[1, 1].set_title('Pixel 20 vs Pixel 21\n(Adjacent Pixels)')axes[1, 1].set_xlabel('Pixel 20 Intensity')axes[1, 1].set_ylabel('Pixel 21 Intensity')axes[1, 1].grid(True, alpha=0.3)plt.tight_layout()plt.show()print("Joint Distribution Properties:")print(f"- Dimensionality: {X.shape[1]} (each sample is 64-dimensional)")print(f"- Sample size: {X.shape[0]} realizations")print(f"- Mean pixel intensity: {X.mean():.2f}")print(f"- Overall variance: {X.var():.2f}")print(f"- Range: [{X.min():.1f}, {X.max():.1f}]")print(f"- Correlation between adjacent pixels (20,21): {np.corrcoef(X[:, 20], X[:, 21])[0,1]:.3f}")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="step-2-principal-component-analysis---finding-joint-distribution-structurepca-helps-us-understand-the-joint-distribution-by-finding-the-directions-of-maximum-variance.-these-directions-reveal-the-underlying-structure-of-how-the-64-pixel-intensities-co-vary." class="level3">
<h3 class="anchored" data-anchor-id="step-2-principal-component-analysis---finding-joint-distribution-structurepca-helps-us-understand-the-joint-distribution-by-finding-the-directions-of-maximum-variance.-these-directions-reveal-the-underlying-structure-of-how-the-64-pixel-intensities-co-vary.">Step 2: Principal Component Analysis - Finding Joint Distribution StructurePCA helps us understand the joint distribution by finding the directions of maximum variance. These directions reveal the underlying structure of how the 64 pixel intensities co-vary.</h3>
<p>â€”## Summary and Key Takeaways### What Weâ€™ve Learned About Joint Distributions:1. <strong>High-Dimensional Reality</strong>: Real data often lives in high-dimensional spaces (64D for 8Ã—8 images), but the effective dimensionality can be much lower2. <strong>PCA Reveals Structure</strong>: Principal Component Analysis uncovers the underlying structure of joint distributions by finding directions of maximum variance3. <strong>Covariance is Key</strong>: The joint distributionâ€™s covariance matrix completely determines the PCA transformation - they are mathematically equivalent4. <strong>Dimensionality Reduction</strong>: Most of the information in a 64-dimensional joint distribution can be captured in just a few principal components5. <strong>Visual Interpretation</strong>: We can visualize complex joint distributions by projecting to 2D or 3D spaces### Mathematical Connections:- <strong>Joint Distribution</strong> â†’ <strong>Covariance Matrix</strong> â†’ <strong>Eigendecomposition</strong> â†’ <strong>Principal Components</strong>- PCA components are eigenvectors of the covariance matrix- PCA eigenvalues represent variance along each principal direction- Reconstruction quality depends on how many components we retain### Practical Applications:1. <strong>Data Compression</strong>: Store images using fewer principal components2. <strong>Noise Reduction</strong>: Reconstruct data using only major components3. <strong>Visualization</strong>: Plot high-dimensional data in 2D/3D4. <strong>Feature Extraction</strong>: Use PC scores as features for machine learning5. <strong>Anomaly Detection</strong>: Identify samples that donâ€™t fit the joint distribution pattern### Key Insights for Data Science:- <strong>Curse of Dimensionality</strong>: High-dimensional spaces are mostly empty- <strong>Intrinsic Dimensionality</strong>: Data often has lower effective dimensionality than the ambient space- <strong>Correlation Structure</strong>: Understanding dependencies between variables is crucial- <strong>Trade-offs</strong>: Dimensionality reduction involves balancing information retention vs.&nbsp;simplicityThis analysis demonstrates how abstract concepts like joint distributions become concrete and actionable through computational tools like PCA, bridging probability theory with practical data analysis.</p>
<div id="cell-10" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Explore the relationship between PCA and covariance structurefrom sklearn.covariance import EmpiricalCovariance# Compute sample covariance matrixcov_estimator = EmpiricalCovariance()cov_matrix = cov_estimator.fit(X).covariance_# Compare with PCA eigendecompositioneigenvals, eigenvecs = np.linalg.eigh(cov_matrix)# Sort in descending order (like PCA)idx = np.argsort(eigenvals)[::-1]eigenvals = eigenvals[idx]eigenvecs = eigenvecs[:, idx]# Create visualizationfig, axes = plt.subplots(2, 3, figsize=(15, 10))# 1. Covariance matrixim1 = axes[0, 0].imshow(cov_matrix, cmap='coolwarm')axes[0, 0].set_title('Sample Covariance Matrix\n(64Ã—64)')axes[0, 0].set_xlabel('Pixel Index')axes[0, 0].set_ylabel('Pixel Index')plt.colorbar(im1, ax=axes[0, 0])# 2. Eigenvalues comparisonaxes[0, 1].plot(range(1, 11), eigenvals[:10], 'bo-', label='Covariance Eigenvalues')axes[0, 1].plot(range(1, 11), pca_detailed.explained_variance_, 'ro-', label='PCA Eigenvalues')axes[0, 1].set_title('Eigenvalues Comparison')axes[0, 1].set_xlabel('Component')axes[0, 1].set_ylabel('Eigenvalue')axes[0, 1].legend()axes[0, 1].grid(True)# 3. Eigenvector comparison (first PC)axes[0, 2].plot(eigenvecs[:, 0], 'b-', label='Cov. Eigenvector 1')axes[0, 2].plot(pca_detailed.components_[0], 'r--', label='PCA Component 1')axes[0, 2].set_title('First Principal Component\n(Eigenvector Comparison)')axes[0, 2].set_xlabel('Pixel Index')axes[0, 2].set_ylabel('Component Weight')axes[0, 2].legend()axes[0, 2].grid(True)# 4. Correlation matrix (easier to interpret)correlation_matrix = np.corrcoef(X.T)im2 = axes[1, 0].imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)axes[1, 0].set_title('Sample Correlation Matrix\n(64Ã—64)')axes[1, 0].set_xlabel('Pixel Index')axes[1, 0].set_ylabel('Pixel Index')plt.colorbar(im2, ax=axes[1, 0])# 5. Local correlation structure (show 8x8 spatial structure)# Reshape correlation matrix to show spatial structurespatial_corr = np.zeros((8, 8))center_pixel = 28  # Middle-ish pixelfor i in range(64):    row, col = i // 8, i % 8    spatial_corr[row, col] = correlation_matrix[center_pixel, i]im3 = axes[1, 1].imshow(spatial_corr, cmap='coolwarm', vmin=-1, vmax=1)axes[1, 1].set_title(f'Correlation with Pixel {center_pixel}\n(Spatial Layout)')plt.colorbar(im3, ax=axes[1, 1])# 6. Distribution of correlationsaxes[1, 2].hist(correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)],                 bins=50, alpha=0.7, density=True)axes[1, 2].set_title('Distribution of Pairwise\nPixel Correlations')axes[1, 2].set_xlabel('Correlation Coefficient')axes[1, 2].set_ylabel('Density')axes[1, 2].axvline(0, color='red', linestyle='--', alpha=0.7)axes[1, 2].grid(True, alpha=0.3)plt.tight_layout()plt.show()# Statistical summaryprint("COVARIANCE STRUCTURE ANALYSIS:")print("="*40)print(f"Covariance matrix shape: {cov_matrix.shape}")print(f"Covariance matrix rank: {np.linalg.matrix_rank(cov_matrix)}")print(f"Trace (total variance): {np.trace(cov_matrix):.2f}")print(f"Maximum correlation: {correlation_matrix[correlation_matrix &lt; 1].max():.3f}")print(f"Minimum correlation: {correlation_matrix.min():.3f}")print(f"Mean absolute correlation: {np.abs(correlation_matrix[np.triu_indices_from(correlation_matrix, k=1)]).mean():.3f}")print("\nPROOF THAT PCA = COVARIANCE EIGENDECOMPOSITION:")print("="*50)print(f"Eigenvalue difference (should be ~0): {np.max(np.abs(eigenvals[:10] - pca_detailed.explained_variance_)):.2e}")print(f"Eigenvector difference (should be ~0): {np.max(np.abs(np.abs(eigenvecs[:, 0]) - np.abs(pca_detailed.components_[0]))):.2e}")print("\nâœ“ PCA components are eigenvectors of the covariance matrix!")print("âœ“ PCA eigenvalues are eigenvalues of the covariance matrix!")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="connecting-pca-to-probability-theory-covariance-matrix-and-eigenstructurethe-principal-components-are-directly-related-to-the-covariance-structure-of-our-joint-distribution" class="level2">
<h2 class="anchored" data-anchor-id="connecting-pca-to-probability-theory-covariance-matrix-and-eigenstructurethe-principal-components-are-directly-related-to-the-covariance-structure-of-our-joint-distribution">Connecting PCA to Probability Theory### Covariance Matrix and EigenstructureThe principal components are directly related to the covariance structure of our joint distribution:</h2>
<div id="cell-12" class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Detailed PCA Analysispca_detailed = PCA(n_components=10)  # Get more components for analysisX_reduced_detailed = pca_detailed.fit_transform(X)# Create comprehensive visualizationfig = plt.figure(figsize=(16, 12))# 1. Explained variance ratioax1 = plt.subplot(3, 4, 1)plt.bar(range(10), pca_detailed.explained_variance_ratio_)plt.title('Explained Variance Ratio\nby Principal Component')plt.xlabel('Component')plt.ylabel('Variance Ratio')plt.xticks(range(10))# 2. Cumulative explained varianceax2 = plt.subplot(3, 4, 2)cumsum_var = np.cumsum(pca_detailed.explained_variance_ratio_)plt.plot(range(10), cumsum_var, 'bo-')plt.title('Cumulative Explained Variance')plt.xlabel('Number of Components')plt.ylabel('Cumulative Variance Ratio')plt.grid(True)# 3-6. First 4 principal components as imagesfor i in range(4):    ax = plt.subplot(3, 4, 3 + i)    component_image = pca_detailed.components_[i].reshape(8, 8)    plt.imshow(component_image, cmap='RdBu_r')    plt.title(f'PC{i+1}\n(Var: {pca_detailed.explained_variance_ratio_[i]:.3f})')    plt.axis('off')    plt.colorbar(shrink=0.6)# 7. 2D projection colored by digitax7 = plt.subplot(3, 4, 7)scatter = plt.scatter(X_reduced_detailed[:, 0], X_reduced_detailed[:, 1],                      c=y, cmap='tab10', alpha=0.6, s=10)plt.title('Joint Distribution Projection\n(First 2 PCs)')plt.xlabel('PC1')plt.ylabel('PC2')plt.colorbar(scatter, shrink=0.6)# 8. Different PC pairsax8 = plt.subplot(3, 4, 8)plt.scatter(X_reduced_detailed[:, 2], X_reduced_detailed[:, 3],            c=y, cmap='tab10', alpha=0.6, s=10)plt.title('Joint Distribution Projection\n(PC3 vs PC4)')plt.xlabel('PC3')plt.ylabel('PC4')# 9. Reconstruction demonstrationax9 = plt.subplot(3, 4, 9)# Original imageoriginal_idx = 100original_image = X[original_idx].reshape(8, 8)plt.imshow(original_image, cmap='gray')plt.title(f'Original\n(Digit {y[original_idx]})')plt.axis('off')# 10. Reconstruction with 2 componentsax10 = plt.subplot(3, 4, 10)pca_2 = PCA(n_components=2)X_2d = pca_2.fit_transform(X)X_reconstructed_2 = pca_2.inverse_transform(X_2d)reconstructed_image_2 = X_reconstructed_2[original_idx].reshape(8, 8)plt.imshow(reconstructed_image_2, cmap='gray')plt.title('2-PC Reconstruction')plt.axis('off')# 11. Reconstruction with 5 componentsax11 = plt.subplot(3, 4, 11)pca_5 = PCA(n_components=5)X_5d = pca_5.fit_transform(X)X_reconstructed_5 = pca_5.inverse_transform(X_5d)reconstructed_image_5 = X_reconstructed_5[original_idx].reshape(8, 8)plt.imshow(reconstructed_image_5, cmap='gray')plt.title('5-PC Reconstruction')plt.axis('off')# 12. Reconstruction error analysisax12 = plt.subplot(3, 4, 12)n_components_range = range(1, 21)reconstruction_errors = []for n_comp in n_components_range:    pca_temp = PCA(n_components=n_comp)    X_temp = pca_temp.fit_transform(X)    X_recon_temp = pca_temp.inverse_transform(X_temp)    error = np.mean((X - X_recon_temp) ** 2)    reconstruction_errors.append(error)plt.plot(n_components_range, reconstruction_errors, 'ro-')plt.title('Reconstruction Error\nvs Number of Components')plt.xlabel('Number of Components')plt.ylabel('Mean Squared Error')plt.grid(True)plt.tight_layout()plt.show()# Print summary statisticsprint("JOINT DISTRIBUTION ANALYSIS SUMMARY:")print("="*50)print(f"Original dimensionality: {X.shape[1]}")print(f"Number of samples: {X.shape[0]}")print(f"First 2 PCs explain {cumsum_var[1]:.1%} of total variance")print(f"First 5 PCs explain {cumsum_var[4]:.1%} of total variance")print(f"First 10 PCs explain {cumsum_var[9]:.1%} of total variance")print(f"\nThis means the 64-dimensional joint distribution has most of its")print(f"structure captured in just a few principal directions!")print(f"\nReconstruction Quality:")print(f"- 2 components: MSE = {reconstruction_errors[1]:.3f}")print(f"- 5 components: MSE = {reconstruction_errors[4]:.3f}")print(f"- 10 components: MSE = {reconstruction_errors[9]:.3f}")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Letâ€™s explore this in more detail by analyzing the principal components and their relationship to the joint distribution:</p>
<p><strong>Understanding the PCA Results:</strong>- <strong>Original space</strong>: 64-dimensional joint distribution (64 pixel intensities)- <strong>Reduced space</strong>: 2-dimensional projection (captures main patterns of co-variation)- <strong>Information loss</strong>: We visualize the most important 2 directions out of 64 possibleThe scatter plot shows how the 64-dimensional samples project onto the first two principal components, revealing the underlying structure of the joint distribution.</p>
</section>
<section id="analyzing-the-joint-distribution-structure-step-1-understanding-marginal-distributionsbefore-applying-pca-lets-examine-the-marginal-distributions---how-individual-pixels-random-variables-behave-across-all-samples." class="level2">
<h2 class="anchored" data-anchor-id="analyzing-the-joint-distribution-structure-step-1-understanding-marginal-distributionsbefore-applying-pca-lets-examine-the-marginal-distributions---how-individual-pixels-random-variables-behave-across-all-samples.">Analyzing the Joint Distribution Structure### Step 1: Understanding Marginal DistributionsBefore applying PCA, letâ€™s examine the marginal distributions - how individual pixels (random variables) behave across all samples.</h2>
<section id="visualizing-individual-sampleslets-examine-individual-realizations-from-our-64-dimensional-joint-distribution" class="level3">
<h3 class="anchored" data-anchor-id="visualizing-individual-sampleslets-examine-individual-realizations-from-our-64-dimensional-joint-distribution">Visualizing Individual SamplesLetâ€™s examine individual realizations from our 64-dimensional joint distribution:</h3>
<div id="cell-17" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> load_digits(return_X_y<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.astype(np.float32)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> y.astype(np.int64)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X.shape, y.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(1797, 64) (1797,)</code></pre>
</div>
</div>
<div id="cell-18" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>X[<span class="dv">0</span>], y[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>(array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,
        15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,
        12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,
         0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,
        10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.],
       dtype=float32),
 np.int64(0))</code></pre>
</div>
</div>
<div id="cell-19" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>dig <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(X[dig].reshape(<span class="dv">8</span>, <span class="dv">8</span>), cmap<span class="op">=</span><span class="st">'gray'</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Target: </span><span class="sc">{</span>y[dig]<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>Text(0.5, 1.0, 'Target: 0')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="images-joint-distribution_files/figure-html/cell-10-output-2.png" width="408" height="434" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-20" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Using PCA to reduce the dimensionality of the data to 2d</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>X_reduced <span class="op">=</span> pca.fit_transform(X)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_reduced.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(1797, 2)</code></pre>
</div>
</div>
<div id="cell-21" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the reduced data</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_reduced[:, <span class="dv">0</span>], X_reduced[:, <span class="dv">1</span>], c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">'tab10'</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>plt.colorbar()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="images-joint-distribution_files/figure-html/cell-12-output-1.png" width="517" height="418" class="figure-img"></p>
</figure>
</div>
</div>
</div>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    // Ensure there is a toggle, if there isn't float one in the top right
    if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
      const a = window.document.createElement('a');
      a.classList.add('top-right');
      a.classList.add('quarto-color-scheme-toggle');
      a.href = "";
      a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
      const i = window.document.createElement("i");
      i.classList.add('bi');
      a.appendChild(i);
      window.document.body.appendChild(a);
    }
    setColorSchemeToggle(hasAlternateSentinel())
    const icon = "î§‹";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/nipunbatra\.github\.io\/psdv-teaching\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>